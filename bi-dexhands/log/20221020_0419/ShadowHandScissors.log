/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-1ua9zmjs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandScissors_ppo_20221020041913
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/1ua9zmjs
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:7
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=7, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=7, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:7', seed=None, sim_device='cuda:7', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandScissors', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_scissors', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 150, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.1, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandScissors', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandScissors_ppo_20221020041913', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 4135
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:7
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 820 steps/s (collection: 15.967s, learning 4.008s)
               Value function loss: 15.7943
                    Surrogate loss: 0.0486
             Mean action noise std: 0.80
                  Mean reward/step: -0.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 19.97s
                        Total time: 19.97s
                               ETA: 1997468.2s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1133 steps/s (collection: 13.465s, learning 0.984s)
               Value function loss: 2.7329
                    Surrogate loss: -0.0171
             Mean action noise std: 0.80
                  Mean reward/step: -0.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 14.45s
                        Total time: 34.42s
                               ETA: 1721149.5s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 966 steps/s (collection: 16.774s, learning 0.174s)
               Value function loss: 0.8206
                    Surrogate loss: -0.0287
             Mean action noise std: 0.80
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 16.95s
                        Total time: 51.37s
                               ETA: 1712353.8s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 924 steps/s (collection: 17.548s, learning 0.179s)
               Value function loss: 0.7016
                    Surrogate loss: 0.0000
             Mean action noise std: 0.80
                  Mean reward/step: -0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 17.73s
                        Total time: 69.10s
                               ETA: 1727425.6s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 880 steps/s (collection: 18.426s, learning 0.183s)
               Value function loss: 0.5065
                    Surrogate loss: -0.0246
             Mean action noise std: 0.80
                  Mean reward/step: -0.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 18.61s
                        Total time: 87.71s
                               ETA: 1754090.7s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 859 steps/s (collection: 18.888s, learning 0.181s)
               Value function loss: 0.5209
                    Surrogate loss: -0.0192
             Mean action noise std: 0.80
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 19.07s
                        Total time: 106.78s
                               ETA: 1779536.8s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 849 steps/s (collection: 19.125s, learning 0.167s)
               Value function loss: 0.3294
                    Surrogate loss: -0.0216
             Mean action noise std: 0.80
                  Mean reward/step: -0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 19.29s
                        Total time: 126.07s
                               ETA: 1800882.8s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 830 steps/s (collection: 19.547s, learning 0.171s)
               Value function loss: 0.3180
                    Surrogate loss: -0.0318
             Mean action noise std: 0.80
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 19.72s
                        Total time: 145.79s
                               ETA: 1822214.8s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 835 steps/s (collection: 19.414s, learning 0.193s)
               Value function loss: 0.3467
                    Surrogate loss: -0.0285
             Mean action noise std: 0.80
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 19.61s
                        Total time: 165.39s
                               ETA: 1837566.8s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 860 steps/s (collection: 18.859s, learning 0.174s)
               Value function loss: 0.4260
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 19.03s
                        Total time: 184.43s
                               ETA: 1844105.8s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 852 steps/s (collection: 19.034s, learning 0.174s)
               Value function loss: 0.8663
                    Surrogate loss: 0.0024
             Mean action noise std: 0.80
                  Mean reward/step: -0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 19.21s
                        Total time: 203.64s
                               ETA: 1851049.7s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 849 steps/s (collection: 19.135s, learning 0.160s)
               Value function loss: 0.8159
                    Surrogate loss: -0.0153
             Mean action noise std: 0.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 19.30s
                        Total time: 222.93s
                               ETA: 1857554.0s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 837 steps/s (collection: 19.384s, learning 0.168s)
               Value function loss: 0.8817
                    Surrogate loss: -0.0121
             Mean action noise std: 0.80
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 19.55s
                        Total time: 242.48s
                               ETA: 1865030.7s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 851 steps/s (collection: 19.067s, learning 0.178s)
               Value function loss: 0.9766
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 19.24s
                        Total time: 261.73s
                               ETA: 1869239.1s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 856 steps/s (collection: 18.961s, learning 0.159s)
               Value function loss: 1.2946
                    Surrogate loss: -0.0154
             Mean action noise std: 0.80
                  Mean reward/step: 0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 19.12s
                        Total time: 280.85s
                               ETA: 1872059.2s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 860 steps/s (collection: 18.880s, learning 0.169s)
               Value function loss: 1.8327
                    Surrogate loss: -0.0202
             Mean action noise std: 0.80
                  Mean reward/step: 0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 19.05s
                        Total time: 299.90s
                               ETA: 1874080.6s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 854 steps/s (collection: 19.018s, learning 0.164s)
               Value function loss: 1.8655
                    Surrogate loss: -0.0176
             Mean action noise std: 0.80
                  Mean reward/step: 0.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 19.18s
                        Total time: 319.08s
                               ETA: 1876637.1s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 870 steps/s (collection: 18.654s, learning 0.173s)
               Value function loss: 1.8066
                    Surrogate loss: -0.0078
             Mean action noise std: 0.80
                  Mean reward/step: 0.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 18.83s
                        Total time: 337.91s
                               ETA: 1876939.1s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 851 steps/s (collection: 19.088s, learning 0.160s)
               Value function loss: 1.6543
                    Surrogate loss: 0.0086
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 19.25s
                        Total time: 357.15s
                               ETA: 1879419.0s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 846 steps/s (collection: 19.191s, learning 0.164s)
               Value function loss: 0.6273
                    Surrogate loss: 0.0687
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 19.36s
                        Total time: 376.51s
                               ETA: 1882190.0s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 871 steps/s (collection: 18.636s, learning 0.165s)
               Value function loss: 0.2164
                    Surrogate loss: -0.0104
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 18.80s
                        Total time: 395.31s
                               ETA: 1882056.7s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 850 steps/s (collection: 19.104s, learning 0.169s)
               Value function loss: 0.1307
                    Surrogate loss: -0.0254
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 19.27s
                        Total time: 414.58s
                               ETA: 1884077.1s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 868 steps/s (collection: 18.685s, learning 0.176s)
               Value function loss: 0.3679
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 18.86s
                        Total time: 433.45s
                               ETA: 1884131.8s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 880 steps/s (collection: 18.428s, learning 0.189s)
               Value function loss: 0.5740
                    Surrogate loss: -0.0197
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 18.62s
                        Total time: 452.06s
                               ETA: 1883163.7s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 859 steps/s (collection: 18.894s, learning 0.163s)
               Value function loss: 0.8935
                    Surrogate loss: -0.0131
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 19.06s
                        Total time: 471.12s
                               ETA: 1884028.4s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 847 steps/s (collection: 19.173s, learning 0.166s)
               Value function loss: 1.4163
                    Surrogate loss: -0.0130
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 19.34s
                        Total time: 490.46s
                               ETA: 1885910.4s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 839 steps/s (collection: 19.356s, learning 0.164s)
               Value function loss: 1.8826
                    Surrogate loss: -0.0107
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 19.52s
                        Total time: 509.98s
                               ETA: 1888320.6s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 873 steps/s (collection: 18.604s, learning 0.161s)
               Value function loss: 2.2870
                    Surrogate loss: -0.0096
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 18.77s
                        Total time: 528.74s
                               ETA: 1887863.2s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 853 steps/s (collection: 19.021s, learning 0.168s)
               Value function loss: 4.3394
                    Surrogate loss: -0.0052
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 19.19s
                        Total time: 547.93s
                               ETA: 1888896.4s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 848 steps/s (collection: 19.152s, learning 0.161s)
               Value function loss: 5.8651
                    Surrogate loss: -0.0111
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 19.31s
                        Total time: 567.25s
                               ETA: 1890270.1s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 860 steps/s (collection: 18.872s, learning 0.169s)
               Value function loss: 7.1181
                    Surrogate loss: 0.0256
             Mean action noise std: 0.80
                       Mean reward: -1.21
               Mean episode length: 149.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 19.04s
                        Total time: 586.29s
                               ETA: 1890679.3s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 836 steps/s (collection: 19.420s, learning 0.169s)
               Value function loss: 8.7811
                    Surrogate loss: 0.0040
             Mean action noise std: 0.80
                       Mean reward: -0.10
               Mean episode length: 148.10
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 19.59s
                        Total time: 605.88s
                               ETA: 1892773.4s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 847 steps/s (collection: 19.166s, learning 0.166s)
               Value function loss: 11.7021
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: -0.10
               Mean episode length: 148.10
                  Mean reward/step: 1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 19.33s
                        Total time: 625.21s
                               ETA: 1893961.1s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 846 steps/s (collection: 19.197s, learning 0.160s)
               Value function loss: 17.5045
                    Surrogate loss: 0.0100
             Mean action noise std: 0.80
                       Mean reward: 2.96
               Mean episode length: 146.61
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 19.36s
                        Total time: 644.57s
                               ETA: 1895154.0s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 835 steps/s (collection: 19.422s, learning 0.176s)
               Value function loss: 23.1083
                    Surrogate loss: 0.0092
             Mean action noise std: 0.80
                       Mean reward: 5.56
               Mean episode length: 145.30
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 19.60s
                        Total time: 664.16s
                               ETA: 1896964.0s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 832 steps/s (collection: 19.510s, learning 0.162s)
               Value function loss: 9.6212
                    Surrogate loss: 0.0032
             Mean action noise std: 0.80
                       Mean reward: 14.59
               Mean episode length: 143.77
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 19.67s
                        Total time: 683.84s
                               ETA: 1898878.2s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 852 steps/s (collection: 19.061s, learning 0.169s)
               Value function loss: 11.1687
                    Surrogate loss: -0.0019
             Mean action noise std: 0.80
                       Mean reward: 25.03
               Mean episode length: 143.05
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 19.23s
                        Total time: 703.07s
                               ETA: 1899492.9s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1122 steps/s (collection: 14.434s, learning 0.165s)
               Value function loss: 209.4223
                    Surrogate loss: 0.0067
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 4.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 14.60s
                        Total time: 717.66s
                               ETA: 1887891.8s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.610s, learning 0.171s)
               Value function loss: 4.7887
                    Surrogate loss: 0.0664
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 9.78s
                        Total time: 727.44s
                               ETA: 1864534.2s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1632 steps/s (collection: 9.871s, learning 0.167s)
               Value function loss: 2.2367
                    Surrogate loss: -0.0002
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 10.04s
                        Total time: 737.48s
                               ETA: 1842986.0s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1750 steps/s (collection: 9.189s, learning 0.172s)
               Value function loss: 0.4933
                    Surrogate loss: 0.0280
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 9.36s
                        Total time: 746.84s
                               ETA: 1820841.2s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1588 steps/s (collection: 10.149s, learning 0.162s)
               Value function loss: 0.3487
                    Surrogate loss: -0.0122
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 10.31s
                        Total time: 757.15s
                               ETA: 1802010.1s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.674s, learning 0.165s)
               Value function loss: 0.8990
                    Surrogate loss: -0.0007
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 9.84s
                        Total time: 766.99s
                               ETA: 1782959.1s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.726s, learning 0.166s)
               Value function loss: 1.7035
                    Surrogate loss: 0.0211
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 9.89s
                        Total time: 776.89s
                               ETA: 1764889.9s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.588s, learning 0.166s)
               Value function loss: 3.4836
                    Surrogate loss: 0.0168
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 9.75s
                        Total time: 786.64s
                               ETA: 1747318.7s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1664 steps/s (collection: 9.682s, learning 0.161s)
               Value function loss: 5.1383
                    Surrogate loss: 0.0194
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 9.84s
                        Total time: 796.48s
                               ETA: 1730704.0s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1745 steps/s (collection: 9.199s, learning 0.187s)
               Value function loss: 5.8332
                    Surrogate loss: 0.0230
             Mean action noise std: 0.80
                       Mean reward: 112.78
               Mean episode length: 150.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 9.39s
                        Total time: 805.87s
                               ETA: 1713823.4s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1632 steps/s (collection: 9.872s, learning 0.164s)
               Value function loss: 7.7424
                    Surrogate loss: -0.0029
             Mean action noise std: 0.80
                       Mean reward: 110.14
               Mean episode length: 147.24
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 10.04s
                        Total time: 815.90s
                               ETA: 1699000.6s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.622s, learning 0.171s)
               Value function loss: 11.1248
                    Surrogate loss: 0.0110
             Mean action noise std: 0.80
                       Mean reward: 103.79
               Mean episode length: 141.13
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 9.79s
                        Total time: 825.70s
                               ETA: 1684286.1s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1687 steps/s (collection: 9.542s, learning 0.169s)
               Value function loss: 16.3801
                    Surrogate loss: 0.0024
             Mean action noise std: 0.80
                       Mean reward: 97.86
               Mean episode length: 135.82
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 9.71s
                        Total time: 835.41s
                               ETA: 1669996.7s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.606s, learning 0.163s)
               Value function loss: 14.1221
                    Surrogate loss: 0.0338
             Mean action noise std: 0.80
                       Mean reward: 97.89
               Mean episode length: 132.86
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 9.77s
                        Total time: 845.18s
                               ETA: 1656380.2s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1630 steps/s (collection: 9.874s, learning 0.177s)
               Value function loss: 14.0055
                    Surrogate loss: -0.0105
             Mean action noise std: 0.80
                       Mean reward: 97.22
               Mean episode length: 130.72
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 10.05s
                        Total time: 855.23s
                               ETA: 1643828.0s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1614 steps/s (collection: 9.962s, learning 0.188s)
               Value function loss: 15.8456
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 99.28
               Mean episode length: 126.49
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 10.15s
                        Total time: 865.38s
                               ETA: 1631936.5s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.633s, learning 0.186s)
               Value function loss: 13.4716
                    Surrogate loss: -0.0040
             Mean action noise std: 0.80
                       Mean reward: 104.03
               Mean episode length: 124.49
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 9.82s
                        Total time: 875.20s
                               ETA: 1619873.1s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1575 steps/s (collection: 10.212s, learning 0.190s)
               Value function loss: 15.4526
                    Surrogate loss: -0.0163
             Mean action noise std: 0.80
                       Mean reward: 115.99
               Mean episode length: 127.08
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 10.40s
                        Total time: 885.60s
                               ETA: 1609307.7s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1710 steps/s (collection: 9.404s, learning 0.177s)
               Value function loss: 13.5312
                    Surrogate loss: -0.0091
             Mean action noise std: 0.80
                       Mean reward: 131.82
               Mean episode length: 136.11
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 9.58s
                        Total time: 895.18s
                               ETA: 1597653.3s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1626 steps/s (collection: 9.903s, learning 0.173s)
               Value function loss: 136.3325
                    Surrogate loss: 0.0291
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 4.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 10.08s
                        Total time: 905.25s
                               ETA: 1587276.0s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.352s, learning 0.175s)
               Value function loss: 2.3575
                    Surrogate loss: 0.0043
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 9.53s
                        Total time: 914.78s
                               ETA: 1576309.4s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.513s, learning 0.179s)
               Value function loss: 3.2141
                    Surrogate loss: 0.0136
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 9.69s
                        Total time: 924.47s
                               ETA: 1565994.5s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1625 steps/s (collection: 9.904s, learning 0.176s)
               Value function loss: 3.5566
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 10.08s
                        Total time: 934.55s
                               ETA: 1556668.6s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.390s, learning 0.169s)
               Value function loss: 1.7805
                    Surrogate loss: 0.0028
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 9.56s
                        Total time: 944.11s
                               ETA: 1546794.7s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1739 steps/s (collection: 9.225s, learning 0.192s)
               Value function loss: 1.1389
                    Surrogate loss: -0.0146
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 9.42s
                        Total time: 953.53s
                               ETA: 1537010.9s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.376s, learning 0.160s)
               Value function loss: 1.0442
                    Surrogate loss: -0.0021
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 9.54s
                        Total time: 963.06s
                               ETA: 1527725.2s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1705 steps/s (collection: 9.429s, learning 0.177s)
               Value function loss: 1.6733
                    Surrogate loss: 0.0279
             Mean action noise std: 0.80
                       Mean reward: 165.58
               Mean episode length: 150.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 9.61s
                        Total time: 972.67s
                               ETA: 1518840.0s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.764s, learning 0.176s)
               Value function loss: 2.8879
                    Surrogate loss: 0.0080
             Mean action noise std: 0.80
                       Mean reward: 163.47
               Mean episode length: 148.51
                  Mean reward/step: 1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 9.94s
                        Total time: 982.61s
                               ETA: 1510740.4s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.545s, learning 0.188s)
               Value function loss: 6.2474
                    Surrogate loss: 0.0043
             Mean action noise std: 0.80
                       Mean reward: 161.90
               Mean episode length: 146.60
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 9.73s
                        Total time: 992.34s
                               ETA: 1502572.0s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.413s, learning 0.178s)
               Value function loss: 7.7878
                    Surrogate loss: 0.0111
             Mean action noise std: 0.80
                       Mean reward: 157.63
               Mean episode length: 143.54
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 9.59s
                        Total time: 1001.93s
                               ETA: 1494436.4s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.715s, learning 0.170s)
               Value function loss: 17.6547
                    Surrogate loss: 0.0108
             Mean action noise std: 0.80
                       Mean reward: 156.78
               Mean episode length: 140.02
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 9.88s
                        Total time: 1011.82s
                               ETA: 1486970.7s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.571s, learning 0.178s)
               Value function loss: 14.0105
                    Surrogate loss: -0.0061
             Mean action noise std: 0.80
                       Mean reward: 154.75
               Mean episode length: 136.99
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 9.75s
                        Total time: 1021.57s
                               ETA: 1479525.3s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.777s, learning 0.185s)
               Value function loss: 17.2950
                    Surrogate loss: -0.0119
             Mean action noise std: 0.80
                       Mean reward: 151.02
               Mean episode length: 134.84
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 9.96s
                        Total time: 1031.53s
                               ETA: 1472596.2s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.470s, learning 0.172s)
               Value function loss: 15.9708
                    Surrogate loss: -0.0161
             Mean action noise std: 0.80
                       Mean reward: 158.13
               Mean episode length: 134.51
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 9.64s
                        Total time: 1041.17s
                               ETA: 1465411.5s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.815s, learning 0.183s)
               Value function loss: 20.6188
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 164.91
               Mean episode length: 134.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 10.00s
                        Total time: 1051.17s
                               ETA: 1458920.9s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.679s, learning 0.174s)
               Value function loss: 25.5924
                    Surrogate loss: -0.0093
             Mean action noise std: 0.80
                       Mean reward: 183.97
               Mean episode length: 140.38
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 9.85s
                        Total time: 1061.02s
                               ETA: 1452409.3s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1620 steps/s (collection: 9.928s, learning 0.180s)
               Value function loss: 25.2849
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 196.15
               Mean episode length: 144.66
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 10.11s
                        Total time: 1071.13s
                               ETA: 1446416.7s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1610 steps/s (collection: 10.006s, learning 0.169s)
               Value function loss: 548.7313
                    Surrogate loss: -0.0005
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 4.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 10.17s
                        Total time: 1081.31s
                               ETA: 1440673.2s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.425s, learning 0.169s)
               Value function loss: 9.7439
                    Surrogate loss: 0.2096
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 9.59s
                        Total time: 1090.90s
                               ETA: 1434317.4s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.322s, learning 0.175s)
               Value function loss: 5.0685
                    Surrogate loss: -0.0050
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 9.50s
                        Total time: 1100.40s
                               ETA: 1427999.9s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.292s, learning 0.167s)
               Value function loss: 5.1694
                    Surrogate loss: 0.0921
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 9.46s
                        Total time: 1109.86s
                               ETA: 1421795.8s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1740 steps/s (collection: 9.229s, learning 0.185s)
               Value function loss: 3.5553
                    Surrogate loss: 0.0005
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 9.41s
                        Total time: 1119.27s
                               ETA: 1415692.2s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1697 steps/s (collection: 9.487s, learning 0.168s)
               Value function loss: 2.7273
                    Surrogate loss: -0.0178
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 9.65s
                        Total time: 1128.92s
                               ETA: 1410040.8s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.293s, learning 0.164s)
               Value function loss: 2.3386
                    Surrogate loss: -0.0101
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 9.46s
                        Total time: 1138.38s
                               ETA: 1404285.1s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.585s, learning 0.170s)
               Value function loss: 2.2974
                    Surrogate loss: -0.0259
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 9.76s
                        Total time: 1148.14s
                               ETA: 1399032.5s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1751 steps/s (collection: 9.190s, learning 0.163s)
               Value function loss: 2.1498
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 194.41
               Mean episode length: 150.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 9.35s
                        Total time: 1157.49s
                               ETA: 1393421.2s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.622s, learning 0.160s)
               Value function loss: 2.1858
                    Surrogate loss: 0.0062
             Mean action noise std: 0.80
                       Mean reward: 194.80
               Mean episode length: 150.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 9.78s
                        Total time: 1167.27s
                               ETA: 1388454.3s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.460s, learning 0.163s)
               Value function loss: 2.3294
                    Surrogate loss: 0.0022
             Mean action noise std: 0.80
                       Mean reward: 195.27
               Mean episode length: 150.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 9.62s
                        Total time: 1176.89s
                               ETA: 1383417.2s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.651s, learning 0.161s)
               Value function loss: 2.3744
                    Surrogate loss: -0.0027
             Mean action noise std: 0.80
                       Mean reward: 198.18
               Mean episode length: 150.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 9.81s
                        Total time: 1186.71s
                               ETA: 1378716.6s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.483s, learning 0.165s)
               Value function loss: 2.4612
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 202.55
               Mean episode length: 150.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 9.65s
                        Total time: 1196.35s
                               ETA: 1373936.2s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1672 steps/s (collection: 9.629s, learning 0.169s)
               Value function loss: 3.0881
                    Surrogate loss: -0.0088
             Mean action noise std: 0.80
                       Mean reward: 199.98
               Mean episode length: 150.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 9.80s
                        Total time: 1206.15s
                               ETA: 1369434.3s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.590s, learning 0.159s)
               Value function loss: 3.5296
                    Surrogate loss: 0.0275
             Mean action noise std: 0.80
                       Mean reward: 199.39
               Mean episode length: 150.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 9.75s
                        Total time: 1215.90s
                               ETA: 1364978.3s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1700 steps/s (collection: 9.474s, learning 0.162s)
               Value function loss: 3.6519
                    Surrogate loss: 0.0080
             Mean action noise std: 0.80
                       Mean reward: 194.43
               Mean episode length: 150.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 9.64s
                        Total time: 1225.54s
                               ETA: 1360496.4s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.629s, learning 0.163s)
               Value function loss: 4.7705
                    Surrogate loss: 0.0169
             Mean action noise std: 0.80
                       Mean reward: 188.21
               Mean episode length: 150.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 9.79s
                        Total time: 1235.33s
                               ETA: 1356283.0s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.542s, learning 0.170s)
               Value function loss: 6.1328
                    Surrogate loss: -0.0080
             Mean action noise std: 0.80
                       Mean reward: 170.10
               Mean episode length: 150.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 9.71s
                        Total time: 1245.04s
                               ETA: 1352074.4s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.608s, learning 0.159s)
               Value function loss: 5.9254
                    Surrogate loss: 0.0679
             Mean action noise std: 0.80
                       Mean reward: 150.58
               Mean episode length: 150.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 9.77s
                        Total time: 1254.81s
                               ETA: 1348015.0s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.568s, learning 0.173s)
               Value function loss: 171.1373
                    Surrogate loss: 0.0112
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 4.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 9.74s
                        Total time: 1264.55s
                               ETA: 1344014.6s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.479s, learning 0.163s)
               Value function loss: 1.2146
                    Surrogate loss: 0.0117
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 9.64s
                        Total time: 1274.19s
                               ETA: 1339993.4s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.633s, learning 0.168s)
               Value function loss: 1.6266
                    Surrogate loss: 0.0063
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 9.80s
                        Total time: 1283.99s
                               ETA: 1336222.4s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.566s, learning 0.161s)
               Value function loss: 1.1067
                    Surrogate loss: -0.0031
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 9.73s
                        Total time: 1293.72s
                               ETA: 1332452.6s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1729 steps/s (collection: 9.308s, learning 0.167s)
               Value function loss: 0.8786
                    Surrogate loss: -0.0150
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 9.47s
                        Total time: 1303.20s
                               ETA: 1328501.5s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1767 steps/s (collection: 9.102s, learning 0.169s)
               Value function loss: 0.9206
                    Surrogate loss: 0.0046
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 9.27s
                        Total time: 1312.47s
                               ETA: 1324424.5s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.288s, learning 0.166s)
               Value function loss: 1.5150
                    Surrogate loss: 0.0076
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 9.45s
                        Total time: 1321.92s
                               ETA: 1320611.9s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1734 steps/s (collection: 9.284s, learning 0.165s)
               Value function loss: 2.3648
                    Surrogate loss: 0.0367
             Mean action noise std: 0.80
                       Mean reward: 77.36
               Mean episode length: 150.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 9.45s
                        Total time: 1331.37s
                               ETA: 1316869.2s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1642 steps/s (collection: 9.815s, learning 0.162s)
               Value function loss: 2.5988
                    Surrogate loss: 0.0077
             Mean action noise std: 0.80
                       Mean reward: 77.79
               Mean episode length: 150.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 9.98s
                        Total time: 1341.35s
                               ETA: 1313718.0s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.678s, learning 0.158s)
               Value function loss: 4.0178
                    Surrogate loss: 0.0596
             Mean action noise std: 0.80
                       Mean reward: 77.87
               Mean episode length: 150.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 9.84s
                        Total time: 1351.18s
                               ETA: 1310490.4s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.647s, learning 0.166s)
               Value function loss: 6.9685
                    Surrogate loss: -0.0008
             Mean action noise std: 0.80
                       Mean reward: 77.56
               Mean episode length: 147.89
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 9.81s
                        Total time: 1361.00s
                               ETA: 1307301.9s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.593s, learning 0.162s)
               Value function loss: 7.8263
                    Surrogate loss: 0.1382
             Mean action noise std: 0.80
                       Mean reward: 83.96
               Mean episode length: 146.64
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 9.76s
                        Total time: 1370.75s
                               ETA: 1304119.6s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.657s, learning 0.162s)
               Value function loss: 7.2217
                    Surrogate loss: 0.0072
             Mean action noise std: 0.80
                       Mean reward: 88.60
               Mean episode length: 146.09
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 9.82s
                        Total time: 1380.57s
                               ETA: 1301057.2s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1639 steps/s (collection: 9.833s, learning 0.161s)
               Value function loss: 7.4336
                    Surrogate loss: -0.0052
             Mean action noise std: 0.80
                       Mean reward: 91.48
               Mean episode length: 145.62
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 9.99s
                        Total time: 1390.56s
                               ETA: 1298215.5s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.615s, learning 0.168s)
               Value function loss: 8.6735
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: 92.99
               Mean episode length: 144.46
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 9.78s
                        Total time: 1400.35s
                               ETA: 1295230.9s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1672 steps/s (collection: 9.638s, learning 0.159s)
               Value function loss: 10.6362
                    Surrogate loss: -0.0132
             Mean action noise std: 0.80
                       Mean reward: 100.71
               Mean episode length: 143.85
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 9.80s
                        Total time: 1410.15s
                               ETA: 1292314.0s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.630s, learning 0.159s)
               Value function loss: 10.8451
                    Surrogate loss: 0.0089
             Mean action noise std: 0.80
                       Mean reward: 108.62
               Mean episode length: 145.71
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 9.79s
                        Total time: 1419.93s
                               ETA: 1289442.5s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1712 steps/s (collection: 9.406s, learning 0.159s)
               Value function loss: 14.0299
                    Surrogate loss: -0.0065
             Mean action noise std: 0.80
                       Mean reward: 114.12
               Mean episode length: 147.51
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 9.57s
                        Total time: 1429.50s
                               ETA: 1286421.4s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1722 steps/s (collection: 9.345s, learning 0.165s)
               Value function loss: 15.1832
                    Surrogate loss: 0.0001
             Mean action noise std: 0.80
                       Mean reward: 120.66
               Mean episode length: 147.74
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 9.51s
                        Total time: 1439.01s
                               ETA: 1283404.2s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1631 steps/s (collection: 9.868s, learning 0.178s)
               Value function loss: 336.7602
                    Surrogate loss: 0.0467
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 4.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 10.05s
                        Total time: 1449.06s
                               ETA: 1280913.6s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.448s, learning 0.167s)
               Value function loss: 0.9077
                    Surrogate loss: -0.0267
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 9.61s
                        Total time: 1458.67s
                               ETA: 1278089.4s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1729 steps/s (collection: 9.307s, learning 0.167s)
               Value function loss: 1.1518
                    Surrogate loss: 0.0065
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 9.47s
                        Total time: 1468.14s
                               ETA: 1275192.4s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.361s, learning 0.164s)
               Value function loss: 1.5578
                    Surrogate loss: -0.0086
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 9.53s
                        Total time: 1477.67s
                               ETA: 1272389.2s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.544s, learning 0.162s)
               Value function loss: 1.0153
                    Surrogate loss: -0.0075
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 9.71s
                        Total time: 1487.38s
                               ETA: 1269787.6s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1717 steps/s (collection: 9.370s, learning 0.167s)
               Value function loss: 0.8893
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 9.54s
                        Total time: 1496.91s
                               ETA: 1267086.9s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.581s, learning 0.164s)
               Value function loss: 1.2867
                    Surrogate loss: 0.0300
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 9.74s
                        Total time: 1506.66s
                               ETA: 1264605.9s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.371s, learning 0.157s)
               Value function loss: 2.2681
                    Surrogate loss: 0.0010
             Mean action noise std: 0.80
                       Mean reward: 130.20
               Mean episode length: 150.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 9.53s
                        Total time: 1516.19s
                               ETA: 1261985.9s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.600s, learning 0.165s)
               Value function loss: 5.3437
                    Surrogate loss: 0.0109
             Mean action noise std: 0.80
                       Mean reward: 130.59
               Mean episode length: 150.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 9.77s
                        Total time: 1525.95s
                               ETA: 1259604.8s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1754 steps/s (collection: 9.173s, learning 0.167s)
               Value function loss: 7.8877
                    Surrogate loss: 0.0187
             Mean action noise std: 0.80
                       Mean reward: 131.73
               Mean episode length: 150.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 9.34s
                        Total time: 1535.29s
                               ETA: 1256914.0s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1636 steps/s (collection: 9.854s, learning 0.157s)
               Value function loss: 12.5295
                    Surrogate loss: 0.0049
             Mean action noise std: 0.80
                       Mean reward: 137.07
               Mean episode length: 150.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 10.01s
                        Total time: 1545.30s
                               ETA: 1254811.9s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1721 steps/s (collection: 9.357s, learning 0.161s)
               Value function loss: 15.8649
                    Surrogate loss: 0.0054
             Mean action noise std: 0.80
                       Mean reward: 141.32
               Mean episode length: 150.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 9.52s
                        Total time: 1554.82s
                               ETA: 1252346.4s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.753s, learning 0.164s)
               Value function loss: 16.1930
                    Surrogate loss: 0.0031
             Mean action noise std: 0.80
                       Mean reward: 146.64
               Mean episode length: 150.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 9.92s
                        Total time: 1564.74s
                               ETA: 1250238.7s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.651s, learning 0.166s)
               Value function loss: 17.3990
                    Surrogate loss: 0.0132
             Mean action noise std: 0.80
                       Mean reward: 149.42
               Mean episode length: 149.07
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 9.82s
                        Total time: 1574.56s
                               ETA: 1248085.6s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.339s, learning 0.162s)
               Value function loss: 16.6677
                    Surrogate loss: 0.0308
             Mean action noise std: 0.80
                       Mean reward: 153.48
               Mean episode length: 148.03
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 9.50s
                        Total time: 1584.06s
                               ETA: 1245718.1s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1557 steps/s (collection: 10.350s, learning 0.168s)
               Value function loss: 20.3481
                    Surrogate loss: -0.0064
             Mean action noise std: 0.80
                       Mean reward: 155.11
               Mean episode length: 147.47
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 10.52s
                        Total time: 1594.58s
                               ETA: 1244181.0s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1717 steps/s (collection: 9.371s, learning 0.169s)
               Value function loss: 19.4761
                    Surrogate loss: 0.0172
             Mean action noise std: 0.80
                       Mean reward: 155.01
               Mean episode length: 146.81
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 9.54s
                        Total time: 1604.12s
                               ETA: 1241909.3s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1606 steps/s (collection: 10.037s, learning 0.160s)
               Value function loss: 21.4956
                    Surrogate loss: 0.0056
             Mean action noise std: 0.80
                       Mean reward: 157.00
               Mean episode length: 147.46
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 10.20s
                        Total time: 1614.31s
                               ETA: 1240177.2s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.335s, learning 0.168s)
               Value function loss: 19.4945
                    Surrogate loss: 0.0191
             Mean action noise std: 0.80
                       Mean reward: 159.57
               Mean episode length: 148.43
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 9.50s
                        Total time: 1623.82s
                               ETA: 1237942.3s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1627 steps/s (collection: 9.899s, learning 0.166s)
               Value function loss: 214.2801
                    Surrogate loss: 0.0193
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 4.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 10.07s
                        Total time: 1633.88s
                               ETA: 1236166.6s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1728 steps/s (collection: 9.312s, learning 0.167s)
               Value function loss: 0.8523
                    Surrogate loss: -0.0188
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 9.48s
                        Total time: 1643.36s
                               ETA: 1233977.2s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.510s, learning 0.166s)
               Value function loss: 1.3118
                    Surrogate loss: 0.0442
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 9.68s
                        Total time: 1653.03s
                               ETA: 1231967.4s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.585s, learning 0.169s)
               Value function loss: 1.3044
                    Surrogate loss: 0.0080
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 9.75s
                        Total time: 1662.79s
                               ETA: 1230044.5s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1676 steps/s (collection: 9.615s, learning 0.158s)
               Value function loss: 1.1587
                    Surrogate loss: 0.0019
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 9.77s
                        Total time: 1672.56s
                               ETA: 1228163.6s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1728 steps/s (collection: 9.321s, learning 0.159s)
               Value function loss: 1.3842
                    Surrogate loss: -0.0094
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 9.48s
                        Total time: 1682.04s
                               ETA: 1226097.1s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1647 steps/s (collection: 9.776s, learning 0.169s)
               Value function loss: 2.9079
                    Surrogate loss: 0.0186
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 9.94s
                        Total time: 1691.99s
                               ETA: 1224396.6s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1777 steps/s (collection: 9.056s, learning 0.163s)
               Value function loss: 4.3252
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: 161.73
               Mean episode length: 150.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 9.22s
                        Total time: 1701.20s
                               ETA: 1222198.6s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.665s, learning 0.162s)
               Value function loss: 5.6400
                    Surrogate loss: 0.0119
             Mean action noise std: 0.80
                       Mean reward: 159.57
               Mean episode length: 149.16
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 9.83s
                        Total time: 1711.03s
                               ETA: 1220466.2s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.575s, learning 0.162s)
               Value function loss: 8.1337
                    Surrogate loss: 0.0051
             Mean action noise std: 0.80
                       Mean reward: 160.66
               Mean episode length: 149.16
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 9.74s
                        Total time: 1720.77s
                               ETA: 1218694.0s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.463s, learning 0.158s)
               Value function loss: 10.1438
                    Surrogate loss: 0.0167
             Mean action noise std: 0.80
                       Mean reward: 158.67
               Mean episode length: 149.16
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 9.62s
                        Total time: 1730.39s
                               ETA: 1216865.4s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.726s, learning 0.160s)
               Value function loss: 13.0515
                    Surrogate loss: 0.0074
             Mean action noise std: 0.80
                       Mean reward: 156.17
               Mean episode length: 149.16
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 9.89s
                        Total time: 1740.28s
                               ETA: 1215247.6s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1748 steps/s (collection: 9.201s, learning 0.167s)
               Value function loss: 13.3673
                    Surrogate loss: 0.0035
             Mean action noise std: 0.80
                       Mean reward: 156.12
               Mean episode length: 149.16
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 9.37s
                        Total time: 1749.64s
                               ETA: 1213292.4s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.378s, learning 0.159s)
               Value function loss: 14.5759
                    Surrogate loss: -0.0167
             Mean action noise std: 0.80
                       Mean reward: 159.71
               Mean episode length: 149.16
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 9.54s
                        Total time: 1759.18s
                               ETA: 1211480.3s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.558s, learning 0.184s)
               Value function loss: 17.1069
                    Surrogate loss: -0.0005
             Mean action noise std: 0.80
                       Mean reward: 162.48
               Mean episode length: 148.97
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 9.74s
                        Total time: 1768.92s
                               ETA: 1209833.8s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1626 steps/s (collection: 9.912s, learning 0.163s)
               Value function loss: 21.5154
                    Surrogate loss: 0.0167
             Mean action noise std: 0.80
                       Mean reward: 157.96
               Mean episode length: 149.36
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 10.08s
                        Total time: 1779.00s
                               ETA: 1208435.3s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1696 steps/s (collection: 9.495s, learning 0.164s)
               Value function loss: 23.2045
                    Surrogate loss: -0.0061
             Mean action noise std: 0.80
                       Mean reward: 160.25
               Mean episode length: 148.96
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 9.66s
                        Total time: 1788.66s
                               ETA: 1206775.0s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1620 steps/s (collection: 9.941s, learning 0.171s)
               Value function loss: 19.3139
                    Surrogate loss: 0.0288
             Mean action noise std: 0.80
                       Mean reward: 159.93
               Mean episode length: 147.78
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 10.11s
                        Total time: 1798.77s
                               ETA: 1205440.7s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1740 steps/s (collection: 9.247s, learning 0.166s)
               Value function loss: 508.0244
                    Surrogate loss: 0.0384
             Mean action noise std: 0.80
                       Mean reward: 171.15
               Mean episode length: 150.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 4.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 9.41s
                        Total time: 1808.18s
                               ETA: 1203658.4s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.736s, learning 0.167s)
               Value function loss: 1.0027
                    Surrogate loss: -0.0175
             Mean action noise std: 0.80
                       Mean reward: 171.15
               Mean episode length: 150.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 9.90s
                        Total time: 1818.09s
                               ETA: 1202223.9s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.369s, learning 0.162s)
               Value function loss: 1.0094
                    Surrogate loss: 0.0095
             Mean action noise std: 0.80
                       Mean reward: 171.15
               Mean episode length: 150.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 9.53s
                        Total time: 1827.62s
                               ETA: 1200563.4s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.527s, learning 0.161s)
               Value function loss: 1.7775
                    Surrogate loss: 0.0575
             Mean action noise std: 0.80
                       Mean reward: 168.66
               Mean episode length: 149.03
                  Mean reward/step: 0.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 9.69s
                        Total time: 1837.30s
                               ETA: 1199027.2s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1752 steps/s (collection: 9.186s, learning 0.161s)
               Value function loss: 1.5671
                    Surrogate loss: 0.0177
             Mean action noise std: 0.80
                       Mean reward: 168.66
               Mean episode length: 149.03
                  Mean reward/step: 0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 9.35s
                        Total time: 1846.65s
                               ETA: 1197289.9s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.737s, learning 0.158s)
               Value function loss: 1.3879
                    Surrogate loss: 0.0142
             Mean action noise std: 0.80
                       Mean reward: 168.66
               Mean episode length: 149.03
                  Mean reward/step: 0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 9.90s
                        Total time: 1856.55s
                               ETA: 1195927.8s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.452s, learning 0.164s)
               Value function loss: 1.2176
                    Surrogate loss: -0.0036
             Mean action noise std: 0.80
                       Mean reward: 168.66
               Mean episode length: 149.03
                  Mean reward/step: 1.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 9.62s
                        Total time: 1866.16s
                               ETA: 1194403.7s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1729 steps/s (collection: 9.306s, learning 0.165s)
               Value function loss: 8.6335
                    Surrogate loss: 0.0014
             Mean action noise std: 0.80
                       Mean reward: 163.09
               Mean episode length: 146.21
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 9.47s
                        Total time: 1875.63s
                               ETA: 1192807.1s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.512s, learning 0.165s)
               Value function loss: 3.3677
                    Surrogate loss: -0.0007
             Mean action noise std: 0.80
                       Mean reward: 163.09
               Mean episode length: 146.21
                  Mean reward/step: 1.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 9.68s
                        Total time: 1885.31s
                               ETA: 1191361.1s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1746 steps/s (collection: 9.218s, learning 0.163s)
               Value function loss: 4.1491
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: 161.84
               Mean episode length: 146.21
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 9.38s
                        Total time: 1894.69s
                               ETA: 1189747.0s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1670 steps/s (collection: 9.640s, learning 0.167s)
               Value function loss: 6.7538
                    Surrogate loss: -0.0025
             Mean action noise std: 0.80
                       Mean reward: 156.67
               Mean episode length: 145.50
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 9.81s
                        Total time: 1904.50s
                               ETA: 1188418.9s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.532s, learning 0.169s)
               Value function loss: 10.7068
                    Surrogate loss: 0.0098
             Mean action noise std: 0.80
                       Mean reward: 156.53
               Mean episode length: 143.39
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 9.70s
                        Total time: 1914.20s
                               ETA: 1187041.0s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.310s, learning 0.158s)
               Value function loss: 9.3790
                    Surrogate loss: -0.0093
             Mean action noise std: 0.80
                       Mean reward: 150.73
               Mean episode length: 142.86
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 9.47s
                        Total time: 1923.67s
                               ETA: 1185536.5s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.342s, learning 0.160s)
               Value function loss: 8.2262
                    Surrogate loss: -0.0025
             Mean action noise std: 0.80
                       Mean reward: 151.15
               Mean episode length: 142.86
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 9.50s
                        Total time: 1933.17s
                               ETA: 1184071.9s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.570s, learning 0.170s)
               Value function loss: 9.8666
                    Surrogate loss: 0.0018
             Mean action noise std: 0.80
                       Mean reward: 148.52
               Mean episode length: 141.26
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 9.74s
                        Total time: 1942.91s
                               ETA: 1182769.7s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1697 steps/s (collection: 9.489s, learning 0.166s)
               Value function loss: 34.5919
                    Surrogate loss: 0.0099
             Mean action noise std: 0.80
                       Mean reward: 145.29
               Mean episode length: 144.10
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 9.65s
                        Total time: 1952.56s
                               ETA: 1181431.1s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.710s, learning 0.167s)
               Value function loss: 11.2992
                    Surrogate loss: 0.0006
             Mean action noise std: 0.80
                       Mean reward: 136.50
               Mean episode length: 145.23
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 9.88s
                        Total time: 1962.44s
                               ETA: 1180242.4s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.675s, learning 0.156s)
               Value function loss: 9.8527
                    Surrogate loss: 0.0126
             Mean action noise std: 0.80
                       Mean reward: 131.02
               Mean episode length: 146.06
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 9.83s
                        Total time: 1972.27s
                               ETA: 1179040.9s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1649 steps/s (collection: 9.769s, learning 0.161s)
               Value function loss: 9.7130
                    Surrogate loss: 0.0017
             Mean action noise std: 0.80
                       Mean reward: 137.82
               Mean episode length: 147.76
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 9.93s
                        Total time: 1982.20s
                               ETA: 1177911.7s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.595s, learning 0.163s)
               Value function loss: 238.8892
                    Surrogate loss: 0.0383
             Mean action noise std: 0.80
                       Mean reward: 143.77
               Mean episode length: 150.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 4.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 9.76s
                        Total time: 1991.96s
                               ETA: 1176694.2s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.444s, learning 0.171s)
               Value function loss: 0.8233
                    Surrogate loss: -0.0209
             Mean action noise std: 0.80
                       Mean reward: 143.77
               Mean episode length: 150.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 9.61s
                        Total time: 2001.57s
                               ETA: 1175407.0s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.410s, learning 0.164s)
               Value function loss: 0.8074
                    Surrogate loss: -0.0185
             Mean action noise std: 0.80
                       Mean reward: 143.77
               Mean episode length: 150.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 9.57s
                        Total time: 2011.15s
                               ETA: 1174111.3s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1748 steps/s (collection: 9.211s, learning 0.162s)
               Value function loss: 1.2147
                    Surrogate loss: -0.0091
             Mean action noise std: 0.80
                       Mean reward: 143.83
               Mean episode length: 150.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 9.37s
                        Total time: 2020.52s
                               ETA: 1172713.4s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.655s, learning 0.170s)
               Value function loss: 1.2861
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 143.83
               Mean episode length: 150.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 9.83s
                        Total time: 2030.35s
                               ETA: 1171592.4s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1746 steps/s (collection: 9.218s, learning 0.165s)
               Value function loss: 1.2800
                    Surrogate loss: 0.0056
             Mean action noise std: 0.80
                       Mean reward: 143.83
               Mean episode length: 150.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 9.38s
                        Total time: 2039.73s
                               ETA: 1170230.3s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.669s, learning 0.161s)
               Value function loss: 3.6926
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: 139.85
               Mean episode length: 146.75
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 9.83s
                        Total time: 2049.56s
                               ETA: 1169139.0s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1676 steps/s (collection: 9.617s, learning 0.158s)
               Value function loss: 4.7188
                    Surrogate loss: 0.0391
             Mean action noise std: 0.80
                       Mean reward: 138.45
               Mean episode length: 144.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 9.78s
                        Total time: 2059.33s
                               ETA: 1168028.9s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.162s)
               Value function loss: 7.4240
                    Surrogate loss: 0.0245
             Mean action noise std: 0.80
                       Mean reward: 135.40
               Mean episode length: 142.52
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 9.59s
                        Total time: 2068.92s
                               ETA: 1166826.5s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.569s, learning 0.159s)
               Value function loss: 5.0506
                    Surrogate loss: 0.0129
             Mean action noise std: 0.80
                       Mean reward: 135.40
               Mean episode length: 142.52
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 9.73s
                        Total time: 2078.65s
                               ETA: 1165715.4s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.531s, learning 0.160s)
               Value function loss: 13.4729
                    Surrogate loss: 0.0118
             Mean action noise std: 0.80
                       Mean reward: 133.51
               Mean episode length: 138.24
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 9.69s
                        Total time: 2088.34s
                               ETA: 1164595.7s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1771 steps/s (collection: 9.085s, learning 0.163s)
               Value function loss: 247.8990
                    Surrogate loss: 0.0030
             Mean action noise std: 0.80
                       Mean reward: 121.98
               Mean episode length: 132.75
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 9.25s
                        Total time: 2097.59s
                               ETA: 1163242.8s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.444s, learning 0.182s)
               Value function loss: 463.3787
                    Surrogate loss: -0.0017
             Mean action noise std: 0.80
                       Mean reward: 107.47
               Mean episode length: 128.43
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 9.63s
                        Total time: 2107.22s
                               ETA: 1162113.0s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.406s, learning 0.183s)
               Value function loss: 40.5623
                    Surrogate loss: -0.0093
             Mean action noise std: 0.80
                       Mean reward: 106.06
               Mean episode length: 128.92
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 9.59s
                        Total time: 2116.81s
                               ETA: 1160975.2s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.712s, learning 0.160s)
               Value function loss: 1009.1901
                    Surrogate loss: 0.0041
             Mean action noise std: 0.80
                       Mean reward: 84.76
               Mean episode length: 134.63
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 9.87s
                        Total time: 2126.68s
                               ETA: 1160004.2s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.528s, learning 0.165s)
               Value function loss: 134.3438
                    Surrogate loss: -0.0031
             Mean action noise std: 0.80
                       Mean reward: 91.58
               Mean episode length: 137.65
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 9.69s
                        Total time: 2136.37s
                               ETA: 1158946.3s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1624 steps/s (collection: 9.925s, learning 0.160s)
               Value function loss: 35.1583
                    Surrogate loss: -0.0132
             Mean action noise std: 0.80
                       Mean reward: 103.25
               Mean episode length: 140.20
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 10.08s
                        Total time: 2146.46s
                               ETA: 1158111.1s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.741s, learning 0.164s)
               Value function loss: 81154.0676
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: -276.71
               Mean episode length: 141.19
                  Mean reward/step: -1.21
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 9.91s
                        Total time: 2156.36s
                               ETA: 1157188.9s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1726 steps/s (collection: 9.321s, learning 0.170s)
               Value function loss: 86.2620
                    Surrogate loss: 0.1900
             Mean action noise std: 0.80
                       Mean reward: -258.50
               Mean episode length: 143.49
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 9.49s
                        Total time: 2165.85s
                               ETA: 1156055.6s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1755 steps/s (collection: 9.175s, learning 0.161s)
               Value function loss: 199.4772
                    Surrogate loss: 0.0281
             Mean action noise std: 0.80
                       Mean reward: 179.73
               Mean episode length: 149.83
                  Mean reward/step: 0.31
       Mean episode length/episode: 4.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 9.34s
                        Total time: 2175.19s
                               ETA: 1154851.2s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.516s, learning 0.167s)
               Value function loss: 2.7621
                    Surrogate loss: -0.0024
             Mean action noise std: 0.80
                       Mean reward: 178.67
               Mean episode length: 148.83
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 9.68s
                        Total time: 2184.87s
                               ETA: 1153843.3s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.667s, learning 0.161s)
               Value function loss: 4.5795
                    Surrogate loss: 0.0029
             Mean action noise std: 0.80
                       Mean reward: 178.67
               Mean episode length: 148.83
                  Mean reward/step: 0.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 9.83s
                        Total time: 2194.70s
                               ETA: 1152921.5s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1751 steps/s (collection: 9.193s, learning 0.162s)
               Value function loss: 2.8119
                    Surrogate loss: 0.0746
             Mean action noise std: 0.80
                       Mean reward: 178.67
               Mean episode length: 148.83
                  Mean reward/step: 0.66
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 9.35s
                        Total time: 2204.05s
                               ETA: 1151762.3s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1705 steps/s (collection: 9.430s, learning 0.175s)
               Value function loss: 2.4715
                    Surrogate loss: -0.0058
             Mean action noise std: 0.80
                       Mean reward: 178.67
               Mean episode length: 148.83
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 9.60s
                        Total time: 2213.66s
                               ETA: 1150744.8s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1726 steps/s (collection: 9.324s, learning 0.163s)
               Value function loss: 1.7686
                    Surrogate loss: 0.0257
             Mean action noise std: 0.80
                       Mean reward: 178.67
               Mean episode length: 148.83
                  Mean reward/step: 1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 9.49s
                        Total time: 2223.15s
                               ETA: 1149677.4s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.539s, learning 0.165s)
               Value function loss: 3.2238
                    Surrogate loss: 0.0067
             Mean action noise std: 0.80
                       Mean reward: 174.69
               Mean episode length: 147.51
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 9.70s
                        Total time: 2232.85s
                               ETA: 1148731.9s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.675s, learning 0.164s)
               Value function loss: 1081.9262
                    Surrogate loss: 0.0004
             Mean action noise std: 0.80
                       Mean reward: 139.68
               Mean episode length: 141.84
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 9.84s
                        Total time: 2242.69s
                               ETA: 1147865.3s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.597s, learning 0.158s)
               Value function loss: 2.6139
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 138.08
               Mean episode length: 140.97
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 9.75s
                        Total time: 2252.44s
                               ETA: 1146964.6s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.295s, learning 0.163s)
               Value function loss: 2.3624
                    Surrogate loss: 0.0266
             Mean action noise std: 0.79
                       Mean reward: 136.59
               Mean episode length: 140.97
                  Mean reward/step: 1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 9.46s
                        Total time: 2261.90s
                               ETA: 1145922.7s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.513s, learning 0.159s)
               Value function loss: 4.1161
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 139.26
               Mean episode length: 140.25
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 9.67s
                        Total time: 2271.57s
                               ETA: 1144998.9s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1729 steps/s (collection: 9.309s, learning 0.162s)
               Value function loss: 4.3292
                    Surrogate loss: 0.0061
             Mean action noise std: 0.79
                       Mean reward: 134.46
               Mean episode length: 139.84
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 9.47s
                        Total time: 2281.04s
                               ETA: 1143983.7s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1643 steps/s (collection: 9.809s, learning 0.163s)
               Value function loss: 4.5719
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: 129.64
               Mean episode length: 138.30
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 9.97s
                        Total time: 2291.02s
                               ETA: 1143228.4s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.430s, learning 0.168s)
               Value function loss: 4.3681
                    Surrogate loss: 0.0712
             Mean action noise std: 0.79
                       Mean reward: 159.25
               Mean episode length: 145.82
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 9.60s
                        Total time: 2300.61s
                               ETA: 1142294.8s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.556s, learning 0.173s)
               Value function loss: 5.9235
                    Surrogate loss: 0.0034
             Mean action noise std: 0.79
                       Mean reward: 141.13
               Mean episode length: 144.41
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 9.73s
                        Total time: 2310.34s
                               ETA: 1141435.5s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.458s, learning 0.164s)
               Value function loss: 6.8374
                    Surrogate loss: 0.0463
             Mean action noise std: 0.79
                       Mean reward: 121.60
               Mean episode length: 144.38
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 9.62s
                        Total time: 2319.97s
                               ETA: 1140531.4s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.499s, learning 0.170s)
               Value function loss: 8.4627
                    Surrogate loss: 0.0042
             Mean action noise std: 0.79
                       Mean reward: 109.91
               Mean episode length: 145.20
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 9.67s
                        Total time: 2329.63s
                               ETA: 1139659.2s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1727 steps/s (collection: 9.327s, learning 0.159s)
               Value function loss: 8.1981
                    Surrogate loss: 0.0158
             Mean action noise std: 0.79
                       Mean reward: 101.01
               Mean episode length: 144.02
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 9.49s
                        Total time: 2339.12s
                               ETA: 1138706.4s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.772s, learning 0.166s)
               Value function loss: 6.9242
                    Surrogate loss: 0.0178
             Mean action noise std: 0.79
                       Mean reward: 92.83
               Mean episode length: 146.79
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 9.94s
                        Total time: 2349.06s
                               ETA: 1137981.4s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.667s, learning 0.157s)
               Value function loss: 21.6283
                    Surrogate loss: 0.0067
             Mean action noise std: 0.79
                       Mean reward: 97.66
               Mean episode length: 146.19
                  Mean reward/step: -0.12
       Mean episode length/episode: 4.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 9.82s
                        Total time: 2358.88s
                               ETA: 1137208.6s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1706 steps/s (collection: 9.442s, learning 0.161s)
               Value function loss: 0.5277
                    Surrogate loss: 0.0153
             Mean action noise std: 0.79
                       Mean reward: 97.66
               Mean episode length: 146.19
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 9.60s
                        Total time: 2368.48s
                               ETA: 1136337.1s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1741 steps/s (collection: 9.239s, learning 0.167s)
               Value function loss: 0.5691
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 97.66
               Mean episode length: 146.19
                  Mean reward/step: 0.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 9.41s
                        Total time: 2377.89s
                               ETA: 1135380.0s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.289s, learning 0.170s)
               Value function loss: 0.8779
                    Surrogate loss: 0.0194
             Mean action noise std: 0.79
                       Mean reward: 97.66
               Mean episode length: 146.19
                  Mean reward/step: 0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 9.46s
                        Total time: 2387.35s
                               ETA: 1134457.1s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.711s, learning 0.157s)
               Value function loss: 1.5509
                    Surrogate loss: 0.0296
             Mean action noise std: 0.79
                       Mean reward: 97.66
               Mean episode length: 146.19
                  Mean reward/step: 0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 9.87s
                        Total time: 2397.22s
                               ETA: 1133735.9s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.545s, learning 0.171s)
               Value function loss: 5.8482
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 87.61
               Mean episode length: 134.69
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 9.72s
                        Total time: 2406.93s
                               ETA: 1132949.7s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1632 steps/s (collection: 9.866s, learning 0.167s)
               Value function loss: 6.3372
                    Surrogate loss: 0.0345
             Mean action noise std: 0.79
                       Mean reward: 80.76
               Mean episode length: 129.61
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 10.03s
                        Total time: 2416.97s
                               ETA: 1132319.8s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1767 steps/s (collection: 9.107s, learning 0.164s)
               Value function loss: 6.3453
                    Surrogate loss: 0.0057
             Mean action noise std: 0.79
                       Mean reward: 78.33
               Mean episode length: 126.81
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 9.27s
                        Total time: 2426.24s
                               ETA: 1131340.1s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.562s, learning 0.172s)
               Value function loss: 5.9856
                    Surrogate loss: 0.0018
             Mean action noise std: 0.79
                       Mean reward: 76.52
               Mean episode length: 124.80
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 9.73s
                        Total time: 2435.97s
                               ETA: 1130584.6s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.668s, learning 0.166s)
               Value function loss: 4.3697
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 76.58
               Mean episode length: 124.07
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 9.83s
                        Total time: 2445.80s
                               ETA: 1129882.3s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.695s, learning 0.159s)
               Value function loss: 4.8801
                    Surrogate loss: 0.0028
             Mean action noise std: 0.79
                       Mean reward: 76.14
               Mean episode length: 122.28
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 9.85s
                        Total time: 2455.66s
                               ETA: 1129195.1s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.590s, learning 0.159s)
               Value function loss: 4.6127
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 89.06
               Mean episode length: 135.53
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 9.75s
                        Total time: 2465.41s
                               ETA: 1128466.2s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1640 steps/s (collection: 9.813s, learning 0.171s)
               Value function loss: 4.2351
                    Surrogate loss: 0.0029
             Mean action noise std: 0.79
                       Mean reward: 95.54
               Mean episode length: 142.07
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 9.98s
                        Total time: 2475.39s
                               ETA: 1127851.3s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1721 steps/s (collection: 9.357s, learning 0.159s)
               Value function loss: 5.6747
                    Surrogate loss: 0.0042
             Mean action noise std: 0.79
                       Mean reward: 97.36
               Mean episode length: 142.69
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 9.52s
                        Total time: 2484.91s
                               ETA: 1127029.1s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.326s, learning 0.168s)
               Value function loss: 746.3876
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: 79.94
               Mean episode length: 143.73
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 9.49s
                        Total time: 2494.40s
                               ETA: 1126204.8s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1717 steps/s (collection: 9.376s, learning 0.162s)
               Value function loss: 6.9480
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 81.80
               Mean episode length: 145.94
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 9.54s
                        Total time: 2503.94s
                               ETA: 1125407.4s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.684s, learning 0.167s)
               Value function loss: 8.0613
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 103.65
               Mean episode length: 146.83
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 9.85s
                        Total time: 2513.79s
                               ETA: 1124757.3s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1734 steps/s (collection: 9.287s, learning 0.159s)
               Value function loss: 5.7893
                    Surrogate loss: 0.0343
             Mean action noise std: 0.79
                       Mean reward: 106.61
               Mean episode length: 146.82
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 9.45s
                        Total time: 2523.24s
                               ETA: 1123932.5s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.522s, learning 0.163s)
               Value function loss: 74.9695
                    Surrogate loss: 0.0019
             Mean action noise std: 0.79
                       Mean reward: 111.15
               Mean episode length: 150.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 4.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 9.68s
                        Total time: 2532.92s
                               ETA: 1123220.4s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1676 steps/s (collection: 9.613s, learning 0.159s)
               Value function loss: 1.9633
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 109.06
               Mean episode length: 148.97
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 9.77s
                        Total time: 2542.69s
                               ETA: 1122553.6s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.684s, learning 0.171s)
               Value function loss: 2.0462
                    Surrogate loss: 0.0113
             Mean action noise std: 0.79
                       Mean reward: 109.06
               Mean episode length: 148.97
                  Mean reward/step: 0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 9.86s
                        Total time: 2552.55s
                               ETA: 1121929.0s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1621 steps/s (collection: 9.941s, learning 0.163s)
               Value function loss: 1.9776
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 108.29
               Mean episode length: 147.92
                  Mean reward/step: 0.63
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 10.10s
                        Total time: 2562.65s
                               ETA: 1121418.4s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.371s, learning 0.163s)
               Value function loss: 2.9068
                    Surrogate loss: 0.0642
             Mean action noise std: 0.79
                       Mean reward: 107.84
               Mean episode length: 147.17
                  Mean reward/step: 0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 9.53s
                        Total time: 2572.18s
                               ETA: 1120663.8s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.587s, learning 0.166s)
               Value function loss: 3.0814
                    Surrogate loss: 0.0038
             Mean action noise std: 0.79
                       Mean reward: 110.73
               Mean episode length: 146.78
                  Mean reward/step: 0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 9.75s
                        Total time: 2581.94s
                               ETA: 1120011.0s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.701s, learning 0.174s)
               Value function loss: 11.2880
                    Surrogate loss: 0.0155
             Mean action noise std: 0.79
                       Mean reward: 103.98
               Mean episode length: 135.73
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 9.88s
                        Total time: 2591.81s
                               ETA: 1119416.5s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.634s, learning 0.166s)
               Value function loss: 10.5567
                    Surrogate loss: 0.0390
             Mean action noise std: 0.79
                       Mean reward: 100.62
               Mean episode length: 126.22
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 9.80s
                        Total time: 2601.61s
                               ETA: 1118794.5s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.165s)
               Value function loss: 9.1844
                    Surrogate loss: 0.0006
             Mean action noise std: 0.79
                       Mean reward: 102.45
               Mean episode length: 125.37
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 9.59s
                        Total time: 2611.20s
                               ETA: 1118088.5s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.728s, learning 0.165s)
               Value function loss: 7.6955
                    Surrogate loss: -0.0039
             Mean action noise std: 0.79
                       Mean reward: 101.88
               Mean episode length: 123.99
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 9.89s
                        Total time: 2621.10s
                               ETA: 1117517.1s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1630 steps/s (collection: 9.884s, learning 0.165s)
               Value function loss: 173.3070
                    Surrogate loss: 0.0006
             Mean action noise std: 0.79
                       Mean reward: 86.21
               Mean episode length: 123.23
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 10.05s
                        Total time: 2631.15s
                               ETA: 1117016.8s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.754s, learning 0.157s)
               Value function loss: 9.6597
                    Surrogate loss: 0.0140
             Mean action noise std: 0.79
                       Mean reward: 101.98
               Mean episode length: 132.71
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 9.91s
                        Total time: 2641.06s
                               ETA: 1116462.3s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.736s, learning 0.166s)
               Value function loss: 8.0902
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: 104.19
               Mean episode length: 138.25
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 9.90s
                        Total time: 2650.96s
                               ETA: 1115909.0s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.706s, learning 0.159s)
               Value function loss: 8.9135
                    Surrogate loss: 0.0151
             Mean action noise std: 0.79
                       Mean reward: 110.73
               Mean episode length: 141.48
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 9.86s
                        Total time: 2660.83s
                               ETA: 1115344.2s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.818s, learning 0.178s)
               Value function loss: 2444.4388
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 96.27
               Mean episode length: 139.57
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 10.00s
                        Total time: 2670.82s
                               ETA: 1114839.1s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1726 steps/s (collection: 9.327s, learning 0.165s)
               Value function loss: 23356.2065
                    Surrogate loss: -0.0017
             Mean action noise std: 0.79
                       Mean reward: -4.28
               Mean episode length: 141.23
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 9.49s
                        Total time: 2680.31s
                               ETA: 1114128.4s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1643 steps/s (collection: 9.805s, learning 0.162s)
               Value function loss: 73.4157
                    Surrogate loss: 0.0114
             Mean action noise std: 0.79
                       Mean reward: 28.00
               Mean episode length: 145.25
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 9.97s
                        Total time: 2690.28s
                               ETA: 1113620.2s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1613 steps/s (collection: 9.989s, learning 0.164s)
               Value function loss: 67.4936
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 127.09
               Mean episode length: 146.37
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 10.15s
                        Total time: 2700.43s
                               ETA: 1113192.5s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1670 steps/s (collection: 9.637s, learning 0.169s)
               Value function loss: 25.8744
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: 131.53
               Mean episode length: 148.50
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 9.81s
                        Total time: 2710.24s
                               ETA: 1112626.1s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.617s, learning 0.172s)
               Value function loss: 96.3249
                    Surrogate loss: 0.0355
             Mean action noise std: 0.79
                       Mean reward: 141.07
               Mean episode length: 148.56
                  Mean reward/step: 0.55
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 9.79s
                        Total time: 2720.03s
                               ETA: 1112057.2s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.560s, learning 0.161s)
               Value function loss: 2.7046
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 141.77
               Mean episode length: 146.28
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 9.72s
                        Total time: 2729.75s
                               ETA: 1111465.1s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.454s, learning 0.168s)
               Value function loss: 2.1878
                    Surrogate loss: 0.0003
             Mean action noise std: 0.79
                       Mean reward: 141.77
               Mean episode length: 146.28
                  Mean reward/step: 0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 9.62s
                        Total time: 2739.37s
                               ETA: 1110837.3s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.744s, learning 0.163s)
               Value function loss: 2.8779
                    Surrogate loss: 0.0067
             Mean action noise std: 0.79
                       Mean reward: 140.40
               Mean episode length: 145.28
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 9.91s
                        Total time: 2749.28s
                               ETA: 1110330.0s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.619s, learning 0.161s)
               Value function loss: 3.1430
                    Surrogate loss: 0.0187
             Mean action noise std: 0.79
                       Mean reward: 140.40
               Mean episode length: 145.28
                  Mean reward/step: 0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 9.78s
                        Total time: 2759.06s
                               ETA: 1109775.5s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1624 steps/s (collection: 9.921s, learning 0.162s)
               Value function loss: 5.6497
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 141.34
               Mean episode length: 142.07
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 10.08s
                        Total time: 2769.14s
                               ETA: 1109347.0s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1635 steps/s (collection: 9.852s, learning 0.166s)
               Value function loss: 8.8328
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 128.88
               Mean episode length: 131.48
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 10.02s
                        Total time: 2779.16s
                               ETA: 1108895.8s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1643 steps/s (collection: 9.803s, learning 0.168s)
               Value function loss: 19.8210
                    Surrogate loss: 0.0442
             Mean action noise std: 0.79
                       Mean reward: 124.63
               Mean episode length: 124.23
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 9.97s
                        Total time: 2789.13s
                               ETA: 1108429.5s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1696 steps/s (collection: 9.487s, learning 0.172s)
               Value function loss: 10.5460
                    Surrogate loss: 0.0251
             Mean action noise std: 0.79
                       Mean reward: 120.59
               Mean episode length: 118.79
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 9.66s
                        Total time: 2798.79s
                               ETA: 1107843.2s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.595s, learning 0.167s)
               Value function loss: 250.1522
                    Surrogate loss: -0.0025
             Mean action noise std: 0.79
                       Mean reward: 95.60
               Mean episode length: 113.99
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 9.76s
                        Total time: 2808.55s
                               ETA: 1107302.2s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1618 steps/s (collection: 9.949s, learning 0.172s)
               Value function loss: 25.7137
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 94.75
               Mean episode length: 116.29
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 10.12s
                        Total time: 2818.67s
                               ETA: 1106906.2s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.613s, learning 0.171s)
               Value function loss: 10.8372
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 107.55
               Mean episode length: 123.22
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 9.78s
                        Total time: 2828.46s
                               ETA: 1106381.5s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1612 steps/s (collection: 9.996s, learning 0.162s)
               Value function loss: 118.6709
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 125.24
               Mean episode length: 131.63
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 10.16s
                        Total time: 2838.61s
                               ETA: 1106006.4s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.517s, learning 0.170s)
               Value function loss: 1548.1364
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 106.73
               Mean episode length: 133.43
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 9.69s
                        Total time: 2848.30s
                               ETA: 1105451.6s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1696 steps/s (collection: 9.485s, learning 0.174s)
               Value function loss: 16.3268
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 134.10
               Mean episode length: 136.89
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 9.66s
                        Total time: 2857.96s
                               ETA: 1104889.9s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.330s, learning 0.164s)
               Value function loss: 22.5716
                    Surrogate loss: 0.0234
             Mean action noise std: 0.79
                       Mean reward: 163.34
               Mean episode length: 142.17
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 9.49s
                        Total time: 2867.45s
                               ETA: 1104269.0s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.507s, learning 0.162s)
               Value function loss: 64.2882
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 156.07
               Mean episode length: 143.58
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 9.67s
                        Total time: 2877.12s
                               ETA: 1103720.1s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1615 steps/s (collection: 9.969s, learning 0.170s)
               Value function loss: 69.4440
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 163.23
               Mean episode length: 144.31
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 10.14s
                        Total time: 2887.26s
                               ETA: 1103354.8s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1591 steps/s (collection: 10.114s, learning 0.181s)
               Value function loss: 17.6041
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 166.40
               Mean episode length: 147.73
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 10.30s
                        Total time: 2897.56s
                               ETA: 1103051.9s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1629 steps/s (collection: 9.881s, learning 0.172s)
               Value function loss: 186.9795
                    Surrogate loss: 0.0241
             Mean action noise std: 0.79
                       Mean reward: 178.24
               Mean episode length: 148.92
                  Mean reward/step: 0.43
       Mean episode length/episode: 4.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 10.05s
                        Total time: 2907.61s
                               ETA: 1102659.4s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1649 steps/s (collection: 9.758s, learning 0.176s)
               Value function loss: 2.8526
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 173.55
               Mean episode length: 145.71
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 9.93s
                        Total time: 2917.55s
                               ETA: 1102224.6s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1649 steps/s (collection: 9.749s, learning 0.184s)
               Value function loss: 2.4152
                    Surrogate loss: 0.0037
             Mean action noise std: 0.79
                       Mean reward: 173.16
               Mean episode length: 145.44
                  Mean reward/step: 0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 9.93s
                        Total time: 2927.48s
                               ETA: 1101792.7s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1705 steps/s (collection: 9.424s, learning 0.183s)
               Value function loss: 2.3695
                    Surrogate loss: 0.0101
             Mean action noise std: 0.79
                       Mean reward: 172.55
               Mean episode length: 145.14
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 9.61s
                        Total time: 2937.09s
                               ETA: 1101241.7s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.544s, learning 0.187s)
               Value function loss: 1.7190
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 171.47
               Mean episode length: 144.83
                  Mean reward/step: 1.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 9.73s
                        Total time: 2946.82s
                               ETA: 1100741.3s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.702s, learning 0.192s)
               Value function loss: 8.6721
                    Surrogate loss: 0.0082
             Mean action noise std: 0.79
                       Mean reward: 158.21
               Mean episode length: 139.23
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 9.89s
                        Total time: 2956.71s
                               ETA: 1100305.2s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1640 steps/s (collection: 9.815s, learning 0.174s)
               Value function loss: 2270.3011
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 98.49
               Mean episode length: 129.10
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 9.99s
                        Total time: 2966.70s
                               ETA: 1099907.4s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1628 steps/s (collection: 9.872s, learning 0.188s)
               Value function loss: 68.0772
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 86.88
               Mean episode length: 124.77
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 10.06s
                        Total time: 2976.76s
                               ETA: 1099538.2s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1646 steps/s (collection: 9.773s, learning 0.179s)
               Value function loss: 20.0771
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 76.78
               Mean episode length: 120.88
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 9.95s
                        Total time: 2986.71s
                               ETA: 1099132.3s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.692s, learning 0.175s)
               Value function loss: 16.6744
                    Surrogate loss: 0.0020
             Mean action noise std: 0.79
                       Mean reward: 112.06
               Mean episode length: 117.61
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 9.87s
                        Total time: 2996.58s
                               ETA: 1098698.1s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1588 steps/s (collection: 10.133s, learning 0.182s)
               Value function loss: 22.1649
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 113.95
               Mean episode length: 118.34
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 10.31s
                        Total time: 3006.89s
                               ETA: 1098430.7s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.492s, learning 0.192s)
               Value function loss: 19.1406
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 129.13
               Mean episode length: 124.63
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 9.68s
                        Total time: 3016.58s
                               ETA: 1097935.4s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.809s, learning 0.174s)
               Value function loss: 328.6878
                    Surrogate loss: 0.0015
             Mean action noise std: 0.79
                       Mean reward: 129.75
               Mean episode length: 128.49
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 9.98s
                        Total time: 3026.56s
                               ETA: 1097552.1s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.706s, learning 0.191s)
               Value function loss: 16.8406
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 150.62
               Mean episode length: 138.52
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 9.90s
                        Total time: 3036.46s
                               ETA: 1097140.6s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1639 steps/s (collection: 9.795s, learning 0.195s)
               Value function loss: 19.7423
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 159.42
               Mean episode length: 140.91
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 9.99s
                        Total time: 3046.45s
                               ETA: 1096765.6s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.646s, learning 0.179s)
               Value function loss: 17.3039
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 159.12
               Mean episode length: 142.99
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 9.83s
                        Total time: 3056.27s
                               ETA: 1096334.0s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1574 steps/s (collection: 10.207s, learning 0.196s)
               Value function loss: 17.0879
                    Surrogate loss: 0.0139
             Mean action noise std: 0.79
                       Mean reward: 166.60
               Mean episode length: 145.53
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 10.40s
                        Total time: 3066.68s
                               ETA: 1096111.7s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.538s, learning 0.165s)
               Value function loss: 13568.2239
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: 61.44
               Mean episode length: 147.97
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 9.70s
                        Total time: 3076.38s
                               ETA: 1095641.5s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.683s, learning 0.164s)
               Value function loss: 12.2984
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 167.38
               Mean episode length: 145.83
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 9.85s
                        Total time: 3086.23s
                               ETA: 1095226.2s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1587 steps/s (collection: 10.155s, learning 0.163s)
               Value function loss: 64.6957
                    Surrogate loss: 0.0234
             Mean action noise std: 0.79
                       Mean reward: 175.61
               Mean episode length: 148.06
                  Mean reward/step: 0.32
       Mean episode length/episode: 4.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 10.32s
                        Total time: 3096.54s
                               ETA: 1094979.9s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.626s, learning 0.165s)
               Value function loss: 2.1216
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 174.47
               Mean episode length: 147.16
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 9.79s
                        Total time: 3106.34s
                               ETA: 1094549.9s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.579s, learning 0.158s)
               Value function loss: 2.8506
                    Surrogate loss: 0.0053
             Mean action noise std: 0.79
                       Mean reward: 173.59
               Mean episode length: 145.61
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 9.74s
                        Total time: 3116.07s
                               ETA: 1094103.7s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.553s, learning 0.167s)
               Value function loss: 6.8390
                    Surrogate loss: 0.0031
             Mean action noise std: 0.79
                       Mean reward: 174.08
               Mean episode length: 145.27
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 9.72s
                        Total time: 3125.79s
                               ETA: 1093654.5s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.564s, learning 0.163s)
               Value function loss: 2.8669
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 173.01
               Mean episode length: 144.42
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 9.73s
                        Total time: 3135.52s
                               ETA: 1093211.2s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1740 steps/s (collection: 9.249s, learning 0.162s)
               Value function loss: 12.9281
                    Surrogate loss: 0.0180
             Mean action noise std: 0.79
                       Mean reward: 159.12
               Mean episode length: 136.26
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 9.41s
                        Total time: 3144.93s
                               ETA: 1092660.9s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1705 steps/s (collection: 9.439s, learning 0.166s)
               Value function loss: 13.5681
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 155.30
               Mean episode length: 132.28
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 9.61s
                        Total time: 3154.54s
                               ETA: 1092181.5s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.480s, learning 0.164s)
               Value function loss: 1273.1023
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: 134.71
               Mean episode length: 135.88
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 9.64s
                        Total time: 3164.18s
                               ETA: 1091718.7s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.794s, learning 0.169s)
               Value function loss: 14473.0697
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: 47.30
               Mean episode length: 133.22
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 9.96s
                        Total time: 3174.14s
                               ETA: 1091369.0s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.602s, learning 0.166s)
               Value function loss: 23.6363
                    Surrogate loss: 0.0105
             Mean action noise std: 0.79
                       Mean reward: 64.99
               Mean episode length: 130.50
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 9.77s
                        Total time: 3183.91s
                               ETA: 1090954.7s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.461s, learning 0.169s)
               Value function loss: 26.4524
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 149.00
               Mean episode length: 133.05
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 9.63s
                        Total time: 3193.54s
                               ETA: 1090495.8s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.457s, learning 0.162s)
               Value function loss: 18.8055
                    Surrogate loss: 0.0037
             Mean action noise std: 0.79
                       Mean reward: 166.61
               Mean episode length: 136.66
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 9.62s
                        Total time: 3203.16s
                               ETA: 1090036.5s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1750 steps/s (collection: 9.203s, learning 0.158s)
               Value function loss: 15.8220
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: 166.26
               Mean episode length: 136.90
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 9.36s
                        Total time: 3212.52s
                               ETA: 1089492.9s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.474s, learning 0.165s)
               Value function loss: 29.0516
                    Surrogate loss: 0.0054
             Mean action noise std: 0.79
                       Mean reward: 163.79
               Mean episode length: 143.06
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 9.64s
                        Total time: 3222.16s
                               ETA: 1089046.6s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.652s, learning 0.165s)
               Value function loss: 15.6273
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 167.28
               Mean episode length: 141.44
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 9.82s
                        Total time: 3231.98s
                               ETA: 1088663.3s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1628 steps/s (collection: 9.896s, learning 0.161s)
               Value function loss: 66.4179
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 158.71
               Mean episode length: 144.22
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 10.06s
                        Total time: 3242.04s
                               ETA: 1088363.3s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.804s, learning 0.161s)
               Value function loss: 30.2441
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 171.44
               Mean episode length: 145.68
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 9.96s
                        Total time: 3252.00s
                               ETA: 1088034.2s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.435s, learning 0.159s)
               Value function loss: 125.3765
                    Surrogate loss: -0.0010
             Mean action noise std: 0.79
                       Mean reward: 161.91
               Mean episode length: 143.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 9.59s
                        Total time: 3261.59s
                               ETA: 1087583.4s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1633 steps/s (collection: 9.873s, learning 0.159s)
               Value function loss: 190.8202
                    Surrogate loss: 0.0325
             Mean action noise std: 0.79
                       Mean reward: 181.08
               Mean episode length: 150.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 4.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 10.03s
                        Total time: 3271.63s
                               ETA: 1087281.1s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1613 steps/s (collection: 9.997s, learning 0.160s)
               Value function loss: 6.8637
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 168.04
               Mean episode length: 144.18
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 10.16s
                        Total time: 3281.78s
                               ETA: 1087022.2s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.571s, learning 0.163s)
               Value function loss: 3.4776
                    Surrogate loss: 0.0052
             Mean action noise std: 0.79
                       Mean reward: 163.94
               Mean episode length: 141.51
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 9.73s
                        Total time: 3291.52s
                               ETA: 1086625.2s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.428s, learning 0.158s)
               Value function loss: 192.1792
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: 151.62
               Mean episode length: 139.78
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 9.59s
                        Total time: 3301.10s
                               ETA: 1086182.1s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.583s, learning 0.165s)
               Value function loss: 4.7438
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: 148.78
               Mean episode length: 137.21
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 9.75s
                        Total time: 3310.85s
                               ETA: 1085795.1s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.504s, learning 0.163s)
               Value function loss: 6.8543
                    Surrogate loss: 0.0132
             Mean action noise std: 0.79
                       Mean reward: 143.70
               Mean episode length: 134.93
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 9.67s
                        Total time: 3320.52s
                               ETA: 1085384.0s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1706 steps/s (collection: 9.437s, learning 0.163s)
               Value function loss: 23.7749
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 120.57
               Mean episode length: 124.39
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 9.60s
                        Total time: 3330.12s
                               ETA: 1084953.9s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.462s, learning 0.162s)
               Value function loss: 14.9754
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 141.97
               Mean episode length: 131.16
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 9.62s
                        Total time: 3339.74s
                               ETA: 1084534.3s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.701s, learning 0.170s)
               Value function loss: 45481.0838
                    Surrogate loss: 0.0029
             Mean action noise std: 0.79
                       Mean reward: 1.01
               Mean episode length: 130.67
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 9.87s
                        Total time: 3349.61s
                               ETA: 1084197.1s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.551s, learning 0.168s)
               Value function loss: 72.4841
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: -0.36
               Mean episode length: 126.08
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 9.72s
                        Total time: 3359.33s
                               ETA: 1083813.3s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.655s, learning 0.161s)
               Value function loss: 20.7422
                    Surrogate loss: 0.0047
             Mean action noise std: 0.79
                       Mean reward: 149.74
               Mean episode length: 130.79
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 9.82s
                        Total time: 3369.15s
                               ETA: 1083463.2s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.422s, learning 0.160s)
               Value function loss: 17.0597
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 163.66
               Mean episode length: 138.52
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 9.58s
                        Total time: 3378.73s
                               ETA: 1083040.2s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.662s, learning 0.174s)
               Value function loss: 32.2562
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 169.58
               Mean episode length: 140.89
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 9.84s
                        Total time: 3388.56s
                               ETA: 1082700.7s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1731 steps/s (collection: 9.304s, learning 0.160s)
               Value function loss: 12.2509
                    Surrogate loss: 0.0099
             Mean action noise std: 0.79
                       Mean reward: 164.22
               Mean episode length: 142.30
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 9.46s
                        Total time: 3398.03s
                               ETA: 1082245.3s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.744s, learning 0.165s)
               Value function loss: 12.6459
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 163.65
               Mean episode length: 141.93
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 9.91s
                        Total time: 3407.94s
                               ETA: 1081933.4s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.745s, learning 0.171s)
               Value function loss: 12.1244
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 174.73
               Mean episode length: 145.48
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 9.92s
                        Total time: 3417.85s
                               ETA: 1081625.8s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1629 steps/s (collection: 9.889s, learning 0.168s)
               Value function loss: 14.6798
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: 178.49
               Mean episode length: 147.93
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 10.06s
                        Total time: 3427.91s
                               ETA: 1081364.8s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.468s, learning 0.163s)
               Value function loss: 61.0639
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 158.51
               Mean episode length: 143.04
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 9.63s
                        Total time: 3437.54s
                               ETA: 1080971.4s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.504s, learning 0.165s)
               Value function loss: 31.6281
                    Surrogate loss: 0.0034
             Mean action noise std: 0.79
                       Mean reward: 148.11
               Mean episode length: 143.28
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 9.67s
                        Total time: 3447.21s
                               ETA: 1080592.0s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.449s, learning 0.165s)
               Value function loss: 119.7117
                    Surrogate loss: 0.0266
             Mean action noise std: 0.79
                       Mean reward: 166.81
               Mean episode length: 144.75
                  Mean reward/step: 0.80
       Mean episode length/episode: 5.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 9.61s
                        Total time: 3456.82s
                               ETA: 1080198.0s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.601s, learning 0.164s)
               Value function loss: 5.0513
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 158.13
               Mean episode length: 139.50
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 9.77s
                        Total time: 3466.59s
                               ETA: 1079853.6s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.551s, learning 0.164s)
               Value function loss: 13.4642
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 149.33
               Mean episode length: 136.05
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 9.72s
                        Total time: 3476.31s
                               ETA: 1079495.6s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.408s, learning 0.167s)
               Value function loss: 4.3369
                    Surrogate loss: 0.0506
             Mean action noise std: 0.79
                       Mean reward: 147.16
               Mean episode length: 134.74
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 9.58s
                        Total time: 3485.88s
                               ETA: 1079096.6s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1697 steps/s (collection: 9.484s, learning 0.170s)
               Value function loss: 26.6229
                    Surrogate loss: 0.0083
             Mean action noise std: 0.79
                       Mean reward: 132.02
               Mean episode length: 128.33
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 9.65s
                        Total time: 3495.53s
                               ETA: 1078724.2s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1670 steps/s (collection: 9.649s, learning 0.159s)
               Value function loss: 8.0269
                    Surrogate loss: 0.0049
             Mean action noise std: 0.79
                       Mean reward: 130.83
               Mean episode length: 123.21
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 9.81s
                        Total time: 3505.34s
                               ETA: 1078401.5s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1642 steps/s (collection: 9.808s, learning 0.164s)
               Value function loss: 48.2892
                    Surrogate loss: 0.0035
             Mean action noise std: 0.79
                       Mean reward: 148.54
               Mean episode length: 132.96
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 9.97s
                        Total time: 3515.32s
                               ETA: 1078131.0s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.295s, learning 0.171s)
               Value function loss: 384.3509
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 114.15
               Mean episode length: 122.44
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 9.47s
                        Total time: 3524.78s
                               ETA: 1077707.3s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.598s, learning 0.163s)
               Value function loss: 95.1327
                    Surrogate loss: 0.0038
             Mean action noise std: 0.79
                       Mean reward: 106.51
               Mean episode length: 115.14
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 9.76s
                        Total time: 3534.54s
                               ETA: 1077376.1s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1700 steps/s (collection: 9.473s, learning 0.164s)
               Value function loss: 1471.0516
                    Surrogate loss: -0.0004
             Mean action noise std: 0.79
                       Mean reward: 104.47
               Mean episode length: 122.11
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 9.64s
                        Total time: 3544.18s
                               ETA: 1077009.2s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1629 steps/s (collection: 9.879s, learning 0.175s)
               Value function loss: 402.8130
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 119.91
               Mean episode length: 125.82
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 10.05s
                        Total time: 3554.23s
                               ETA: 1076770.9s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.476s, learning 0.162s)
               Value function loss: 26.0502
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: 155.26
               Mean episode length: 136.72
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 9.64s
                        Total time: 3563.87s
                               ETA: 1076408.2s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.478s, learning 0.167s)
               Value function loss: 11.6241
                    Surrogate loss: -0.0003
             Mean action noise std: 0.79
                       Mean reward: 166.22
               Mean episode length: 142.44
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 9.65s
                        Total time: 3573.52s
                               ETA: 1076049.8s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1760 steps/s (collection: 9.134s, learning 0.171s)
               Value function loss: 41.2216
                    Surrogate loss: 0.0006
             Mean action noise std: 0.79
                       Mean reward: 168.42
               Mean episode length: 145.15
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 9.31s
                        Total time: 3582.82s
                               ETA: 1075591.5s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1734 steps/s (collection: 9.278s, learning 0.168s)
               Value function loss: 12.8722
                    Surrogate loss: 0.0060
             Mean action noise std: 0.79
                       Mean reward: 172.75
               Mean episode length: 146.10
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 9.45s
                        Total time: 3592.27s
                               ETA: 1075177.9s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.646s, learning 0.167s)
               Value function loss: 51.2332
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 152.53
               Mean episode length: 142.83
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 9.81s
                        Total time: 3602.08s
                               ETA: 1074876.3s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1635 steps/s (collection: 9.839s, learning 0.178s)
               Value function loss: 14.1576
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 161.78
               Mean episode length: 144.33
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 10.02s
                        Total time: 3612.10s
                               ETA: 1074637.2s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.727s, learning 0.166s)
               Value function loss: 10.0643
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 161.69
               Mean episode length: 148.41
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 9.89s
                        Total time: 3621.99s
                               ETA: 1074362.6s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.596s, learning 0.167s)
               Value function loss: 10.1923
                    Surrogate loss: -0.0022
             Mean action noise std: 0.79
                       Mean reward: 161.07
               Mean episode length: 147.12
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 9.76s
                        Total time: 3631.75s
                               ETA: 1074051.1s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1592 steps/s (collection: 10.125s, learning 0.162s)
               Value function loss: 911.6113
                    Surrogate loss: 0.0051
             Mean action noise std: 0.79
                       Mean reward: 153.08
               Mean episode length: 147.25
                  Mean reward/step: 0.56
       Mean episode length/episode: 5.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 10.29s
                        Total time: 3642.04s
                               ETA: 1073896.0s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1611 steps/s (collection: 9.998s, learning 0.168s)
               Value function loss: 3.9395
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 157.05
               Mean episode length: 146.76
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 10.17s
                        Total time: 3652.21s
                               ETA: 1073705.9s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.376s, learning 0.187s)
               Value function loss: 16.3563
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: 154.50
               Mean episode length: 145.48
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 9.56s
                        Total time: 3661.77s
                               ETA: 1073340.2s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1721 steps/s (collection: 9.342s, learning 0.175s)
               Value function loss: 4.2577
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: 151.81
               Mean episode length: 145.41
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 9.52s
                        Total time: 3671.29s
                               ETA: 1072963.3s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.742s, learning 0.174s)
               Value function loss: 5.1667
                    Surrogate loss: 0.0101
             Mean action noise std: 0.79
                       Mean reward: 152.69
               Mean episode length: 145.41
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 9.92s
                        Total time: 3681.20s
                               ETA: 1072704.8s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.645s, learning 0.186s)
               Value function loss: 11.2310
                    Surrogate loss: 0.0130
             Mean action noise std: 0.79
                       Mean reward: 177.98
               Mean episode length: 145.40
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 9.83s
                        Total time: 3691.03s
                               ETA: 1072423.1s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1592 steps/s (collection: 10.120s, learning 0.172s)
               Value function loss: 14.1276
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 157.61
               Mean episode length: 138.43
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 10.29s
                        Total time: 3701.33s
                               ETA: 1072276.2s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.776s, learning 0.187s)
               Value function loss: 124.1341
                    Surrogate loss: 0.0050
             Mean action noise std: 0.79
                       Mean reward: 152.60
               Mean episode length: 139.34
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 9.96s
                        Total time: 3711.29s
                               ETA: 1072035.5s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1577 steps/s (collection: 10.204s, learning 0.184s)
               Value function loss: 104.1580
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 169.42
               Mean episode length: 139.56
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 10.39s
                        Total time: 3721.68s
                               ETA: 1071918.4s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1633 steps/s (collection: 9.848s, learning 0.181s)
               Value function loss: 14.9453
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 176.45
               Mean episode length: 145.97
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 10.03s
                        Total time: 3731.71s
                               ETA: 1071698.7s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.719s, learning 0.171s)
               Value function loss: 12.2272
                    Surrogate loss: 0.0116
             Mean action noise std: 0.79
                       Mean reward: 184.49
               Mean episode length: 146.76
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 9.89s
                        Total time: 3741.60s
                               ETA: 1071440.4s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.734s, learning 0.170s)
               Value function loss: 13.1303
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 175.40
               Mean episode length: 147.97
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 9.90s
                        Total time: 3751.50s
                               ETA: 1071187.5s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.766s, learning 0.197s)
               Value function loss: 11.7994
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 175.74
               Mean episode length: 147.12
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 9.96s
                        Total time: 3761.46s
                               ETA: 1070952.9s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.523s, learning 0.180s)
               Value function loss: 281.4196
                    Surrogate loss: -0.0008
             Mean action noise std: 0.79
                       Mean reward: 169.15
               Mean episode length: 148.10
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 9.70s
                        Total time: 3771.17s
                               ETA: 1070645.9s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1623 steps/s (collection: 9.919s, learning 0.173s)
               Value function loss: 10.1492
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 173.61
               Mean episode length: 147.69
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 10.09s
                        Total time: 3781.26s
                               ETA: 1070450.6s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.577s, learning 0.163s)
               Value function loss: 19.7979
                    Surrogate loss: -0.0006
             Mean action noise std: 0.79
                       Mean reward: 174.83
               Mean episode length: 144.34
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 9.74s
                        Total time: 3791.00s
                               ETA: 1070156.9s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.541s, learning 0.160s)
               Value function loss: 12.9798
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 177.74
               Mean episode length: 149.92
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 9.70s
                        Total time: 3800.70s
                               ETA: 1069854.0s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.598s, learning 0.158s)
               Value function loss: 34.2585
                    Surrogate loss: 0.0061
             Mean action noise std: 0.79
                       Mean reward: 176.88
               Mean episode length: 148.65
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 9.76s
                        Total time: 3810.46s
                               ETA: 1069567.9s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1625 steps/s (collection: 9.911s, learning 0.171s)
               Value function loss: 12.5799
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 188.05
               Mean episode length: 149.01
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 10.08s
                        Total time: 3820.54s
                               ETA: 1069374.8s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1650 steps/s (collection: 9.752s, learning 0.175s)
               Value function loss: 38.0951
                    Surrogate loss: 0.0153
             Mean action noise std: 0.79
                       Mean reward: 180.83
               Mean episode length: 148.95
                  Mean reward/step: 0.63
       Mean episode length/episode: 5.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 9.93s
                        Total time: 3830.46s
                               ETA: 1069139.4s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1742 steps/s (collection: 9.228s, learning 0.173s)
               Value function loss: 6.5068
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 175.25
               Mean episode length: 146.50
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 9.40s
                        Total time: 3839.86s
                               ETA: 1068758.8s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.470s, learning 0.159s)
               Value function loss: 7.7593
                    Surrogate loss: 0.0067
             Mean action noise std: 0.79
                       Mean reward: 169.83
               Mean episode length: 142.57
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 9.63s
                        Total time: 3849.49s
                               ETA: 1068443.7s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1651 steps/s (collection: 9.755s, learning 0.163s)
               Value function loss: 5.7579
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 169.71
               Mean episode length: 142.57
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 9.92s
                        Total time: 3859.41s
                               ETA: 1068210.2s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.428s, learning 0.159s)
               Value function loss: 296.2883
                    Surrogate loss: 0.0064
             Mean action noise std: 0.79
                       Mean reward: 156.73
               Mean episode length: 140.57
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 9.59s
                        Total time: 3869.00s
                               ETA: 1067886.4s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.504s, learning 0.167s)
               Value function loss: 14.0305
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 164.21
               Mean episode length: 141.62
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 9.67s
                        Total time: 3878.67s
                               ETA: 1067587.4s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.652s, learning 0.161s)
               Value function loss: 16.8586
                    Surrogate loss: 0.0105
             Mean action noise std: 0.79
                       Mean reward: 175.29
               Mean episode length: 140.98
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 9.81s
                        Total time: 3888.48s
                               ETA: 1067329.3s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.523s, learning 0.162s)
               Value function loss: 17.4583
                    Surrogate loss: 0.0145
             Mean action noise std: 0.79
                       Mean reward: 188.23
               Mean episode length: 143.63
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 9.69s
                        Total time: 3898.17s
                               ETA: 1067037.6s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1610 steps/s (collection: 10.007s, learning 0.166s)
               Value function loss: 19.4149
                    Surrogate loss: 0.0193
             Mean action noise std: 0.79
                       Mean reward: 183.39
               Mean episode length: 142.27
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 10.17s
                        Total time: 3908.34s
                               ETA: 1066880.5s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.529s, learning 0.176s)
               Value function loss: 19.7108
                    Surrogate loss: 0.0019
             Mean action noise std: 0.79
                       Mean reward: 189.45
               Mean episode length: 147.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 9.70s
                        Total time: 3918.04s
                               ETA: 1066596.7s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.399s, learning 0.160s)
               Value function loss: 15.1958
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 185.58
               Mean episode length: 148.26
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 9.56s
                        Total time: 3927.60s
                               ETA: 1066274.8s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1634 steps/s (collection: 9.851s, learning 0.173s)
               Value function loss: 17.3827
                    Surrogate loss: 0.0128
             Mean action noise std: 0.79
                       Mean reward: 189.50
               Mean episode length: 149.06
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 10.02s
                        Total time: 3937.63s
                               ETA: 1066080.4s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.542s, learning 0.159s)
               Value function loss: 14.0364
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 184.11
               Mean episode length: 148.04
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 9.70s
                        Total time: 3947.33s
                               ETA: 1065799.8s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.739s, learning 0.160s)
               Value function loss: 12.9842
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 200.86
               Mean episode length: 149.02
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 9.90s
                        Total time: 3957.23s
                               ETA: 1065574.1s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.586s, learning 0.164s)
               Value function loss: 12.0597
                    Surrogate loss: 0.0071
             Mean action noise std: 0.79
                       Mean reward: 194.39
               Mean episode length: 148.68
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 9.75s
                        Total time: 3966.98s
                               ETA: 1065309.5s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.570s, learning 0.186s)
               Value function loss: 18.4755
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 193.18
               Mean episode length: 147.21
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 9.76s
                        Total time: 3976.73s
                               ETA: 1065047.8s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1777 steps/s (collection: 9.047s, learning 0.169s)
               Value function loss: 16.4080
                    Surrogate loss: 0.0178
             Mean action noise std: 0.79
                       Mean reward: 201.17
               Mean episode length: 148.54
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 9.22s
                        Total time: 3985.95s
                               ETA: 1064643.2s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.355s, learning 0.168s)
               Value function loss: 14.8382
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 187.23
               Mean episode length: 148.86
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 9.52s
                        Total time: 3995.47s
                               ETA: 1064322.7s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1676 steps/s (collection: 9.591s, learning 0.179s)
               Value function loss: 98.7773
                    Surrogate loss: 0.0757
             Mean action noise std: 0.79
                       Mean reward: 193.83
               Mean episode length: 150.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 5.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 9.77s
                        Total time: 4005.24s
                               ETA: 1064069.5s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.658s, learning 0.177s)
               Value function loss: 8.1971
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 193.08
               Mean episode length: 149.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 9.83s
                        Total time: 4015.07s
                               ETA: 1063834.6s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.523s, learning 0.167s)
               Value function loss: 6.7033
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 188.56
               Mean episode length: 148.69
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 9.69s
                        Total time: 4024.76s
                               ETA: 1063562.7s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.448s, learning 0.167s)
               Value function loss: 7.1851
                    Surrogate loss: 0.0163
             Mean action noise std: 0.79
                       Mean reward: 186.65
               Mean episode length: 148.69
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 9.61s
                        Total time: 4034.38s
                               ETA: 1063272.5s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1755 steps/s (collection: 9.173s, learning 0.161s)
               Value function loss: 6.4517
                    Surrogate loss: 0.0069
             Mean action noise std: 0.79
                       Mean reward: 187.84
               Mean episode length: 148.49
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 9.33s
                        Total time: 4043.71s
                               ETA: 1062909.7s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1630 steps/s (collection: 9.881s, learning 0.170s)
               Value function loss: 7.9515
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 189.68
               Mean episode length: 148.49
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 10.05s
                        Total time: 4053.76s
                               ETA: 1062736.8s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1735 steps/s (collection: 9.267s, learning 0.171s)
               Value function loss: 14.5565
                    Surrogate loss: 0.0154
             Mean action noise std: 0.79
                       Mean reward: 171.34
               Mean episode length: 148.51
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 9.44s
                        Total time: 4063.20s
                               ETA: 1062404.7s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.738s, learning 0.175s)
               Value function loss: 12.4643
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 162.69
               Mean episode length: 148.79
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 9.91s
                        Total time: 4073.11s
                               ETA: 1062197.8s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.616s, learning 0.177s)
               Value function loss: 15.0272
                    Surrogate loss: -0.0022
             Mean action noise std: 0.79
                       Mean reward: 160.31
               Mean episode length: 146.75
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 9.79s
                        Total time: 4082.91s
                               ETA: 1061960.9s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1619 steps/s (collection: 9.932s, learning 0.185s)
               Value function loss: 15.3149
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 163.43
               Mean episode length: 147.35
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 10.12s
                        Total time: 4093.02s
                               ETA: 1061809.3s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.736s, learning 0.170s)
               Value function loss: 13.2222
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 180.99
               Mean episode length: 150.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 9.91s
                        Total time: 4102.93s
                               ETA: 1061604.0s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.388s, learning 0.183s)
               Value function loss: 11.9543
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 167.85
               Mean episode length: 149.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 9.57s
                        Total time: 4112.50s
                               ETA: 1061312.9s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.675s, learning 0.176s)
               Value function loss: 13.4604
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 183.50
               Mean episode length: 149.34
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 9.85s
                        Total time: 4122.35s
                               ETA: 1061095.7s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1642 steps/s (collection: 9.792s, learning 0.181s)
               Value function loss: 12.3815
                    Surrogate loss: 0.0192
             Mean action noise std: 0.79
                       Mean reward: 201.00
               Mean episode length: 149.06
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 9.97s
                        Total time: 4132.33s
                               ETA: 1060910.6s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.711s, learning 0.194s)
               Value function loss: 12.5641
                    Surrogate loss: 0.0121
             Mean action noise std: 0.79
                       Mean reward: 198.43
               Mean episode length: 149.58
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 9.91s
                        Total time: 4142.23s
                               ETA: 1060709.2s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.503s, learning 0.209s)
               Value function loss: 14.5507
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 185.98
               Mean episode length: 150.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 9.71s
                        Total time: 4151.94s
                               ETA: 1060459.4s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.710s, learning 0.164s)
               Value function loss: 19.1094
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 198.41
               Mean episode length: 148.04
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 9.87s
                        Total time: 4161.82s
                               ETA: 1060252.1s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1715 steps/s (collection: 9.392s, learning 0.160s)
               Value function loss: 18.3990
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 189.64
               Mean episode length: 150.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 9.55s
                        Total time: 4171.37s
                               ETA: 1059964.0s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.709s, learning 0.166s)
               Value function loss: 17.2422
                    Surrogate loss: 0.0010
             Mean action noise std: 0.79
                       Mean reward: 182.09
               Mean episode length: 149.16
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 9.88s
                        Total time: 4181.24s
                               ETA: 1059759.2s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.556s, learning 0.174s)
               Value function loss: 102.2853
                    Surrogate loss: -0.0004
             Mean action noise std: 0.79
                       Mean reward: 182.36
               Mean episode length: 149.13
                  Mean reward/step: 1.09
       Mean episode length/episode: 5.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 9.73s
                        Total time: 4190.97s
                               ETA: 1059518.7s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.694s, learning 0.171s)
               Value function loss: 17.6596
                    Surrogate loss: 0.0005
             Mean action noise std: 0.79
                       Mean reward: 182.60
               Mean episode length: 148.67
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 9.86s
                        Total time: 4200.84s
                               ETA: 1059313.3s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.671s, learning 0.163s)
               Value function loss: 12.0061
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 183.78
               Mean episode length: 148.67
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 9.83s
                        Total time: 4210.67s
                               ETA: 1059101.2s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.486s, learning 0.160s)
               Value function loss: 11.1431
                    Surrogate loss: -0.0141
             Mean action noise std: 0.79
                       Mean reward: 189.99
               Mean episode length: 148.67
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 9.65s
                        Total time: 4220.32s
                               ETA: 1058842.9s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1687 steps/s (collection: 9.545s, learning 0.166s)
               Value function loss: 11.8134
                    Surrogate loss: -0.0025
             Mean action noise std: 0.79
                       Mean reward: 190.27
               Mean episode length: 148.67
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 9.71s
                        Total time: 4230.03s
                               ETA: 1058602.1s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1664 steps/s (collection: 9.675s, learning 0.169s)
               Value function loss: 17.2603
                    Surrogate loss: 0.0298
             Mean action noise std: 0.79
                       Mean reward: 202.75
               Mean episode length: 147.83
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 9.84s
                        Total time: 4239.87s
                               ETA: 1058395.7s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1735 steps/s (collection: 9.276s, learning 0.162s)
               Value function loss: 23.2609
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 222.14
               Mean episode length: 148.25
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 9.44s
                        Total time: 4249.31s
                               ETA: 1058089.3s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.429s, learning 0.158s)
               Value function loss: 22.8967
                    Surrogate loss: 0.0791
             Mean action noise std: 0.79
                       Mean reward: 213.11
               Mean episode length: 146.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 9.59s
                        Total time: 4258.90s
                               ETA: 1057821.1s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1639 steps/s (collection: 9.827s, learning 0.168s)
               Value function loss: 23.3254
                    Surrogate loss: 0.0128
             Mean action noise std: 0.79
                       Mean reward: 208.12
               Mean episode length: 143.22
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 10.00s
                        Total time: 4268.89s
                               ETA: 1057655.5s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.550s, learning 0.165s)
               Value function loss: 23.1459
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 242.57
               Mean episode length: 148.81
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 9.72s
                        Total time: 4278.61s
                               ETA: 1057421.5s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1621 steps/s (collection: 9.949s, learning 0.156s)
               Value function loss: 20.0664
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 236.59
               Mean episode length: 150.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 10.11s
                        Total time: 4288.71s
                               ETA: 1057284.7s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.737s, learning 0.163s)
               Value function loss: 20.4886
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 248.74
               Mean episode length: 149.60
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 9.90s
                        Total time: 4298.61s
                               ETA: 1057098.2s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.568s, learning 0.160s)
               Value function loss: 19.9705
                    Surrogate loss: 0.0106
             Mean action noise std: 0.79
                       Mean reward: 242.53
               Mean episode length: 150.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 9.73s
                        Total time: 4308.34s
                               ETA: 1056870.3s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.552s, learning 0.168s)
               Value function loss: 19.7383
                    Surrogate loss: -0.0140
             Mean action noise std: 0.79
                       Mean reward: 262.18
               Mean episode length: 150.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 9.72s
                        Total time: 4318.06s
                               ETA: 1056641.4s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1613 steps/s (collection: 9.995s, learning 0.161s)
               Value function loss: 20.0395
                    Surrogate loss: 0.0086
             Mean action noise std: 0.79
                       Mean reward: 259.93
               Mean episode length: 150.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 10.16s
                        Total time: 4328.22s
                               ETA: 1056520.2s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1735 steps/s (collection: 9.277s, learning 0.161s)
               Value function loss: 23.8348
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 262.65
               Mean episode length: 150.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 9.44s
                        Total time: 4337.66s
                               ETA: 1056224.6s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1736 steps/s (collection: 9.263s, learning 0.169s)
               Value function loss: 25.8921
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 230.85
               Mean episode length: 150.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 9.43s
                        Total time: 4347.09s
                               ETA: 1055929.1s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.439s, learning 0.158s)
               Value function loss: 27.9475
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 227.12
               Mean episode length: 149.67
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 9.60s
                        Total time: 4356.69s
                               ETA: 1055674.8s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.697s, learning 0.194s)
               Value function loss: 29.6624
                    Surrogate loss: 0.0010
             Mean action noise std: 0.79
                       Mean reward: 215.12
               Mean episode length: 149.75
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 9.89s
                        Total time: 4366.58s
                               ETA: 1055492.7s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1661 steps/s (collection: 9.704s, learning 0.158s)
               Value function loss: 80.7309
                    Surrogate loss: 0.0148
             Mean action noise std: 0.79
                       Mean reward: 193.06
               Mean episode length: 150.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 5.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 9.86s
                        Total time: 4376.44s
                               ETA: 1055304.6s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.553s, learning 0.168s)
               Value function loss: 6.9971
                    Surrogate loss: -0.0243
             Mean action noise std: 0.79
                       Mean reward: 194.21
               Mean episode length: 150.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 9.72s
                        Total time: 4386.16s
                               ETA: 1055083.3s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.617s, learning 0.164s)
               Value function loss: 6.5236
                    Surrogate loss: -0.0200
             Mean action noise std: 0.79
                       Mean reward: 191.60
               Mean episode length: 149.14
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 9.78s
                        Total time: 4395.94s
                               ETA: 1054877.4s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.745s, learning 0.169s)
               Value function loss: 6.2825
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 196.47
               Mean episode length: 149.14
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 9.91s
                        Total time: 4405.85s
                               ETA: 1054704.4s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.625s, learning 0.168s)
               Value function loss: 6.2005
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 182.84
               Mean episode length: 149.14
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 9.79s
                        Total time: 4415.65s
                               ETA: 1054503.2s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1728 steps/s (collection: 9.312s, learning 0.166s)
               Value function loss: 6.4653
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 157.85
               Mean episode length: 149.14
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 9.48s
                        Total time: 4425.13s
                               ETA: 1054227.9s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.468s, learning 0.160s)
               Value function loss: 7.1318
                    Surrogate loss: -0.0008
             Mean action noise std: 0.79
                       Mean reward: 110.76
               Mean episode length: 150.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 9.63s
                        Total time: 4434.75s
                               ETA: 1053989.6s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.569s, learning 0.161s)
               Value function loss: 8.5938
                    Surrogate loss: 0.0122
             Mean action noise std: 0.79
                       Mean reward: 104.18
               Mean episode length: 148.97
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 9.73s
                        Total time: 4444.48s
                               ETA: 1053776.6s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.726s, learning 0.160s)
               Value function loss: 10.5100
                    Surrogate loss: 0.0090
             Mean action noise std: 0.79
                       Mean reward: 111.46
               Mean episode length: 148.28
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 9.89s
                        Total time: 4454.37s
                               ETA: 1053601.4s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.366s, learning 0.162s)
               Value function loss: 9.8194
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 115.65
               Mean episode length: 150.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 9.53s
                        Total time: 4463.90s
                               ETA: 1053342.5s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1645 steps/s (collection: 9.796s, learning 0.162s)
               Value function loss: 9.2848
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 126.83
               Mean episode length: 150.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 9.96s
                        Total time: 4473.86s
                               ETA: 1053185.9s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1759 steps/s (collection: 9.136s, learning 0.176s)
               Value function loss: 8.9628
                    Surrogate loss: -0.0261
             Mean action noise std: 0.79
                       Mean reward: 125.72
               Mean episode length: 150.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 9.31s
                        Total time: 4483.17s
                               ETA: 1052878.4s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1597 steps/s (collection: 10.086s, learning 0.170s)
               Value function loss: 8.7898
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 136.53
               Mean episode length: 149.45
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 10.26s
                        Total time: 4493.42s
                               ETA: 1052793.5s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1620 steps/s (collection: 9.952s, learning 0.160s)
               Value function loss: 9.0537
                    Surrogate loss: -0.0200
             Mean action noise std: 0.79
                       Mean reward: 138.16
               Mean episode length: 150.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 10.11s
                        Total time: 4503.54s
                               ETA: 1052675.4s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1737 steps/s (collection: 9.258s, learning 0.172s)
               Value function loss: 9.1889
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 141.45
               Mean episode length: 149.10
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 9.43s
                        Total time: 4512.97s
                               ETA: 1052398.5s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.734s, learning 0.164s)
               Value function loss: 9.9684
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 138.68
               Mean episode length: 149.10
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 9.90s
                        Total time: 4522.87s
                               ETA: 1052231.9s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1710 steps/s (collection: 9.403s, learning 0.176s)
               Value function loss: 9.3792
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 130.16
               Mean episode length: 150.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 9.58s
                        Total time: 4532.44s
                               ETA: 1051991.8s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.688s, learning 0.165s)
               Value function loss: 9.0944
                    Surrogate loss: -0.0167
             Mean action noise std: 0.79
                       Mean reward: 140.65
               Mean episode length: 150.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 9.85s
                        Total time: 4542.30s
                               ETA: 1051816.3s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.587s, learning 0.161s)
               Value function loss: 8.9683
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: 150.30
               Mean episode length: 150.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 9.75s
                        Total time: 4552.04s
                               ETA: 1051617.3s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.538s, learning 0.164s)
               Value function loss: 11.5718
                    Surrogate loss: -0.0003
             Mean action noise std: 0.79
                       Mean reward: 137.65
               Mean episode length: 150.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 5.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 9.70s
                        Total time: 4561.75s
                               ETA: 1051408.7s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1731 steps/s (collection: 9.284s, learning 0.180s)
               Value function loss: 7.3810
                    Surrogate loss: -0.0158
             Mean action noise std: 0.79
                       Mean reward: 138.86
               Mean episode length: 148.18
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 9.46s
                        Total time: 4571.21s
                               ETA: 1051146.1s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.580s, learning 0.186s)
               Value function loss: 8.1234
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 145.44
               Mean episode length: 148.18
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 9.77s
                        Total time: 4580.98s
                               ETA: 1050954.0s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.417s, learning 0.165s)
               Value function loss: 9.0693
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 149.81
               Mean episode length: 148.18
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 9.58s
                        Total time: 4590.56s
                               ETA: 1050720.7s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1754 steps/s (collection: 9.177s, learning 0.160s)
               Value function loss: 11.0308
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 148.78
               Mean episode length: 148.18
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 9.34s
                        Total time: 4599.89s
                               ETA: 1050432.4s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.467s, learning 0.177s)
               Value function loss: 12.9446
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: 169.50
               Mean episode length: 148.79
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 9.64s
                        Total time: 4609.54s
                               ETA: 1050215.3s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.541s, learning 0.173s)
               Value function loss: 14.7801
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 165.02
               Mean episode length: 149.05
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 9.71s
                        Total time: 4619.25s
                               ETA: 1050015.1s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.322s, learning 0.175s)
               Value function loss: 13.9047
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 167.95
               Mean episode length: 149.05
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 9.50s
                        Total time: 4628.75s
                               ETA: 1049766.5s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.557s, learning 0.181s)
               Value function loss: 15.2354
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 177.25
               Mean episode length: 150.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 9.74s
                        Total time: 4638.49s
                               ETA: 1049573.7s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1637 steps/s (collection: 9.835s, learning 0.173s)
               Value function loss: 16.0151
                    Surrogate loss: -0.0176
             Mean action noise std: 0.79
                       Mean reward: 176.64
               Mean episode length: 149.38
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 10.01s
                        Total time: 4648.49s
                               ETA: 1049442.4s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.363s, learning 0.165s)
               Value function loss: 15.9179
                    Surrogate loss: 0.0049
             Mean action noise std: 0.79
                       Mean reward: 172.24
               Mean episode length: 149.34
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 9.53s
                        Total time: 4658.02s
                               ETA: 1049203.6s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1731 steps/s (collection: 9.291s, learning 0.173s)
               Value function loss: 17.9934
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 182.38
               Mean episode length: 148.78
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 9.46s
                        Total time: 4667.49s
                               ETA: 1048951.6s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.685s, learning 0.183s)
               Value function loss: 14.5459
                    Surrogate loss: -0.0280
             Mean action noise std: 0.79
                       Mean reward: 185.93
               Mean episode length: 149.42
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 9.87s
                        Total time: 4677.35s
                               ETA: 1048791.1s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.592s, learning 0.159s)
               Value function loss: 19.0354
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 179.27
               Mean episode length: 149.88
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 9.75s
                        Total time: 4687.10s
                               ETA: 1048605.4s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1629 steps/s (collection: 9.891s, learning 0.166s)
               Value function loss: 16.2324
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 194.78
               Mean episode length: 149.88
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 10.06s
                        Total time: 4697.16s
                               ETA: 1048488.7s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1651 steps/s (collection: 9.753s, learning 0.166s)
               Value function loss: 23.2799
                    Surrogate loss: 0.0356
             Mean action noise std: 0.79
                       Mean reward: 189.42
               Mean episode length: 150.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 9.92s
                        Total time: 4707.08s
                               ETA: 1048341.6s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1695 steps/s (collection: 9.487s, learning 0.176s)
               Value function loss: 25.5704
                    Surrogate loss: 0.0078
             Mean action noise std: 0.79
                       Mean reward: 173.30
               Mean episode length: 150.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 9.66s
                        Total time: 4716.74s
                               ETA: 1048138.3s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.813s, learning 0.167s)
               Value function loss: 26.5320
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 167.27
               Mean episode length: 149.27
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 9.98s
                        Total time: 4726.72s
                               ETA: 1048006.2s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.567s, learning 0.168s)
               Value function loss: 77.5123
                    Surrogate loss: 0.0101
             Mean action noise std: 0.79
                       Mean reward: 173.10
               Mean episode length: 150.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 5.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 9.73s
                        Total time: 4736.46s
                               ETA: 1047820.4s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1636 steps/s (collection: 9.834s, learning 0.178s)
               Value function loss: 13.1666
                    Surrogate loss: -0.0296
             Mean action noise std: 0.79
                       Mean reward: 176.09
               Mean episode length: 149.15
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 10.01s
                        Total time: 4746.47s
                               ETA: 1047696.6s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.449s, learning 0.172s)
               Value function loss: 22.7099
                    Surrogate loss: 0.0285
             Mean action noise std: 0.79
                       Mean reward: 189.61
               Mean episode length: 149.15
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 9.62s
                        Total time: 4756.09s
                               ETA: 1047487.0s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.527s, learning 0.164s)
               Value function loss: 13.7834
                    Surrogate loss: -0.0070
             Mean action noise std: 0.79
                       Mean reward: 196.35
               Mean episode length: 149.15
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 9.69s
                        Total time: 4765.78s
                               ETA: 1047293.9s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.510s, learning 0.161s)
               Value function loss: 14.5083
                    Surrogate loss: -0.0185
             Mean action noise std: 0.79
                       Mean reward: 201.15
               Mean episode length: 149.15
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 9.67s
                        Total time: 4775.45s
                               ETA: 1047097.2s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1629 steps/s (collection: 9.890s, learning 0.166s)
               Value function loss: 15.5745
                    Surrogate loss: -0.0223
             Mean action noise std: 0.79
                       Mean reward: 210.49
               Mean episode length: 149.15
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 10.06s
                        Total time: 4785.51s
                               ETA: 1046985.5s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.705s, learning 0.162s)
               Value function loss: 23.4915
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 232.83
               Mean episode length: 149.29
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 9.87s
                        Total time: 4795.38s
                               ETA: 1046832.8s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.499s, learning 0.168s)
               Value function loss: 29.4290
                    Surrogate loss: -0.0017
             Mean action noise std: 0.79
                       Mean reward: 219.69
               Mean episode length: 147.11
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 9.67s
                        Total time: 4805.04s
                               ETA: 1046637.4s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.717s, learning 0.166s)
               Value function loss: 31.3129
                    Surrogate loss: 0.0019
             Mean action noise std: 0.79
                       Mean reward: 222.56
               Mean episode length: 145.29
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 9.88s
                        Total time: 4814.93s
                               ETA: 1046489.8s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.671s, learning 0.165s)
               Value function loss: 32.0844
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 233.70
               Mean episode length: 146.63
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 9.84s
                        Total time: 4824.76s
                               ETA: 1046332.5s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.564s, learning 0.165s)
               Value function loss: 35.7244
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 255.77
               Mean episode length: 147.14
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 9.73s
                        Total time: 4834.49s
                               ETA: 1046152.9s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.578s, learning 0.165s)
               Value function loss: 34.9641
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 248.13
               Mean episode length: 144.56
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 9.74s
                        Total time: 4844.24s
                               ETA: 1045976.8s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1676 steps/s (collection: 9.606s, learning 0.165s)
               Value function loss: 33.1024
                    Surrogate loss: -0.0181
             Mean action noise std: 0.79
                       Mean reward: 249.03
               Mean episode length: 148.98
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 9.77s
                        Total time: 4854.01s
                               ETA: 1045807.5s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.695s, learning 0.161s)
               Value function loss: 26.3623
                    Surrogate loss: 0.0100
             Mean action noise std: 0.79
                       Mean reward: 274.83
               Mean episode length: 149.10
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 9.86s
                        Total time: 4863.86s
                               ETA: 1045657.3s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.739s, learning 0.164s)
               Value function loss: 34.5084
                    Surrogate loss: -0.0100
             Mean action noise std: 0.79
                       Mean reward: 276.97
               Mean episode length: 149.48
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 9.90s
                        Total time: 4873.77s
                               ETA: 1045517.8s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.563s, learning 0.177s)
               Value function loss: 32.1125
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 277.22
               Mean episode length: 147.89
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 9.74s
                        Total time: 4883.51s
                               ETA: 1045343.8s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1635 steps/s (collection: 9.861s, learning 0.159s)
               Value function loss: 38.7449
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 283.46
               Mean episode length: 148.95
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 10.02s
                        Total time: 4893.53s
                               ETA: 1045230.4s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1632 steps/s (collection: 9.872s, learning 0.162s)
               Value function loss: 56.8424
                    Surrogate loss: 0.0284
             Mean action noise std: 0.78
                       Mean reward: 232.11
               Mean episode length: 147.69
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 10.03s
                        Total time: 4903.56s
                               ETA: 1045120.3s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.655s, learning 0.163s)
               Value function loss: 56.4888
                    Surrogate loss: 0.0103
             Mean action noise std: 0.78
                       Mean reward: 226.34
               Mean episode length: 146.33
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 9.82s
                        Total time: 4913.38s
                               ETA: 1044964.8s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.822s, learning 0.158s)
               Value function loss: 297.7074
                    Surrogate loss: 0.0050
             Mean action noise std: 0.78
                       Mean reward: 228.96
               Mean episode length: 149.56
                  Mean reward/step: 1.48
       Mean episode length/episode: 5.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 9.98s
                        Total time: 4923.36s
                               ETA: 1044844.3s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.570s, learning 0.161s)
               Value function loss: 25.9652
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 231.12
               Mean episode length: 148.84
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 9.73s
                        Total time: 4933.09s
                               ETA: 1044671.4s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1717 steps/s (collection: 9.365s, learning 0.173s)
               Value function loss: 11.3668
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 225.42
               Mean episode length: 148.84
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 9.54s
                        Total time: 4942.63s
                               ETA: 1044458.5s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1651 steps/s (collection: 9.756s, learning 0.163s)
               Value function loss: 12.2744
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 227.59
               Mean episode length: 148.19
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 9.92s
                        Total time: 4952.55s
                               ETA: 1044327.0s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.467s, learning 0.165s)
               Value function loss: 14.7098
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 230.10
               Mean episode length: 147.83
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 9.63s
                        Total time: 4962.18s
                               ETA: 1044135.3s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.301s, learning 0.158s)
               Value function loss: 24.4140
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 221.11
               Mean episode length: 145.93
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 9.46s
                        Total time: 4971.64s
                               ETA: 1043908.2s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1610 steps/s (collection: 10.015s, learning 0.160s)
               Value function loss: 42.6325
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 241.38
               Mean episode length: 146.72
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 10.18s
                        Total time: 4981.82s
                               ETA: 1043832.0s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1722 steps/s (collection: 9.340s, learning 0.170s)
               Value function loss: 30.6504
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 235.91
               Mean episode length: 148.54
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 9.51s
                        Total time: 4991.33s
                               ETA: 1043617.1s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.590s, learning 0.174s)
               Value function loss: 39.1842
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: 213.58
               Mean episode length: 143.70
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 9.76s
                        Total time: 5001.09s
                               ETA: 1043456.0s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1640 steps/s (collection: 9.814s, learning 0.172s)
               Value function loss: 66.5941
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 216.10
               Mean episode length: 143.86
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 9.99s
                        Total time: 5011.08s
                               ETA: 1043341.8s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.551s, learning 0.173s)
               Value function loss: 31.8899
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 237.05
               Mean episode length: 147.09
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 9.72s
                        Total time: 5020.80s
                               ETA: 1043173.6s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.703s, learning 0.167s)
               Value function loss: 114.8415
                    Surrogate loss: 0.0094
             Mean action noise std: 0.78
                       Mean reward: 203.88
               Mean episode length: 145.35
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 9.87s
                        Total time: 5030.67s
                               ETA: 1043036.3s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1614 steps/s (collection: 9.983s, learning 0.168s)
               Value function loss: 30.3299
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 210.45
               Mean episode length: 143.33
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 10.15s
                        Total time: 5040.82s
                               ETA: 1042957.5s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1739 steps/s (collection: 9.257s, learning 0.164s)
               Value function loss: 45.1390
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 209.23
               Mean episode length: 145.96
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 9.42s
                        Total time: 5050.24s
                               ETA: 1042728.3s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.816s, learning 0.181s)
               Value function loss: 25.9482
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 223.30
               Mean episode length: 146.89
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 10.00s
                        Total time: 5060.24s
                               ETA: 1042618.7s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.493s, learning 0.178s)
               Value function loss: 50.3463
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 201.28
               Mean episode length: 146.12
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 9.67s
                        Total time: 5069.91s
                               ETA: 1042442.5s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1615 steps/s (collection: 9.968s, learning 0.176s)
               Value function loss: 66.9261
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 217.29
               Mean episode length: 146.25
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 10.14s
                        Total time: 5080.05s
                               ETA: 1042364.2s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1642 steps/s (collection: 9.804s, learning 0.168s)
               Value function loss: 3974.5610
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: 155.85
               Mean episode length: 145.57
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 9.97s
                        Total time: 5090.03s
                               ETA: 1042250.9s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.363s, learning 0.164s)
               Value function loss: 34.8081
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 183.00
               Mean episode length: 146.29
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 9.53s
                        Total time: 5099.55s
                               ETA: 1042047.1s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.605s, learning 0.176s)
               Value function loss: 138.0291
                    Surrogate loss: 0.0064
             Mean action noise std: 0.78
                       Mean reward: 171.50
               Mean episode length: 147.47
                  Mean reward/step: 0.92
       Mean episode length/episode: 5.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 9.78s
                        Total time: 5109.33s
                               ETA: 1041895.9s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.536s, learning 0.159s)
               Value function loss: 15.4626
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 167.39
               Mean episode length: 145.24
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 9.69s
                        Total time: 5119.03s
                               ETA: 1041727.6s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.844s, learning 0.158s)
               Value function loss: 14.6578
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 170.61
               Mean episode length: 144.21
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 10.00s
                        Total time: 5129.03s
                               ETA: 1041622.5s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.600s, learning 0.160s)
               Value function loss: 867.4245
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 151.09
               Mean episode length: 141.93
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 9.76s
                        Total time: 5138.79s
                               ETA: 1041468.7s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.508s, learning 0.165s)
               Value function loss: 26.6008
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 159.33
               Mean episode length: 141.93
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 9.67s
                        Total time: 5148.46s
                               ETA: 1041297.7s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1728 steps/s (collection: 9.316s, learning 0.165s)
               Value function loss: 48.0054
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 171.96
               Mean episode length: 139.76
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 9.48s
                        Total time: 5157.94s
                               ETA: 1041088.8s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.690s, learning 0.165s)
               Value function loss: 52.2493
                    Surrogate loss: -0.0227
             Mean action noise std: 0.78
                       Mean reward: 205.07
               Mean episode length: 138.78
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 9.85s
                        Total time: 5167.80s
                               ETA: 1040955.9s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1625 steps/s (collection: 9.919s, learning 0.163s)
               Value function loss: 47.0442
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 201.31
               Mean episode length: 142.47
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 10.08s
                        Total time: 5177.88s
                               ETA: 1040869.3s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1645 steps/s (collection: 9.797s, learning 0.161s)
               Value function loss: 542.1050
                    Surrogate loss: -0.0024
             Mean action noise std: 0.78
                       Mean reward: 179.91
               Mean episode length: 136.69
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 9.96s
                        Total time: 5187.84s
                               ETA: 1040758.0s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1702 steps/s (collection: 9.459s, learning 0.167s)
               Value function loss: 69.0465
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 239.20
               Mean episode length: 137.92
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 9.63s
                        Total time: 5197.47s
                               ETA: 1040580.6s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.625s, learning 0.160s)
               Value function loss: 55.7346
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 232.06
               Mean episode length: 144.40
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 9.78s
                        Total time: 5207.25s
                               ETA: 1040435.7s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1637 steps/s (collection: 9.842s, learning 0.161s)
               Value function loss: 75.7924
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 227.16
               Mean episode length: 146.21
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 10.00s
                        Total time: 5217.25s
                               ETA: 1040334.8s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1614 steps/s (collection: 9.969s, learning 0.179s)
               Value function loss: 57.3615
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 218.48
               Mean episode length: 142.06
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 10.15s
                        Total time: 5227.40s
                               ETA: 1040263.3s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1672 steps/s (collection: 9.631s, learning 0.163s)
               Value function loss: 67.4488
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 218.41
               Mean episode length: 141.30
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 9.79s
                        Total time: 5237.20s
                               ETA: 1040121.7s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1755 steps/s (collection: 9.171s, learning 0.161s)
               Value function loss: 60.9606
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 215.47
               Mean episode length: 142.20
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 9.33s
                        Total time: 5246.53s
                               ETA: 1039889.0s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.705s, learning 0.163s)
               Value function loss: 83.1069
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 221.43
               Mean episode length: 140.35
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 9.87s
                        Total time: 5256.40s
                               ETA: 1039763.1s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1676 steps/s (collection: 9.604s, learning 0.166s)
               Value function loss: 544.8685
                    Surrogate loss: -0.0039
             Mean action noise std: 0.78
                       Mean reward: 192.62
               Mean episode length: 136.49
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 9.77s
                        Total time: 5266.17s
                               ETA: 1039618.4s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1706 steps/s (collection: 9.436s, learning 0.165s)
               Value function loss: 334.8441
                    Surrogate loss: -0.0035
             Mean action noise std: 0.78
                       Mean reward: 201.25
               Mean episode length: 134.04
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 9.60s
                        Total time: 5275.77s
                               ETA: 1039440.8s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.371s, learning 0.164s)
               Value function loss: 144.1112
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 187.51
               Mean episode length: 135.17
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 9.53s
                        Total time: 5285.30s
                               ETA: 1039251.0s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.578s, learning 0.167s)
               Value function loss: 182.4150
                    Surrogate loss: 0.0042
             Mean action noise std: 0.78
                       Mean reward: 202.79
               Mean episode length: 138.37
                  Mean reward/step: 0.89
       Mean episode length/episode: 5.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 9.74s
                        Total time: 5295.04s
                               ETA: 1039102.9s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.549s, learning 0.177s)
               Value function loss: 52.9238
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 175.55
               Mean episode length: 127.57
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 9.73s
                        Total time: 5304.77s
                               ETA: 1038951.9s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.707s, learning 0.181s)
               Value function loss: 72.5380
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 144.20
               Mean episode length: 115.19
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 9.89s
                        Total time: 5314.66s
                               ETA: 1038833.1s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1712 steps/s (collection: 9.396s, learning 0.171s)
               Value function loss: 58.7041
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 119.80
               Mean episode length: 109.36
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 9.57s
                        Total time: 5324.23s
                               ETA: 1038652.0s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.447s, learning 0.163s)
               Value function loss: 75.6981
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 112.76
               Mean episode length: 104.77
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 9.61s
                        Total time: 5333.84s
                               ETA: 1038480.2s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1749 steps/s (collection: 9.203s, learning 0.164s)
               Value function loss: 141.0441
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 130.88
               Mean episode length: 111.11
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 9.37s
                        Total time: 5343.20s
                               ETA: 1038261.5s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1670 steps/s (collection: 9.647s, learning 0.159s)
               Value function loss: 225.6913
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 132.97
               Mean episode length: 112.48
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 9.81s
                        Total time: 5353.01s
                               ETA: 1038128.9s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1716 steps/s (collection: 9.386s, learning 0.162s)
               Value function loss: 221.4959
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 165.24
               Mean episode length: 110.92
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 9.55s
                        Total time: 5362.56s
                               ETA: 1037946.7s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.581s, learning 0.170s)
               Value function loss: 299.2851
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 187.72
               Mean episode length: 125.71
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 9.75s
                        Total time: 5372.31s
                               ETA: 1037804.5s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.443s, learning 0.170s)
               Value function loss: 1661.5673
                    Surrogate loss: 0.0101
             Mean action noise std: 0.78
                       Mean reward: 154.55
               Mean episode length: 126.07
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 9.61s
                        Total time: 5381.92s
                               ETA: 1037636.3s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1631 steps/s (collection: 9.878s, learning 0.161s)
               Value function loss: 263.1545
                    Surrogate loss: 0.0017
             Mean action noise std: 0.78
                       Mean reward: 160.88
               Mean episode length: 123.63
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 10.04s
                        Total time: 5391.96s
                               ETA: 1037550.7s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.414s, learning 0.168s)
               Value function loss: 329.6053
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 168.07
               Mean episode length: 129.59
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 9.58s
                        Total time: 5401.54s
                               ETA: 1037377.6s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.430s, learning 0.166s)
               Value function loss: 1225.4202
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 148.32
               Mean episode length: 124.22
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 9.60s
                        Total time: 5411.14s
                               ETA: 1037207.6s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.444s, learning 0.168s)
               Value function loss: 513.3323
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 133.56
               Mean episode length: 127.95
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 9.61s
                        Total time: 5420.75s
                               ETA: 1037041.5s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.499s, learning 0.170s)
               Value function loss: 83.2699
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 163.10
               Mean episode length: 123.66
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 9.67s
                        Total time: 5430.42s
                               ETA: 1036886.9s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.540s, learning 0.160s)
               Value function loss: 383.7636
                    Surrogate loss: 0.0021
             Mean action noise std: 0.78
                       Mean reward: 166.62
               Mean episode length: 124.69
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 9.70s
                        Total time: 5440.12s
                               ETA: 1036738.8s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.707s, learning 0.164s)
               Value function loss: 156.4015
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 144.57
               Mean episode length: 124.95
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 9.87s
                        Total time: 5449.99s
                               ETA: 1036623.6s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.818s, learning 0.162s)
               Value function loss: 246.9044
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 115.92
               Mean episode length: 121.95
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 9.98s
                        Total time: 5459.97s
                               ETA: 1036529.6s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.345s, learning 0.158s)
               Value function loss: 89294.7613
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: 159.66
               Mean episode length: 144.79
                  Mean reward/step: -0.34
       Mean episode length/episode: 6.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 9.50s
                        Total time: 5469.47s
                               ETA: 1036345.3s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1736 steps/s (collection: 9.271s, learning 0.162s)
               Value function loss: 189.8424
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 90.30
               Mean episode length: 110.21
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 9.43s
                        Total time: 5478.91s
                               ETA: 1036148.7s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.577s, learning 0.163s)
               Value function loss: 148.8652
                    Surrogate loss: -0.0032
             Mean action noise std: 0.78
                       Mean reward: 62.70
               Mean episode length: 106.88
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 9.74s
                        Total time: 5488.65s
                               ETA: 1036010.5s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.684s, learning 0.164s)
               Value function loss: 41.0235
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 77.58
               Mean episode length: 106.48
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 9.85s
                        Total time: 5498.49s
                               ETA: 1035893.3s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1716 steps/s (collection: 9.382s, learning 0.165s)
               Value function loss: 63.4111
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 113.17
               Mean episode length: 112.35
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 9.55s
                        Total time: 5508.04s
                               ETA: 1035719.8s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.543s, learning 0.157s)
               Value function loss: 69.1769
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 106.18
               Mean episode length: 111.62
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 9.70s
                        Total time: 5517.74s
                               ETA: 1035575.8s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1745 steps/s (collection: 9.221s, learning 0.166s)
               Value function loss: 452.2908
                    Surrogate loss: -0.0020
             Mean action noise std: 0.78
                       Mean reward: 142.16
               Mean episode length: 124.54
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 9.39s
                        Total time: 5527.13s
                               ETA: 1035373.6s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1639 steps/s (collection: 9.831s, learning 0.161s)
               Value function loss: 70.7835
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 132.31
               Mean episode length: 126.38
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 9.99s
                        Total time: 5537.12s
                               ETA: 1035285.3s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.631s, learning 0.172s)
               Value function loss: 344.4075
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 114.78
               Mean episode length: 129.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 9.80s
                        Total time: 5546.92s
                               ETA: 1035161.9s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1742 steps/s (collection: 9.246s, learning 0.158s)
               Value function loss: 81.3015
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 162.92
               Mean episode length: 130.27
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 9.40s
                        Total time: 5556.33s
                               ETA: 1034964.8s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.802s, learning 0.162s)
               Value function loss: 65.5569
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 181.82
               Mean episode length: 133.92
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 9.96s
                        Total time: 5566.29s
                               ETA: 1034872.4s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.573s, learning 0.174s)
               Value function loss: 173.6272
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 159.98
               Mean episode length: 129.69
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 9.75s
                        Total time: 5576.04s
                               ETA: 1034739.9s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.422s, learning 0.166s)
               Value function loss: 116.5086
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 139.89
               Mean episode length: 127.47
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 9.59s
                        Total time: 5585.63s
                               ETA: 1034578.5s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.298s, learning 0.159s)
               Value function loss: 79.3818
                    Surrogate loss: -0.0215
             Mean action noise std: 0.78
                       Mean reward: 172.30
               Mean episode length: 136.81
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 9.46s
                        Total time: 5595.08s
                               ETA: 1034393.5s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1639 steps/s (collection: 9.824s, learning 0.171s)
               Value function loss: 591.5890
                    Surrogate loss: -0.0038
             Mean action noise std: 0.78
                       Mean reward: 114.75
               Mean episode length: 130.73
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 9.99s
                        Total time: 5605.08s
                               ETA: 1034308.4s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1747 steps/s (collection: 9.198s, learning 0.175s)
               Value function loss: 2604.2862
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 133.01
               Mean episode length: 134.97
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 9.37s
                        Total time: 5614.45s
                               ETA: 1034109.1s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.470s, learning 0.160s)
               Value function loss: 158.6851
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 189.70
               Mean episode length: 137.69
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 9.63s
                        Total time: 5624.08s
                               ETA: 1033957.6s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1772 steps/s (collection: 9.079s, learning 0.162s)
               Value function loss: 130.6075
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 170.52
               Mean episode length: 139.19
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 9.24s
                        Total time: 5633.32s
                               ETA: 1033735.4s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1650 steps/s (collection: 9.746s, learning 0.179s)
               Value function loss: 1665.4277
                    Surrogate loss: 0.0026
             Mean action noise std: 0.78
                       Mean reward: 149.28
               Mean episode length: 134.46
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 9.92s
                        Total time: 5643.25s
                               ETA: 1033639.0s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.690s, learning 0.184s)
               Value function loss: 161.3513
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 217.56
               Mean episode length: 145.53
                  Mean reward/step: 1.15
       Mean episode length/episode: 6.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 9.87s
                        Total time: 5653.12s
                               ETA: 1033533.8s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1647 steps/s (collection: 9.775s, learning 0.168s)
               Value function loss: 88.2143
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 163.80
               Mean episode length: 131.36
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 9.94s
                        Total time: 5663.06s
                               ETA: 1033441.6s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1620 steps/s (collection: 9.932s, learning 0.177s)
               Value function loss: 71.9437
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 144.36
               Mean episode length: 121.39
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 10.11s
                        Total time: 5673.17s
                               ETA: 1033379.8s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.527s, learning 0.159s)
               Value function loss: 66.5395
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 150.80
               Mean episode length: 124.85
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 9.69s
                        Total time: 5682.86s
                               ETA: 1033241.5s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.581s, learning 0.160s)
               Value function loss: 1762.2737
                    Surrogate loss: 0.0028
             Mean action noise std: 0.78
                       Mean reward: 118.10
               Mean episode length: 128.07
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 9.74s
                        Total time: 5692.60s
                               ETA: 1033113.4s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.585s, learning 0.161s)
               Value function loss: 1486.8832
                    Surrogate loss: -0.0020
             Mean action noise std: 0.78
                       Mean reward: 98.10
               Mean episode length: 129.69
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 9.75s
                        Total time: 5702.35s
                               ETA: 1032986.8s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.728s, learning 0.170s)
               Value function loss: 193.5957
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 150.50
               Mean episode length: 128.77
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 9.90s
                        Total time: 5712.24s
                               ETA: 1032888.1s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.299s, learning 0.169s)
               Value function loss: 96.8999
                    Surrogate loss: -0.0251
             Mean action noise std: 0.78
                       Mean reward: 156.37
               Mean episode length: 129.93
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 9.47s
                        Total time: 5721.71s
                               ETA: 1032712.0s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1603 steps/s (collection: 10.054s, learning 0.163s)
               Value function loss: 2266.2375
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 99.29
               Mean episode length: 130.06
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 10.22s
                        Total time: 5731.93s
                               ETA: 1032671.5s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.402s, learning 0.169s)
               Value function loss: 913.5302
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 191.91
               Mean episode length: 137.99
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 9.57s
                        Total time: 5741.50s
                               ETA: 1032514.8s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1716 steps/s (collection: 9.387s, learning 0.160s)
               Value function loss: 194.1566
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 173.39
               Mean episode length: 137.17
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 9.55s
                        Total time: 5751.05s
                               ETA: 1032354.5s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.418s, learning 0.172s)
               Value function loss: 4490.5604
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: 216.34
               Mean episode length: 140.46
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 9.59s
                        Total time: 5760.64s
                               ETA: 1032202.3s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.547s, learning 0.176s)
               Value function loss: 78.2050
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 168.59
               Mean episode length: 141.54
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 9.72s
                        Total time: 5770.36s
                               ETA: 1032074.5s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1635 steps/s (collection: 9.851s, learning 0.166s)
               Value function loss: 304.1882
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 190.13
               Mean episode length: 141.12
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 10.02s
                        Total time: 5780.38s
                               ETA: 1031999.5s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.605s, learning 0.173s)
               Value function loss: 224.0165
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 146.53
               Mean episode length: 140.17
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 9.78s
                        Total time: 5790.15s
                               ETA: 1031882.3s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.516s, learning 0.174s)
               Value function loss: 73.5673
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 208.92
               Mean episode length: 143.72
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 9.69s
                        Total time: 5799.84s
                               ETA: 1031749.8s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.731s, learning 0.159s)
               Value function loss: 77.2557
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 212.49
               Mean episode length: 142.09
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 9.89s
                        Total time: 5809.73s
                               ETA: 1031653.1s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1208 steps/s (collection: 13.376s, learning 0.178s)
               Value function loss: 232.4871
                    Surrogate loss: 0.0079
             Mean action noise std: 0.78
                       Mean reward: 154.73
               Mean episode length: 138.44
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 13.55s
                        Total time: 5823.29s
                               ETA: 1032206.3s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 872 steps/s (collection: 18.603s, learning 0.172s)
               Value function loss: 81.4990
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 196.59
               Mean episode length: 140.08
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 18.77s
                        Total time: 5842.06s
                               ETA: 1033681.2s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 878 steps/s (collection: 18.470s, learning 0.171s)
               Value function loss: 140.0122
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: 205.07
               Mean episode length: 141.90
                  Mean reward/step: 1.03
       Mean episode length/episode: 6.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 18.64s
                        Total time: 5860.70s
                               ETA: 1035127.4s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 919 steps/s (collection: 17.639s, learning 0.179s)
               Value function loss: 53.8599
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 197.38
               Mean episode length: 134.36
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 17.82s
                        Total time: 5878.52s
                               ETA: 1036423.0s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 878 steps/s (collection: 18.470s, learning 0.178s)
               Value function loss: 790.5833
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 158.62
               Mean episode length: 133.73
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 18.65s
                        Total time: 5897.17s
                               ETA: 1037860.0s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 877 steps/s (collection: 18.501s, learning 0.175s)
               Value function loss: 30.8034
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 176.69
               Mean episode length: 140.41
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 18.68s
                        Total time: 5915.85s
                               ETA: 1039297.0s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 867 steps/s (collection: 18.708s, learning 0.176s)
               Value function loss: 35.5403
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 171.18
               Mean episode length: 145.13
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 18.88s
                        Total time: 5934.73s
                               ETA: 1040765.2s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 901 steps/s (collection: 18.011s, learning 0.168s)
               Value function loss: 32.8248
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 148.24
               Mean episode length: 140.96
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 18.18s
                        Total time: 5952.91s
                               ETA: 1042104.8s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 865 steps/s (collection: 18.737s, learning 0.186s)
               Value function loss: 36.1085
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 183.22
               Mean episode length: 144.76
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 18.92s
                        Total time: 5971.83s
                               ETA: 1043569.6s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 907 steps/s (collection: 17.881s, learning 0.177s)
               Value function loss: 45.1399
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 180.59
               Mean episode length: 148.22
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 18.06s
                        Total time: 5989.89s
                               ETA: 1044878.4s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 878 steps/s (collection: 18.467s, learning 0.175s)
               Value function loss: 35.8679
                    Surrogate loss: 0.0108
             Mean action noise std: 0.78
                       Mean reward: 154.40
               Mean episode length: 147.69
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 18.64s
                        Total time: 6008.53s
                               ETA: 1046284.1s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 865 steps/s (collection: 18.767s, learning 0.169s)
               Value function loss: 33.4991
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 161.30
               Mean episode length: 149.07
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 18.94s
                        Total time: 6027.47s
                               ETA: 1047735.9s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 894 steps/s (collection: 18.138s, learning 0.172s)
               Value function loss: 46.1696
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 165.15
               Mean episode length: 149.90
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 18.31s
                        Total time: 6045.78s
                               ETA: 1049074.1s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 869 steps/s (collection: 18.672s, learning 0.174s)
               Value function loss: 53.2194
                    Surrogate loss: -0.0038
             Mean action noise std: 0.78
                       Mean reward: 164.72
               Mean episode length: 148.54
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 18.85s
                        Total time: 6064.62s
                               ETA: 1050500.3s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 851 steps/s (collection: 19.066s, learning 0.177s)
               Value function loss: 50.1345
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 169.50
               Mean episode length: 149.78
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 19.24s
                        Total time: 6083.87s
                               ETA: 1051990.2s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 856 steps/s (collection: 18.957s, learning 0.176s)
               Value function loss: 109.9298
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 175.07
               Mean episode length: 148.54
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 19.13s
                        Total time: 6103.00s
                               ETA: 1053455.8s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 852 steps/s (collection: 19.038s, learning 0.187s)
               Value function loss: 52.9926
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 179.33
               Mean episode length: 148.64
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 19.22s
                        Total time: 6122.22s
                               ETA: 1054932.2s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 886 steps/s (collection: 18.302s, learning 0.180s)
               Value function loss: 71.8758
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 180.71
               Mean episode length: 150.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 18.48s
                        Total time: 6140.71s
                               ETA: 1056275.7s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 886 steps/s (collection: 18.328s, learning 0.161s)
               Value function loss: 73.5869
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 182.90
               Mean episode length: 149.74
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 18.49s
                        Total time: 6159.20s
                               ETA: 1057615.7s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 900 steps/s (collection: 18.037s, learning 0.163s)
               Value function loss: 75.0388
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 183.17
               Mean episode length: 150.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 18.20s
                        Total time: 6177.39s
                               ETA: 1058901.3s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 892 steps/s (collection: 18.198s, learning 0.162s)
               Value function loss: 64.2140
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 174.06
               Mean episode length: 149.89
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 18.36s
                        Total time: 6195.75s
                               ETA: 1060209.9s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 849 steps/s (collection: 19.119s, learning 0.163s)
               Value function loss: 333.2579
                    Surrogate loss: 0.0223
             Mean action noise std: 0.78
                       Mean reward: 187.53
               Mean episode length: 149.30
                  Mean reward/step: 0.87
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 19.28s
                        Total time: 6215.04s
                               ETA: 1061671.4s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 867 steps/s (collection: 18.696s, learning 0.187s)
               Value function loss: 53.9967
                    Surrogate loss: -0.0012
             Mean action noise std: 0.78
                       Mean reward: 176.71
               Mean episode length: 149.30
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 18.88s
                        Total time: 6233.92s
                               ETA: 1063059.7s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 874 steps/s (collection: 18.577s, learning 0.161s)
               Value function loss: 81.4915
                    Surrogate loss: 0.0118
             Mean action noise std: 0.78
                       Mean reward: 180.26
               Mean episode length: 150.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 18.74s
                        Total time: 6252.66s
                               ETA: 1064418.4s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 881 steps/s (collection: 18.424s, learning 0.159s)
               Value function loss: 61.9887
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 186.45
               Mean episode length: 149.12
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 18.58s
                        Total time: 6271.24s
                               ETA: 1065746.3s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 878 steps/s (collection: 18.485s, learning 0.171s)
               Value function loss: 66.0424
                    Surrogate loss: 0.0261
             Mean action noise std: 0.78
                       Mean reward: 175.30
               Mean episode length: 148.98
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 18.66s
                        Total time: 6289.90s
                               ETA: 1067081.9s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 892 steps/s (collection: 18.198s, learning 0.161s)
               Value function loss: 58.8529
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 173.34
               Mean episode length: 148.78
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 18.36s
                        Total time: 6308.25s
                               ETA: 1068362.5s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 874 steps/s (collection: 18.577s, learning 0.161s)
               Value function loss: 50.8775
                    Surrogate loss: 0.0037
             Mean action noise std: 0.78
                       Mean reward: 166.61
               Mean episode length: 148.64
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 18.74s
                        Total time: 6326.99s
                               ETA: 1069702.8s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 860 steps/s (collection: 18.881s, learning 0.163s)
               Value function loss: 43.3920
                    Surrogate loss: -0.0042
             Mean action noise std: 0.78
                       Mean reward: 155.48
               Mean episode length: 149.42
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 19.04s
                        Total time: 6346.04s
                               ETA: 1071090.2s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 869 steps/s (collection: 18.680s, learning 0.165s)
               Value function loss: 37.5278
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 152.00
               Mean episode length: 147.86
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 18.84s
                        Total time: 6364.88s
                               ETA: 1072439.2s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 894 steps/s (collection: 18.147s, learning 0.165s)
               Value function loss: 29.5570
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 144.04
               Mean episode length: 147.83
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 18.31s
                        Total time: 6383.19s
                               ETA: 1073694.0s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 866 steps/s (collection: 18.738s, learning 0.160s)
               Value function loss: 26.6729
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 142.71
               Mean episode length: 147.37
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 18.90s
                        Total time: 6402.09s
                               ETA: 1075042.9s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 871 steps/s (collection: 18.635s, learning 0.161s)
               Value function loss: 24.4405
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 136.21
               Mean episode length: 148.65
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 18.80s
                        Total time: 6420.89s
                               ETA: 1076370.0s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 863 steps/s (collection: 18.795s, learning 0.176s)
               Value function loss: 20.0748
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 140.46
               Mean episode length: 148.34
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 18.97s
                        Total time: 6439.86s
                               ETA: 1077722.1s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 863 steps/s (collection: 18.809s, learning 0.168s)
               Value function loss: 16.0461
                    Surrogate loss: 0.0036
             Mean action noise std: 0.78
                       Mean reward: 133.15
               Mean episode length: 147.81
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 18.98s
                        Total time: 6458.83s
                               ETA: 1079070.5s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 889 steps/s (collection: 18.259s, learning 0.162s)
               Value function loss: 14.0731
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 126.53
               Mean episode length: 147.93
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 18.42s
                        Total time: 6477.26s
                               ETA: 1080321.6s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 866 steps/s (collection: 18.747s, learning 0.165s)
               Value function loss: 16.4671
                    Surrogate loss: 0.0061
             Mean action noise std: 0.78
                       Mean reward: 122.90
               Mean episode length: 146.81
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 18.91s
                        Total time: 6496.17s
                               ETA: 1081650.2s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 864 steps/s (collection: 18.801s, learning 0.160s)
               Value function loss: 14.5318
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 122.11
               Mean episode length: 149.93
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 18.96s
                        Total time: 6515.13s
                               ETA: 1082982.4s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.164s, learning 0.166s)
               Value function loss: 17.4934
                    Surrogate loss: -0.0012
             Mean action noise std: 0.78
                       Mean reward: 124.90
               Mean episode length: 148.32
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 12.33s
                        Total time: 6527.46s
                               ETA: 1083209.6s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1626 steps/s (collection: 9.911s, learning 0.162s)
               Value function loss: 33.3762
                    Surrogate loss: 0.0137
             Mean action noise std: 0.78
                       Mean reward: 139.46
               Mean episode length: 149.06
                  Mean reward/step: 0.83
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 10.07s
                        Total time: 6537.53s
                               ETA: 1083062.2s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.486s, learning 0.162s)
               Value function loss: 12.2468
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 139.36
               Mean episode length: 145.35
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 9.65s
                        Total time: 6547.18s
                               ETA: 1082844.9s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.394s, learning 0.169s)
               Value function loss: 12.4098
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 132.07
               Mean episode length: 146.64
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 9.56s
                        Total time: 6556.74s
                               ETA: 1082614.3s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1615 steps/s (collection: 9.979s, learning 0.161s)
               Value function loss: 11.5315
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 134.16
               Mean episode length: 148.05
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 10.14s
                        Total time: 6566.88s
                               ETA: 1082479.4s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.405s, learning 0.167s)
               Value function loss: 12.1285
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 127.66
               Mean episode length: 146.77
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 9.57s
                        Total time: 6576.46s
                               ETA: 1082251.5s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1640 steps/s (collection: 9.820s, learning 0.166s)
               Value function loss: 12.3437
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 133.04
               Mean episode length: 147.35
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 9.99s
                        Total time: 6586.44s
                               ETA: 1082092.3s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1646 steps/s (collection: 9.780s, learning 0.168s)
               Value function loss: 11.5475
                    Surrogate loss: 0.0025
             Mean action noise std: 0.78
                       Mean reward: 126.82
               Mean episode length: 146.94
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 9.95s
                        Total time: 6596.39s
                               ETA: 1081927.5s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1744 steps/s (collection: 9.229s, learning 0.163s)
               Value function loss: 11.6097
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 123.98
               Mean episode length: 145.90
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 9.39s
                        Total time: 6605.78s
                               ETA: 1081672.1s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.450s, learning 0.171s)
               Value function loss: 11.2136
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 122.94
               Mean episode length: 145.48
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 9.62s
                        Total time: 6615.40s
                               ETA: 1081454.9s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1589 steps/s (collection: 10.143s, learning 0.165s)
               Value function loss: 10.5822
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 111.93
               Mean episode length: 145.88
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 10.31s
                        Total time: 6625.71s
                               ETA: 1081350.5s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.691s, learning 0.177s)
               Value function loss: 10.1742
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 109.37
               Mean episode length: 143.44
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 9.87s
                        Total time: 6635.58s
                               ETA: 1081174.8s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.571s, learning 0.160s)
               Value function loss: 10.2875
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 118.19
               Mean episode length: 146.22
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 9.73s
                        Total time: 6645.31s
                               ETA: 1080977.3s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1615 steps/s (collection: 9.975s, learning 0.164s)
               Value function loss: 9.7241
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 118.03
               Mean episode length: 148.97
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 10.14s
                        Total time: 6655.45s
                               ETA: 1080846.8s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1729 steps/s (collection: 9.311s, learning 0.163s)
               Value function loss: 8.8363
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 106.10
               Mean episode length: 145.20
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 9.47s
                        Total time: 6664.92s
                               ETA: 1080608.8s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.531s, learning 0.162s)
               Value function loss: 7.7870
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 108.66
               Mean episode length: 145.80
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 9.69s
                        Total time: 6674.61s
                               ETA: 1080406.9s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.296s, learning 0.173s)
               Value function loss: 8.0154
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 104.41
               Mean episode length: 141.82
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 9.47s
                        Total time: 6684.08s
                               ETA: 1080169.5s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.793s, learning 0.170s)
               Value function loss: 8.0053
                    Surrogate loss: 0.0033
             Mean action noise std: 0.78
                       Mean reward: 101.21
               Mean episode length: 143.08
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 9.96s
                        Total time: 6694.05s
                               ETA: 1080012.6s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1606 steps/s (collection: 10.031s, learning 0.165s)
               Value function loss: 8.0912
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 104.46
               Mean episode length: 144.15
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 10.20s
                        Total time: 6704.24s
                               ETA: 1079893.7s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.417s, learning 0.174s)
               Value function loss: 7.8223
                    Surrogate loss: 0.0028
             Mean action noise std: 0.78
                       Mean reward: 99.47
               Mean episode length: 143.98
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 9.59s
                        Total time: 6713.83s
                               ETA: 1079677.8s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.464s, learning 0.164s)
               Value function loss: 9.5390
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 93.78
               Mean episode length: 144.63
                  Mean reward/step: 0.58
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 9.63s
                        Total time: 6723.46s
                               ETA: 1079468.5s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1637 steps/s (collection: 9.842s, learning 0.162s)
               Value function loss: 6.0849
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 93.50
               Mean episode length: 137.01
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 10.00s
                        Total time: 6733.47s
                               ETA: 1079320.1s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.474s, learning 0.174s)
               Value function loss: 6.5054
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 92.60
               Mean episode length: 136.99
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 9.65s
                        Total time: 6743.11s
                               ETA: 1079115.2s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1747 steps/s (collection: 9.213s, learning 0.164s)
               Value function loss: 6.3577
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 91.58
               Mean episode length: 136.85
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 9.38s
                        Total time: 6752.49s
                               ETA: 1078867.6s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1706 steps/s (collection: 9.439s, learning 0.163s)
               Value function loss: 6.6262
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 94.84
               Mean episode length: 136.98
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 9.60s
                        Total time: 6762.09s
                               ETA: 1078656.6s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.771s, learning 0.166s)
               Value function loss: 6.4731
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 91.60
               Mean episode length: 136.35
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 9.94s
                        Total time: 6772.03s
                               ETA: 1078499.6s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.565s, learning 0.166s)
               Value function loss: 6.4271
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 95.48
               Mean episode length: 137.75
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 9.73s
                        Total time: 6781.76s
                               ETA: 1078310.4s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.544s, learning 0.160s)
               Value function loss: 6.8238
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 92.97
               Mean episode length: 135.53
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 9.70s
                        Total time: 6791.46s
                               ETA: 1078117.4s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.693s, learning 0.158s)
               Value function loss: 6.4341
                    Surrogate loss: -0.0215
             Mean action noise std: 0.78
                       Mean reward: 90.56
               Mean episode length: 135.41
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 9.85s
                        Total time: 6801.31s
                               ETA: 1077948.4s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.387s, learning 0.168s)
               Value function loss: 6.1844
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 92.02
               Mean episode length: 135.74
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 9.55s
                        Total time: 6810.87s
                               ETA: 1077733.0s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1617 steps/s (collection: 9.972s, learning 0.160s)
               Value function loss: 6.3066
                    Surrogate loss: 0.0038
             Mean action noise std: 0.78
                       Mean reward: 92.04
               Mean episode length: 135.46
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 10.13s
                        Total time: 6821.00s
                               ETA: 1077609.5s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.513s, learning 0.164s)
               Value function loss: 5.5386
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 87.18
               Mean episode length: 138.15
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 9.68s
                        Total time: 6830.68s
                               ETA: 1077414.6s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.721s, learning 0.165s)
               Value function loss: 5.8013
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 97.76
               Mean episode length: 141.62
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 9.89s
                        Total time: 6840.56s
                               ETA: 1077253.1s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.432s, learning 0.161s)
               Value function loss: 5.8349
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 91.79
               Mean episode length: 136.72
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 9.59s
                        Total time: 6850.16s
                               ETA: 1077046.2s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1649 steps/s (collection: 9.768s, learning 0.164s)
               Value function loss: 5.2417
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 82.35
               Mean episode length: 134.69
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 9.93s
                        Total time: 6860.09s
                               ETA: 1076893.0s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.500s, learning 0.167s)
               Value function loss: 6.2434
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 85.55
               Mean episode length: 132.37
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 9.67s
                        Total time: 6869.76s
                               ETA: 1076698.7s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1643 steps/s (collection: 9.807s, learning 0.161s)
               Value function loss: 5.7656
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 81.93
               Mean episode length: 135.74
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 9.97s
                        Total time: 6879.72s
                               ETA: 1076552.0s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.572s, learning 0.161s)
               Value function loss: 5.6026
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 84.72
               Mean episode length: 138.60
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 9.73s
                        Total time: 6889.46s
                               ETA: 1076369.2s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.569s, learning 0.162s)
               Value function loss: 5.7240
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 83.89
               Mean episode length: 137.70
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 9.73s
                        Total time: 6899.19s
                               ETA: 1076186.4s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.778s, learning 0.159s)
               Value function loss: 5.8385
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 92.94
               Mean episode length: 138.71
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 9.94s
                        Total time: 6909.12s
                               ETA: 1076036.4s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.661s, learning 0.163s)
               Value function loss: 4.9940
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 85.02
               Mean episode length: 131.66
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 9.82s
                        Total time: 6918.95s
                               ETA: 1075869.3s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.592s, learning 0.173s)
               Value function loss: 5.4417
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 89.08
               Mean episode length: 134.85
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 9.77s
                        Total time: 6928.71s
                               ETA: 1075693.6s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.638s, learning 0.161s)
               Value function loss: 5.7180
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 86.55
               Mean episode length: 131.54
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 9.80s
                        Total time: 6938.51s
                               ETA: 1075523.6s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.814s, learning 0.169s)
               Value function loss: 6.6381
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 89.78
               Mean episode length: 133.64
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 9.98s
                        Total time: 6948.50s
                               ETA: 1075382.5s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1697 steps/s (collection: 9.480s, learning 0.173s)
               Value function loss: 6.5120
                    Surrogate loss: -0.0213
             Mean action noise std: 0.78
                       Mean reward: 89.65
               Mean episode length: 135.04
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 9.65s
                        Total time: 6958.15s
                               ETA: 1075190.7s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.370s, learning 0.165s)
               Value function loss: 7.2078
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 86.52
               Mean episode length: 134.16
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 9.53s
                        Total time: 6967.68s
                               ETA: 1074981.4s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1759 steps/s (collection: 9.154s, learning 0.157s)
               Value function loss: 6.9091
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 83.07
               Mean episode length: 129.06
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 9.31s
                        Total time: 6976.99s
                               ETA: 1074738.3s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.706s, learning 0.171s)
               Value function loss: 6.9213
                    Surrogate loss: 0.0017
             Mean action noise std: 0.78
                       Mean reward: 84.92
               Mean episode length: 132.05
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 9.88s
                        Total time: 6986.87s
                               ETA: 1074583.0s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.831s, learning 0.169s)
               Value function loss: 6.6956
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 83.45
               Mean episode length: 133.23
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 10.00s
                        Total time: 6996.87s
                               ETA: 1074446.8s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.538s, learning 0.186s)
               Value function loss: 6.0730
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 86.58
               Mean episode length: 133.84
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 9.72s
                        Total time: 7006.59s
                               ETA: 1074268.8s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.703s, learning 0.180s)
               Value function loss: 6.1721
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 83.73
               Mean episode length: 135.12
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 9.88s
                        Total time: 7016.48s
                               ETA: 1074115.7s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.600s, learning 0.160s)
               Value function loss: 6.4162
                    Surrogate loss: -0.0023
             Mean action noise std: 0.78
                       Mean reward: 86.32
               Mean episode length: 133.43
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 9.76s
                        Total time: 7026.24s
                               ETA: 1073944.1s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.689s, learning 0.163s)
               Value function loss: 5.8694
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 84.61
               Mean episode length: 135.90
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 9.85s
                        Total time: 7036.09s
                               ETA: 1073787.3s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.361s, learning 0.161s)
               Value function loss: 5.4505
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 90.05
               Mean episode length: 133.22
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 9.52s
                        Total time: 7045.61s
                               ETA: 1073580.5s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1622 steps/s (collection: 9.937s, learning 0.163s)
               Value function loss: 4.9159
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 86.49
               Mean episode length: 136.41
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 10.10s
                        Total time: 7055.71s
                               ETA: 1073462.3s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1696 steps/s (collection: 9.491s, learning 0.167s)
               Value function loss: 5.3686
                    Surrogate loss: 0.0079
             Mean action noise std: 0.78
                       Mean reward: 85.21
               Mean episode length: 133.40
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 9.66s
                        Total time: 7065.37s
                               ETA: 1073277.2s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1625 steps/s (collection: 9.921s, learning 0.160s)
               Value function loss: 5.1678
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 88.58
               Mean episode length: 138.11
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 10.08s
                        Total time: 7075.45s
                               ETA: 1073156.9s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1687 steps/s (collection: 9.545s, learning 0.166s)
               Value function loss: 5.4248
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: 77.34
               Mean episode length: 132.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 9.71s
                        Total time: 7085.16s
                               ETA: 1072980.8s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1617 steps/s (collection: 9.970s, learning 0.161s)
               Value function loss: 5.2949
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 91.32
               Mean episode length: 137.15
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 10.13s
                        Total time: 7095.29s
                               ETA: 1072868.7s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1729 steps/s (collection: 9.306s, learning 0.168s)
               Value function loss: 4.9218
                    Surrogate loss: 0.0132
             Mean action noise std: 0.78
                       Mean reward: 85.72
               Mean episode length: 130.76
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 9.47s
                        Total time: 7104.77s
                               ETA: 1072657.8s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.467s, learning 0.161s)
               Value function loss: 5.1520
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 78.95
               Mean episode length: 131.54
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 9.63s
                        Total time: 7114.39s
                               ETA: 1072470.7s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1755 steps/s (collection: 9.157s, learning 0.178s)
               Value function loss: 4.8786
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 78.78
               Mean episode length: 128.25
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 9.34s
                        Total time: 7123.73s
                               ETA: 1072240.1s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1636 steps/s (collection: 9.840s, learning 0.170s)
               Value function loss: 5.0774
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 78.03
               Mean episode length: 129.26
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 10.01s
                        Total time: 7133.74s
                               ETA: 1072111.5s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1749 steps/s (collection: 9.186s, learning 0.179s)
               Value function loss: 5.5715
                    Surrogate loss: 0.0144
             Mean action noise std: 0.78
                       Mean reward: 79.49
               Mean episode length: 124.53
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 9.37s
                        Total time: 7143.10s
                               ETA: 1071886.5s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1706 steps/s (collection: 9.433s, learning 0.167s)
               Value function loss: 5.7368
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: 80.74
               Mean episode length: 127.81
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 9.60s
                        Total time: 7152.70s
                               ETA: 1071697.4s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1736 steps/s (collection: 9.264s, learning 0.171s)
               Value function loss: 5.7730
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 74.02
               Mean episode length: 126.53
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 9.43s
                        Total time: 7162.14s
                               ETA: 1071484.1s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1670 steps/s (collection: 9.639s, learning 0.170s)
               Value function loss: 5.6537
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 75.39
               Mean episode length: 126.70
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 9.81s
                        Total time: 7171.95s
                               ETA: 1071327.3s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.577s, learning 0.158s)
               Value function loss: 5.1635
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 74.64
               Mean episode length: 129.46
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 9.73s
                        Total time: 7181.68s
                               ETA: 1071159.9s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.492s, learning 0.191s)
               Value function loss: 4.2881
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 75.60
               Mean episode length: 130.27
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 9.68s
                        Total time: 7191.37s
                               ETA: 1070985.4s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.456s, learning 0.171s)
               Value function loss: 4.4887
                    Surrogate loss: 0.0092
             Mean action noise std: 0.78
                       Mean reward: 72.23
               Mean episode length: 130.01
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 9.63s
                        Total time: 7200.99s
                               ETA: 1070803.0s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.476s, learning 0.190s)
               Value function loss: 4.4250
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 82.25
               Mean episode length: 133.51
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 9.67s
                        Total time: 7210.66s
                               ETA: 1070626.8s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.748s, learning 0.162s)
               Value function loss: 4.3266
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 75.36
               Mean episode length: 133.06
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 9.91s
                        Total time: 7220.57s
                               ETA: 1070487.2s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.684s, learning 0.166s)
               Value function loss: 4.7097
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: 80.70
               Mean episode length: 132.21
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 9.85s
                        Total time: 7230.42s
                               ETA: 1070339.2s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.426s, learning 0.162s)
               Value function loss: 4.5573
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 77.65
               Mean episode length: 130.86
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 9.59s
                        Total time: 7240.01s
                               ETA: 1070152.8s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1723 steps/s (collection: 9.344s, learning 0.161s)
               Value function loss: 3.9777
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 81.29
               Mean episode length: 131.07
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 9.50s
                        Total time: 7249.51s
                               ETA: 1069954.8s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.380s, learning 0.176s)
               Value function loss: 3.9082
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 81.87
               Mean episode length: 132.79
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 9.56s
                        Total time: 7259.07s
                               ETA: 1069764.8s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.919s, learning 0.163s)
               Value function loss: 4.6570
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 80.65
               Mean episode length: 135.29
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 9.08s
                        Total time: 7268.15s
                               ETA: 1069505.5s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.532s, learning 0.173s)
               Value function loss: 4.3814
                    Surrogate loss: 0.0233
             Mean action noise std: 0.78
                       Mean reward: 73.72
               Mean episode length: 126.85
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 9.71s
                        Total time: 7277.85s
                               ETA: 1069338.6s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.529s, learning 0.164s)
               Value function loss: 4.5217
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 74.77
               Mean episode length: 127.61
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 9.69s
                        Total time: 7287.55s
                               ETA: 1069170.3s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.327s, learning 0.175s)
               Value function loss: 4.8121
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 70.65
               Mean episode length: 119.06
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 9.50s
                        Total time: 7297.05s
                               ETA: 1068974.6s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1692 steps/s (collection: 9.500s, learning 0.180s)
               Value function loss: 5.0652
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 76.85
               Mean episode length: 126.54
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 9.68s
                        Total time: 7306.73s
                               ETA: 1068805.6s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.520s, learning 0.178s)
               Value function loss: 4.7449
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 74.11
               Mean episode length: 131.17
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 9.70s
                        Total time: 7316.43s
                               ETA: 1068639.5s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.535s, learning 0.162s)
               Value function loss: 4.7659
                    Surrogate loss: 0.0238
             Mean action noise std: 0.78
                       Mean reward: 74.12
               Mean episode length: 128.33
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 9.70s
                        Total time: 7326.12s
                               ETA: 1068473.9s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.409s, learning 0.175s)
               Value function loss: 4.7435
                    Surrogate loss: 0.0075
             Mean action noise std: 0.78
                       Mean reward: 71.07
               Mean episode length: 126.46
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 9.58s
                        Total time: 7335.71s
                               ETA: 1068292.2s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1642 steps/s (collection: 9.795s, learning 0.179s)
               Value function loss: 3.9105
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 79.59
               Mean episode length: 132.83
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 9.97s
                        Total time: 7345.68s
                               ETA: 1068167.8s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1723 steps/s (collection: 9.324s, learning 0.180s)
               Value function loss: 3.5480
                    Surrogate loss: 0.0100
             Mean action noise std: 0.78
                       Mean reward: 69.70
               Mean episode length: 127.35
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 9.50s
                        Total time: 7355.19s
                               ETA: 1067975.4s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.488s, learning 0.161s)
               Value function loss: 3.7161
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: 75.86
               Mean episode length: 128.57
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 9.65s
                        Total time: 7364.84s
                               ETA: 1067804.5s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1746 steps/s (collection: 9.216s, learning 0.164s)
               Value function loss: 4.0321
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 67.23
               Mean episode length: 122.78
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 9.38s
                        Total time: 7374.22s
                               ETA: 1067595.3s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1707 steps/s (collection: 9.432s, learning 0.161s)
               Value function loss: 3.9203
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 76.35
               Mean episode length: 129.96
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 9.59s
                        Total time: 7383.81s
                               ETA: 1067417.3s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.821s, learning 0.161s)
               Value function loss: 4.2986
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 68.47
               Mean episode length: 126.93
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 9.98s
                        Total time: 7393.79s
                               ETA: 1067296.0s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.566s, learning 0.167s)
               Value function loss: 4.2793
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 67.50
               Mean episode length: 128.45
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 9.73s
                        Total time: 7403.52s
                               ETA: 1067139.1s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.563s, learning 0.166s)
               Value function loss: 4.9958
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 69.99
               Mean episode length: 127.25
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 9.73s
                        Total time: 7413.25s
                               ETA: 1066981.9s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1603 steps/s (collection: 10.060s, learning 0.159s)
               Value function loss: 5.1787
                    Surrogate loss: -0.0265
             Mean action noise std: 0.78
                       Mean reward: 64.62
               Mean episode length: 127.13
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 10.22s
                        Total time: 7423.47s
                               ETA: 1066895.8s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.297s, learning 0.161s)
               Value function loss: 5.7182
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: 66.93
               Mean episode length: 129.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 9.46s
                        Total time: 7432.93s
                               ETA: 1066700.6s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.676s, learning 0.163s)
               Value function loss: 6.1853
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 69.96
               Mean episode length: 129.52
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 9.84s
                        Total time: 7442.77s
                               ETA: 1066560.4s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.567s, learning 0.162s)
               Value function loss: 6.8200
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 67.13
               Mean episode length: 126.95
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 9.73s
                        Total time: 7452.50s
                               ETA: 1066405.0s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.694s, learning 0.161s)
               Value function loss: 6.0976
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 70.77
               Mean episode length: 132.07
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 9.85s
                        Total time: 7462.35s
                               ETA: 1066268.0s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1746 steps/s (collection: 9.207s, learning 0.172s)
               Value function loss: 6.6358
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 71.84
               Mean episode length: 129.94
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 9.38s
                        Total time: 7471.73s
                               ETA: 1066063.6s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1737 steps/s (collection: 9.250s, learning 0.179s)
               Value function loss: 7.2043
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 81.21
               Mean episode length: 132.17
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 9.43s
                        Total time: 7481.16s
                               ETA: 1065866.9s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1722 steps/s (collection: 9.331s, learning 0.182s)
               Value function loss: 7.2848
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 81.88
               Mean episode length: 138.96
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 9.51s
                        Total time: 7490.67s
                               ETA: 1065682.5s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.639s, learning 0.164s)
               Value function loss: 7.3408
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 91.04
               Mean episode length: 137.37
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 9.80s
                        Total time: 7500.48s
                               ETA: 1065539.8s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.632s, learning 0.160s)
               Value function loss: 7.7963
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 89.60
               Mean episode length: 138.69
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 9.79s
                        Total time: 7510.27s
                               ETA: 1065395.9s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.677s, learning 0.163s)
               Value function loss: 7.7963
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 99.57
               Mean episode length: 141.20
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 9.84s
                        Total time: 7520.11s
                               ETA: 1065259.3s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1750 steps/s (collection: 9.186s, learning 0.174s)
               Value function loss: 7.9570
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 98.70
               Mean episode length: 141.63
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 9.36s
                        Total time: 7529.47s
                               ETA: 1065055.1s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1745 steps/s (collection: 9.223s, learning 0.165s)
               Value function loss: 7.7955
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 102.12
               Mean episode length: 143.60
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 9.39s
                        Total time: 7538.86s
                               ETA: 1064855.4s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.428s, learning 0.158s)
               Value function loss: 7.8359
                    Surrogate loss: -0.0249
             Mean action noise std: 0.78
                       Mean reward: 115.28
               Mean episode length: 144.29
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 9.59s
                        Total time: 7548.44s
                               ETA: 1064684.2s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1752 steps/s (collection: 9.177s, learning 0.171s)
               Value function loss: 8.5306
                    Surrogate loss: -0.0288
             Mean action noise std: 0.78
                       Mean reward: 121.26
               Mean episode length: 147.77
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 9.35s
                        Total time: 7557.79s
                               ETA: 1064479.9s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1651 steps/s (collection: 9.757s, learning 0.165s)
               Value function loss: 7.6145
                    Surrogate loss: -0.0309
             Mean action noise std: 0.78
                       Mean reward: 124.14
               Mean episode length: 145.94
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 9.92s
                        Total time: 7567.71s
                               ETA: 1064356.9s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1756 steps/s (collection: 9.157s, learning 0.172s)
               Value function loss: 7.5898
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 130.38
               Mean episode length: 148.04
                  Mean reward/step: 0.84
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 9.33s
                        Total time: 7577.04s
                               ETA: 1064151.1s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1695 steps/s (collection: 9.505s, learning 0.160s)
               Value function loss: 7.5377
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 130.68
               Mean episode length: 145.19
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 9.67s
                        Total time: 7586.71s
                               ETA: 1063992.9s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.517s, learning 0.167s)
               Value function loss: 7.2256
                    Surrogate loss: -0.0284
             Mean action noise std: 0.78
                       Mean reward: 132.12
               Mean episode length: 146.64
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 9.68s
                        Total time: 7596.39s
                               ETA: 1063837.7s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.355s, learning 0.166s)
               Value function loss: 6.6650
                    Surrogate loss: 0.0021
             Mean action noise std: 0.78
                       Mean reward: 138.42
               Mean episode length: 143.85
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 9.52s
                        Total time: 7605.91s
                               ETA: 1063660.0s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.562s, learning 0.171s)
               Value function loss: 6.7199
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 131.55
               Mean episode length: 144.04
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 9.73s
                        Total time: 7615.64s
                               ETA: 1063512.5s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.558s, learning 0.172s)
               Value function loss: 6.6941
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 130.67
               Mean episode length: 143.49
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 9.73s
                        Total time: 7625.38s
                               ETA: 1063365.0s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.645s, learning 0.159s)
               Value function loss: 6.7283
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 134.74
               Mean episode length: 142.19
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 9.80s
                        Total time: 7635.18s
                               ETA: 1063228.1s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.354s, learning 0.169s)
               Value function loss: 7.6450
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 140.42
               Mean episode length: 144.37
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 9.52s
                        Total time: 7644.70s
                               ETA: 1063052.6s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.729s, learning 0.158s)
               Value function loss: 7.7101
                    Surrogate loss: -0.0216
             Mean action noise std: 0.78
                       Mean reward: 134.92
               Mean episode length: 141.65
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 9.89s
                        Total time: 7654.59s
                               ETA: 1062928.0s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.368s, learning 0.159s)
               Value function loss: 8.3243
                    Surrogate loss: 0.0088
             Mean action noise std: 0.78
                       Mean reward: 140.74
               Mean episode length: 144.55
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 9.53s
                        Total time: 7664.12s
                               ETA: 1062753.9s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.697s, learning 0.157s)
               Value function loss: 8.0229
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 145.83
               Mean episode length: 145.76
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 9.85s
                        Total time: 7673.97s
                               ETA: 1062625.6s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1723 steps/s (collection: 9.342s, learning 0.162s)
               Value function loss: 8.5317
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 137.99
               Mean episode length: 143.72
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 9.50s
                        Total time: 7683.48s
                               ETA: 1062449.1s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.510s, learning 0.163s)
               Value function loss: 8.7735
                    Surrogate loss: -0.0252
             Mean action noise std: 0.78
                       Mean reward: 134.18
               Mean episode length: 142.96
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 9.67s
                        Total time: 7693.15s
                               ETA: 1062296.4s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1700 steps/s (collection: 9.471s, learning 0.163s)
               Value function loss: 8.0738
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 132.78
               Mean episode length: 142.60
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 9.63s
                        Total time: 7702.78s
                               ETA: 1062138.7s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1661 steps/s (collection: 9.689s, learning 0.170s)
               Value function loss: 8.4974
                    Surrogate loss: -0.0255
             Mean action noise std: 0.78
                       Mean reward: 142.71
               Mean episode length: 143.59
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 9.86s
                        Total time: 7712.64s
                               ETA: 1062012.3s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1715 steps/s (collection: 9.391s, learning 0.160s)
               Value function loss: 7.3730
                    Surrogate loss: -0.0268
             Mean action noise std: 0.78
                       Mean reward: 137.85
               Mean episode length: 143.90
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 9.55s
                        Total time: 7722.19s
                               ETA: 1061844.0s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1745 steps/s (collection: 9.229s, learning 0.159s)
               Value function loss: 7.7410
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 138.28
               Mean episode length: 146.17
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 9.39s
                        Total time: 7731.58s
                               ETA: 1061653.8s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.621s, learning 0.165s)
               Value function loss: 8.4951
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 140.64
               Mean episode length: 147.40
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 9.79s
                        Total time: 7741.37s
                               ETA: 1061518.6s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1688 steps/s (collection: 9.531s, learning 0.176s)
               Value function loss: 7.3302
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 134.77
               Mean episode length: 146.36
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 9.71s
                        Total time: 7751.07s
                               ETA: 1061372.9s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1715 steps/s (collection: 9.373s, learning 0.178s)
               Value function loss: 7.8985
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 137.08
               Mean episode length: 144.94
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 9.55s
                        Total time: 7760.62s
                               ETA: 1061206.2s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1738 steps/s (collection: 9.252s, learning 0.171s)
               Value function loss: 7.2041
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 136.42
               Mean episode length: 143.90
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 9.42s
                        Total time: 7770.04s
                               ETA: 1061022.6s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.364s, learning 0.167s)
               Value function loss: 8.1292
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 142.83
               Mean episode length: 146.48
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 9.53s
                        Total time: 7779.58s
                               ETA: 1060854.1s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1731 steps/s (collection: 9.299s, learning 0.165s)
               Value function loss: 7.9339
                    Surrogate loss: -0.0249
             Mean action noise std: 0.78
                       Mean reward: 135.39
               Mean episode length: 145.95
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 9.46s
                        Total time: 7789.04s
                               ETA: 1060677.0s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.719s, learning 0.157s)
               Value function loss: 7.3914
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 142.09
               Mean episode length: 148.90
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 9.88s
                        Total time: 7798.92s
                               ETA: 1060556.4s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1717 steps/s (collection: 9.383s, learning 0.159s)
               Value function loss: 8.3135
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 144.85
               Mean episode length: 142.53
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 9.54s
                        Total time: 7808.46s
                               ETA: 1060390.7s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.762s, learning 0.174s)
               Value function loss: 7.4407
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 138.77
               Mean episode length: 144.65
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 9.94s
                        Total time: 7818.39s
                               ETA: 1060278.9s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.815s, learning 0.163s)
               Value function loss: 7.8927
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 142.62
               Mean episode length: 145.64
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 9.98s
                        Total time: 7828.37s
                               ETA: 1060173.1s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1643 steps/s (collection: 9.799s, learning 0.168s)
               Value function loss: 8.3237
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 142.35
               Mean episode length: 144.47
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 9.97s
                        Total time: 7838.34s
                               ETA: 1060066.0s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.725s, learning 0.168s)
               Value function loss: 8.7809
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 141.05
               Mean episode length: 145.72
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 9.89s
                        Total time: 7848.23s
                               ETA: 1059949.2s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1710 steps/s (collection: 9.414s, learning 0.162s)
               Value function loss: 9.7137
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 141.76
               Mean episode length: 147.48
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 9.58s
                        Total time: 7857.81s
                               ETA: 1059789.9s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1760 steps/s (collection: 9.134s, learning 0.171s)
               Value function loss: 9.4406
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 133.09
               Mean episode length: 142.03
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 9.31s
                        Total time: 7867.11s
                               ETA: 1059594.6s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1705 steps/s (collection: 9.439s, learning 0.165s)
               Value function loss: 8.9169
                    Surrogate loss: -0.0260
             Mean action noise std: 0.78
                       Mean reward: 143.96
               Mean episode length: 148.79
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 9.60s
                        Total time: 7876.72s
                               ETA: 1059440.0s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1759 steps/s (collection: 9.137s, learning 0.176s)
               Value function loss: 8.8244
                    Surrogate loss: -0.0294
             Mean action noise std: 0.78
                       Mean reward: 139.01
               Mean episode length: 146.52
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 9.31s
                        Total time: 7886.03s
                               ETA: 1059246.7s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.353s, learning 0.176s)
               Value function loss: 9.7746
                    Surrogate loss: -0.0231
             Mean action noise std: 0.78
                       Mean reward: 140.48
               Mean episode length: 149.19
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 9.53s
                        Total time: 7895.56s
                               ETA: 1059082.8s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1614 steps/s (collection: 9.982s, learning 0.168s)
               Value function loss: 9.7102
                    Surrogate loss: -0.0287
             Mean action noise std: 0.78
                       Mean reward: 141.16
               Mean episode length: 146.37
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 10.15s
                        Total time: 7905.71s
                               ETA: 1059002.6s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.361s, learning 0.162s)
               Value function loss: 10.8593
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 145.12
               Mean episode length: 147.37
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 9.52s
                        Total time: 7915.23s
                               ETA: 1058838.6s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1787 steps/s (collection: 8.998s, learning 0.170s)
               Value function loss: 11.3296
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 144.42
               Mean episode length: 147.95
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 9.17s
                        Total time: 7924.40s
                               ETA: 1058627.6s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1696 steps/s (collection: 9.485s, learning 0.174s)
               Value function loss: 10.7379
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 141.29
               Mean episode length: 148.03
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 9.66s
                        Total time: 7934.06s
                               ETA: 1058482.7s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.715s, learning 0.166s)
               Value function loss: 10.2189
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 146.67
               Mean episode length: 148.36
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 9.88s
                        Total time: 7943.94s
                               ETA: 1058367.7s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.722s, learning 0.159s)
               Value function loss: 11.7530
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 149.53
               Mean episode length: 147.31
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 9.88s
                        Total time: 7953.82s
                               ETA: 1058253.0s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.159s)
               Value function loss: 10.5484
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 158.04
               Mean episode length: 148.13
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 9.70s
                        Total time: 7963.52s
                               ETA: 1058114.1s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.618s, learning 0.167s)
               Value function loss: 10.1642
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 163.61
               Mean episode length: 148.48
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 9.79s
                        Total time: 7973.31s
                               ETA: 1057987.3s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1687 steps/s (collection: 9.544s, learning 0.167s)
               Value function loss: 9.6636
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 158.02
               Mean episode length: 148.02
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 9.71s
                        Total time: 7983.02s
                               ETA: 1057850.9s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1735 steps/s (collection: 9.275s, learning 0.164s)
               Value function loss: 10.5907
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 160.22
               Mean episode length: 147.37
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 9.44s
                        Total time: 7992.46s
                               ETA: 1057678.9s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.387s, learning 0.167s)
               Value function loss: 10.7980
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 167.99
               Mean episode length: 146.70
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 9.55s
                        Total time: 8002.01s
                               ETA: 1057522.5s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.535s, learning 0.182s)
               Value function loss: 10.0262
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 161.84
               Mean episode length: 146.55
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 9.72s
                        Total time: 8011.73s
                               ETA: 1057388.1s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.601s, learning 0.164s)
               Value function loss: 10.5453
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 165.98
               Mean episode length: 147.94
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 9.76s
                        Total time: 8021.49s
                               ETA: 1057260.2s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.529s, learning 0.168s)
               Value function loss: 11.2854
                    Surrogate loss: -0.0280
             Mean action noise std: 0.78
                       Mean reward: 180.32
               Mean episode length: 148.40
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 9.70s
                        Total time: 8031.19s
                               ETA: 1057123.7s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.507s, learning 0.168s)
               Value function loss: 11.5035
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 160.07
               Mean episode length: 144.97
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 9.67s
                        Total time: 8040.86s
                               ETA: 1056984.7s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1672 steps/s (collection: 9.625s, learning 0.173s)
               Value function loss: 10.7512
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 170.73
               Mean episode length: 147.72
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 9.80s
                        Total time: 8050.66s
                               ETA: 1056862.1s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1721 steps/s (collection: 9.361s, learning 0.159s)
               Value function loss: 12.0415
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 165.32
               Mean episode length: 147.02
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 9.52s
                        Total time: 8060.18s
                               ETA: 1056703.3s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1650 steps/s (collection: 9.758s, learning 0.170s)
               Value function loss: 10.8067
                    Surrogate loss: -0.0187
             Mean action noise std: 0.78
                       Mean reward: 167.28
               Mean episode length: 147.51
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 9.93s
                        Total time: 8070.11s
                               ETA: 1056598.4s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1747 steps/s (collection: 9.198s, learning 0.175s)
               Value function loss: 12.4754
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 165.05
               Mean episode length: 146.73
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 9.37s
                        Total time: 8079.48s
                               ETA: 1056421.3s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.584s, learning 0.166s)
               Value function loss: 11.9838
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 163.44
               Mean episode length: 145.01
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 9.75s
                        Total time: 8089.23s
                               ETA: 1056293.8s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.637s, learning 0.175s)
               Value function loss: 11.8792
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 167.01
               Mean episode length: 147.77
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 9.81s
                        Total time: 8099.04s
                               ETA: 1056174.6s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1705 steps/s (collection: 9.443s, learning 0.162s)
               Value function loss: 12.2778
                    Surrogate loss: -0.0233
             Mean action noise std: 0.78
                       Mean reward: 171.97
               Mean episode length: 148.42
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 9.61s
                        Total time: 8108.65s
                               ETA: 1056028.9s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1683 steps/s (collection: 9.568s, learning 0.162s)
               Value function loss: 12.3186
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 169.50
               Mean episode length: 147.40
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 9.73s
                        Total time: 8118.38s
                               ETA: 1055899.7s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.668s, learning 0.158s)
               Value function loss: 12.7870
                    Surrogate loss: -0.0252
             Mean action noise std: 0.78
                       Mean reward: 159.05
               Mean episode length: 146.09
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 9.83s
                        Total time: 8128.20s
                               ETA: 1055783.4s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.568s, learning 0.171s)
               Value function loss: 12.6463
                    Surrogate loss: -0.0238
             Mean action noise std: 0.78
                       Mean reward: 175.24
               Mean episode length: 149.84
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 9.74s
                        Total time: 8137.94s
                               ETA: 1055655.9s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.335s, learning 0.165s)
               Value function loss: 12.4439
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 170.81
               Mean episode length: 146.02
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 9.50s
                        Total time: 8147.44s
                               ETA: 1055497.8s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.391s, learning 0.163s)
               Value function loss: 12.3835
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 156.50
               Mean episode length: 146.32
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 9.55s
                        Total time: 8157.00s
                               ETA: 1055347.2s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1731 steps/s (collection: 9.302s, learning 0.160s)
               Value function loss: 11.4262
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 169.73
               Mean episode length: 144.51
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 9.46s
                        Total time: 8166.46s
                               ETA: 1055185.0s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.833s, learning 0.166s)
               Value function loss: 11.4394
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 166.51
               Mean episode length: 145.10
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 10.00s
                        Total time: 8176.46s
                               ETA: 1055092.5s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.402s, learning 0.158s)
               Value function loss: 11.6158
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 164.81
               Mean episode length: 144.89
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 9.56s
                        Total time: 8186.02s
                               ETA: 1054943.7s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.580s, learning 0.165s)
               Value function loss: 12.0646
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 163.85
               Mean episode length: 143.80
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 9.74s
                        Total time: 8195.76s
                               ETA: 1054819.0s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.397s, learning 0.160s)
               Value function loss: 10.8883
                    Surrogate loss: -0.0220
             Mean action noise std: 0.78
                       Mean reward: 156.06
               Mean episode length: 147.86
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 9.56s
                        Total time: 8205.32s
                               ETA: 1054670.3s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1622 steps/s (collection: 9.937s, learning 0.163s)
               Value function loss: 15.2156
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 155.43
               Mean episode length: 144.69
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 10.10s
                        Total time: 8215.42s
                               ETA: 1054591.8s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.576s, learning 0.170s)
               Value function loss: 15.2957
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 152.79
               Mean episode length: 142.67
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 9.75s
                        Total time: 8225.16s
                               ETA: 1054468.2s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1616 steps/s (collection: 9.970s, learning 0.167s)
               Value function loss: 12.9683
                    Surrogate loss: -0.0231
             Mean action noise std: 0.78
                       Mean reward: 148.97
               Mean episode length: 142.49
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 10.14s
                        Total time: 8235.30s
                               ETA: 1054394.8s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1719 steps/s (collection: 9.358s, learning 0.171s)
               Value function loss: 19.1329
                    Surrogate loss: -0.0251
             Mean action noise std: 0.78
                       Mean reward: 151.64
               Mean episode length: 147.05
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 9.53s
                        Total time: 8244.83s
                               ETA: 1054243.9s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1611 steps/s (collection: 10.008s, learning 0.161s)
               Value function loss: 19.3402
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 147.18
               Mean episode length: 141.03
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 10.17s
                        Total time: 8255.00s
                               ETA: 1054174.9s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1697 steps/s (collection: 9.486s, learning 0.166s)
               Value function loss: 10.7074
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 142.07
               Mean episode length: 142.79
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 9.65s
                        Total time: 8264.65s
                               ETA: 1054040.2s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.688s, learning 0.162s)
               Value function loss: 8.9617
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 148.21
               Mean episode length: 147.29
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 9.85s
                        Total time: 8274.50s
                               ETA: 1053931.1s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1744 steps/s (collection: 9.232s, learning 0.158s)
               Value function loss: 10.8444
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 147.54
               Mean episode length: 147.36
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 9.39s
                        Total time: 8283.89s
                               ETA: 1053763.8s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.458s, learning 0.161s)
               Value function loss: 10.0194
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 140.89
               Mean episode length: 146.76
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 9.62s
                        Total time: 8293.51s
                               ETA: 1053626.1s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1700 steps/s (collection: 9.476s, learning 0.159s)
               Value function loss: 8.7074
                    Surrogate loss: -0.0204
             Mean action noise std: 0.78
                       Mean reward: 138.80
               Mean episode length: 143.17
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 9.63s
                        Total time: 8303.14s
                               ETA: 1053490.5s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.473s, learning 0.168s)
               Value function loss: 9.0316
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 137.88
               Mean episode length: 145.30
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 9.64s
                        Total time: 8312.78s
                               ETA: 1053356.2s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.453s, learning 0.165s)
               Value function loss: 9.5357
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 140.23
               Mean episode length: 145.75
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 9.62s
                        Total time: 8322.40s
                               ETA: 1053219.2s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.598s, learning 0.181s)
               Value function loss: 10.8607
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 136.79
               Mean episode length: 143.40
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 9.78s
                        Total time: 8332.18s
                               ETA: 1053102.9s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1649 steps/s (collection: 9.761s, learning 0.173s)
               Value function loss: 10.1498
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 146.90
               Mean episode length: 142.83
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 9.93s
                        Total time: 8342.12s
                               ETA: 1053006.4s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.559s, learning 0.160s)
               Value function loss: 9.2118
                    Surrogate loss: -0.0249
             Mean action noise std: 0.78
                       Mean reward: 148.84
               Mean episode length: 147.61
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 9.72s
                        Total time: 8351.83s
                               ETA: 1052883.0s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.540s, learning 0.180s)
               Value function loss: 8.7637
                    Surrogate loss: -0.0281
             Mean action noise std: 0.78
                       Mean reward: 147.48
               Mean episode length: 144.59
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 9.72s
                        Total time: 8361.56s
                               ETA: 1052760.1s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1686 steps/s (collection: 9.553s, learning 0.161s)
               Value function loss: 10.5589
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 153.56
               Mean episode length: 147.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 9.71s
                        Total time: 8371.27s
                               ETA: 1052636.6s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1634 steps/s (collection: 9.857s, learning 0.164s)
               Value function loss: 11.1838
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 138.89
               Mean episode length: 141.15
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 10.02s
                        Total time: 8381.29s
                               ETA: 1052552.1s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.395s, learning 0.166s)
               Value function loss: 11.0773
                    Surrogate loss: -0.0231
             Mean action noise std: 0.78
                       Mean reward: 139.62
               Mean episode length: 142.82
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 9.56s
                        Total time: 8390.85s
                               ETA: 1052410.0s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1638 steps/s (collection: 9.833s, learning 0.163s)
               Value function loss: 13.1677
                    Surrogate loss: -0.0267
             Mean action noise std: 0.78
                       Mean reward: 150.05
               Mean episode length: 144.79
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 10.00s
                        Total time: 8400.85s
                               ETA: 1052322.8s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.568s, learning 0.161s)
               Value function loss: 14.6220
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 152.92
               Mean episode length: 146.86
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 9.73s
                        Total time: 8410.58s
                               ETA: 1052202.3s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1597 steps/s (collection: 10.089s, learning 0.165s)
               Value function loss: 13.0562
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 151.47
               Mean episode length: 145.22
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 10.25s
                        Total time: 8420.83s
                               ETA: 1052147.6s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1580 steps/s (collection: 10.201s, learning 0.164s)
               Value function loss: 14.4224
                    Surrogate loss: -0.0260
             Mean action noise std: 0.78
                       Mean reward: 157.06
               Mean episode length: 146.46
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 10.37s
                        Total time: 8431.19s
                               ETA: 1052107.0s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1634 steps/s (collection: 9.851s, learning 0.173s)
               Value function loss: 16.5633
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 155.44
               Mean episode length: 146.56
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 10.02s
                        Total time: 8441.22s
                               ETA: 1052024.0s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.398s, learning 0.172s)
               Value function loss: 15.3343
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 153.67
               Mean episode length: 143.46
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 9.57s
                        Total time: 8450.79s
                               ETA: 1051884.7s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1641 steps/s (collection: 9.820s, learning 0.162s)
               Value function loss: 13.9890
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 172.89
               Mean episode length: 146.96
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 9.98s
                        Total time: 8460.77s
                               ETA: 1051796.8s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1619 steps/s (collection: 9.945s, learning 0.171s)
               Value function loss: 15.8668
                    Surrogate loss: -0.0207
             Mean action noise std: 0.78
                       Mean reward: 172.71
               Mean episode length: 149.11
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 10.12s
                        Total time: 8470.89s
                               ETA: 1051725.8s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.390s, learning 0.169s)
               Value function loss: 14.8852
                    Surrogate loss: -0.0246
             Mean action noise std: 0.78
                       Mean reward: 163.57
               Mean episode length: 149.51
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 9.56s
                        Total time: 8480.45s
                               ETA: 1051585.9s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1663 steps/s (collection: 9.692s, learning 0.158s)
               Value function loss: 15.1704
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 162.91
               Mean episode length: 149.23
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 9.85s
                        Total time: 8490.29s
                               ETA: 1051482.2s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.552s, learning 0.169s)
               Value function loss: 13.4642
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: 154.94
               Mean episode length: 148.66
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 9.72s
                        Total time: 8500.02s
                               ETA: 1051363.0s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1710 steps/s (collection: 9.400s, learning 0.179s)
               Value function loss: 14.6952
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 175.10
               Mean episode length: 148.37
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 9.58s
                        Total time: 8509.60s
                               ETA: 1051226.4s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1695 steps/s (collection: 9.497s, learning 0.165s)
               Value function loss: 14.0606
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 170.92
               Mean episode length: 146.09
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 9.66s
                        Total time: 8519.26s
                               ETA: 1051100.4s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1770 steps/s (collection: 9.082s, learning 0.172s)
               Value function loss: 12.9859
                    Surrogate loss: -0.0206
             Mean action noise std: 0.78
                       Mean reward: 168.16
               Mean episode length: 146.35
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 9.25s
                        Total time: 8528.51s
                               ETA: 1050924.3s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.316s, learning 0.181s)
               Value function loss: 13.4824
                    Surrogate loss: -0.0263
             Mean action noise std: 0.78
                       Mean reward: 164.74
               Mean episode length: 149.17
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 9.50s
                        Total time: 8538.01s
                               ETA: 1050778.8s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1753 steps/s (collection: 9.185s, learning 0.161s)
               Value function loss: 12.7145
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 170.42
               Mean episode length: 147.08
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 9.35s
                        Total time: 8547.35s
                               ETA: 1050614.9s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.713s, learning 0.159s)
               Value function loss: 11.9938
                    Surrogate loss: -0.0230
             Mean action noise std: 0.78
                       Mean reward: 174.94
               Mean episode length: 145.24
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 9.87s
                        Total time: 8557.23s
                               ETA: 1050515.9s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.699s, learning 0.175s)
               Value function loss: 12.5256
                    Surrogate loss: -0.0224
             Mean action noise std: 0.78
                       Mean reward: 168.84
               Mean episode length: 143.62
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 9.87s
                        Total time: 8567.10s
                               ETA: 1050417.5s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.680s, learning 0.172s)
               Value function loss: 14.8363
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 171.26
               Mean episode length: 145.94
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 9.85s
                        Total time: 8576.95s
                               ETA: 1050316.7s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.593s, learning 0.167s)
               Value function loss: 15.1754
                    Surrogate loss: -0.0264
             Mean action noise std: 0.78
                       Mean reward: 176.41
               Mean episode length: 149.11
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 9.76s
                        Total time: 8586.71s
                               ETA: 1050204.7s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.674s, learning 0.166s)
               Value function loss: 15.7444
                    Surrogate loss: -0.0279
             Mean action noise std: 0.78
                       Mean reward: 184.37
               Mean episode length: 149.30
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 9.84s
                        Total time: 8596.55s
                               ETA: 1050102.7s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1632 steps/s (collection: 9.879s, learning 0.158s)
               Value function loss: 14.8115
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 174.78
               Mean episode length: 147.53
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 10.04s
                        Total time: 8606.59s
                               ETA: 1050025.0s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1721 steps/s (collection: 9.332s, learning 0.185s)
               Value function loss: 13.2846
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 188.93
               Mean episode length: 148.48
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 9.52s
                        Total time: 8616.11s
                               ETA: 1049884.2s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1726 steps/s (collection: 9.324s, learning 0.164s)
               Value function loss: 13.7173
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 186.31
               Mean episode length: 148.51
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 9.49s
                        Total time: 8625.59s
                               ETA: 1049740.1s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1700 steps/s (collection: 9.462s, learning 0.173s)
               Value function loss: 13.0752
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 171.40
               Mean episode length: 148.33
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 9.63s
                        Total time: 8635.23s
                               ETA: 1049614.1s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1700 steps/s (collection: 9.477s, learning 0.158s)
               Value function loss: 12.5242
                    Surrogate loss: -0.0219
             Mean action noise std: 0.78
                       Mean reward: 188.53
               Mean episode length: 150.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 9.63s
                        Total time: 8644.86s
                               ETA: 1049488.5s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1606 steps/s (collection: 10.018s, learning 0.180s)
               Value function loss: 14.2885
                    Surrogate loss: -0.0205
             Mean action noise std: 0.78
                       Mean reward: 171.69
               Mean episode length: 149.82
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 10.20s
                        Total time: 8655.06s
                               ETA: 1049431.4s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.630s, learning 0.174s)
               Value function loss: 13.5437
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 184.44
               Mean episode length: 149.34
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 9.80s
                        Total time: 8664.86s
                               ETA: 1049326.7s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.727s, learning 0.186s)
               Value function loss: 13.2686
                    Surrogate loss: -0.0221
             Mean action noise std: 0.78
                       Mean reward: 174.86
               Mean episode length: 149.65
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 9.91s
                        Total time: 8674.78s
                               ETA: 1049235.4s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.322s, learning 0.175s)
               Value function loss: 15.0134
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 190.74
               Mean episode length: 149.61
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 9.50s
                        Total time: 8684.27s
                               ETA: 1049094.2s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1629 steps/s (collection: 9.886s, learning 0.166s)
               Value function loss: 17.8778
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 197.98
               Mean episode length: 149.49
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 10.05s
                        Total time: 8694.33s
                               ETA: 1049020.2s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1733 steps/s (collection: 9.285s, learning 0.168s)
               Value function loss: 13.3762
                    Surrogate loss: -0.0243
             Mean action noise std: 0.78
                       Mean reward: 189.41
               Mean episode length: 148.81
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 9.45s
                        Total time: 8703.78s
                               ETA: 1048874.2s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.639s, learning 0.163s)
               Value function loss: 13.6737
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 184.25
               Mean episode length: 148.70
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 9.80s
                        Total time: 8713.58s
                               ETA: 1048770.4s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1724 steps/s (collection: 9.342s, learning 0.161s)
               Value function loss: 14.4990
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 191.83
               Mean episode length: 148.19
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 9.50s
                        Total time: 8723.08s
                               ETA: 1048631.0s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1696 steps/s (collection: 9.496s, learning 0.162s)
               Value function loss: 13.3362
                    Surrogate loss: -0.0281
             Mean action noise std: 0.78
                       Mean reward: 197.98
               Mean episode length: 147.54
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 9.66s
                        Total time: 8732.74s
                               ETA: 1048510.6s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.669s, learning 0.159s)
               Value function loss: 13.3104
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 195.24
               Mean episode length: 148.17
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 9.83s
                        Total time: 8742.57s
                               ETA: 1048410.7s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1738 steps/s (collection: 9.254s, learning 0.170s)
               Value function loss: 12.9184
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 193.81
               Mean episode length: 147.21
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 9.42s
                        Total time: 8751.99s
                               ETA: 1048262.6s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.504s, learning 0.171s)
               Value function loss: 13.7726
                    Surrogate loss: -0.0252
             Mean action noise std: 0.78
                       Mean reward: 185.07
               Mean episode length: 148.15
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 9.68s
                        Total time: 8761.67s
                               ETA: 1048145.0s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.461s, learning 0.169s)
               Value function loss: 13.2928
                    Surrogate loss: -0.0192
             Mean action noise std: 0.78
                       Mean reward: 201.96
               Mean episode length: 148.84
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 9.63s
                        Total time: 8771.30s
                               ETA: 1048022.2s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1692 steps/s (collection: 9.517s, learning 0.160s)
               Value function loss: 12.7079
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 189.50
               Mean episode length: 148.50
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 9.68s
                        Total time: 8780.98s
                               ETA: 1047905.4s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1737 steps/s (collection: 9.269s, learning 0.161s)
               Value function loss: 13.8150
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 196.12
               Mean episode length: 148.19
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 9.43s
                        Total time: 8790.40s
                               ETA: 1047759.2s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1657 steps/s (collection: 9.716s, learning 0.166s)
               Value function loss: 13.8146
                    Surrogate loss: -0.0234
             Mean action noise std: 0.78
                       Mean reward: 180.77
               Mean episode length: 146.91
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 9.88s
                        Total time: 8800.29s
                               ETA: 1047667.3s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.582s, learning 0.170s)
               Value function loss: 13.6180
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 183.76
               Mean episode length: 147.01
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 9.75s
                        Total time: 8810.04s
                               ETA: 1047560.1s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1650 steps/s (collection: 9.751s, learning 0.173s)
               Value function loss: 12.2045
                    Surrogate loss: -0.0211
             Mean action noise std: 0.78
                       Mean reward: 190.31
               Mean episode length: 148.61
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 9.92s
                        Total time: 8819.96s
                               ETA: 1047473.6s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1659 steps/s (collection: 9.709s, learning 0.167s)
               Value function loss: 12.2144
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 187.25
               Mean episode length: 148.67
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 9.88s
                        Total time: 8829.84s
                               ETA: 1047381.4s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1692 steps/s (collection: 9.510s, learning 0.171s)
               Value function loss: 12.2537
                    Surrogate loss: -0.0208
             Mean action noise std: 0.78
                       Mean reward: 181.05
               Mean episode length: 148.14
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 9.68s
                        Total time: 8839.52s
                               ETA: 1047266.5s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.856s, learning 0.165s)
               Value function loss: 11.1334
                    Surrogate loss: -0.0317
             Mean action noise std: 0.78
                       Mean reward: 186.43
               Mean episode length: 148.80
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 9.02s
                        Total time: 8848.54s
                               ETA: 1047073.7s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1616 steps/s (collection: 9.963s, learning 0.172s)
               Value function loss: 12.6152
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 186.34
               Mean episode length: 148.82
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 10.13s
                        Total time: 8858.67s
                               ETA: 1047013.0s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1662 steps/s (collection: 9.681s, learning 0.173s)
               Value function loss: 11.7496
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 180.74
               Mean episode length: 148.95
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 9.85s
                        Total time: 8868.53s
                               ETA: 1046919.2s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.712s, learning 0.167s)
               Value function loss: 11.9098
                    Surrogate loss: -0.0180
             Mean action noise std: 0.78
                       Mean reward: 178.32
               Mean episode length: 149.13
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 9.88s
                        Total time: 8878.41s
                               ETA: 1046828.6s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1715 steps/s (collection: 9.387s, learning 0.164s)
               Value function loss: 10.0904
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 197.43
               Mean episode length: 149.60
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 9.55s
                        Total time: 8887.96s
                               ETA: 1046699.6s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.795s, learning 0.170s)
               Value function loss: 11.8978
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 189.23
               Mean episode length: 149.66
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 9.97s
                        Total time: 8897.92s
                               ETA: 1046619.6s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.466s, learning 0.180s)
               Value function loss: 11.3779
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 191.09
               Mean episode length: 149.63
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 9.65s
                        Total time: 8907.57s
                               ETA: 1046502.1s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.724s, learning 0.164s)
               Value function loss: 10.4033
                    Surrogate loss: -0.0258
             Mean action noise std: 0.78
                       Mean reward: 181.81
               Mean episode length: 148.18
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 9.89s
                        Total time: 8917.46s
                               ETA: 1046413.4s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.703s, learning 0.164s)
               Value function loss: 10.2659
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 179.77
               Mean episode length: 148.35
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 9.87s
                        Total time: 8927.32s
                               ETA: 1046322.4s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.354s, learning 0.167s)
               Value function loss: 9.6065
                    Surrogate loss: -0.0253
             Mean action noise std: 0.78
                       Mean reward: 192.50
               Mean episode length: 149.06
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 9.52s
                        Total time: 8936.85s
                               ETA: 1046191.2s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.648s, learning 0.181s)
               Value function loss: 12.1953
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 183.02
               Mean episode length: 149.25
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 9.83s
                        Total time: 8946.67s
                               ETA: 1046096.3s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1778 steps/s (collection: 9.052s, learning 0.159s)
               Value function loss: 11.7913
                    Surrogate loss: -0.0242
             Mean action noise std: 0.78
                       Mean reward: 194.39
               Mean episode length: 149.45
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 9.21s
                        Total time: 8955.89s
                               ETA: 1045929.3s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1647 steps/s (collection: 9.784s, learning 0.160s)
               Value function loss: 11.4602
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 186.54
               Mean episode length: 147.88
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 9.94s
                        Total time: 8965.83s
                               ETA: 1045848.2s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1617 steps/s (collection: 9.968s, learning 0.161s)
               Value function loss: 12.2921
                    Surrogate loss: -0.0229
             Mean action noise std: 0.78
                       Mean reward: 185.17
               Mean episode length: 147.83
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 10.13s
                        Total time: 8975.96s
                               ETA: 1045788.8s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1661 steps/s (collection: 9.690s, learning 0.172s)
               Value function loss: 14.0085
                    Surrogate loss: -0.0215
             Mean action noise std: 0.78
                       Mean reward: 184.02
               Mean episode length: 149.90
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 9.86s
                        Total time: 8985.82s
                               ETA: 1045698.4s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1802 steps/s (collection: 8.922s, learning 0.169s)
               Value function loss: 14.9773
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 184.45
               Mean episode length: 147.01
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 9.09s
                        Total time: 8994.91s
                               ETA: 1045518.8s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.480s, learning 0.159s)
               Value function loss: 14.1014
                    Surrogate loss: -0.0236
             Mean action noise std: 0.78
                       Mean reward: 185.80
               Mean episode length: 146.94
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 9.64s
                        Total time: 9004.55s
                               ETA: 1045403.0s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1716 steps/s (collection: 9.376s, learning 0.171s)
               Value function loss: 15.5942
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 195.52
               Mean episode length: 148.33
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 9.55s
                        Total time: 9014.10s
                               ETA: 1045276.8s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.696s, learning 0.174s)
               Value function loss: 15.7974
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 191.16
               Mean episode length: 148.49
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 9.87s
                        Total time: 9023.97s
                               ETA: 1045188.3s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.453s, learning 0.162s)
               Value function loss: 15.6614
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 195.18
               Mean episode length: 148.46
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 9.62s
                        Total time: 9033.58s
                               ETA: 1045070.5s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.657s, learning 0.179s)
               Value function loss: 14.1825
                    Surrogate loss: -0.0215
             Mean action noise std: 0.78
                       Mean reward: 196.00
               Mean episode length: 148.64
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 9.84s
                        Total time: 9043.42s
                               ETA: 1044978.5s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1760 steps/s (collection: 9.144s, learning 0.161s)
               Value function loss: 15.4888
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 196.71
               Mean episode length: 148.33
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 9.31s
                        Total time: 9052.72s
                               ETA: 1044825.5s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.729s, learning 0.172s)
               Value function loss: 14.0656
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 202.34
               Mean episode length: 148.97
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 9.90s
                        Total time: 9062.62s
                               ETA: 1044741.5s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1715 steps/s (collection: 9.372s, learning 0.179s)
               Value function loss: 12.8867
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 194.15
               Mean episode length: 148.41
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 9.55s
                        Total time: 9072.18s
                               ETA: 1044617.3s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1737 steps/s (collection: 9.258s, learning 0.169s)
               Value function loss: 12.1078
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 199.54
               Mean episode length: 148.58
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 9.43s
                        Total time: 9081.60s
                               ETA: 1044479.1s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.779s, learning 0.160s)
               Value function loss: 12.9811
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 206.38
               Mean episode length: 149.36
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 9.94s
                        Total time: 9091.54s
                               ETA: 1044400.1s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.549s, learning 0.169s)
               Value function loss: 10.8477
                    Surrogate loss: -0.0299
             Mean action noise std: 0.78
                       Mean reward: 199.43
               Mean episode length: 149.72
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 9.72s
                        Total time: 9101.26s
                               ETA: 1044295.9s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1649 steps/s (collection: 9.759s, learning 0.170s)
               Value function loss: 13.5305
                    Surrogate loss: -0.0192
             Mean action noise std: 0.78
                       Mean reward: 202.33
               Mean episode length: 149.65
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 9.93s
                        Total time: 9111.19s
                               ETA: 1044216.1s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1769 steps/s (collection: 9.102s, learning 0.158s)
               Value function loss: 15.1163
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 204.89
               Mean episode length: 149.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 9.26s
                        Total time: 9120.45s
                               ETA: 1044059.8s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1678 steps/s (collection: 9.591s, learning 0.169s)
               Value function loss: 15.5626
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 199.33
               Mean episode length: 147.84
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 9.76s
                        Total time: 9130.21s
                               ETA: 1043961.0s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1695 steps/s (collection: 9.504s, learning 0.160s)
               Value function loss: 14.0159
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 203.28
               Mean episode length: 148.88
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 9.66s
                        Total time: 9139.87s
                               ETA: 1043851.4s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.558s, learning 0.167s)
               Value function loss: 15.1830
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: 204.00
               Mean episode length: 147.77
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 9.73s
                        Total time: 9149.60s
                               ETA: 1043749.1s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1671 steps/s (collection: 9.621s, learning 0.179s)
               Value function loss: 16.0881
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 199.19
               Mean episode length: 149.70
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 9.80s
                        Total time: 9159.40s
                               ETA: 1043655.5s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1674 steps/s (collection: 9.609s, learning 0.177s)
               Value function loss: 18.2086
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 213.77
               Mean episode length: 148.81
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 9.79s
                        Total time: 9169.18s
                               ETA: 1043560.5s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.592s, learning 0.189s)
               Value function loss: 14.9037
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 202.43
               Mean episode length: 148.95
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 9.78s
                        Total time: 9178.96s
                               ETA: 1043465.2s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1651 steps/s (collection: 9.752s, learning 0.169s)
               Value function loss: 15.1834
                    Surrogate loss: -0.0209
             Mean action noise std: 0.78
                       Mean reward: 203.34
               Mean episode length: 149.11
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 9.92s
                        Total time: 9188.89s
                               ETA: 1043385.9s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1690 steps/s (collection: 9.522s, learning 0.171s)
               Value function loss: 15.8425
                    Surrogate loss: -0.0260
             Mean action noise std: 0.78
                       Mean reward: 205.14
               Mean episode length: 147.18
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 9.69s
                        Total time: 9198.58s
                               ETA: 1043281.0s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.416s, learning 0.170s)
               Value function loss: 17.7830
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 208.61
               Mean episode length: 148.63
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 9.59s
                        Total time: 9208.17s
                               ETA: 1043164.2s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.528s, learning 0.171s)
               Value function loss: 16.7456
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 217.78
               Mean episode length: 149.90
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 9.70s
                        Total time: 9217.87s
                               ETA: 1043060.4s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1654 steps/s (collection: 9.737s, learning 0.167s)
               Value function loss: 13.4526
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 210.62
               Mean episode length: 150.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 9.90s
                        Total time: 9227.77s
                               ETA: 1042979.9s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1701 steps/s (collection: 9.467s, learning 0.162s)
               Value function loss: 15.7531
                    Surrogate loss: -0.0241
             Mean action noise std: 0.78
                       Mean reward: 209.88
               Mean episode length: 148.18
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 9.63s
                        Total time: 9237.40s
                               ETA: 1042868.6s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1628 steps/s (collection: 9.895s, learning 0.166s)
               Value function loss: 15.9218
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 212.23
               Mean episode length: 148.49
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 10.06s
                        Total time: 9247.46s
                               ETA: 1042806.3s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.595s, learning 0.160s)
               Value function loss: 13.3716
                    Surrogate loss: -0.0244
             Mean action noise std: 0.78
                       Mean reward: 202.03
               Mean episode length: 147.17
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 9.76s
                        Total time: 9257.22s
                               ETA: 1042709.6s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.365s, learning 0.171s)
               Value function loss: 15.1304
                    Surrogate loss: -0.0221
             Mean action noise std: 0.78
                       Mean reward: 205.29
               Mean episode length: 147.95
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 9.54s
                        Total time: 9266.75s
                               ETA: 1042588.4s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.401s, learning 0.163s)
               Value function loss: 14.7983
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 201.73
               Mean episode length: 147.73
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 9.56s
                        Total time: 9276.32s
                               ETA: 1042470.6s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1613 steps/s (collection: 9.996s, learning 0.162s)
               Value function loss: 13.4805
                    Surrogate loss: -0.0231
             Mean action noise std: 0.78
                       Mean reward: 201.40
               Mean episode length: 149.24
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 10.16s
                        Total time: 9286.47s
                               ETA: 1042419.7s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1647 steps/s (collection: 9.780s, learning 0.167s)
               Value function loss: 17.1671
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 205.83
               Mean episode length: 148.81
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 9.95s
                        Total time: 9296.42s
                               ETA: 1042345.3s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1726 steps/s (collection: 9.324s, learning 0.168s)
               Value function loss: 18.8315
                    Surrogate loss: -0.0193
             Mean action noise std: 0.78
                       Mean reward: 206.47
               Mean episode length: 148.98
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 9.49s
                        Total time: 9305.91s
                               ETA: 1042220.0s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1627 steps/s (collection: 9.894s, learning 0.171s)
               Value function loss: 20.3937
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 211.13
               Mean episode length: 147.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 10.07s
                        Total time: 9315.98s
                               ETA: 1042159.2s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1732 steps/s (collection: 9.278s, learning 0.177s)
               Value function loss: 18.8088
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 211.21
               Mean episode length: 148.61
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 9.46s
                        Total time: 9325.43s
                               ETA: 1042030.3s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1718 steps/s (collection: 9.366s, learning 0.167s)
               Value function loss: 19.8328
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 228.50
               Mean episode length: 149.32
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 9.53s
                        Total time: 9334.97s
                               ETA: 1041910.4s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1736 steps/s (collection: 9.278s, learning 0.159s)
               Value function loss: 22.9949
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 217.50
               Mean episode length: 148.81
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 9.44s
                        Total time: 9344.40s
                               ETA: 1041779.9s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.351s, learning 0.171s)
               Value function loss: 21.8527
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 219.15
               Mean episode length: 148.88
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 9.52s
                        Total time: 9353.92s
                               ETA: 1041659.2s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1728 steps/s (collection: 9.304s, learning 0.176s)
               Value function loss: 19.7646
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 225.68
               Mean episode length: 150.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 9.48s
                        Total time: 9363.40s
                               ETA: 1041534.1s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.660s, learning 0.177s)
               Value function loss: 20.3023
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 223.75
               Mean episode length: 149.92
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 9.84s
                        Total time: 9373.24s
                               ETA: 1041449.0s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1682 steps/s (collection: 9.568s, learning 0.168s)
               Value function loss: 21.0157
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 219.27
               Mean episode length: 150.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 9.74s
                        Total time: 9382.98s
                               ETA: 1041352.8s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1763 steps/s (collection: 9.119s, learning 0.173s)
               Value function loss: 21.5927
                    Surrogate loss: -0.0210
             Mean action noise std: 0.78
                       Mean reward: 224.35
               Mean episode length: 148.69
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 9.29s
                        Total time: 9392.27s
                               ETA: 1041207.6s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.748s, learning 0.160s)
               Value function loss: 22.8834
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 227.05
               Mean episode length: 150.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 9.91s
                        Total time: 9402.18s
                               ETA: 1041130.8s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1640 steps/s (collection: 9.829s, learning 0.161s)
               Value function loss: 18.3558
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 231.33
               Mean episode length: 149.06
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 9.99s
                        Total time: 9412.17s
                               ETA: 1041063.3s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.616s, learning 0.174s)
               Value function loss: 22.0287
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 218.65
               Mean episode length: 148.63
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 9.79s
                        Total time: 9421.96s
                               ETA: 1040973.8s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.387s, learning 0.172s)
               Value function loss: 18.8167
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 219.47
               Mean episode length: 147.57
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 9.56s
                        Total time: 9431.51s
                               ETA: 1040859.0s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1679 steps/s (collection: 9.581s, learning 0.177s)
               Value function loss: 18.9734
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 219.55
               Mean episode length: 149.15
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 9.76s
                        Total time: 9441.27s
                               ETA: 1040766.4s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.303s, learning 0.164s)
               Value function loss: 19.5868
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: 229.79
               Mean episode length: 148.68
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 9.47s
                        Total time: 9450.74s
                               ETA: 1040642.0s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.438s, learning 0.174s)
               Value function loss: 19.0977
                    Surrogate loss: -0.0228
             Mean action noise std: 0.78
                       Mean reward: 228.05
               Mean episode length: 148.42
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 9.61s
                        Total time: 9460.35s
                               ETA: 1040533.8s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.741s, learning 0.168s)
               Value function loss: 20.1371
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 226.84
               Mean episode length: 147.01
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 9.91s
                        Total time: 9470.26s
                               ETA: 1040458.3s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.653s, learning 0.162s)
               Value function loss: 20.4647
                    Surrogate loss: -0.0196
             Mean action noise std: 0.78
                       Mean reward: 239.09
               Mean episode length: 149.79
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 9.82s
                        Total time: 9480.08s
                               ETA: 1040372.8s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1626 steps/s (collection: 9.904s, learning 0.171s)
               Value function loss: 21.1149
                    Surrogate loss: -0.0240
             Mean action noise std: 0.78
                       Mean reward: 219.09
               Mean episode length: 148.53
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 10.08s
                        Total time: 9490.15s
                               ETA: 1040315.9s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.652s, learning 0.167s)
               Value function loss: 21.4352
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 231.86
               Mean episode length: 148.69
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 9.82s
                        Total time: 9499.97s
                               ETA: 1040231.0s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.624s, learning 0.166s)
               Value function loss: 23.1453
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 236.83
               Mean episode length: 149.51
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 9.79s
                        Total time: 9509.76s
                               ETA: 1040143.2s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1762 steps/s (collection: 9.133s, learning 0.161s)
               Value function loss: 21.0331
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 224.38
               Mean episode length: 147.43
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 9.29s
                        Total time: 9519.05s
                               ETA: 1040001.3s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.527s, learning 0.168s)
               Value function loss: 21.5098
                    Surrogate loss: -0.0225
             Mean action noise std: 0.78
                       Mean reward: 230.00
               Mean episode length: 149.02
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 9.69s
                        Total time: 9528.75s
                               ETA: 1039903.5s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1736 steps/s (collection: 9.272s, learning 0.163s)
               Value function loss: 24.1514
                    Surrogate loss: -0.0254
             Mean action noise std: 0.77
                       Mean reward: 228.28
               Mean episode length: 148.29
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 9.43s
                        Total time: 9538.18s
                               ETA: 1039777.5s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1695 steps/s (collection: 9.497s, learning 0.164s)
               Value function loss: 23.2885
                    Surrogate loss: -0.0031
             Mean action noise std: 0.77
                       Mean reward: 222.79
               Mean episode length: 146.82
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 9.66s
                        Total time: 9547.84s
                               ETA: 1039676.3s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.589s, learning 0.161s)
               Value function loss: 22.9405
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 228.29
               Mean episode length: 148.51
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 9.75s
                        Total time: 9557.59s
                               ETA: 1039585.1s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1669 steps/s (collection: 9.642s, learning 0.170s)
               Value function loss: 23.0824
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 242.47
               Mean episode length: 149.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 9.81s
                        Total time: 9567.41s
                               ETA: 1039500.8s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.497s, learning 0.179s)
               Value function loss: 22.4925
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 225.03
               Mean episode length: 148.52
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 9.68s
                        Total time: 9577.08s
                               ETA: 1039401.8s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.669s, learning 0.160s)
               Value function loss: 22.9486
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 228.85
               Mean episode length: 148.25
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 9.83s
                        Total time: 9586.91s
                               ETA: 1039319.8s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1721 steps/s (collection: 9.354s, learning 0.164s)
               Value function loss: 21.5714
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 245.32
               Mean episode length: 149.52
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 9.52s
                        Total time: 9596.43s
                               ETA: 1039204.1s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1735 steps/s (collection: 9.276s, learning 0.165s)
               Value function loss: 22.3512
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 240.75
               Mean episode length: 147.88
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 9.44s
                        Total time: 9605.87s
                               ETA: 1039080.3s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1765 steps/s (collection: 9.123s, learning 0.159s)
               Value function loss: 19.2706
                    Surrogate loss: -0.0213
             Mean action noise std: 0.77
                       Mean reward: 230.57
               Mean episode length: 149.60
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 9.28s
                        Total time: 9615.15s
                               ETA: 1038939.6s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.555s, learning 0.173s)
               Value function loss: 20.0818
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 237.00
               Mean episode length: 147.74
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 9.73s
                        Total time: 9624.88s
                               ETA: 1038847.4s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.539s, learning 0.159s)
               Value function loss: 20.1445
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 242.78
               Mean episode length: 149.88
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 9.70s
                        Total time: 9634.58s
                               ETA: 1038752.1s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1684 steps/s (collection: 9.552s, learning 0.177s)
               Value function loss: 20.8383
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 248.33
               Mean episode length: 148.90
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 9.73s
                        Total time: 9644.31s
                               ETA: 1038660.3s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.572s, learning 0.173s)
               Value function loss: 22.1717
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 241.14
               Mean episode length: 147.83
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 9.74s
                        Total time: 9654.05s
                               ETA: 1038570.4s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.481s, learning 0.163s)
               Value function loss: 19.7775
                    Surrogate loss: -0.0214
             Mean action noise std: 0.77
                       Mean reward: 243.80
               Mean episode length: 149.89
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 9.64s
                        Total time: 9663.69s
                               ETA: 1038469.8s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1694 steps/s (collection: 9.504s, learning 0.167s)
               Value function loss: 21.8369
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 239.08
               Mean episode length: 149.47
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 9.67s
                        Total time: 9673.37s
                               ETA: 1038372.4s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1734 steps/s (collection: 9.267s, learning 0.181s)
               Value function loss: 20.6719
                    Surrogate loss: -0.0220
             Mean action noise std: 0.77
                       Mean reward: 244.54
               Mean episode length: 149.38
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 9.45s
                        Total time: 9682.81s
                               ETA: 1038251.2s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1670 steps/s (collection: 9.644s, learning 0.165s)
               Value function loss: 23.9448
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 232.52
               Mean episode length: 149.62
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 9.81s
                        Total time: 9692.62s
                               ETA: 1038168.9s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1711 steps/s (collection: 9.405s, learning 0.166s)
               Value function loss: 23.9018
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 250.05
               Mean episode length: 149.10
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 9.57s
                        Total time: 9702.19s
                               ETA: 1038061.2s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1695 steps/s (collection: 9.499s, learning 0.165s)
               Value function loss: 23.8238
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 241.78
               Mean episode length: 149.22
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 9.66s
                        Total time: 9711.86s
                               ETA: 1037963.8s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.681s, learning 0.158s)
               Value function loss: 24.3857
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 241.94
               Mean episode length: 149.21
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 9.84s
                        Total time: 9721.70s
                               ETA: 1037885.3s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1717 steps/s (collection: 9.378s, learning 0.163s)
               Value function loss: 27.3129
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 235.56
               Mean episode length: 148.83
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 9.54s
                        Total time: 9731.24s
                               ETA: 1037775.1s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.934s, learning 0.166s)
               Value function loss: 26.8834
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 237.79
               Mean episode length: 148.87
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 9.10s
                        Total time: 9740.34s
                               ETA: 1037618.1s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1623 steps/s (collection: 9.932s, learning 0.159s)
               Value function loss: 26.3055
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 235.51
               Mean episode length: 148.96
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 10.09s
                        Total time: 9750.43s
                               ETA: 1037567.0s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1712 steps/s (collection: 9.405s, learning 0.160s)
               Value function loss: 24.4129
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 228.88
               Mean episode length: 148.95
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 9.57s
                        Total time: 9759.99s
                               ETA: 1037460.0s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1664 steps/s (collection: 9.662s, learning 0.178s)
               Value function loss: 25.8702
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 240.37
               Mean episode length: 149.87
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 9.84s
                        Total time: 9769.83s
                               ETA: 1037382.5s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1656 steps/s (collection: 9.730s, learning 0.163s)
               Value function loss: 21.4546
                    Surrogate loss: 0.0008
             Mean action noise std: 0.77
                       Mean reward: 245.86
               Mean episode length: 150.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 9.89s
                        Total time: 9779.73s
                               ETA: 1037310.7s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1628 steps/s (collection: 9.897s, learning 0.165s)
               Value function loss: 20.8787
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 242.15
               Mean episode length: 149.63
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 10.06s
                        Total time: 9789.79s
                               ETA: 1037256.9s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1608 steps/s (collection: 10.019s, learning 0.164s)
               Value function loss: 18.1328
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 248.88
               Mean episode length: 149.37
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 10.18s
                        Total time: 9799.97s
                               ETA: 1037216.0s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.593s, learning 0.176s)
               Value function loss: 17.2156
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 242.39
               Mean episode length: 150.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 9.77s
                        Total time: 9809.74s
                               ETA: 1037131.5s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1765 steps/s (collection: 9.115s, learning 0.165s)
               Value function loss: 17.3656
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 236.13
               Mean episode length: 149.63
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 9.28s
                        Total time: 9819.02s
                               ETA: 1036995.4s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1699 steps/s (collection: 9.474s, learning 0.165s)
               Value function loss: 17.0392
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 233.70
               Mean episode length: 148.42
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 9.64s
                        Total time: 9828.66s
                               ETA: 1036897.4s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1652 steps/s (collection: 9.753s, learning 0.163s)
               Value function loss: 17.9026
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 228.69
               Mean episode length: 149.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 9.92s
                        Total time: 9838.58s
                               ETA: 1036828.9s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1687 steps/s (collection: 9.543s, learning 0.164s)
               Value function loss: 23.6602
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 237.32
               Mean episode length: 147.64
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 9.71s
                        Total time: 9848.28s
                               ETA: 1036738.5s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1632 steps/s (collection: 9.881s, learning 0.159s)
               Value function loss: 22.5013
                    Surrogate loss: -0.0240
             Mean action noise std: 0.77
                       Mean reward: 236.14
               Mean episode length: 149.17
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 10.04s
                        Total time: 9858.32s
                               ETA: 1036683.2s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1668 steps/s (collection: 9.646s, learning 0.174s)
               Value function loss: 21.0468
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 233.69
               Mean episode length: 150.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 9.82s
                        Total time: 9868.14s
                               ETA: 1036605.0s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1664 steps/s (collection: 9.676s, learning 0.169s)
               Value function loss: 23.1596
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 250.32
               Mean episode length: 149.55
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 9.85s
                        Total time: 9877.99s
                               ETA: 1036529.6s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1760 steps/s (collection: 9.148s, learning 0.157s)
               Value function loss: 23.8511
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 242.36
               Mean episode length: 149.47
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 9.30s
                        Total time: 9887.29s
                               ETA: 1036397.6s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1625 steps/s (collection: 9.907s, learning 0.169s)
               Value function loss: 24.2510
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 242.20
               Mean episode length: 149.70
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 10.08s
                        Total time: 9897.37s
                               ETA: 1036346.7s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1633 steps/s (collection: 9.869s, learning 0.161s)
               Value function loss: 21.9743
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 231.64
               Mean episode length: 149.24
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 10.03s
                        Total time: 9907.40s
                               ETA: 1036291.0s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1664 steps/s (collection: 9.673s, learning 0.169s)
               Value function loss: 22.6383
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 238.62
               Mean episode length: 149.87
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 9.84s
                        Total time: 9917.24s
                               ETA: 1036215.8s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1648 steps/s (collection: 9.776s, learning 0.162s)
               Value function loss: 21.4535
                    Surrogate loss: -0.0246
             Mean action noise std: 0.77
                       Mean reward: 242.75
               Mean episode length: 148.76
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 9.94s
                        Total time: 9927.18s
                               ETA: 1036150.7s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1713 steps/s (collection: 9.394s, learning 0.166s)
               Value function loss: 22.0376
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 231.61
               Mean episode length: 148.79
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 9.56s
                        Total time: 9936.74s
                               ETA: 1036046.4s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.300s, learning 0.166s)
               Value function loss: 23.3338
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 243.25
               Mean episode length: 149.37
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 9.47s
                        Total time: 9946.21s
                               ETA: 1035932.4s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1680 steps/s (collection: 9.578s, learning 0.173s)
               Value function loss: 21.8396
                    Surrogate loss: -0.0227
             Mean action noise std: 0.77
                       Mean reward: 251.07
               Mean episode length: 149.18
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 9.75s
                        Total time: 9955.96s
                               ETA: 1035848.4s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1672 steps/s (collection: 9.636s, learning 0.162s)
               Value function loss: 26.8867
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 241.84
               Mean episode length: 148.23
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 9.80s
                        Total time: 9965.76s
                               ETA: 1035769.3s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.628s, learning 0.161s)
               Value function loss: 30.8519
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 242.73
               Mean episode length: 149.78
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 9.79s
                        Total time: 9975.54s
                               ETA: 1035689.4s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1708 steps/s (collection: 9.424s, learning 0.166s)
               Value function loss: 30.8328
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 245.71
               Mean episode length: 149.16
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 9.59s
                        Total time: 9985.13s
                               ETA: 1035589.0s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1714 steps/s (collection: 9.397s, learning 0.159s)
               Value function loss: 23.6857
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 243.60
               Mean episode length: 148.37
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 9.56s
                        Total time: 9994.69s
                               ETA: 1035485.3s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1744 steps/s (collection: 9.223s, learning 0.167s)
               Value function loss: 29.2586
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 246.77
               Mean episode length: 149.42
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 9.39s
                        Total time: 10004.08s
                               ETA: 1035364.7s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1681 steps/s (collection: 9.583s, learning 0.159s)
               Value function loss: 21.3905
                    Surrogate loss: -0.0228
             Mean action noise std: 0.77
                       Mean reward: 256.10
               Mean episode length: 148.80
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 9.74s
                        Total time: 10013.82s
                               ETA: 1035280.6s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.737s, learning 0.161s)
               Value function loss: 22.3386
                    Surrogate loss: -0.0230
             Mean action noise std: 0.77
                       Mean reward: 245.61
               Mean episode length: 150.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 9.90s
                        Total time: 10023.72s
                               ETA: 1035212.9s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1730 steps/s (collection: 9.302s, learning 0.166s)
               Value function loss: 23.9062
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 242.26
               Mean episode length: 149.33
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 9.47s
                        Total time: 10033.19s
                               ETA: 1035100.9s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1661 steps/s (collection: 9.696s, learning 0.163s)
               Value function loss: 26.8115
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 256.23
               Mean episode length: 149.44
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 9.86s
                        Total time: 10043.05s
                               ETA: 1035029.3s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1710 steps/s (collection: 9.419s, learning 0.159s)
               Value function loss: 27.1720
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 251.11
               Mean episode length: 148.04
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 9.58s
                        Total time: 10052.62s
                               ETA: 1034929.1s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1636 steps/s (collection: 9.849s, learning 0.166s)
               Value function loss: 30.8175
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 257.22
               Mean episode length: 150.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 10.01s
                        Total time: 10062.64s
                               ETA: 1034873.9s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.509s, learning 0.163s)
               Value function loss: 27.5596
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 241.72
               Mean episode length: 149.36
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 9.67s
                        Total time: 10072.31s
                               ETA: 1034783.6s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1665 steps/s (collection: 9.674s, learning 0.164s)
               Value function loss: 24.9910
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 241.81
               Mean episode length: 149.52
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 9.84s
                        Total time: 10082.15s
                               ETA: 1034710.5s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.519s, learning 0.158s)
               Value function loss: 25.2480
                    Surrogate loss: -0.0220
             Mean action noise std: 0.77
                       Mean reward: 251.50
               Mean episode length: 148.66
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 9.68s
                        Total time: 10091.83s
                               ETA: 1034621.0s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1762 steps/s (collection: 9.138s, learning 0.158s)
               Value function loss: 26.0903
                    Surrogate loss: -0.0243
             Mean action noise std: 0.77
                       Mean reward: 245.22
               Mean episode length: 148.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 9.30s
                        Total time: 10101.12s
                               ETA: 1034492.7s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1685 steps/s (collection: 9.558s, learning 0.164s)
               Value function loss: 23.8294
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 238.97
               Mean episode length: 149.83
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 9.72s
                        Total time: 10110.84s
                               ETA: 1034408.2s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1660 steps/s (collection: 9.699s, learning 0.170s)
               Value function loss: 26.5522
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 255.80
               Mean episode length: 150.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 9.87s
                        Total time: 10120.71s
                               ETA: 1034338.8s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1644 steps/s (collection: 9.800s, learning 0.164s)
               Value function loss: 28.2951
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 248.72
               Mean episode length: 149.10
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 9.96s
                        Total time: 10130.68s
                               ETA: 1034279.3s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1720 steps/s (collection: 9.363s, learning 0.161s)
               Value function loss: 31.2459
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 251.71
               Mean episode length: 148.85
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 9.52s
                        Total time: 10140.20s
                               ETA: 1034175.1s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1618 steps/s (collection: 9.955s, learning 0.167s)
               Value function loss: 35.2199
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 247.34
               Mean episode length: 149.06
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 10.12s
                        Total time: 10150.32s
                               ETA: 1034132.0s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.612s, learning 0.177s)
               Value function loss: 30.3046
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 254.97
               Mean episode length: 148.32
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 9.79s
                        Total time: 10160.11s
                               ETA: 1034055.1s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1666 steps/s (collection: 9.672s, learning 0.160s)
               Value function loss: 25.5139
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 250.52
               Mean episode length: 149.16
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 9.83s
                        Total time: 10169.94s
                               ETA: 1033982.7s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1745 steps/s (collection: 9.213s, learning 0.174s)
               Value function loss: 25.6344
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 253.14
               Mean episode length: 148.46
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 9.39s
                        Total time: 10179.33s
                               ETA: 1033865.2s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1723 steps/s (collection: 9.339s, learning 0.168s)
               Value function loss: 26.4422
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 249.37
               Mean episode length: 149.70
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 9.51s
                        Total time: 10188.84s
                               ETA: 1033760.0s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1655 steps/s (collection: 9.733s, learning 0.161s)
               Value function loss: 26.4684
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 249.61
               Mean episode length: 150.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 9.89s
                        Total time: 10198.73s
                               ETA: 1033694.3s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1618 steps/s (collection: 9.967s, learning 0.157s)
               Value function loss: 30.2768
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 259.74
               Mean episode length: 149.38
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 10.12s
                        Total time: 10208.86s
                               ETA: 1033652.0s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1703 steps/s (collection: 9.458s, learning 0.159s)
               Value function loss: 32.5132
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 262.53
               Mean episode length: 149.54
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 9.62s
                        Total time: 10218.47s
                               ETA: 1033558.5s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1675 steps/s (collection: 9.613s, learning 0.167s)
               Value function loss: 34.2862
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 243.97
               Mean episode length: 149.11
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 9.78s
                        Total time: 10228.25s
                               ETA: 1033481.7s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1712 steps/s (collection: 9.406s, learning 0.163s)
               Value function loss: 39.1483
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 266.27
               Mean episode length: 149.88
                  Mean reward/step: 1.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 9.57s
                        Total time: 10237.82s
                               ETA: 1033383.5s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1709 steps/s (collection: 9.402s, learning 0.180s)
               Value function loss: 38.9084
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 253.14
               Mean episode length: 149.64
                  Mean reward/step: 1.73
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 9.58s
                        Total time: 10247.41s
                               ETA: 1033287.0s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1677 steps/s (collection: 9.603s, learning 0.165s)
               Value function loss: 53.1972
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 266.69
               Mean episode length: 148.01
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 9.77s
                        Total time: 10257.17s
                               ETA: 1033209.3s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1731 steps/s (collection: 9.302s, learning 0.160s)
               Value function loss: 39.9009
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 258.61
               Mean episode length: 148.23
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 9.46s
                        Total time: 10266.64s
                               ETA: 1033101.0s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1617 steps/s (collection: 9.965s, learning 0.164s)
               Value function loss: 44.2132
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 264.92
               Mean episode length: 149.54
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 10.13s
                        Total time: 10276.76s
                               ETA: 1033060.0s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.935s, learning 0.171s)
               Value function loss: 49.4574
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 250.60
               Mean episode length: 150.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 9.11s
                        Total time: 10285.87s
                               ETA: 1032916.3s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1712 steps/s (collection: 9.407s, learning 0.163s)
               Value function loss: 49.9407
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 263.29
               Mean episode length: 150.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 9.57s
                        Total time: 10295.44s
                               ETA: 1032819.3s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1704 steps/s (collection: 9.445s, learning 0.166s)
               Value function loss: 47.2375
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 261.00
               Mean episode length: 149.13
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 9.61s
                        Total time: 10305.05s
                               ETA: 1032726.8s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1689 steps/s (collection: 9.527s, learning 0.173s)
               Value function loss: 38.1728
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 239.01
               Mean episode length: 150.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 9.70s
                        Total time: 10314.75s
                               ETA: 1032643.2s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1762 steps/s (collection: 9.123s, learning 0.174s)
               Value function loss: 38.3214
                    Surrogate loss: -0.0198
             Mean action noise std: 0.77
                       Mean reward: 245.12
               Mean episode length: 149.82
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 9.30s
                        Total time: 10324.05s
                               ETA: 1032519.5s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1693 steps/s (collection: 9.514s, learning 0.163s)
               Value function loss: 48.2514
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 265.32
               Mean episode length: 149.42
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 9.68s
                        Total time: 10333.72s
                               ETA: 1032434.0s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1710 steps/s (collection: 9.404s, learning 0.172s)
               Value function loss: 37.5166
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 246.27
               Mean episode length: 149.22
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 9.58s
                        Total time: 10343.30s
                               ETA: 1032338.6s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1653 steps/s (collection: 9.749s, learning 0.159s)
               Value function loss: 39.6786
                    Surrogate loss: 0.0067
             Mean action noise std: 0.77
                       Mean reward: 238.06
               Mean episode length: 149.24
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 9.91s
                        Total time: 10353.21s
                               ETA: 1032276.5s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1742 steps/s (collection: 9.237s, learning 0.165s)
               Value function loss: 34.0672
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 251.88
               Mean episode length: 149.21
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 9.40s
                        Total time: 10362.61s
                               ETA: 1032164.1s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1752 steps/s (collection: 9.193s, learning 0.158s)
               Value function loss: 34.8571
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 239.83
               Mean episode length: 149.21
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 9.35s
                        Total time: 10371.96s
                               ETA: 1032046.7s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1756 steps/s (collection: 9.168s, learning 0.158s)
               Value function loss: 36.6347
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: 242.87
               Mean episode length: 148.66
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 9.33s
                        Total time: 10381.29s
                               ETA: 1031927.2s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1667 steps/s (collection: 9.656s, learning 0.172s)
               Value function loss: 33.2120
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 255.97
               Mean episode length: 149.96
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 9.83s
                        Total time: 10391.12s
                               ETA: 1031857.7s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1698 steps/s (collection: 9.475s, learning 0.172s)
               Value function loss: 40.2586
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: 240.25
               Mean episode length: 148.68
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 9.65s
                        Total time: 10400.76s
                               ETA: 1031770.4s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1691 steps/s (collection: 9.520s, learning 0.164s)
               Value function loss: 34.7451
                    Surrogate loss: -0.0231
             Mean action noise std: 0.77
                       Mean reward: 259.40
               Mean episode length: 149.79
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 9.68s
                        Total time: 10410.45s
                               ETA: 1031686.9s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1673 steps/s (collection: 9.628s, learning 0.165s)
               Value function loss: 42.8952
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 255.50
               Mean episode length: 149.45
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 9.79s
                        Total time: 10420.24s
                               ETA: 1031614.3s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.468s, learning 0.159s)
               Value function loss: 43.3649
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 233.62
               Mean episode length: 147.50
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 9.63s
                        Total time: 10429.87s
                               ETA: 1031525.4s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1631 steps/s (collection: 9.878s, learning 0.166s)
               Value function loss: 49.3752
                    Surrogate loss: -0.0207
             Mean action noise std: 0.77
                       Mean reward: 231.34
               Mean episode length: 146.86
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 10.04s
                        Total time: 10439.91s
                               ETA: 1031477.8s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.254s, learning 0.162s)
               Value function loss: 69.2720
                    Surrogate loss: -0.0220
             Mean action noise std: 0.77
                       Mean reward: 240.99
               Mean episode length: 148.42
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 9.42s
                        Total time: 10449.33s
                               ETA: 1031368.5s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.587s, learning 0.168s)
               Value function loss: 74.1185
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 244.92
               Mean episode length: 149.76
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 9.75s
                        Total time: 10459.08s
                               ETA: 1031292.6s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.442s, learning 0.162s)
               Value function loss: 42.9668
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 231.81
               Mean episode length: 150.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 9.60s
                        Total time: 10468.69s
                               ETA: 1031202.1s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1610 steps/s (collection: 10.001s, learning 0.173s)
               Value function loss: 43.1024
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 229.23
               Mean episode length: 149.74
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 10.17s
                        Total time: 10478.86s
                               ETA: 1031167.8s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.171s, learning 0.190s)
               Value function loss: 36.1164
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 231.43
               Mean episode length: 149.08
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 9.36s
                        Total time: 10488.22s
                               ETA: 1031053.5s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.774s, learning 0.172s)
               Value function loss: 32.9436
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 223.52
               Mean episode length: 149.28
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 9.95s
                        Total time: 10498.17s
                               ETA: 1030997.0s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1623 steps/s (collection: 9.928s, learning 0.161s)
               Value function loss: 31.1653
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 227.92
               Mean episode length: 149.45
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 10.09s
                        Total time: 10508.26s
                               ETA: 1030954.7s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.567s, learning 0.162s)
               Value function loss: 37.6974
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 227.86
               Mean episode length: 149.69
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 9.73s
                        Total time: 10517.98s
                               ETA: 1030877.0s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.528s, learning 0.160s)
               Value function loss: 30.9299
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 227.27
               Mean episode length: 148.97
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 9.69s
                        Total time: 10527.67s
                               ETA: 1030795.5s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.398s, learning 0.162s)
               Value function loss: 33.2502
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 226.71
               Mean episode length: 147.88
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 9.56s
                        Total time: 10537.23s
                               ETA: 1030701.7s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.452s, learning 0.160s)
               Value function loss: 34.6788
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: 225.33
               Mean episode length: 147.74
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 9.61s
                        Total time: 10546.84s
                               ETA: 1030613.0s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.480s, learning 0.168s)
               Value function loss: 30.1405
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 218.92
               Mean episode length: 148.10
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 9.65s
                        Total time: 10556.49s
                               ETA: 1030528.1s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.647s, learning 0.168s)
               Value function loss: 35.4050
                    Surrogate loss: -0.0212
             Mean action noise std: 0.77
                       Mean reward: 222.11
               Mean episode length: 147.24
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 9.82s
                        Total time: 10566.31s
                               ETA: 1030459.6s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.544s, learning 0.169s)
               Value function loss: 38.8193
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 232.30
               Mean episode length: 148.63
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 9.71s
                        Total time: 10576.02s
                               ETA: 1030381.2s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.977s, learning 0.165s)
               Value function loss: 41.7980
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: 232.32
               Mean episode length: 149.70
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 9.14s
                        Total time: 10585.16s
                               ETA: 1030247.5s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.265s, learning 0.166s)
               Value function loss: 37.3447
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 223.68
               Mean episode length: 149.25
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 9.43s
                        Total time: 10594.59s
                               ETA: 1030142.0s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.346s, learning 0.162s)
               Value function loss: 42.8166
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 219.01
               Mean episode length: 147.96
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 9.51s
                        Total time: 10604.10s
                               ETA: 1030044.3s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.473s, learning 0.167s)
               Value function loss: 42.3895
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 215.44
               Mean episode length: 148.58
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 9.64s
                        Total time: 10613.74s
                               ETA: 1029959.5s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.475s, learning 0.166s)
               Value function loss: 49.7193
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 227.23
               Mean episode length: 148.69
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 9.64s
                        Total time: 10623.38s
                               ETA: 1029875.1s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.355s, learning 0.165s)
               Value function loss: 47.8419
                    Surrogate loss: -0.0232
             Mean action noise std: 0.77
                       Mean reward: 239.50
               Mean episode length: 148.43
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 9.52s
                        Total time: 10632.90s
                               ETA: 1029778.9s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.487s, learning 0.160s)
               Value function loss: 48.2285
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 222.49
               Mean episode length: 148.91
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 9.65s
                        Total time: 10642.55s
                               ETA: 1029695.2s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.530s, learning 0.161s)
               Value function loss: 49.7430
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 257.21
               Mean episode length: 148.58
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 9.69s
                        Total time: 10652.24s
                               ETA: 1029616.1s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.613s, learning 0.164s)
               Value function loss: 54.3257
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 241.62
               Mean episode length: 149.24
                  Mean reward/step: 1.65
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 9.78s
                        Total time: 10662.02s
                               ETA: 1029545.2s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.541s, learning 0.170s)
               Value function loss: 53.9995
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 251.64
               Mean episode length: 149.50
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 9.71s
                        Total time: 10671.73s
                               ETA: 1029468.1s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.410s, learning 0.164s)
               Value function loss: 52.9994
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 234.77
               Mean episode length: 148.99
                  Mean reward/step: 1.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 9.57s
                        Total time: 10681.30s
                               ETA: 1029378.0s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.197s, learning 0.165s)
               Value function loss: 51.8966
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 235.55
               Mean episode length: 147.86
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 9.36s
                        Total time: 10690.66s
                               ETA: 1029267.6s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.382s, learning 0.161s)
               Value function loss: 52.0778
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 239.72
               Mean episode length: 150.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 9.54s
                        Total time: 10700.21s
                               ETA: 1029174.8s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.604s, learning 0.169s)
               Value function loss: 45.1670
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 249.10
               Mean episode length: 148.07
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 9.77s
                        Total time: 10709.98s
                               ETA: 1029104.3s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.296s, learning 0.166s)
               Value function loss: 57.2009
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 254.78
               Mean episode length: 147.87
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 9.46s
                        Total time: 10719.44s
                               ETA: 1029004.0s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.488s, learning 0.167s)
               Value function loss: 54.7965
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 250.32
               Mean episode length: 149.24
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 9.66s
                        Total time: 10729.10s
                               ETA: 1028922.5s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.927s, learning 0.171s)
               Value function loss: 58.2013
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 268.85
               Mean episode length: 147.06
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 9.10s
                        Total time: 10738.19s
                               ETA: 1028787.7s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.595s, learning 0.159s)
               Value function loss: 73.2317
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 274.30
               Mean episode length: 148.58
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 9.75s
                        Total time: 10747.95s
                               ETA: 1028715.9s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.511s, learning 0.168s)
               Value function loss: 77.2835
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 284.70
               Mean episode length: 149.08
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 9.68s
                        Total time: 10757.63s
                               ETA: 1028637.1s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.326s, learning 0.163s)
               Value function loss: 86.3410
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 293.18
               Mean episode length: 149.58
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 9.49s
                        Total time: 10767.12s
                               ETA: 1028540.3s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.165s)
               Value function loss: 63.2901
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 287.50
               Mean episode length: 149.40
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 9.69s
                        Total time: 10776.80s
                               ETA: 1028462.4s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.510s, learning 0.165s)
               Value function loss: 83.2183
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 307.72
               Mean episode length: 149.56
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 9.67s
                        Total time: 10786.48s
                               ETA: 1028383.6s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.408s, learning 0.159s)
               Value function loss: 66.2943
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 274.84
               Mean episode length: 150.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 9.57s
                        Total time: 10796.05s
                               ETA: 1028294.7s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.621s, learning 0.162s)
               Value function loss: 74.6650
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 290.91
               Mean episode length: 149.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 9.78s
                        Total time: 10805.83s
                               ETA: 1028226.5s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.470s, learning 0.164s)
               Value function loss: 79.6759
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 318.03
               Mean episode length: 148.88
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 9.63s
                        Total time: 10815.46s
                               ETA: 1028144.2s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.854s, learning 0.169s)
               Value function loss: 68.5107
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 276.18
               Mean episode length: 147.73
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 10.02s
                        Total time: 10825.49s
                               ETA: 1028099.1s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1602 steps/s (collection: 10.042s, learning 0.179s)
               Value function loss: 70.4317
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 300.95
               Mean episode length: 150.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 10.22s
                        Total time: 10835.71s
                               ETA: 1028072.7s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.615s, learning 0.165s)
               Value function loss: 84.2376
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 290.82
               Mean episode length: 147.12
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 9.78s
                        Total time: 10845.49s
                               ETA: 1028004.6s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.355s, learning 0.174s)
               Value function loss: 69.7135
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 300.59
               Mean episode length: 148.25
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 9.53s
                        Total time: 10855.02s
                               ETA: 1027912.9s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.529s, learning 0.160s)
               Value function loss: 68.7872
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 299.33
               Mean episode length: 148.75
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 9.69s
                        Total time: 10864.71s
                               ETA: 1027836.4s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.597s, learning 0.161s)
               Value function loss: 73.3796
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 296.15
               Mean episode length: 147.82
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 9.76s
                        Total time: 10874.46s
                               ETA: 1027766.6s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.707s, learning 0.163s)
               Value function loss: 59.7881
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 302.55
               Mean episode length: 150.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 9.87s
                        Total time: 10884.33s
                               ETA: 1027707.5s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.438s, learning 0.158s)
               Value function loss: 69.0573
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 298.90
               Mean episode length: 147.49
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 9.60s
                        Total time: 10893.93s
                               ETA: 1027622.6s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.534s, learning 0.163s)
               Value function loss: 65.0419
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 298.27
               Mean episode length: 148.65
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 9.70s
                        Total time: 10903.63s
                               ETA: 1027547.3s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.534s, learning 0.163s)
               Value function loss: 48.6410
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 289.10
               Mean episode length: 147.81
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 9.70s
                        Total time: 10913.32s
                               ETA: 1027472.2s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.871s, learning 0.162s)
               Value function loss: 53.8337
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 287.08
               Mean episode length: 148.07
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 10.03s
                        Total time: 10923.36s
                               ETA: 1027428.8s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.597s, learning 0.168s)
               Value function loss: 51.5920
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 284.06
               Mean episode length: 147.47
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 9.76s
                        Total time: 10933.12s
                               ETA: 1027360.2s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.514s, learning 0.168s)
               Value function loss: 68.9300
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 310.07
               Mean episode length: 150.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 9.68s
                        Total time: 10942.80s
                               ETA: 1027284.1s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.440s, learning 0.161s)
               Value function loss: 68.0198
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 313.49
               Mean episode length: 146.93
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 9.60s
                        Total time: 10952.40s
                               ETA: 1027200.4s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.761s, learning 0.171s)
               Value function loss: 81.5218
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 319.29
               Mean episode length: 150.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 9.93s
                        Total time: 10962.33s
                               ETA: 1027147.9s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.496s, learning 0.172s)
               Value function loss: 72.1572
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 315.26
               Mean episode length: 148.40
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 9.67s
                        Total time: 10972.00s
                               ETA: 1027070.8s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.461s, learning 0.168s)
               Value function loss: 71.9302
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 317.33
               Mean episode length: 148.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 9.63s
                        Total time: 10981.63s
                               ETA: 1026990.1s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.468s, learning 0.169s)
               Value function loss: 64.7922
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 293.77
               Mean episode length: 146.58
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 9.64s
                        Total time: 10991.27s
                               ETA: 1026910.3s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.870s, learning 0.166s)
               Value function loss: 65.4695
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 299.68
               Mean episode length: 145.60
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 10.04s
                        Total time: 11001.30s
                               ETA: 1026867.9s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.237s, learning 0.185s)
               Value function loss: 60.0040
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 309.24
               Mean episode length: 148.83
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 9.42s
                        Total time: 11010.73s
                               ETA: 1026768.4s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.356s, learning 0.166s)
               Value function loss: 54.0121
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 289.87
               Mean episode length: 149.08
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 9.52s
                        Total time: 11020.25s
                               ETA: 1026678.2s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.736s, learning 0.170s)
               Value function loss: 74.2555
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 321.16
               Mean episode length: 148.93
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 9.91s
                        Total time: 11030.15s
                               ETA: 1026624.0s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.390s, learning 0.167s)
               Value function loss: 62.0800
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 294.20
               Mean episode length: 147.64
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 9.56s
                        Total time: 11039.71s
                               ETA: 1026537.4s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.476s, learning 0.166s)
               Value function loss: 69.2178
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 310.90
               Mean episode length: 145.87
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 9.64s
                        Total time: 11049.35s
                               ETA: 1026458.8s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.753s, learning 0.170s)
               Value function loss: 57.5544
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 302.30
               Mean episode length: 147.36
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 9.92s
                        Total time: 11059.28s
                               ETA: 1026406.6s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1622 steps/s (collection: 9.933s, learning 0.163s)
               Value function loss: 54.6897
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 309.54
               Mean episode length: 147.63
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 10.10s
                        Total time: 11069.37s
                               ETA: 1026370.5s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.610s, learning 0.162s)
               Value function loss: 57.4194
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 296.54
               Mean episode length: 144.88
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 9.77s
                        Total time: 11079.14s
                               ETA: 1026304.3s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.595s, learning 0.163s)
               Value function loss: 51.3374
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 305.13
               Mean episode length: 148.72
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 9.76s
                        Total time: 11088.90s
                               ETA: 1026236.9s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.628s, learning 0.161s)
               Value function loss: 49.7350
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 294.61
               Mean episode length: 148.66
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 9.79s
                        Total time: 11098.69s
                               ETA: 1026172.5s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.547s, learning 0.161s)
               Value function loss: 53.2043
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 290.27
               Mean episode length: 148.48
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 9.71s
                        Total time: 11108.40s
                               ETA: 1026100.8s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.687s, learning 0.164s)
               Value function loss: 43.2917
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 282.50
               Mean episode length: 150.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 9.85s
                        Total time: 11118.25s
                               ETA: 1026042.3s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.563s, learning 0.161s)
               Value function loss: 52.0042
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 277.57
               Mean episode length: 149.25
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 9.72s
                        Total time: 11127.97s
                               ETA: 1025972.2s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.475s, learning 0.165s)
               Value function loss: 48.1051
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 265.92
               Mean episode length: 146.90
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 9.64s
                        Total time: 11137.61s
                               ETA: 1025894.5s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.600s, learning 0.159s)
               Value function loss: 67.4082
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 288.02
               Mean episode length: 149.14
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 9.76s
                        Total time: 11147.37s
                               ETA: 1025827.9s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.267s, learning 0.170s)
               Value function loss: 72.8331
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 289.28
               Mean episode length: 149.07
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 9.44s
                        Total time: 11156.81s
                               ETA: 1025731.8s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.600s, learning 0.159s)
               Value function loss: 75.6930
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 304.29
               Mean episode length: 148.72
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 9.76s
                        Total time: 11166.57s
                               ETA: 1025665.4s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.431s, learning 0.164s)
               Value function loss: 74.8890
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 300.34
               Mean episode length: 148.84
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 9.60s
                        Total time: 11176.16s
                               ETA: 1025584.1s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.514s, learning 0.162s)
               Value function loss: 77.5283
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 309.07
               Mean episode length: 148.45
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 9.68s
                        Total time: 11185.84s
                               ETA: 1025510.2s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.451s, learning 0.160s)
               Value function loss: 69.9138
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 298.92
               Mean episode length: 148.53
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 9.61s
                        Total time: 11195.45s
                               ETA: 1025430.7s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.479s, learning 0.180s)
               Value function loss: 75.2086
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 301.09
               Mean episode length: 149.71
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 9.66s
                        Total time: 11205.11s
                               ETA: 1025355.6s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.638s, learning 0.157s)
               Value function loss: 74.8554
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 304.53
               Mean episode length: 148.80
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 9.79s
                        Total time: 11214.90s
                               ETA: 1025293.1s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.210s, learning 0.159s)
               Value function loss: 71.0649
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 313.13
               Mean episode length: 150.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 9.37s
                        Total time: 11224.27s
                               ETA: 1025191.8s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.812s, learning 0.163s)
               Value function loss: 72.1759
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 322.92
               Mean episode length: 149.21
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 9.98s
                        Total time: 11234.25s
                               ETA: 1025145.9s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.614s, learning 0.164s)
               Value function loss: 71.3222
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 315.75
               Mean episode length: 149.03
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 9.78s
                        Total time: 11244.03s
                               ETA: 1025082.1s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.599s, learning 0.158s)
               Value function loss: 66.2515
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 319.03
               Mean episode length: 150.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 9.76s
                        Total time: 11253.78s
                               ETA: 1025016.5s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.680s, learning 0.162s)
               Value function loss: 60.8198
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 309.85
               Mean episode length: 150.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 9.84s
                        Total time: 11263.63s
                               ETA: 1024958.8s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.703s, learning 0.164s)
               Value function loss: 56.2934
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 293.96
               Mean episode length: 149.74
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 9.87s
                        Total time: 11273.49s
                               ETA: 1024903.5s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.507s, learning 0.168s)
               Value function loss: 66.8937
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 320.37
               Mean episode length: 150.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 9.67s
                        Total time: 11283.17s
                               ETA: 1024830.7s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.588s, learning 0.160s)
               Value function loss: 74.9724
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 309.45
               Mean episode length: 150.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 9.75s
                        Total time: 11292.91s
                               ETA: 1024764.7s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.416s, learning 0.166s)
               Value function loss: 79.8441
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 320.96
               Mean episode length: 150.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 9.58s
                        Total time: 11302.50s
                               ETA: 1024683.7s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.582s, learning 0.162s)
               Value function loss: 75.3557
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 304.24
               Mean episode length: 150.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 9.74s
                        Total time: 11312.24s
                               ETA: 1024617.6s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.328s, learning 0.162s)
               Value function loss: 71.7949
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 313.21
               Mean episode length: 150.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 9.49s
                        Total time: 11321.73s
                               ETA: 1024528.5s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.295s, learning 0.164s)
               Value function loss: 78.3547
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 326.43
               Mean episode length: 150.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 9.46s
                        Total time: 11331.19s
                               ETA: 1024436.8s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1762 steps/s (collection: 9.138s, learning 0.158s)
               Value function loss: 83.3751
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 337.16
               Mean episode length: 149.66
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 9.30s
                        Total time: 11340.48s
                               ETA: 1024330.6s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.711s, learning 0.160s)
               Value function loss: 94.1406
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 332.23
               Mean episode length: 149.60
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 9.87s
                        Total time: 11350.36s
                               ETA: 1024276.4s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.532s, learning 0.163s)
               Value function loss: 86.1708
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 314.24
               Mean episode length: 150.00
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 9.70s
                        Total time: 11360.05s
                               ETA: 1024206.5s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.637s, learning 0.158s)
               Value function loss: 80.6189
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 318.00
               Mean episode length: 150.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 9.79s
                        Total time: 11369.85s
                               ETA: 1024145.6s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.562s, learning 0.170s)
               Value function loss: 60.4828
                    Surrogate loss: -0.0246
             Mean action noise std: 0.77
                       Mean reward: 303.23
               Mean episode length: 150.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 9.73s
                        Total time: 11379.58s
                               ETA: 1024079.2s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1787 steps/s (collection: 9.007s, learning 0.162s)
               Value function loss: 81.5561
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 324.93
               Mean episode length: 150.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 9.17s
                        Total time: 11388.75s
                               ETA: 1023962.2s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.439s, learning 0.174s)
               Value function loss: 78.8738
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 307.62
               Mean episode length: 148.54
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 9.61s
                        Total time: 11398.36s
                               ETA: 1023885.3s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.240s, learning 0.163s)
               Value function loss: 71.0954
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 346.77
               Mean episode length: 149.02
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 9.40s
                        Total time: 11407.76s
                               ETA: 1023789.8s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.486s, learning 0.164s)
               Value function loss: 72.4936
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 347.30
               Mean episode length: 150.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 9.65s
                        Total time: 11417.41s
                               ETA: 1023716.5s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.657s, learning 0.159s)
               Value function loss: 71.6190
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 325.60
               Mean episode length: 150.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 9.82s
                        Total time: 11427.23s
                               ETA: 1023658.1s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.508s, learning 0.166s)
               Value function loss: 71.0081
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 343.31
               Mean episode length: 150.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 9.67s
                        Total time: 11436.90s
                               ETA: 1023587.2s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.471s, learning 0.160s)
               Value function loss: 64.6078
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 293.59
               Mean episode length: 150.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 9.63s
                        Total time: 11446.53s
                               ETA: 1023512.6s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.552s, learning 0.169s)
               Value function loss: 70.8249
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 316.76
               Mean episode length: 148.80
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 9.72s
                        Total time: 11456.25s
                               ETA: 1023446.1s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.929s, learning 0.165s)
               Value function loss: 85.4076
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 345.53
               Mean episode length: 147.64
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 12.09s
                        Total time: 11468.35s
                               ETA: 1023591.5s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 873 steps/s (collection: 18.570s, learning 0.177s)
               Value function loss: 79.6635
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 350.23
               Mean episode length: 150.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 18.75s
                        Total time: 11487.10s
                               ETA: 1024329.9s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 865 steps/s (collection: 18.759s, learning 0.166s)
               Value function loss: 96.0230
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 345.82
               Mean episode length: 149.78
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 18.92s
                        Total time: 11506.02s
                               ETA: 1025082.7s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 865 steps/s (collection: 18.769s, learning 0.166s)
               Value function loss: 88.7911
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 323.47
               Mean episode length: 148.68
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 18.94s
                        Total time: 11524.96s
                               ETA: 1025835.2s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 884 steps/s (collection: 18.363s, learning 0.170s)
               Value function loss: 76.0601
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 317.29
               Mean episode length: 149.99
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 18.53s
                        Total time: 11543.49s
                               ETA: 1026550.4s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 881 steps/s (collection: 18.413s, learning 0.173s)
               Value function loss: 82.3275
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 306.63
               Mean episode length: 149.34
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 18.59s
                        Total time: 11562.07s
                               ETA: 1027269.0s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 865 steps/s (collection: 18.770s, learning 0.166s)
               Value function loss: 86.0441
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 324.99
               Mean episode length: 148.84
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 18.94s
                        Total time: 11581.01s
                               ETA: 1028017.3s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 868 steps/s (collection: 18.701s, learning 0.164s)
               Value function loss: 94.3180
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 319.04
               Mean episode length: 149.09
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 18.86s
                        Total time: 11599.87s
                               ETA: 1028758.0s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.401s, learning 0.158s)
               Value function loss: 107.0662
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 322.58
               Mean episode length: 149.41
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 18.56s
                        Total time: 11618.43s
                               ETA: 1029470.2s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 871 steps/s (collection: 18.626s, learning 0.174s)
               Value function loss: 95.2347
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 321.51
               Mean episode length: 150.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 18.80s
                        Total time: 11637.23s
                               ETA: 1030202.5s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 851 steps/s (collection: 19.083s, learning 0.166s)
               Value function loss: 83.2867
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 343.77
               Mean episode length: 150.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 19.25s
                        Total time: 11656.48s
                               ETA: 1030973.1s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.401s, learning 0.173s)
               Value function loss: 86.2058
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 314.10
               Mean episode length: 150.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 18.57s
                        Total time: 11675.06s
                               ETA: 1031682.7s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 886 steps/s (collection: 18.314s, learning 0.177s)
               Value function loss: 76.9233
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 312.47
               Mean episode length: 149.95
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 18.49s
                        Total time: 11693.55s
                               ETA: 1032383.7s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 859 steps/s (collection: 18.891s, learning 0.174s)
               Value function loss: 74.9625
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 309.69
               Mean episode length: 150.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 19.07s
                        Total time: 11712.61s
                               ETA: 1033134.0s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 856 steps/s (collection: 18.956s, learning 0.166s)
               Value function loss: 72.8638
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 304.04
               Mean episode length: 150.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 19.12s
                        Total time: 11731.74s
                               ETA: 1033888.0s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.520s, learning 0.170s)
               Value function loss: 72.1634
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 310.05
               Mean episode length: 149.52
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 18.69s
                        Total time: 11750.43s
                               ETA: 1034602.5s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 885 steps/s (collection: 18.336s, learning 0.173s)
               Value function loss: 76.3758
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 314.42
               Mean episode length: 150.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 18.51s
                        Total time: 11768.93s
                               ETA: 1035299.8s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 855 steps/s (collection: 18.968s, learning 0.173s)
               Value function loss: 76.0417
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 313.66
               Mean episode length: 150.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 19.14s
                        Total time: 11788.08s
                               ETA: 1036051.4s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 854 steps/s (collection: 19.002s, learning 0.163s)
               Value function loss: 85.6665
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 312.06
               Mean episode length: 150.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 19.16s
                        Total time: 11807.24s
                               ETA: 1036803.7s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 871 steps/s (collection: 18.645s, learning 0.164s)
               Value function loss: 71.9038
                    Surrogate loss: -0.0225
             Mean action noise std: 0.77
                       Mean reward: 314.88
               Mean episode length: 149.81
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 18.81s
                        Total time: 11826.05s
                               ETA: 1037523.5s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 890 steps/s (collection: 18.228s, learning 0.173s)
               Value function loss: 72.8937
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 302.05
               Mean episode length: 150.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 18.40s
                        Total time: 11844.45s
                               ETA: 1038206.2s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 856 steps/s (collection: 18.972s, learning 0.163s)
               Value function loss: 75.4030
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 296.65
               Mean episode length: 150.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 19.13s
                        Total time: 11863.59s
                               ETA: 1038951.8s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.572s, learning 0.172s)
               Value function loss: 74.6363
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 278.46
               Mean episode length: 150.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 18.74s
                        Total time: 11882.33s
                               ETA: 1039661.9s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.510s, learning 0.163s)
               Value function loss: 88.4344
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 312.71
               Mean episode length: 150.00
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 18.67s
                        Total time: 11901.00s
                               ETA: 1040364.6s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 865 steps/s (collection: 18.766s, learning 0.174s)
               Value function loss: 93.3504
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 300.61
               Mean episode length: 149.88
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 18.94s
                        Total time: 11919.94s
                               ETA: 1041089.2s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 861 steps/s (collection: 18.854s, learning 0.169s)
               Value function loss: 97.0522
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 314.01
               Mean episode length: 148.60
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 19.02s
                        Total time: 11938.97s
                               ETA: 1041819.8s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 863 steps/s (collection: 18.814s, learning 0.169s)
               Value function loss: 88.9763
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 311.36
               Mean episode length: 150.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 18.98s
                        Total time: 11957.95s
                               ETA: 1042545.6s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.579s, learning 0.164s)
               Value function loss: 92.6950
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 313.14
               Mean episode length: 150.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 18.74s
                        Total time: 11976.69s
                               ETA: 1043249.2s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 864 steps/s (collection: 18.789s, learning 0.160s)
               Value function loss: 95.8131
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 315.41
               Mean episode length: 150.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 18.95s
                        Total time: 11995.64s
                               ETA: 1043969.4s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 894 steps/s (collection: 18.160s, learning 0.162s)
               Value function loss: 78.1453
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 307.46
               Mean episode length: 149.37
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 18.32s
                        Total time: 12013.96s
                               ETA: 1044633.8s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.506s, learning 0.172s)
               Value function loss: 72.0865
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 294.85
               Mean episode length: 150.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 18.68s
                        Total time: 12032.64s
                               ETA: 1045327.9s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 867 steps/s (collection: 18.725s, learning 0.164s)
               Value function loss: 83.2388
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 307.06
               Mean episode length: 148.45
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 18.89s
                        Total time: 12051.53s
                               ETA: 1046039.1s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 868 steps/s (collection: 18.699s, learning 0.166s)
               Value function loss: 68.9897
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 275.55
               Mean episode length: 150.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 18.86s
                        Total time: 12070.40s
                               ETA: 1046746.9s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 891 steps/s (collection: 18.222s, learning 0.163s)
               Value function loss: 87.0788
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 331.35
               Mean episode length: 150.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 18.38s
                        Total time: 12088.78s
                               ETA: 1047411.9s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.509s, learning 0.165s)
               Value function loss: 60.9065
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 303.19
               Mean episode length: 150.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 18.67s
                        Total time: 12107.46s
                               ETA: 1048100.7s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.503s, learning 0.162s)
               Value function loss: 78.4679
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 291.62
               Mean episode length: 149.95
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 18.67s
                        Total time: 12126.12s
                               ETA: 1048787.5s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 892 steps/s (collection: 18.195s, learning 0.165s)
               Value function loss: 69.5563
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 320.71
               Mean episode length: 150.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 18.36s
                        Total time: 12144.48s
                               ETA: 1049446.6s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.547s, learning 0.181s)
               Value function loss: 82.2399
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 313.91
               Mean episode length: 150.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 18.73s
                        Total time: 12163.21s
                               ETA: 1050136.4s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1239 steps/s (collection: 13.041s, learning 0.179s)
               Value function loss: 80.3554
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 306.69
               Mean episode length: 150.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 13.22s
                        Total time: 12176.43s
                               ETA: 1050349.8s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.274s, learning 0.182s)
               Value function loss: 67.1905
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 314.84
               Mean episode length: 150.00
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 9.46s
                        Total time: 12185.88s
                               ETA: 1050238.4s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.746s, learning 0.184s)
               Value function loss: 89.5883
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 289.93
               Mean episode length: 150.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 9.93s
                        Total time: 12195.81s
                               ETA: 1050168.0s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.549s, learning 0.174s)
               Value function loss: 73.5027
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 316.00
               Mean episode length: 150.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 9.72s
                        Total time: 12205.54s
                               ETA: 1050079.9s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.660s, learning 0.174s)
               Value function loss: 80.1852
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 333.00
               Mean episode length: 148.99
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 9.83s
                        Total time: 12215.37s
                               ETA: 1050001.5s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.534s, learning 0.181s)
               Value function loss: 88.8461
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 311.62
               Mean episode length: 150.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 9.71s
                        Total time: 12225.09s
                               ETA: 1049912.9s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.738s, learning 0.162s)
               Value function loss: 97.4105
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 322.94
               Mean episode length: 150.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 9.90s
                        Total time: 12234.99s
                               ETA: 1049840.4s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.519s, learning 0.172s)
               Value function loss: 102.4687
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 318.14
               Mean episode length: 150.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 9.69s
                        Total time: 12244.68s
                               ETA: 1049750.1s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.413s, learning 0.175s)
               Value function loss: 98.2930
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 340.07
               Mean episode length: 148.65
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 9.59s
                        Total time: 12254.27s
                               ETA: 1049651.2s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1646 steps/s (collection: 9.786s, learning 0.163s)
               Value function loss: 89.7289
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 323.74
               Mean episode length: 150.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 9.95s
                        Total time: 12264.22s
                               ETA: 1049583.3s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1646 steps/s (collection: 9.786s, learning 0.163s)
               Value function loss: 90.6253
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 362.78
               Mean episode length: 150.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 9.95s
                        Total time: 12274.16s
                               ETA: 1049515.4s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.590s, learning 0.166s)
               Value function loss: 99.0641
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 342.70
               Mean episode length: 150.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 9.76s
                        Total time: 12283.92s
                               ETA: 1049431.1s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.404s, learning 0.164s)
               Value function loss: 81.5351
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 331.02
               Mean episode length: 149.09
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 9.57s
                        Total time: 12293.49s
                               ETA: 1049331.0s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.642s, learning 0.163s)
               Value function loss: 100.0520
                    Surrogate loss: -0.0203
             Mean action noise std: 0.77
                       Mean reward: 344.62
               Mean episode length: 150.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 9.81s
                        Total time: 12303.29s
                               ETA: 1049251.2s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.410s, learning 0.160s)
               Value function loss: 80.7409
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 337.87
               Mean episode length: 150.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 9.57s
                        Total time: 12312.86s
                               ETA: 1049151.5s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.469s, learning 0.164s)
               Value function loss: 82.4726
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 321.69
               Mean episode length: 150.00
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 9.63s
                        Total time: 12322.50s
                               ETA: 1049057.4s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.515s, learning 0.164s)
               Value function loss: 70.2705
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: 294.55
               Mean episode length: 149.27
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 9.68s
                        Total time: 12332.18s
                               ETA: 1048967.2s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.631s, learning 0.166s)
               Value function loss: 79.0553
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 331.19
               Mean episode length: 149.18
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 9.80s
                        Total time: 12341.97s
                               ETA: 1048887.3s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.509s, learning 0.160s)
               Value function loss: 82.3736
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 329.69
               Mean episode length: 150.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 9.67s
                        Total time: 12351.64s
                               ETA: 1048796.7s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.597s, learning 0.159s)
               Value function loss: 79.9024
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 344.45
               Mean episode length: 150.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 9.76s
                        Total time: 12361.40s
                               ETA: 1048713.5s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1610 steps/s (collection: 10.013s, learning 0.160s)
               Value function loss: 94.5867
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 342.84
               Mean episode length: 148.88
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 10.17s
                        Total time: 12371.57s
                               ETA: 1048665.8s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.162s)
               Value function loss: 90.2976
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 347.01
               Mean episode length: 150.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 9.59s
                        Total time: 12381.16s
                               ETA: 1048568.7s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.467s, learning 0.166s)
               Value function loss: 82.7101
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 344.15
               Mean episode length: 150.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 9.63s
                        Total time: 12390.79s
                               ETA: 1048475.5s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.816s, learning 0.164s)
               Value function loss: 94.6190
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 328.93
               Mean episode length: 150.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 9.98s
                        Total time: 12400.77s
                               ETA: 1048411.7s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.110s, learning 0.167s)
               Value function loss: 91.6272
                    Surrogate loss: -0.0193
             Mean action noise std: 0.77
                       Mean reward: 331.68
               Mean episode length: 149.73
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 9.28s
                        Total time: 12410.05s
                               ETA: 1048288.6s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.383s, learning 0.160s)
               Value function loss: 97.1982
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 359.21
               Mean episode length: 150.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 9.54s
                        Total time: 12419.59s
                               ETA: 1048188.2s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.629s, learning 0.164s)
               Value function loss: 96.6680
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 352.22
               Mean episode length: 150.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 9.79s
                        Total time: 12429.39s
                               ETA: 1048109.1s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.174s, learning 0.165s)
               Value function loss: 84.3347
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 324.96
               Mean episode length: 149.85
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 9.34s
                        Total time: 12438.73s
                               ETA: 1047991.8s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.452s, learning 0.163s)
               Value function loss: 92.4529
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 338.88
               Mean episode length: 150.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 9.62s
                        Total time: 12448.34s
                               ETA: 1047897.9s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.651s, learning 0.164s)
               Value function loss: 111.9034
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 343.40
               Mean episode length: 149.50
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 9.82s
                        Total time: 12458.16s
                               ETA: 1047821.1s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.712s, learning 0.165s)
               Value function loss: 109.2538
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 357.67
               Mean episode length: 150.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 9.88s
                        Total time: 12468.03s
                               ETA: 1047749.5s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.317s, learning 0.164s)
               Value function loss: 91.8600
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 350.70
               Mean episode length: 150.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 9.48s
                        Total time: 12477.51s
                               ETA: 1047644.8s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.728s, learning 0.160s)
               Value function loss: 92.3699
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 346.36
               Mean episode length: 150.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 9.89s
                        Total time: 12487.40s
                               ETA: 1047574.3s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.629s, learning 0.160s)
               Value function loss: 97.4951
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 349.25
               Mean episode length: 149.44
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 9.79s
                        Total time: 12497.19s
                               ETA: 1047495.7s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.499s, learning 0.175s)
               Value function loss: 91.7937
                    Surrogate loss: -0.0217
             Mean action noise std: 0.77
                       Mean reward: 371.01
               Mean episode length: 150.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 9.67s
                        Total time: 12506.87s
                               ETA: 1047407.6s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.186s, learning 0.166s)
               Value function loss: 95.7424
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 369.02
               Mean episode length: 149.01
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 9.35s
                        Total time: 12516.22s
                               ETA: 1047292.6s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.715s, learning 0.164s)
               Value function loss: 91.8732
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 353.22
               Mean episode length: 149.01
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 9.88s
                        Total time: 12526.10s
                               ETA: 1047221.9s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.256s, learning 0.171s)
               Value function loss: 94.3954
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 352.05
               Mean episode length: 149.09
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 9.43s
                        Total time: 12535.52s
                               ETA: 1047113.5s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.525s, learning 0.162s)
               Value function loss: 87.7670
                    Surrogate loss: -0.0196
             Mean action noise std: 0.77
                       Mean reward: 371.33
               Mean episode length: 150.00
                  Mean reward/step: 2.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 9.69s
                        Total time: 12545.21s
                               ETA: 1047027.0s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.614s, learning 0.170s)
               Value function loss: 95.9397
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 342.56
               Mean episode length: 148.81
                  Mean reward/step: 2.53
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 9.78s
                        Total time: 12554.99s
                               ETA: 1046948.8s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.428s, learning 0.174s)
               Value function loss: 108.7218
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 362.42
               Mean episode length: 149.85
                  Mean reward/step: 2.54
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 9.60s
                        Total time: 12564.60s
                               ETA: 1046855.4s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.594s, learning 0.165s)
               Value function loss: 81.1998
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 340.72
               Mean episode length: 149.25
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 9.76s
                        Total time: 12574.35s
                               ETA: 1046775.3s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.757s, learning 0.175s)
               Value function loss: 109.9920
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 352.80
               Mean episode length: 149.70
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 9.93s
                        Total time: 12584.29s
                               ETA: 1046709.7s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.554s, learning 0.170s)
               Value function loss: 119.8478
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 356.20
               Mean episode length: 149.96
                  Mean reward/step: 2.54
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 9.72s
                        Total time: 12594.01s
                               ETA: 1046626.8s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.287s, learning 0.176s)
               Value function loss: 116.6249
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 374.04
               Mean episode length: 150.00
                  Mean reward/step: 2.53
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 9.46s
                        Total time: 12603.47s
                               ETA: 1046522.4s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.619s, learning 0.170s)
               Value function loss: 103.5198
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 375.47
               Mean episode length: 149.28
                  Mean reward/step: 2.47
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 9.79s
                        Total time: 12613.26s
                               ETA: 1046445.2s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.489s, learning 0.166s)
               Value function loss: 115.4375
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 374.14
               Mean episode length: 150.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 9.66s
                        Total time: 12622.92s
                               ETA: 1046357.1s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.808s, learning 0.180s)
               Value function loss: 104.1874
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 371.01
               Mean episode length: 149.93
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 9.99s
                        Total time: 12632.90s
                               ETA: 1046296.7s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.541s, learning 0.165s)
               Value function loss: 117.9591
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 353.30
               Mean episode length: 150.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 9.71s
                        Total time: 12642.61s
                               ETA: 1046213.0s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.692s, learning 0.166s)
               Value function loss: 121.3031
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 377.21
               Mean episode length: 149.74
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 9.86s
                        Total time: 12652.47s
                               ETA: 1046142.0s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.460s, learning 0.163s)
               Value function loss: 111.5946
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 371.45
               Mean episode length: 150.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 9.62s
                        Total time: 12662.09s
                               ETA: 1046051.8s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.769s, learning 0.175s)
               Value function loss: 106.3745
                    Surrogate loss: 0.0037
             Mean action noise std: 0.77
                       Mean reward: 372.58
               Mean episode length: 150.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 9.94s
                        Total time: 12672.04s
                               ETA: 1045988.2s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.548s, learning 0.167s)
               Value function loss: 110.4721
                    Surrogate loss: 0.0061
             Mean action noise std: 0.77
                       Mean reward: 375.90
               Mean episode length: 149.70
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 9.72s
                        Total time: 12681.75s
                               ETA: 1045905.7s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.469s, learning 0.161s)
               Value function loss: 101.8320
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 368.57
               Mean episode length: 149.64
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 9.63s
                        Total time: 12691.38s
                               ETA: 1045816.3s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.761s, learning 0.162s)
               Value function loss: 93.0566
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 369.74
               Mean episode length: 149.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 9.92s
                        Total time: 12701.30s
                               ETA: 1045751.2s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.538s, learning 0.169s)
               Value function loss: 92.8992
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 348.55
               Mean episode length: 149.00
                  Mean reward/step: 2.47
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 9.71s
                        Total time: 12711.01s
                               ETA: 1045668.5s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.345s, learning 0.162s)
               Value function loss: 109.9821
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 371.10
               Mean episode length: 148.98
                  Mean reward/step: 2.53
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 9.51s
                        Total time: 12720.52s
                               ETA: 1045569.4s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.586s, learning 0.169s)
               Value function loss: 92.7007
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 360.45
               Mean episode length: 150.00
                  Mean reward/step: 2.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 9.75s
                        Total time: 12730.27s
                               ETA: 1045490.8s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.273s, learning 0.157s)
               Value function loss: 109.5967
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 374.30
               Mean episode length: 149.46
                  Mean reward/step: 2.61
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 9.43s
                        Total time: 12739.70s
                               ETA: 1045385.7s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1617 steps/s (collection: 9.957s, learning 0.172s)
               Value function loss: 97.8417
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 356.70
               Mean episode length: 149.18
                  Mean reward/step: 2.60
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 10.13s
                        Total time: 12749.83s
                               ETA: 1045338.1s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.512s, learning 0.164s)
               Value function loss: 100.4130
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 360.50
               Mean episode length: 149.86
                  Mean reward/step: 2.60
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 9.68s
                        Total time: 12759.51s
                               ETA: 1045253.4s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.367s, learning 0.159s)
               Value function loss: 120.4210
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 375.82
               Mean episode length: 148.92
                  Mean reward/step: 2.56
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 9.53s
                        Total time: 12769.03s
                               ETA: 1045156.5s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.608s, learning 0.164s)
               Value function loss: 110.7723
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 355.10
               Mean episode length: 150.00
                  Mean reward/step: 2.52
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 9.77s
                        Total time: 12778.81s
                               ETA: 1045079.9s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.572s, learning 0.163s)
               Value function loss: 118.7144
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 366.44
               Mean episode length: 148.79
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 9.74s
                        Total time: 12788.54s
                               ETA: 1045000.4s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.367s, learning 0.157s)
               Value function loss: 110.0960
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 363.92
               Mean episode length: 148.62
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 9.52s
                        Total time: 12798.06s
                               ETA: 1044903.8s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1628 steps/s (collection: 9.897s, learning 0.161s)
               Value function loss: 119.5040
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 370.51
               Mean episode length: 148.62
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 10.06s
                        Total time: 12808.12s
                               ETA: 1044851.0s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.396s, learning 0.162s)
               Value function loss: 107.1496
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 373.49
               Mean episode length: 149.96
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 9.56s
                        Total time: 12817.68s
                               ETA: 1044757.4s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.535s, learning 0.162s)
               Value function loss: 116.9547
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 373.39
               Mean episode length: 148.88
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 9.70s
                        Total time: 12827.38s
                               ETA: 1044675.3s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.848s, learning 0.165s)
               Value function loss: 98.3302
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 354.96
               Mean episode length: 150.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 10.01s
                        Total time: 12837.39s
                               ETA: 1044619.0s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.675s, learning 0.167s)
               Value function loss: 91.4113
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 346.25
               Mean episode length: 149.37
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 9.84s
                        Total time: 12847.23s
                               ETA: 1044548.9s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1617 steps/s (collection: 9.971s, learning 0.159s)
               Value function loss: 84.7691
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 365.79
               Mean episode length: 149.09
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 10.13s
                        Total time: 12857.36s
                               ETA: 1044502.2s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.417s, learning 0.166s)
               Value function loss: 75.7772
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 336.80
               Mean episode length: 150.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 9.58s
                        Total time: 12866.95s
                               ETA: 1044411.3s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.753s, learning 0.164s)
               Value function loss: 69.4598
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 340.89
               Mean episode length: 147.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 9.92s
                        Total time: 12876.86s
                               ETA: 1044347.5s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.219s, learning 0.183s)
               Value function loss: 63.5500
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 340.97
               Mean episode length: 148.98
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 9.40s
                        Total time: 12886.27s
                               ETA: 1044242.1s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.349s, learning 0.160s)
               Value function loss: 79.7230
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 339.42
               Mean episode length: 149.11
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 9.51s
                        Total time: 12895.78s
                               ETA: 1044145.6s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.150s, learning 0.164s)
               Value function loss: 76.2988
                    Surrogate loss: -0.0004
             Mean action noise std: 0.77
                       Mean reward: 290.92
               Mean episode length: 148.31
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 9.31s
                        Total time: 12905.09s
                               ETA: 1044033.4s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.710s, learning 0.163s)
               Value function loss: 66.3263
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 288.76
               Mean episode length: 147.23
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 9.87s
                        Total time: 12914.96s
                               ETA: 1043966.5s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.686s, learning 0.161s)
               Value function loss: 70.6571
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 278.12
               Mean episode length: 147.46
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 9.85s
                        Total time: 12924.81s
                               ETA: 1043897.7s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.380s, learning 0.166s)
               Value function loss: 65.9284
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 329.70
               Mean episode length: 148.89
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 9.55s
                        Total time: 12934.36s
                               ETA: 1043804.7s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1597 steps/s (collection: 10.086s, learning 0.169s)
               Value function loss: 84.1384
                    Surrogate loss: 0.0018
             Mean action noise std: 0.77
                       Mean reward: 300.07
               Mean episode length: 149.12
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 10.25s
                        Total time: 12944.61s
                               ETA: 1043768.9s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.381s, learning 0.163s)
               Value function loss: 80.5155
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 282.27
               Mean episode length: 148.81
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 9.54s
                        Total time: 12954.16s
                               ETA: 1043675.9s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.111s, learning 0.161s)
               Value function loss: 73.7525
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 295.74
               Mean episode length: 148.42
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 9.27s
                        Total time: 12963.43s
                               ETA: 1043561.2s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.301s, learning 0.170s)
               Value function loss: 80.8513
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 276.53
               Mean episode length: 146.41
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 9.47s
                        Total time: 12972.90s
                               ETA: 1043462.7s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.501s, learning 0.180s)
               Value function loss: 84.7563
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 304.95
               Mean episode length: 148.56
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 9.68s
                        Total time: 12982.58s
                               ETA: 1043381.1s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.483s, learning 0.180s)
               Value function loss: 68.2220
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 270.36
               Mean episode length: 148.59
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 9.66s
                        Total time: 12992.24s
                               ETA: 1043298.2s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.834s, learning 0.176s)
               Value function loss: 84.3541
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 306.24
               Mean episode length: 148.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 10.01s
                        Total time: 13002.25s
                               ETA: 1043243.2s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.388s, learning 0.177s)
               Value function loss: 77.7043
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 301.97
               Mean episode length: 149.31
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 9.57s
                        Total time: 13011.82s
                               ETA: 1043152.8s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.567s, learning 0.172s)
               Value function loss: 73.3165
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 304.81
               Mean episode length: 148.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 9.74s
                        Total time: 13021.56s
                               ETA: 1043076.3s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1611 steps/s (collection: 9.978s, learning 0.187s)
               Value function loss: 87.6220
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 324.10
               Mean episode length: 148.59
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 10.17s
                        Total time: 13031.72s
                               ETA: 1043034.1s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.480s, learning 0.158s)
               Value function loss: 88.1320
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 327.02
               Mean episode length: 148.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 9.64s
                        Total time: 13041.36s
                               ETA: 1042949.8s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.368s, learning 0.160s)
               Value function loss: 70.6391
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 299.75
               Mean episode length: 148.31
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 9.53s
                        Total time: 13050.89s
                               ETA: 1042856.7s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.563s, learning 0.162s)
               Value function loss: 65.3729
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 310.91
               Mean episode length: 148.34
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 9.73s
                        Total time: 13060.61s
                               ETA: 1042779.6s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.807s, learning 0.168s)
               Value function loss: 57.1412
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 302.41
               Mean episode length: 149.70
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 9.97s
                        Total time: 13070.59s
                               ETA: 1042722.5s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.507s, learning 0.169s)
               Value function loss: 73.0825
                    Surrogate loss: 0.0084
             Mean action noise std: 0.77
                       Mean reward: 330.13
               Mean episode length: 149.61
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 9.68s
                        Total time: 13080.26s
                               ETA: 1042641.7s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.478s, learning 0.170s)
               Value function loss: 81.5897
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 336.64
               Mean episode length: 148.12
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 9.65s
                        Total time: 13089.91s
                               ETA: 1042558.7s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.560s, learning 0.165s)
               Value function loss: 81.5503
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 326.31
               Mean episode length: 147.14
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 9.73s
                        Total time: 13099.64s
                               ETA: 1042482.0s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.682s, learning 0.170s)
               Value function loss: 82.3257
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 329.43
               Mean episode length: 149.42
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 9.85s
                        Total time: 13109.49s
                               ETA: 1042415.5s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.525s, learning 0.167s)
               Value function loss: 77.5749
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 308.33
               Mean episode length: 148.52
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 9.69s
                        Total time: 13119.18s
                               ETA: 1042336.4s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.440s, learning 0.171s)
               Value function loss: 98.0291
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 343.78
               Mean episode length: 149.66
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 9.61s
                        Total time: 13128.79s
                               ETA: 1042251.0s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.584s, learning 0.160s)
               Value function loss: 103.8303
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 339.78
               Mean episode length: 148.47
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 9.74s
                        Total time: 13138.54s
                               ETA: 1042176.3s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.529s, learning 0.159s)
               Value function loss: 100.6135
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 319.40
               Mean episode length: 149.55
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 9.69s
                        Total time: 13148.23s
                               ETA: 1042097.2s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.422s, learning 0.170s)
               Value function loss: 109.3551
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 352.96
               Mean episode length: 149.71
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 9.59s
                        Total time: 13157.82s
                               ETA: 1042010.7s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1646 steps/s (collection: 9.781s, learning 0.170s)
               Value function loss: 95.0300
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 327.83
               Mean episode length: 147.75
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 9.95s
                        Total time: 13167.77s
                               ETA: 1041952.6s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.548s, learning 0.161s)
               Value function loss: 84.0130
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 319.00
               Mean episode length: 148.45
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 9.71s
                        Total time: 13177.48s
                               ETA: 1041875.5s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.229s, learning 0.167s)
               Value function loss: 96.9590
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 351.73
               Mean episode length: 149.58
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 9.40s
                        Total time: 13186.88s
                               ETA: 1041773.7s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.267s, learning 0.169s)
               Value function loss: 100.0959
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 348.76
               Mean episode length: 150.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 9.44s
                        Total time: 13196.31s
                               ETA: 1041675.3s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.434s, learning 0.158s)
               Value function loss: 82.0390
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 361.10
               Mean episode length: 148.88
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 9.59s
                        Total time: 13205.90s
                               ETA: 1041589.3s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.747s, learning 0.163s)
               Value function loss: 84.6962
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 342.97
               Mean episode length: 149.91
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 9.91s
                        Total time: 13215.81s
                               ETA: 1041528.5s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.115s, learning 0.167s)
               Value function loss: 77.9027
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 331.87
               Mean episode length: 149.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 9.28s
                        Total time: 13225.10s
                               ETA: 1041418.3s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.720s, learning 0.163s)
               Value function loss: 80.3056
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 317.11
               Mean episode length: 150.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 9.88s
                        Total time: 13234.98s
                               ETA: 1041355.6s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.422s, learning 0.162s)
               Value function loss: 88.8502
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 332.22
               Mean episode length: 149.87
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 9.58s
                        Total time: 13244.56s
                               ETA: 1041269.5s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.521s, learning 0.161s)
               Value function loss: 78.1276
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 324.91
               Mean episode length: 150.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 9.68s
                        Total time: 13254.25s
                               ETA: 1041191.1s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.459s, learning 0.172s)
               Value function loss: 84.4109
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 319.51
               Mean episode length: 149.64
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 9.63s
                        Total time: 13263.88s
                               ETA: 1041108.9s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.602s, learning 0.176s)
               Value function loss: 72.0935
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 312.79
               Mean episode length: 149.24
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 9.78s
                        Total time: 13273.65s
                               ETA: 1041038.3s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.560s, learning 0.181s)
               Value function loss: 74.4094
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 323.40
               Mean episode length: 149.44
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 9.74s
                        Total time: 13283.39s
                               ETA: 1040964.8s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.709s, learning 0.162s)
               Value function loss: 77.9599
                    Surrogate loss: -0.0254
             Mean action noise std: 0.77
                       Mean reward: 291.78
               Mean episode length: 149.64
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 9.87s
                        Total time: 13293.27s
                               ETA: 1040901.7s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.468s, learning 0.194s)
               Value function loss: 79.5445
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 338.30
               Mean episode length: 148.21
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 9.66s
                        Total time: 13302.93s
                               ETA: 1040822.4s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.336s, learning 0.160s)
               Value function loss: 87.1102
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 305.96
               Mean episode length: 148.57
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 9.50s
                        Total time: 13312.42s
                               ETA: 1040730.1s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.653s, learning 0.165s)
               Value function loss: 80.3548
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 293.49
               Mean episode length: 149.20
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 9.82s
                        Total time: 13322.24s
                               ETA: 1040663.1s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.548s, learning 0.158s)
               Value function loss: 89.9110
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 319.82
               Mean episode length: 150.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 9.71s
                        Total time: 13331.95s
                               ETA: 1040587.5s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.189s, learning 0.169s)
               Value function loss: 82.0284
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 297.74
               Mean episode length: 149.54
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 9.36s
                        Total time: 13341.31s
                               ETA: 1040484.9s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.215s, learning 0.165s)
               Value function loss: 88.8417
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 311.46
               Mean episode length: 149.53
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 9.38s
                        Total time: 13350.69s
                               ETA: 1040384.0s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.703s, learning 0.164s)
               Value function loss: 92.4441
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 317.50
               Mean episode length: 149.04
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 9.87s
                        Total time: 13360.55s
                               ETA: 1040321.3s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.293s, learning 0.162s)
               Value function loss: 122.6940
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 311.69
               Mean episode length: 148.28
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 9.46s
                        Total time: 13370.01s
                               ETA: 1040226.7s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.390s, learning 0.169s)
               Value function loss: 154.0724
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 294.99
               Mean episode length: 149.46
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 9.56s
                        Total time: 13379.57s
                               ETA: 1040140.2s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.608s, learning 0.161s)
               Value function loss: 217.5061
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 322.37
               Mean episode length: 149.24
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 9.77s
                        Total time: 13389.34s
                               ETA: 1040070.1s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1616 steps/s (collection: 9.970s, learning 0.164s)
               Value function loss: 74.3627
                    Surrogate loss: 0.0182
             Mean action noise std: 0.77
                       Mean reward: 316.49
               Mean episode length: 148.70
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 10.13s
                        Total time: 13399.47s
                               ETA: 1040028.5s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.217s, learning 0.167s)
               Value function loss: 63.5016
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 339.30
               Mean episode length: 150.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 9.38s
                        Total time: 13408.85s
                               ETA: 1039928.8s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1630 steps/s (collection: 9.885s, learning 0.161s)
               Value function loss: 74.0025
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 331.56
               Mean episode length: 148.93
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 10.05s
                        Total time: 13418.90s
                               ETA: 1039880.5s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.351s, learning 0.163s)
               Value function loss: 74.2022
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 326.94
               Mean episode length: 149.10
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 9.51s
                        Total time: 13428.41s
                               ETA: 1039791.1s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.661s, learning 0.166s)
               Value function loss: 68.1935
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 334.56
               Mean episode length: 148.86
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 9.83s
                        Total time: 13438.24s
                               ETA: 1039726.0s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1770 steps/s (collection: 9.091s, learning 0.162s)
               Value function loss: 79.2422
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 346.40
               Mean episode length: 148.98
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 9.25s
                        Total time: 13447.49s
                               ETA: 1039616.6s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.352s, learning 0.164s)
               Value function loss: 70.7485
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 359.03
               Mean episode length: 148.93
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 9.52s
                        Total time: 13457.01s
                               ETA: 1039527.7s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.835s, learning 0.159s)
               Value function loss: 80.2734
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 354.70
               Mean episode length: 149.65
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 9.99s
                        Total time: 13467.00s
                               ETA: 1039475.8s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.273s, learning 0.163s)
               Value function loss: 76.3680
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 324.10
               Mean episode length: 150.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 9.44s
                        Total time: 13476.44s
                               ETA: 1039380.9s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.455s, learning 0.166s)
               Value function loss: 92.3971
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 374.35
               Mean episode length: 149.95
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 9.62s
                        Total time: 13486.06s
                               ETA: 1039300.5s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.178s, learning 0.158s)
               Value function loss: 101.0122
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 358.15
               Mean episode length: 150.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 9.34s
                        Total time: 13495.40s
                               ETA: 1039198.1s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.733s, learning 0.159s)
               Value function loss: 90.7650
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 356.80
               Mean episode length: 149.75
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 9.89s
                        Total time: 13505.29s
                               ETA: 1039138.7s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.473s, learning 0.182s)
               Value function loss: 96.4698
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 359.57
               Mean episode length: 149.05
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 9.65s
                        Total time: 13514.94s
                               ETA: 1039061.2s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.555s, learning 0.160s)
               Value function loss: 112.1691
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 372.00
               Mean episode length: 148.54
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 9.72s
                        Total time: 13524.66s
                               ETA: 1038988.4s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.312s, learning 0.164s)
               Value function loss: 106.6395
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 336.17
               Mean episode length: 149.08
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 9.48s
                        Total time: 13534.13s
                               ETA: 1038897.3s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.322s, learning 0.159s)
               Value function loss: 99.5856
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 352.04
               Mean episode length: 149.97
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 9.48s
                        Total time: 13543.61s
                               ETA: 1038806.8s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.792s, learning 0.175s)
               Value function loss: 78.7456
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 324.07
               Mean episode length: 149.33
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 9.97s
                        Total time: 13553.58s
                               ETA: 1038753.6s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.546s, learning 0.170s)
               Value function loss: 81.4327
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 339.71
               Mean episode length: 149.98
                  Mean reward/step: 2.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 9.72s
                        Total time: 13563.30s
                               ETA: 1038681.3s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.587s, learning 0.167s)
               Value function loss: 86.7979
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 340.27
               Mean episode length: 149.05
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 9.75s
                        Total time: 13573.05s
                               ETA: 1038612.0s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.149s, learning 0.157s)
               Value function loss: 78.3144
                    Surrogate loss: 0.0105
             Mean action noise std: 0.77
                       Mean reward: 306.81
               Mean episode length: 149.45
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 9.31s
                        Total time: 13582.36s
                               ETA: 1038508.5s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1621 steps/s (collection: 9.942s, learning 0.165s)
               Value function loss: 79.0717
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 335.36
               Mean episode length: 149.10
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 10.11s
                        Total time: 13592.46s
                               ETA: 1038466.3s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.315s, learning 0.168s)
               Value function loss: 79.8572
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 336.33
               Mean episode length: 148.52
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 9.48s
                        Total time: 13601.95s
                               ETA: 1038376.6s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.592s, learning 0.161s)
               Value function loss: 87.0734
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 358.70
               Mean episode length: 149.52
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 9.75s
                        Total time: 13611.70s
                               ETA: 1038307.6s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.330s, learning 0.162s)
               Value function loss: 79.0616
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 343.36
               Mean episode length: 149.10
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 9.49s
                        Total time: 13621.19s
                               ETA: 1038218.8s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.564s, learning 0.164s)
               Value function loss: 82.5487
                    Surrogate loss: 0.0089
             Mean action noise std: 0.77
                       Mean reward: 317.27
               Mean episode length: 150.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 9.73s
                        Total time: 13630.92s
                               ETA: 1038148.0s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.512s, learning 0.166s)
               Value function loss: 97.5463
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 359.96
               Mean episode length: 150.00
                  Mean reward/step: 2.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 9.68s
                        Total time: 13640.60s
                               ETA: 1038073.6s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.853s, learning 0.162s)
               Value function loss: 102.6536
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 328.05
               Mean episode length: 149.46
                  Mean reward/step: 2.56
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 10.01s
                        Total time: 13650.61s
                               ETA: 1038024.9s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.105s, learning 0.165s)
               Value function loss: 85.4570
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 325.99
               Mean episode length: 150.00
                  Mean reward/step: 2.60
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 9.27s
                        Total time: 13659.88s
                               ETA: 1037919.6s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1646 steps/s (collection: 9.789s, learning 0.159s)
               Value function loss: 97.6802
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 345.12
               Mean episode length: 149.33
                  Mean reward/step: 2.62
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 9.95s
                        Total time: 13669.83s
                               ETA: 1037866.0s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.644s, learning 0.171s)
               Value function loss: 120.5976
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 360.09
               Mean episode length: 149.95
                  Mean reward/step: 2.63
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 9.81s
                        Total time: 13679.64s
                               ETA: 1037802.4s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.563s, learning 0.163s)
               Value function loss: 110.2993
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 342.56
               Mean episode length: 149.20
                  Mean reward/step: 2.60
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 9.73s
                        Total time: 13689.37s
                               ETA: 1037732.1s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.357s, learning 0.172s)
               Value function loss: 113.7108
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 346.49
               Mean episode length: 150.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 9.53s
                        Total time: 13698.90s
                               ETA: 1037646.9s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.557s, learning 0.167s)
               Value function loss: 115.0248
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 373.48
               Mean episode length: 150.00
                  Mean reward/step: 2.54
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 9.72s
                        Total time: 13708.62s
                               ETA: 1037576.6s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.736s, learning 0.166s)
               Value function loss: 103.9802
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 369.75
               Mean episode length: 149.95
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 9.90s
                        Total time: 13718.52s
                               ETA: 1037519.9s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1644 steps/s (collection: 9.794s, learning 0.168s)
               Value function loss: 96.3825
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 378.53
               Mean episode length: 149.78
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 9.96s
                        Total time: 13728.49s
                               ETA: 1037467.9s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.234s, learning 0.174s)
               Value function loss: 108.8572
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 372.27
               Mean episode length: 149.01
                  Mean reward/step: 2.44
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 9.41s
                        Total time: 13737.90s
                               ETA: 1037374.0s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1596 steps/s (collection: 10.091s, learning 0.169s)
               Value function loss: 103.7327
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 363.78
               Mean episode length: 150.00
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 10.26s
                        Total time: 13748.16s
                               ETA: 1037344.6s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.321s, learning 0.179s)
               Value function loss: 107.7342
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 412.79
               Mean episode length: 149.34
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 9.50s
                        Total time: 13757.66s
                               ETA: 1037257.9s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.318s, learning 0.164s)
               Value function loss: 101.0493
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 369.06
               Mean episode length: 148.98
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 9.48s
                        Total time: 13767.14s
                               ETA: 1037169.9s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.780s, learning 0.162s)
               Value function loss: 91.5391
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 372.57
               Mean episode length: 149.63
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 9.94s
                        Total time: 13777.08s
                               ETA: 1037116.7s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.612s, learning 0.173s)
               Value function loss: 103.8596
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 356.28
               Mean episode length: 149.60
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 9.78s
                        Total time: 13786.86s
                               ETA: 1037051.7s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.186s, learning 0.159s)
               Value function loss: 97.4995
                    Surrogate loss: 0.0002
             Mean action noise std: 0.76
                       Mean reward: 393.18
               Mean episode length: 149.37
                  Mean reward/step: 2.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 9.34s
                        Total time: 13796.21s
                               ETA: 1036953.7s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.466s, learning 0.171s)
               Value function loss: 99.5792
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 383.25
               Mean episode length: 150.00
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 9.64s
                        Total time: 13805.85s
                               ETA: 1036877.9s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.658s, learning 0.164s)
               Value function loss: 107.9791
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 393.33
               Mean episode length: 149.30
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 9.82s
                        Total time: 13815.67s
                               ETA: 1036816.0s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.426s, learning 0.163s)
               Value function loss: 120.9649
                    Surrogate loss: 0.0061
             Mean action noise std: 0.76
                       Mean reward: 376.84
               Mean episode length: 149.70
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 9.59s
                        Total time: 13825.26s
                               ETA: 1036736.7s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.757s, learning 0.161s)
               Value function loss: 94.7733
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 375.49
               Mean episode length: 150.00
                  Mean reward/step: 2.47
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 9.92s
                        Total time: 13835.18s
                               ETA: 1036682.2s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.486s, learning 0.170s)
               Value function loss: 81.3887
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 372.48
               Mean episode length: 150.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 9.66s
                        Total time: 13844.83s
                               ETA: 1036608.1s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.271s, learning 0.161s)
               Value function loss: 103.9978
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 374.77
               Mean episode length: 149.71
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 9.43s
                        Total time: 13854.26s
                               ETA: 1036517.3s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.311s, learning 0.169s)
               Value function loss: 94.2770
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 343.12
               Mean episode length: 149.67
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 9.48s
                        Total time: 13863.74s
                               ETA: 1036430.2s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.622s, learning 0.162s)
               Value function loss: 93.0053
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 322.85
               Mean episode length: 149.79
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 9.78s
                        Total time: 13873.53s
                               ETA: 1036366.0s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.609s, learning 0.159s)
               Value function loss: 95.8012
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 361.90
               Mean episode length: 150.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 9.77s
                        Total time: 13883.29s
                               ETA: 1036300.7s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.526s, learning 0.166s)
               Value function loss: 94.6359
                    Surrogate loss: 0.0074
             Mean action noise std: 0.76
                       Mean reward: 347.13
               Mean episode length: 148.91
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 9.69s
                        Total time: 13892.99s
                               ETA: 1036229.8s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.432s, learning 0.161s)
               Value function loss: 78.8591
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 357.74
               Mean episode length: 149.72
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 9.59s
                        Total time: 13902.58s
                               ETA: 1036151.7s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1622 steps/s (collection: 9.941s, learning 0.159s)
               Value function loss: 86.8147
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 336.69
               Mean episode length: 149.53
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 10.10s
                        Total time: 13912.68s
                               ETA: 1036111.3s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.393s, learning 0.165s)
               Value function loss: 101.5690
                    Surrogate loss: 0.0016
             Mean action noise std: 0.76
                       Mean reward: 360.29
               Mean episode length: 149.42
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 9.56s
                        Total time: 13922.24s
                               ETA: 1036030.7s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.492s, learning 0.161s)
               Value function loss: 82.4190
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 374.08
               Mean episode length: 149.52
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 9.65s
                        Total time: 13931.89s
                               ETA: 1035957.2s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.499s, learning 0.162s)
               Value function loss: 98.0655
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 379.34
               Mean episode length: 150.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 9.66s
                        Total time: 13941.55s
                               ETA: 1035884.5s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.176s, learning 0.165s)
               Value function loss: 90.5305
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 389.07
               Mean episode length: 148.36
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 9.34s
                        Total time: 13950.89s
                               ETA: 1035788.1s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.614s, learning 0.160s)
               Value function loss: 82.8624
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 338.70
               Mean episode length: 148.73
                  Mean reward/step: 2.29
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 9.77s
                        Total time: 13960.67s
                               ETA: 1035724.0s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.337s, learning 0.158s)
               Value function loss: 80.2098
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 347.31
               Mean episode length: 150.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 9.50s
                        Total time: 13970.16s
                               ETA: 1035639.3s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1581 steps/s (collection: 10.202s, learning 0.160s)
               Value function loss: 66.1651
                    Surrogate loss: -0.0168
             Mean action noise std: 0.76
                       Mean reward: 352.04
               Mean episode length: 150.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 10.36s
                        Total time: 13980.52s
                               ETA: 1035618.9s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.229s, learning 0.163s)
               Value function loss: 76.8302
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 360.12
               Mean episode length: 149.81
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 9.39s
                        Total time: 13989.92s
                               ETA: 1035526.7s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.500s, learning 0.175s)
               Value function loss: 78.2958
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 352.18
               Mean episode length: 148.90
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 9.68s
                        Total time: 13999.59s
                               ETA: 1035455.6s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.655s, learning 0.158s)
               Value function loss: 78.4443
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 376.75
               Mean episode length: 148.48
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 9.81s
                        Total time: 14009.41s
                               ETA: 1035394.8s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.509s, learning 0.166s)
               Value function loss: 73.3278
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 340.58
               Mean episode length: 150.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 9.67s
                        Total time: 14019.08s
                               ETA: 1035323.8s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.833s, learning 0.166s)
               Value function loss: 61.5366
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 352.26
               Mean episode length: 148.68
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 10.00s
                        Total time: 14029.08s
                               ETA: 1035276.8s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.680s, learning 0.175s)
               Value function loss: 67.3198
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 311.51
               Mean episode length: 147.36
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 9.85s
                        Total time: 14038.93s
                               ETA: 1035219.3s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.462s, learning 0.158s)
               Value function loss: 64.4957
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 291.06
               Mean episode length: 149.90
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 9.62s
                        Total time: 14048.55s
                               ETA: 1035144.5s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1762 steps/s (collection: 9.135s, learning 0.159s)
               Value function loss: 86.5021
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 337.90
               Mean episode length: 149.73
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 9.29s
                        Total time: 14057.85s
                               ETA: 1035045.8s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.557s, learning 0.163s)
               Value function loss: 79.7298
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 303.90
               Mean episode length: 149.12
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 9.72s
                        Total time: 14067.57s
                               ETA: 1034978.6s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.673s, learning 0.164s)
               Value function loss: 82.4305
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 328.47
               Mean episode length: 148.77
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 9.84s
                        Total time: 14077.41s
                               ETA: 1034920.1s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.803s, learning 0.166s)
               Value function loss: 80.4121
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 341.58
               Mean episode length: 149.54
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 9.97s
                        Total time: 14087.37s
                               ETA: 1034871.3s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.348s, learning 0.162s)
               Value function loss: 87.2032
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 372.93
               Mean episode length: 149.41
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 9.51s
                        Total time: 14096.88s
                               ETA: 1034788.9s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.687s, learning 0.166s)
               Value function loss: 83.9971
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 350.76
               Mean episode length: 148.49
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 9.85s
                        Total time: 14106.74s
                               ETA: 1034731.8s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.809s, learning 0.170s)
               Value function loss: 79.3991
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 338.94
               Mean episode length: 148.29
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 9.98s
                        Total time: 14116.72s
                               ETA: 1034684.0s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.481s, learning 0.164s)
               Value function loss: 76.7834
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 356.83
               Mean episode length: 148.60
                  Mean reward/step: 2.48
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 9.64s
                        Total time: 14126.36s
                               ETA: 1034611.7s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.740s, learning 0.173s)
               Value function loss: 70.9820
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 336.18
               Mean episode length: 149.47
                  Mean reward/step: 2.51
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 9.91s
                        Total time: 14136.27s
                               ETA: 1034559.2s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.381s, learning 0.163s)
               Value function loss: 87.1078
                    Surrogate loss: 0.0030
             Mean action noise std: 0.76
                       Mean reward: 371.49
               Mean episode length: 148.41
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 9.54s
                        Total time: 14145.82s
                               ETA: 1034479.8s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.309s, learning 0.173s)
               Value function loss: 75.0958
                    Surrogate loss: 0.0108
             Mean action noise std: 0.76
                       Mean reward: 366.13
               Mean episode length: 148.77
                  Mean reward/step: 2.59
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 9.48s
                        Total time: 14155.30s
                               ETA: 1034395.9s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.418s, learning 0.165s)
               Value function loss: 67.1708
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 360.42
               Mean episode length: 148.66
                  Mean reward/step: 2.66
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 9.58s
                        Total time: 14164.88s
                               ETA: 1034319.6s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.829s, learning 0.158s)
               Value function loss: 88.4324
                    Surrogate loss: -0.0144
             Mean action noise std: 0.76
                       Mean reward: 391.84
               Mean episode length: 148.78
                  Mean reward/step: 2.71
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 9.99s
                        Total time: 14174.87s
                               ETA: 1034272.7s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.622s, learning 0.159s)
               Value function loss: 68.4003
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 369.43
               Mean episode length: 149.19
                  Mean reward/step: 2.74
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 9.78s
                        Total time: 14184.65s
                               ETA: 1034211.0s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.487s, learning 0.174s)
               Value function loss: 85.5497
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 374.85
               Mean episode length: 148.98
                  Mean reward/step: 2.78
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 9.66s
                        Total time: 14194.31s
                               ETA: 1034140.5s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.336s, learning 0.160s)
               Value function loss: 85.2630
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 393.60
               Mean episode length: 148.24
                  Mean reward/step: 2.77
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 9.50s
                        Total time: 14203.81s
                               ETA: 1034058.1s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.617s, learning 0.164s)
               Value function loss: 90.8252
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 378.89
               Mean episode length: 145.96
                  Mean reward/step: 2.73
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 9.78s
                        Total time: 14213.59s
                               ETA: 1033996.6s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.366s, learning 0.160s)
               Value function loss: 93.3854
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 367.30
               Mean episode length: 149.69
                  Mean reward/step: 2.67
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 9.53s
                        Total time: 14223.11s
                               ETA: 1033916.6s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.522s, learning 0.159s)
               Value function loss: 102.4743
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 417.53
               Mean episode length: 148.39
                  Mean reward/step: 2.62
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 9.68s
                        Total time: 14232.79s
                               ETA: 1033848.0s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.268s, learning 0.159s)
               Value function loss: 93.1491
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 376.54
               Mean episode length: 148.22
                  Mean reward/step: 2.57
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 9.43s
                        Total time: 14242.22s
                               ETA: 1033761.0s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.864s, learning 0.156s)
               Value function loss: 101.1706
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 362.23
               Mean episode length: 149.10
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 10.02s
                        Total time: 14252.24s
                               ETA: 1033717.2s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.650s, learning 0.159s)
               Value function loss: 91.0066
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 370.26
               Mean episode length: 149.55
                  Mean reward/step: 2.51
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 9.81s
                        Total time: 14262.05s
                               ETA: 1033658.1s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.681s, learning 0.170s)
               Value function loss: 79.2526
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 393.32
               Mean episode length: 149.60
                  Mean reward/step: 2.52
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 9.85s
                        Total time: 14271.90s
                               ETA: 1033602.1s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.398s, learning 0.159s)
               Value function loss: 83.6633
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 381.53
               Mean episode length: 149.90
                  Mean reward/step: 2.54
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 9.56s
                        Total time: 14281.46s
                               ETA: 1033525.0s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1625 steps/s (collection: 9.921s, learning 0.161s)
               Value function loss: 85.9582
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 412.13
               Mean episode length: 148.86
                  Mean reward/step: 2.52
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 10.08s
                        Total time: 14291.54s
                               ETA: 1033485.9s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.504s, learning 0.164s)
               Value function loss: 78.2257
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 387.65
               Mean episode length: 149.04
                  Mean reward/step: 2.53
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 9.67s
                        Total time: 14301.21s
                               ETA: 1033416.9s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.642s, learning 0.164s)
               Value function loss: 81.0501
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 374.43
               Mean episode length: 148.88
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 9.81s
                        Total time: 14311.02s
                               ETA: 1033358.0s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.446s, learning 0.163s)
               Value function loss: 78.5395
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 376.34
               Mean episode length: 148.80
                  Mean reward/step: 2.59
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 9.61s
                        Total time: 14320.62s
                               ETA: 1033284.8s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1620 steps/s (collection: 9.951s, learning 0.159s)
               Value function loss: 77.2493
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 403.97
               Mean episode length: 148.39
                  Mean reward/step: 2.63
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 10.11s
                        Total time: 14330.73s
                               ETA: 1033248.0s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.412s, learning 0.169s)
               Value function loss: 78.0616
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 430.84
               Mean episode length: 149.85
                  Mean reward/step: 2.62
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 9.58s
                        Total time: 14340.31s
                               ETA: 1033173.0s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.620s, learning 0.165s)
               Value function loss: 75.7464
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 375.28
               Mean episode length: 148.76
                  Mean reward/step: 2.60
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 9.78s
                        Total time: 14350.10s
                               ETA: 1033112.8s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1631 steps/s (collection: 9.880s, learning 0.163s)
               Value function loss: 79.4373
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 365.97
               Mean episode length: 148.42
                  Mean reward/step: 2.60
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 10.04s
                        Total time: 14360.14s
                               ETA: 1033071.3s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.645s, learning 0.170s)
               Value function loss: 77.5916
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 389.07
               Mean episode length: 149.53
                  Mean reward/step: 2.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 9.82s
                        Total time: 14369.96s
                               ETA: 1033013.4s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.754s, learning 0.167s)
               Value function loss: 81.7551
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 360.82
               Mean episode length: 146.98
                  Mean reward/step: 2.56
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 9.92s
                        Total time: 14379.88s
                               ETA: 1032963.2s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.585s, learning 0.160s)
               Value function loss: 79.2613
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 404.01
               Mean episode length: 148.56
                  Mean reward/step: 2.52
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 9.75s
                        Total time: 14389.62s
                               ETA: 1032900.6s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.626s, learning 0.165s)
               Value function loss: 80.6536
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 381.54
               Mean episode length: 149.76
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 9.79s
                        Total time: 14399.41s
                               ETA: 1032841.1s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.660s, learning 0.159s)
               Value function loss: 91.9863
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 367.48
               Mean episode length: 149.54
                  Mean reward/step: 2.39
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 9.82s
                        Total time: 14409.23s
                               ETA: 1032783.9s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.704s, learning 0.177s)
               Value function loss: 72.3784
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 384.01
               Mean episode length: 148.49
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 9.88s
                        Total time: 14419.12s
                               ETA: 1032731.2s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.570s, learning 0.174s)
               Value function loss: 73.4936
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 360.30
               Mean episode length: 149.95
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 9.74s
                        Total time: 14428.86s
                               ETA: 1032668.6s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.405s, learning 0.161s)
               Value function loss: 77.2230
                    Surrogate loss: 0.0107
             Mean action noise std: 0.76
                       Mean reward: 362.70
               Mean episode length: 149.21
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 9.57s
                        Total time: 14438.42s
                               ETA: 1032593.4s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.125s, learning 0.164s)
               Value function loss: 67.4262
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 339.67
               Mean episode length: 149.78
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 9.29s
                        Total time: 14447.71s
                               ETA: 1032498.5s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.387s, learning 0.163s)
               Value function loss: 69.2113
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 343.38
               Mean episode length: 149.80
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 9.55s
                        Total time: 14457.26s
                               ETA: 1032422.4s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.622s, learning 0.163s)
               Value function loss: 75.2222
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 349.54
               Mean episode length: 148.30
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 9.78s
                        Total time: 14467.05s
                               ETA: 1032363.1s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.495s, learning 0.171s)
               Value function loss: 72.9020
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 359.36
               Mean episode length: 149.17
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 9.67s
                        Total time: 14476.71s
                               ETA: 1032295.5s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.418s, learning 0.161s)
               Value function loss: 83.0736
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 352.34
               Mean episode length: 148.63
                  Mean reward/step: 2.25
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 9.58s
                        Total time: 14486.29s
                               ETA: 1032221.6s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.462s, learning 0.158s)
               Value function loss: 90.3582
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 326.84
               Mean episode length: 148.40
                  Mean reward/step: 2.33
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 9.62s
                        Total time: 14495.91s
                               ETA: 1032150.9s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.600s, learning 0.161s)
               Value function loss: 87.7667
                    Surrogate loss: 0.0589
             Mean action noise std: 0.76
                       Mean reward: 340.60
               Mean episode length: 147.27
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 9.76s
                        Total time: 14505.67s
                               ETA: 1032090.2s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.668s, learning 0.177s)
               Value function loss: 75.3381
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 365.88
               Mean episode length: 149.07
                  Mean reward/step: 2.43
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 9.85s
                        Total time: 14515.52s
                               ETA: 1032035.6s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.868s, learning 0.165s)
               Value function loss: 68.8204
                    Surrogate loss: 0.0031
             Mean action noise std: 0.76
                       Mean reward: 352.40
               Mean episode length: 148.67
                  Mean reward/step: 2.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 10.03s
                        Total time: 14525.55s
                               ETA: 1031994.4s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.599s, learning 0.160s)
               Value function loss: 65.7731
                    Surrogate loss: 0.0254
             Mean action noise std: 0.76
                       Mean reward: 325.65
               Mean episode length: 149.43
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 9.76s
                        Total time: 14535.31s
                               ETA: 1031933.8s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1621 steps/s (collection: 9.941s, learning 0.160s)
               Value function loss: 89.5604
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 338.35
               Mean episode length: 149.16
                  Mean reward/step: 2.36
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 10.10s
                        Total time: 14545.41s
                               ETA: 1031897.5s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.696s, learning 0.157s)
               Value function loss: 76.8517
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 345.20
               Mean episode length: 149.57
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 9.85s
                        Total time: 14555.26s
                               ETA: 1031843.7s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1587 steps/s (collection: 10.159s, learning 0.162s)
               Value function loss: 73.2691
                    Surrogate loss: 0.0129
             Mean action noise std: 0.76
                       Mean reward: 323.65
               Mean episode length: 149.06
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 10.32s
                        Total time: 14565.58s
                               ETA: 1031823.1s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.411s, learning 0.162s)
               Value function loss: 63.0241
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 332.21
               Mean episode length: 148.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 9.57s
                        Total time: 14575.16s
                               ETA: 1031749.5s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.835s, learning 0.161s)
               Value function loss: 70.9940
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 335.15
               Mean episode length: 150.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 10.00s
                        Total time: 14585.15s
                               ETA: 1031706.0s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.621s, learning 0.158s)
               Value function loss: 70.9626
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 341.05
               Mean episode length: 147.67
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 9.78s
                        Total time: 14594.93s
                               ETA: 1031647.2s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.557s, learning 0.173s)
               Value function loss: 66.8385
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 300.78
               Mean episode length: 148.40
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 9.73s
                        Total time: 14604.66s
                               ETA: 1031585.0s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.637s, learning 0.161s)
               Value function loss: 67.4148
                    Surrogate loss: -0.0228
             Mean action noise std: 0.76
                       Mean reward: 323.17
               Mean episode length: 149.82
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 9.80s
                        Total time: 14614.46s
                               ETA: 1031527.7s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.357s, learning 0.171s)
               Value function loss: 60.2647
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 325.21
               Mean episode length: 148.82
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 9.53s
                        Total time: 14623.99s
                               ETA: 1031451.4s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.844s, learning 0.164s)
               Value function loss: 58.3284
                    Surrogate loss: -0.0246
             Mean action noise std: 0.76
                       Mean reward: 318.19
               Mean episode length: 150.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 10.01s
                        Total time: 14634.00s
                               ETA: 1031409.1s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.543s, learning 0.166s)
               Value function loss: 61.8327
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 307.67
               Mean episode length: 149.71
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 9.71s
                        Total time: 14643.71s
                               ETA: 1031345.7s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.834s, learning 0.170s)
               Value function loss: 62.1572
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 310.88
               Mean episode length: 148.80
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 10.00s
                        Total time: 14653.71s
                               ETA: 1031303.1s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.675s, learning 0.160s)
               Value function loss: 61.5487
                    Surrogate loss: -0.0217
             Mean action noise std: 0.76
                       Mean reward: 341.80
               Mean episode length: 149.24
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 9.83s
                        Total time: 14663.54s
                               ETA: 1031248.7s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.340s, learning 0.169s)
               Value function loss: 66.7511
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 325.10
               Mean episode length: 149.45
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 9.51s
                        Total time: 14673.05s
                               ETA: 1031171.6s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.402s, learning 0.161s)
               Value function loss: 75.6252
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 319.26
               Mean episode length: 149.01
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 9.56s
                        Total time: 14682.62s
                               ETA: 1031098.2s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.737s, learning 0.167s)
               Value function loss: 69.8594
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 316.70
               Mean episode length: 149.35
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 9.90s
                        Total time: 14692.52s
                               ETA: 1031048.9s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.631s, learning 0.161s)
               Value function loss: 69.1373
                    Surrogate loss: 0.0071
             Mean action noise std: 0.76
                       Mean reward: 290.88
               Mean episode length: 148.75
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 9.79s
                        Total time: 14702.31s
                               ETA: 1030991.8s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.541s, learning 0.172s)
               Value function loss: 66.4334
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 290.88
               Mean episode length: 148.22
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 9.71s
                        Total time: 14712.02s
                               ETA: 1030929.2s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.505s, learning 0.169s)
               Value function loss: 66.2090
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 293.44
               Mean episode length: 148.05
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 9.67s
                        Total time: 14721.70s
                               ETA: 1030864.0s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.537s, learning 0.164s)
               Value function loss: 66.4371
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 278.64
               Mean episode length: 147.66
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 9.70s
                        Total time: 14731.40s
                               ETA: 1030800.8s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.253s, learning 0.164s)
               Value function loss: 60.1764
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 309.45
               Mean episode length: 149.70
                  Mean reward/step: 2.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 9.42s
                        Total time: 14740.82s
                               ETA: 1030717.8s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.701s, learning 0.159s)
               Value function loss: 69.1704
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 298.87
               Mean episode length: 148.78
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 9.86s
                        Total time: 14750.68s
                               ETA: 1030665.8s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.558s, learning 0.166s)
               Value function loss: 65.8195
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 316.36
               Mean episode length: 149.40
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 9.72s
                        Total time: 14760.40s
                               ETA: 1030604.4s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.167s)
               Value function loss: 61.2644
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 292.78
               Mean episode length: 149.02
                  Mean reward/step: 2.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 9.69s
                        Total time: 14770.09s
                               ETA: 1030540.5s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.333s, learning 0.162s)
               Value function loss: 64.8471
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 299.14
               Mean episode length: 148.44
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 9.49s
                        Total time: 14779.59s
                               ETA: 1030463.2s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.375s, learning 0.178s)
               Value function loss: 70.0685
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 302.18
               Mean episode length: 149.40
                  Mean reward/step: 2.22
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 9.55s
                        Total time: 14789.14s
                               ETA: 1030390.1s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.478s, learning 0.167s)
               Value function loss: 66.0778
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 318.63
               Mean episode length: 149.34
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 9.65s
                        Total time: 14798.78s
                               ETA: 1030323.5s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.672s, learning 0.166s)
               Value function loss: 72.1974
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 335.70
               Mean episode length: 149.07
                  Mean reward/step: 2.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 9.84s
                        Total time: 14808.62s
                               ETA: 1030270.4s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.580s, learning 0.160s)
               Value function loss: 66.9426
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 312.90
               Mean episode length: 150.00
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 9.74s
                        Total time: 14818.36s
                               ETA: 1030210.5s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.239s, learning 0.172s)
               Value function loss: 70.4681
                    Surrogate loss: -0.0238
             Mean action noise std: 0.76
                       Mean reward: 341.67
               Mean episode length: 147.99
                  Mean reward/step: 2.35
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 9.41s
                        Total time: 14827.77s
                               ETA: 1030127.9s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.779s, learning 0.158s)
               Value function loss: 85.4298
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 343.10
               Mean episode length: 149.82
                  Mean reward/step: 2.38
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 9.94s
                        Total time: 14837.71s
                               ETA: 1030081.8s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.731s, learning 0.161s)
               Value function loss: 78.9190
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 367.83
               Mean episode length: 148.55
                  Mean reward/step: 2.40
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 9.89s
                        Total time: 14847.60s
                               ETA: 1030032.6s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.224s, learning 0.173s)
               Value function loss: 69.9575
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 320.98
               Mean episode length: 149.75
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 9.40s
                        Total time: 14857.00s
                               ETA: 1029949.3s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.236s, learning 0.160s)
               Value function loss: 73.0849
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 363.02
               Mean episode length: 149.73
                  Mean reward/step: 2.51
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 9.40s
                        Total time: 14866.39s
                               ETA: 1029866.0s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.670s, learning 0.163s)
               Value function loss: 73.9981
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 388.93
               Mean episode length: 149.28
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 9.83s
                        Total time: 14876.23s
                               ETA: 1029813.0s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.200s, learning 0.162s)
               Value function loss: 71.3337
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 364.02
               Mean episode length: 148.95
                  Mean reward/step: 2.61
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 9.36s
                        Total time: 14885.59s
                               ETA: 1029727.5s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.822s, learning 0.173s)
               Value function loss: 72.7345
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 350.14
               Mean episode length: 146.38
                  Mean reward/step: 2.65
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 10.00s
                        Total time: 14895.58s
                               ETA: 1029685.9s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.298s, learning 0.161s)
               Value function loss: 70.1271
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 351.56
               Mean episode length: 149.26
                  Mean reward/step: 2.75
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 9.46s
                        Total time: 14905.04s
                               ETA: 1029607.3s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.330s, learning 0.160s)
               Value function loss: 67.7391
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 370.37
               Mean episode length: 149.23
                  Mean reward/step: 2.78
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 9.49s
                        Total time: 14914.53s
                               ETA: 1029530.9s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.504s, learning 0.161s)
               Value function loss: 83.4436
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 407.76
               Mean episode length: 149.57
                  Mean reward/step: 2.84
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 9.67s
                        Total time: 14924.20s
                               ETA: 1029466.7s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.661s, learning 0.162s)
               Value function loss: 85.3880
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: 368.41
               Mean episode length: 148.85
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 9.82s
                        Total time: 14934.02s
                               ETA: 1029413.5s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.580s, learning 0.173s)
               Value function loss: 84.7441
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 387.73
               Mean episode length: 150.00
                  Mean reward/step: 2.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 9.75s
                        Total time: 14943.77s
                               ETA: 1029355.5s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.707s, learning 0.171s)
               Value function loss: 81.5597
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 420.22
               Mean episode length: 149.63
                  Mean reward/step: 2.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 9.88s
                        Total time: 14953.65s
                               ETA: 1029306.1s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.866s, learning 0.165s)
               Value function loss: 81.2715
                    Surrogate loss: -0.0240
             Mean action noise std: 0.76
                       Mean reward: 385.11
               Mean episode length: 150.00
                  Mean reward/step: 2.80
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 10.03s
                        Total time: 14963.68s
                               ETA: 1029267.4s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.358s, learning 0.165s)
               Value function loss: 106.9147
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 414.80
               Mean episode length: 149.86
                  Mean reward/step: 2.75
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 9.52s
                        Total time: 14973.20s
                               ETA: 1029193.8s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.272s, learning 0.171s)
               Value function loss: 89.3144
                    Surrogate loss: -0.0258
             Mean action noise std: 0.76
                       Mean reward: 407.15
               Mean episode length: 149.73
                  Mean reward/step: 2.72
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 9.44s
                        Total time: 14982.65s
                               ETA: 1029114.8s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1595 steps/s (collection: 10.105s, learning 0.161s)
               Value function loss: 89.7846
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 401.45
               Mean episode length: 149.60
                  Mean reward/step: 2.70
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 10.27s
                        Total time: 14992.91s
                               ETA: 1029092.3s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1776 steps/s (collection: 9.059s, learning 0.164s)
               Value function loss: 78.0817
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 392.78
               Mean episode length: 148.45
                  Mean reward/step: 2.73
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 9.22s
                        Total time: 15002.14s
                               ETA: 1028998.3s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.045s, learning 0.160s)
               Value function loss: 87.9994
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 458.38
               Mean episode length: 150.00
                  Mean reward/step: 2.78
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 9.21s
                        Total time: 15011.34s
                               ETA: 1028903.3s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.481s, learning 0.164s)
               Value function loss: 86.7040
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 418.23
               Mean episode length: 150.00
                  Mean reward/step: 2.78
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 9.65s
                        Total time: 15020.99s
                               ETA: 1028838.5s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.410s, learning 0.175s)
               Value function loss: 81.2724
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 437.64
               Mean episode length: 149.35
                  Mean reward/step: 2.82
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 9.58s
                        Total time: 15030.57s
                               ETA: 1028769.6s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.731s, learning 0.160s)
               Value function loss: 79.3697
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 373.84
               Mean episode length: 150.00
                  Mean reward/step: 2.88
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 9.89s
                        Total time: 15040.46s
                               ETA: 1028721.7s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.401s, learning 0.164s)
               Value function loss: 82.4889
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 429.22
               Mean episode length: 150.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 9.56s
                        Total time: 15050.03s
                               ETA: 1028651.6s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.741s, learning 0.168s)
               Value function loss: 73.0995
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 426.61
               Mean episode length: 149.26
                  Mean reward/step: 2.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 9.91s
                        Total time: 15059.94s
                               ETA: 1028605.2s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.473s, learning 0.168s)
               Value function loss: 76.6579
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 431.86
               Mean episode length: 149.18
                  Mean reward/step: 2.99
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 9.64s
                        Total time: 15069.58s
                               ETA: 1028540.4s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.538s, learning 0.165s)
               Value function loss: 78.9001
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 443.24
               Mean episode length: 149.45
                  Mean reward/step: 3.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 9.70s
                        Total time: 15079.28s
                               ETA: 1028480.0s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.518s, learning 0.161s)
               Value function loss: 84.7020
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 428.13
               Mean episode length: 149.60
                  Mean reward/step: 3.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 9.68s
                        Total time: 15088.96s
                               ETA: 1028418.0s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.264s, learning 0.168s)
               Value function loss: 85.5660
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 449.72
               Mean episode length: 149.37
                  Mean reward/step: 3.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 9.43s
                        Total time: 15098.39s
                               ETA: 1028339.2s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.733s, learning 0.163s)
               Value function loss: 86.3290
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 431.40
               Mean episode length: 150.00
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 9.90s
                        Total time: 15108.29s
                               ETA: 1028292.1s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.335s, learning 0.163s)
               Value function loss: 91.7852
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 466.31
               Mean episode length: 149.69
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 9.50s
                        Total time: 15117.78s
                               ETA: 1028218.0s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.198s, learning 0.159s)
               Value function loss: 92.5013
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 450.23
               Mean episode length: 149.26
                  Mean reward/step: 2.99
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 9.36s
                        Total time: 15127.14s
                               ETA: 1028134.5s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.249s, learning 0.166s)
               Value function loss: 102.8268
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 448.55
               Mean episode length: 149.40
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 9.42s
                        Total time: 15136.56s
                               ETA: 1028054.9s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.349s, learning 0.166s)
               Value function loss: 108.4016
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 431.40
               Mean episode length: 150.00
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 9.52s
                        Total time: 15146.07s
                               ETA: 1027982.3s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.499s, learning 0.157s)
               Value function loss: 102.5854
                    Surrogate loss: 0.0037
             Mean action noise std: 0.76
                       Mean reward: 426.18
               Mean episode length: 148.80
                  Mean reward/step: 2.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 9.66s
                        Total time: 15155.73s
                               ETA: 1027919.3s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.831s, learning 0.158s)
               Value function loss: 98.5409
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 447.86
               Mean episode length: 149.39
                  Mean reward/step: 2.91
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 9.99s
                        Total time: 15165.72s
                               ETA: 1027878.9s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.386s, learning 0.161s)
               Value function loss: 93.6902
                    Surrogate loss: -0.0213
             Mean action noise std: 0.76
                       Mean reward: 448.49
               Mean episode length: 149.54
                  Mean reward/step: 2.84
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 9.55s
                        Total time: 15175.26s
                               ETA: 1027808.6s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.326s, learning 0.162s)
               Value function loss: 96.8223
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 427.36
               Mean episode length: 149.44
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 9.49s
                        Total time: 15184.75s
                               ETA: 1027734.5s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.591s, learning 0.168s)
               Value function loss: 100.7362
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 450.75
               Mean episode length: 149.64
                  Mean reward/step: 2.84
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 9.76s
                        Total time: 15194.51s
                               ETA: 1027678.7s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.460s, learning 0.168s)
               Value function loss: 97.9045
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 438.04
               Mean episode length: 149.82
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 9.63s
                        Total time: 15204.14s
                               ETA: 1027614.2s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.689s, learning 0.172s)
               Value function loss: 98.1354
                    Surrogate loss: -0.0164
             Mean action noise std: 0.76
                       Mean reward: 426.75
               Mean episode length: 148.13
                  Mean reward/step: 2.86
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 9.86s
                        Total time: 15214.00s
                               ETA: 1027565.5s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.613s, learning 0.159s)
               Value function loss: 101.9617
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 425.09
               Mean episode length: 149.59
                  Mean reward/step: 2.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 9.77s
                        Total time: 15223.77s
                               ETA: 1027510.8s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.713s, learning 0.161s)
               Value function loss: 100.0999
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 450.81
               Mean episode length: 149.71
                  Mean reward/step: 2.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 9.87s
                        Total time: 15233.65s
                               ETA: 1027463.1s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.410s, learning 0.162s)
               Value function loss: 101.4138
                    Surrogate loss: -0.0154
             Mean action noise std: 0.76
                       Mean reward: 479.38
               Mean episode length: 149.32
                  Mean reward/step: 2.91
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 9.57s
                        Total time: 15243.22s
                               ETA: 1027395.0s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.542s, learning 0.170s)
               Value function loss: 91.7414
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 448.56
               Mean episode length: 150.00
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 9.71s
                        Total time: 15252.93s
                               ETA: 1027336.5s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1619 steps/s (collection: 9.951s, learning 0.168s)
               Value function loss: 108.0728
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 414.54
               Mean episode length: 147.99
                  Mean reward/step: 2.98
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 10.12s
                        Total time: 15263.05s
                               ETA: 1027305.4s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.480s, learning 0.168s)
               Value function loss: 157.7825
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 457.42
               Mean episode length: 149.04
                  Mean reward/step: 3.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 9.65s
                        Total time: 15272.70s
                               ETA: 1027242.6s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.349s, learning 0.158s)
               Value function loss: 84.5926
                    Surrogate loss: 0.0101
             Mean action noise std: 0.76
                       Mean reward: 449.03
               Mean episode length: 149.61
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 9.51s
                        Total time: 15282.20s
                               ETA: 1027170.5s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.361s, learning 0.174s)
               Value function loss: 87.0398
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 444.00
               Mean episode length: 149.06
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 9.53s
                        Total time: 15291.74s
                               ETA: 1027100.3s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.185s, learning 0.158s)
               Value function loss: 80.0775
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 420.70
               Mean episode length: 148.37
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 9.34s
                        Total time: 15301.08s
                               ETA: 1027017.3s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.764s, learning 0.173s)
               Value function loss: 92.2208
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 450.35
               Mean episode length: 149.81
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 9.94s
                        Total time: 15311.02s
                               ETA: 1026974.3s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.311s, learning 0.167s)
               Value function loss: 87.1982
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 420.35
               Mean episode length: 149.84
                  Mean reward/step: 3.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 9.48s
                        Total time: 15320.50s
                               ETA: 1026900.6s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.469s, learning 0.159s)
               Value function loss: 93.0559
                    Surrogate loss: -0.0209
             Mean action noise std: 0.76
                       Mean reward: 427.23
               Mean episode length: 150.00
                  Mean reward/step: 3.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 9.63s
                        Total time: 15330.12s
                               ETA: 1026837.0s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.284s, learning 0.164s)
               Value function loss: 92.2220
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 448.60
               Mean episode length: 149.76
                  Mean reward/step: 2.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 9.45s
                        Total time: 15339.57s
                               ETA: 1026761.3s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.243s, learning 0.164s)
               Value function loss: 97.6193
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 434.34
               Mean episode length: 150.00
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 9.41s
                        Total time: 15348.98s
                               ETA: 1026683.1s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.315s, learning 0.161s)
               Value function loss: 78.0840
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 416.05
               Mean episode length: 148.97
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 9.48s
                        Total time: 15358.46s
                               ETA: 1026609.6s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.549s, learning 0.161s)
               Value function loss: 97.3664
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 455.26
               Mean episode length: 149.56
                  Mean reward/step: 3.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 9.71s
                        Total time: 15368.17s
                               ETA: 1026551.8s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.518s, learning 0.160s)
               Value function loss: 106.9426
                    Surrogate loss: -0.0199
             Mean action noise std: 0.76
                       Mean reward: 443.56
               Mean episode length: 149.17
                  Mean reward/step: 3.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 9.68s
                        Total time: 15377.84s
                               ETA: 1026491.9s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.419s, learning 0.164s)
               Value function loss: 89.6966
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 440.98
               Mean episode length: 148.68
                  Mean reward/step: 3.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 9.58s
                        Total time: 15387.43s
                               ETA: 1026425.7s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.488s, learning 0.161s)
               Value function loss: 91.3918
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 474.11
               Mean episode length: 149.42
                  Mean reward/step: 3.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 9.65s
                        Total time: 15397.08s
                               ETA: 1026364.1s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.323s, learning 0.164s)
               Value function loss: 105.5590
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 458.46
               Mean episode length: 149.15
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 9.49s
                        Total time: 15406.56s
                               ETA: 1026291.6s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.418s, learning 0.158s)
               Value function loss: 96.4368
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 472.42
               Mean episode length: 149.06
                  Mean reward/step: 3.13
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 9.58s
                        Total time: 15416.14s
                               ETA: 1026225.2s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.114s, learning 0.165s)
               Value function loss: 86.3402
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 467.43
               Mean episode length: 148.78
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 9.28s
                        Total time: 15425.42s
                               ETA: 1026139.2s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.596s, learning 0.176s)
               Value function loss: 81.9620
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 451.70
               Mean episode length: 150.00
                  Mean reward/step: 3.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 9.77s
                        Total time: 15435.19s
                               ETA: 1026086.0s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.154s, learning 0.163s)
               Value function loss: 83.6483
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 451.83
               Mean episode length: 149.52
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 9.32s
                        Total time: 15444.51s
                               ETA: 1026002.6s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.596s, learning 0.161s)
               Value function loss: 95.7982
                    Surrogate loss: -0.0132
             Mean action noise std: 0.76
                       Mean reward: 467.87
               Mean episode length: 148.17
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 9.76s
                        Total time: 15454.26s
                               ETA: 1025948.6s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.421s, learning 0.161s)
               Value function loss: 87.9813
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 441.01
               Mean episode length: 149.43
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 9.58s
                        Total time: 15463.85s
                               ETA: 1025883.0s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.415s, learning 0.173s)
               Value function loss: 105.5837
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 482.41
               Mean episode length: 149.91
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 9.59s
                        Total time: 15473.43s
                               ETA: 1025817.9s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.349s, learning 0.158s)
               Value function loss: 101.7747
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 474.77
               Mean episode length: 149.29
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 9.51s
                        Total time: 15482.94s
                               ETA: 1025747.5s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.520s, learning 0.158s)
               Value function loss: 111.6972
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 469.30
               Mean episode length: 149.15
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 9.68s
                        Total time: 15492.62s
                               ETA: 1025688.5s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.318s, learning 0.165s)
               Value function loss: 98.3248
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 474.76
               Mean episode length: 150.00
                  Mean reward/step: 3.22
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 9.48s
                        Total time: 15502.10s
                               ETA: 1025616.6s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.585s, learning 0.165s)
               Value function loss: 112.6719
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 492.40
               Mean episode length: 149.42
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 9.75s
                        Total time: 15511.85s
                               ETA: 1025562.5s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.633s, learning 0.158s)
               Value function loss: 114.8061
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 471.06
               Mean episode length: 149.09
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 9.79s
                        Total time: 15521.64s
                               ETA: 1025511.2s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.573s, learning 0.168s)
               Value function loss: 108.9580
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 469.69
               Mean episode length: 150.00
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 9.74s
                        Total time: 15531.38s
                               ETA: 1025456.6s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.262s, learning 0.169s)
               Value function loss: 89.3608
                    Surrogate loss: -0.0266
             Mean action noise std: 0.76
                       Mean reward: 487.55
               Mean episode length: 149.26
                  Mean reward/step: 3.10
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 9.43s
                        Total time: 15540.82s
                               ETA: 1025381.5s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.486s, learning 0.161s)
               Value function loss: 106.0361
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 466.17
               Mean episode length: 149.57
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 9.65s
                        Total time: 15550.46s
                               ETA: 1025320.9s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.470s, learning 0.165s)
               Value function loss: 109.6961
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 473.96
               Mean episode length: 150.00
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 9.64s
                        Total time: 15560.10s
                               ETA: 1025259.5s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.512s, learning 0.177s)
               Value function loss: 111.3949
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 474.54
               Mean episode length: 149.48
                  Mean reward/step: 3.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 9.69s
                        Total time: 15569.79s
                               ETA: 1025201.8s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.528s, learning 0.165s)
               Value function loss: 95.1975
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 441.72
               Mean episode length: 149.17
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 9.69s
                        Total time: 15579.48s
                               ETA: 1025144.4s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.631s, learning 0.163s)
               Value function loss: 100.3667
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 495.00
               Mean episode length: 149.82
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 9.79s
                        Total time: 15589.27s
                               ETA: 1025093.6s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.596s, learning 0.161s)
               Value function loss: 101.9378
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 487.76
               Mean episode length: 149.53
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 9.76s
                        Total time: 15599.03s
                               ETA: 1025040.5s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.053s, learning 0.161s)
               Value function loss: 100.8823
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 467.86
               Mean episode length: 149.31
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 9.21s
                        Total time: 15608.25s
                               ETA: 1024951.8s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.476s, learning 0.159s)
               Value function loss: 100.8054
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 493.10
               Mean episode length: 150.00
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 9.64s
                        Total time: 15617.88s
                               ETA: 1024890.9s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.541s, learning 0.159s)
               Value function loss: 97.2158
                    Surrogate loss: -0.0228
             Mean action noise std: 0.76
                       Mean reward: 468.76
               Mean episode length: 148.64
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 9.70s
                        Total time: 15627.58s
                               ETA: 1024834.3s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.262s, learning 0.161s)
               Value function loss: 113.3685
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 487.77
               Mean episode length: 149.43
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 9.42s
                        Total time: 15637.00s
                               ETA: 1024759.5s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.432s, learning 0.168s)
               Value function loss: 97.4196
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 484.17
               Mean episode length: 150.00
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 9.60s
                        Total time: 15646.60s
                               ETA: 1024696.5s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.641s, learning 0.190s)
               Value function loss: 121.7743
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 484.64
               Mean episode length: 149.08
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 9.83s
                        Total time: 15656.43s
                               ETA: 1024648.6s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.735s, learning 0.184s)
               Value function loss: 117.6699
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 495.05
               Mean episode length: 149.83
                  Mean reward/step: 3.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 9.92s
                        Total time: 15666.35s
                               ETA: 1024606.5s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.715s, learning 0.191s)
               Value function loss: 108.6993
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 482.06
               Mean episode length: 149.46
                  Mean reward/step: 3.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 9.91s
                        Total time: 15676.26s
                               ETA: 1024563.7s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.635s, learning 0.180s)
               Value function loss: 101.6361
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 479.95
               Mean episode length: 149.43
                  Mean reward/step: 3.24
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 9.82s
                        Total time: 15686.07s
                               ETA: 1024514.9s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.599s, learning 0.193s)
               Value function loss: 110.2622
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 479.03
               Mean episode length: 149.77
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 9.79s
                        Total time: 15695.87s
                               ETA: 1024464.8s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.586s, learning 0.172s)
               Value function loss: 140.1228
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 497.08
               Mean episode length: 150.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 9.76s
                        Total time: 15705.63s
                               ETA: 1024412.4s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.820s, learning 0.161s)
               Value function loss: 125.8099
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 478.50
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 9.98s
                        Total time: 15715.61s
                               ETA: 1024374.7s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.881s, learning 0.172s)
               Value function loss: 90.2514
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 466.13
               Mean episode length: 149.92
                  Mean reward/step: 3.10
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 10.05s
                        Total time: 15725.66s
                               ETA: 1024341.6s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.175s, learning 0.177s)
               Value function loss: 113.6165
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 480.38
               Mean episode length: 149.09
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 9.35s
                        Total time: 15735.01s
                               ETA: 1024263.0s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.303s, learning 0.162s)
               Value function loss: 106.7842
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 488.11
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 9.46s
                        Total time: 15744.48s
                               ETA: 1024191.8s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.440s, learning 0.177s)
               Value function loss: 94.4127
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 470.84
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 9.62s
                        Total time: 15754.09s
                               ETA: 1024130.5s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.656s, learning 0.160s)
               Value function loss: 94.0123
                    Surrogate loss: -0.0182
             Mean action noise std: 0.76
                       Mean reward: 480.01
               Mean episode length: 150.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 9.82s
                        Total time: 15763.91s
                               ETA: 1024082.2s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.284s, learning 0.175s)
               Value function loss: 97.3725
                    Surrogate loss: -0.0187
             Mean action noise std: 0.76
                       Mean reward: 487.31
               Mean episode length: 149.89
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 9.46s
                        Total time: 15773.37s
                               ETA: 1024010.8s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.505s, learning 0.161s)
               Value function loss: 97.3786
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 478.55
               Mean episode length: 150.00
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 9.67s
                        Total time: 15783.04s
                               ETA: 1023953.0s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.533s, learning 0.184s)
               Value function loss: 89.8567
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 472.55
               Mean episode length: 149.63
                  Mean reward/step: 3.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 9.72s
                        Total time: 15792.75s
                               ETA: 1023898.5s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.298s, learning 0.162s)
               Value function loss: 87.8203
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 486.73
               Mean episode length: 149.76
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 9.46s
                        Total time: 15802.21s
                               ETA: 1023827.4s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.301s, learning 0.167s)
               Value function loss: 104.9415
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 490.48
               Mean episode length: 149.29
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 9.47s
                        Total time: 15811.68s
                               ETA: 1023756.8s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.612s, learning 0.170s)
               Value function loss: 90.0677
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 509.69
               Mean episode length: 150.00
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 9.78s
                        Total time: 15821.46s
                               ETA: 1023706.8s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.444s, learning 0.161s)
               Value function loss: 94.1178
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 490.31
               Mean episode length: 150.00
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 9.61s
                        Total time: 15831.07s
                               ETA: 1023645.3s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.733s, learning 0.175s)
               Value function loss: 103.9393
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 480.46
               Mean episode length: 150.00
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 8.91s
                        Total time: 15839.98s
                               ETA: 1023538.9s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.591s, learning 0.171s)
               Value function loss: 107.3792
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 474.95
               Mean episode length: 150.00
                  Mean reward/step: 3.32
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 9.76s
                        Total time: 15849.74s
                               ETA: 1023487.7s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.068s, learning 0.162s)
               Value function loss: 90.3680
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 467.04
               Mean episode length: 149.48
                  Mean reward/step: 3.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 9.23s
                        Total time: 15858.97s
                               ETA: 1023402.2s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.473s, learning 0.166s)
               Value function loss: 101.2864
                    Surrogate loss: -0.0156
             Mean action noise std: 0.76
                       Mean reward: 478.96
               Mean episode length: 150.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 9.64s
                        Total time: 15868.61s
                               ETA: 1023343.3s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.411s, learning 0.158s)
               Value function loss: 103.9180
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 461.69
               Mean episode length: 149.53
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 9.57s
                        Total time: 15878.18s
                               ETA: 1023279.9s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.841s, learning 0.158s)
               Value function loss: 109.8331
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 466.19
               Mean episode length: 150.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 10.00s
                        Total time: 15888.17s
                               ETA: 1023244.2s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.376s, learning 0.159s)
               Value function loss: 104.2962
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 512.56
               Mean episode length: 149.47
                  Mean reward/step: 3.13
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 9.53s
                        Total time: 15897.71s
                               ETA: 1023178.6s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.563s, learning 0.163s)
               Value function loss: 103.0041
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 503.28
               Mean episode length: 150.00
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 9.73s
                        Total time: 15907.43s
                               ETA: 1023125.5s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.562s, learning 0.160s)
               Value function loss: 100.8947
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 467.52
               Mean episode length: 150.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 9.72s
                        Total time: 15917.16s
                               ETA: 1023072.1s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.611s, learning 0.175s)
               Value function loss: 100.1510
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 473.54
               Mean episode length: 150.00
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 9.79s
                        Total time: 15926.94s
                               ETA: 1023023.0s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1780 steps/s (collection: 9.024s, learning 0.178s)
               Value function loss: 105.0929
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 500.08
               Mean episode length: 150.00
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 9.20s
                        Total time: 15936.14s
                               ETA: 1022936.4s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.167s)
               Value function loss: 104.7421
                    Surrogate loss: -0.0228
             Mean action noise std: 0.76
                       Mean reward: 481.25
               Mean episode length: 149.48
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 9.69s
                        Total time: 15945.83s
                               ETA: 1022881.0s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.582s, learning 0.172s)
               Value function loss: 124.9185
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 483.19
               Mean episode length: 149.15
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 9.75s
                        Total time: 15955.59s
                               ETA: 1022830.0s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1601 steps/s (collection: 10.059s, learning 0.169s)
               Value function loss: 120.9851
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 517.07
               Mean episode length: 150.00
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 10.23s
                        Total time: 15965.82s
                               ETA: 1022809.4s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.540s, learning 0.168s)
               Value function loss: 122.7476
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 490.30
               Mean episode length: 150.00
                  Mean reward/step: 3.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 9.71s
                        Total time: 15975.52s
                               ETA: 1022755.5s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.630s, learning 0.165s)
               Value function loss: 124.3084
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 462.62
               Mean episode length: 150.00
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 9.79s
                        Total time: 15985.32s
                               ETA: 1022707.2s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.566s, learning 0.187s)
               Value function loss: 143.1970
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 496.84
               Mean episode length: 150.00
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 9.75s
                        Total time: 15995.07s
                               ETA: 1022656.3s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.712s, learning 0.176s)
               Value function loss: 136.1675
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 500.35
               Mean episode length: 150.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 9.89s
                        Total time: 16004.96s
                               ETA: 1022614.1s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.569s, learning 0.174s)
               Value function loss: 140.9549
                    Surrogate loss: -0.0050
             Mean action noise std: 0.76
                       Mean reward: 485.65
               Mean episode length: 150.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 9.74s
                        Total time: 16014.70s
                               ETA: 1022562.7s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.553s, learning 0.160s)
               Value function loss: 132.3631
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 509.80
               Mean episode length: 149.57
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 9.71s
                        Total time: 16024.42s
                               ETA: 1022509.4s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1644 steps/s (collection: 9.793s, learning 0.170s)
               Value function loss: 149.0697
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 512.37
               Mean episode length: 150.00
                  Mean reward/step: 3.43
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 9.96s
                        Total time: 16034.38s
                               ETA: 1022472.1s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.438s, learning 0.172s)
               Value function loss: 128.7334
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 502.66
               Mean episode length: 150.00
                  Mean reward/step: 3.41
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 9.61s
                        Total time: 16043.99s
                               ETA: 1022412.3s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1597 steps/s (collection: 10.091s, learning 0.162s)
               Value function loss: 133.6961
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 507.48
               Mean episode length: 150.00
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 10.25s
                        Total time: 16054.24s
                               ETA: 1022393.5s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.158s, learning 0.162s)
               Value function loss: 149.9366
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 523.03
               Mean episode length: 150.00
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 9.32s
                        Total time: 16063.56s
                               ETA: 1022315.4s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.543s, learning 0.159s)
               Value function loss: 137.9965
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 523.56
               Mean episode length: 150.00
                  Mean reward/step: 3.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 9.70s
                        Total time: 16073.26s
                               ETA: 1022261.7s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.176s, learning 0.161s)
               Value function loss: 97.7219
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 495.97
               Mean episode length: 150.00
                  Mean reward/step: 3.23
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 9.34s
                        Total time: 16082.60s
                               ETA: 1022184.8s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.323s, learning 0.168s)
               Value function loss: 125.7175
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 487.84
               Mean episode length: 150.00
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 9.49s
                        Total time: 16092.09s
                               ETA: 1022117.8s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.413s, learning 0.160s)
               Value function loss: 136.8477
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 494.04
               Mean episode length: 150.00
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 9.57s
                        Total time: 16101.67s
                               ETA: 1022056.1s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.456s, learning 0.164s)
               Value function loss: 121.0941
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 496.51
               Mean episode length: 150.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 9.62s
                        Total time: 16111.29s
                               ETA: 1021997.4s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.287s, learning 0.160s)
               Value function loss: 112.0506
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 501.72
               Mean episode length: 150.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 9.45s
                        Total time: 16120.73s
                               ETA: 1021927.8s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.307s, learning 0.162s)
               Value function loss: 112.1124
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 497.55
               Mean episode length: 149.37
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 9.47s
                        Total time: 16130.20s
                               ETA: 1021859.7s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.584s, learning 0.174s)
               Value function loss: 106.8414
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 511.85
               Mean episode length: 150.00
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 9.76s
                        Total time: 16139.96s
                               ETA: 1021809.9s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.544s, learning 0.161s)
               Value function loss: 102.4877
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 504.77
               Mean episode length: 150.00
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 9.71s
                        Total time: 16149.66s
                               ETA: 1021756.9s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.684s, learning 0.174s)
               Value function loss: 98.1450
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 483.01
               Mean episode length: 149.47
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 9.86s
                        Total time: 16159.52s
                               ETA: 1021713.6s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.753s, learning 0.182s)
               Value function loss: 100.7466
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 515.16
               Mean episode length: 150.00
                  Mean reward/step: 3.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 9.93s
                        Total time: 16169.46s
                               ETA: 1021675.2s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.680s, learning 0.161s)
               Value function loss: 100.2378
                    Surrogate loss: -0.0221
             Mean action noise std: 0.76
                       Mean reward: 505.15
               Mean episode length: 150.00
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 9.84s
                        Total time: 16179.30s
                               ETA: 1021630.9s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.851s, learning 0.165s)
               Value function loss: 92.6317
                    Surrogate loss: -0.0218
             Mean action noise std: 0.76
                       Mean reward: 470.25
               Mean episode length: 148.60
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 9.02s
                        Total time: 16188.32s
                               ETA: 1021534.6s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.373s, learning 0.160s)
               Value function loss: 111.3691
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 510.03
               Mean episode length: 150.00
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 9.53s
                        Total time: 16197.85s
                               ETA: 1021471.0s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.640s, learning 0.158s)
               Value function loss: 116.1460
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 537.30
               Mean episode length: 149.62
                  Mean reward/step: 3.43
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 9.80s
                        Total time: 16207.65s
                               ETA: 1021424.1s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.530s, learning 0.167s)
               Value function loss: 119.5319
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 496.21
               Mean episode length: 150.00
                  Mean reward/step: 3.37
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 9.70s
                        Total time: 16217.34s
                               ETA: 1021371.0s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.253s, learning 0.171s)
               Value function loss: 107.4131
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 494.17
               Mean episode length: 150.00
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 9.42s
                        Total time: 16226.77s
                               ETA: 1021300.7s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.581s, learning 0.173s)
               Value function loss: 118.1304
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 500.57
               Mean episode length: 150.00
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 9.75s
                        Total time: 16236.52s
                               ETA: 1021251.3s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.604s, learning 0.186s)
               Value function loss: 130.9920
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 493.91
               Mean episode length: 150.00
                  Mean reward/step: 3.23
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 9.79s
                        Total time: 16246.31s
                               ETA: 1021204.1s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.798s, learning 0.181s)
               Value function loss: 134.6682
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 495.49
               Mean episode length: 150.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 9.98s
                        Total time: 16256.29s
                               ETA: 1021168.9s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.953s, learning 0.166s)
               Value function loss: 91.5529
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 467.37
               Mean episode length: 150.00
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 9.12s
                        Total time: 16265.41s
                               ETA: 1021079.7s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.335s, learning 0.158s)
               Value function loss: 110.3373
                    Surrogate loss: -0.0149
             Mean action noise std: 0.76
                       Mean reward: 498.11
               Mean episode length: 149.86
                  Mean reward/step: 3.13
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 9.49s
                        Total time: 16274.90s
                               ETA: 1021014.1s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.637s, learning 0.169s)
               Value function loss: 118.7853
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 489.51
               Mean episode length: 150.00
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 9.81s
                        Total time: 16284.71s
                               ETA: 1020968.2s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.190s, learning 0.161s)
               Value function loss: 111.8935
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 493.01
               Mean episode length: 150.00
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 9.35s
                        Total time: 16294.06s
                               ETA: 1020893.9s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.405s, learning 0.165s)
               Value function loss: 111.0757
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 486.15
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 9.57s
                        Total time: 16303.63s
                               ETA: 1020833.3s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.558s, learning 0.165s)
               Value function loss: 111.4117
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 512.27
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 9.72s
                        Total time: 16313.35s
                               ETA: 1020782.3s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.625s, learning 0.166s)
               Value function loss: 100.3760
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 484.29
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 9.79s
                        Total time: 16323.14s
                               ETA: 1020735.7s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.648s, learning 0.159s)
               Value function loss: 100.0665
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 475.11
               Mean episode length: 150.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 9.81s
                        Total time: 16332.95s
                               ETA: 1020690.1s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.807s, learning 0.160s)
               Value function loss: 104.0477
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 482.88
               Mean episode length: 149.79
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 9.97s
                        Total time: 16342.92s
                               ETA: 1020654.5s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.455s, learning 0.162s)
               Value function loss: 119.0723
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 472.65
               Mean episode length: 150.00
                  Mean reward/step: 3.20
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 9.62s
                        Total time: 16352.53s
                               ETA: 1020597.2s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.500s, learning 0.163s)
               Value function loss: 107.3301
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 482.43
               Mean episode length: 150.00
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 9.66s
                        Total time: 16362.20s
                               ETA: 1020542.8s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.475s, learning 0.165s)
               Value function loss: 110.8398
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 453.20
               Mean episode length: 149.03
                  Mean reward/step: 3.24
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 9.64s
                        Total time: 16371.84s
                               ETA: 1020487.0s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.661s, learning 0.168s)
               Value function loss: 115.6938
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 472.42
               Mean episode length: 150.00
                  Mean reward/step: 3.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 9.83s
                        Total time: 16381.67s
                               ETA: 1020443.0s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.898s, learning 0.159s)
               Value function loss: 120.4620
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 467.29
               Mean episode length: 149.30
                  Mean reward/step: 3.20
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 9.06s
                        Total time: 16390.72s
                               ETA: 1020351.0s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.560s, learning 0.163s)
               Value function loss: 111.2260
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: 438.11
               Mean episode length: 149.96
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 9.72s
                        Total time: 16400.45s
                               ETA: 1020300.5s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.620s, learning 0.173s)
               Value function loss: 105.3341
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: 485.80
               Mean episode length: 150.00
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 9.79s
                        Total time: 16410.24s
                               ETA: 1020254.5s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.145s, learning 0.160s)
               Value function loss: 126.9458
                    Surrogate loss: -0.0222
             Mean action noise std: 0.76
                       Mean reward: 469.60
               Mean episode length: 150.00
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 9.30s
                        Total time: 16419.54s
                               ETA: 1020178.1s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.682s, learning 0.167s)
               Value function loss: 149.9077
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 478.24
               Mean episode length: 150.00
                  Mean reward/step: 2.93
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 9.85s
                        Total time: 16429.39s
                               ETA: 1020135.7s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.495s, learning 0.175s)
               Value function loss: 135.7968
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 451.29
               Mean episode length: 150.00
                  Mean reward/step: 2.89
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 9.67s
                        Total time: 16439.06s
                               ETA: 1020082.2s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.385s, learning 0.171s)
               Value function loss: 120.2857
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 464.31
               Mean episode length: 150.00
                  Mean reward/step: 2.91
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 9.56s
                        Total time: 16448.62s
                               ETA: 1020021.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.117s, learning 0.172s)
               Value function loss: 121.4171
                    Surrogate loss: -0.0242
             Mean action noise std: 0.76
                       Mean reward: 429.11
               Mean episode length: 150.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 9.29s
                        Total time: 16457.91s
                               ETA: 1019944.6s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.587s, learning 0.174s)
               Value function loss: 111.5257
                    Surrogate loss: -0.0181
             Mean action noise std: 0.76
                       Mean reward: 463.66
               Mean episode length: 150.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 9.76s
                        Total time: 16467.67s
                               ETA: 1019896.9s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.320s, learning 0.178s)
               Value function loss: 105.5491
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 445.30
               Mean episode length: 150.00
                  Mean reward/step: 2.92
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 9.50s
                        Total time: 16477.17s
                               ETA: 1019833.0s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.391s, learning 0.172s)
               Value function loss: 96.7057
                    Surrogate loss: -0.0218
             Mean action noise std: 0.76
                       Mean reward: 435.78
               Mean episode length: 150.00
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 9.56s
                        Total time: 16486.73s
                               ETA: 1019773.1s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.479s, learning 0.173s)
               Value function loss: 101.8981
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 450.89
               Mean episode length: 149.51
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 9.65s
                        Total time: 16496.38s
                               ETA: 1019718.8s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.407s, learning 0.172s)
               Value function loss: 89.1991
                    Surrogate loss: -0.0188
             Mean action noise std: 0.76
                       Mean reward: 433.49
               Mean episode length: 150.00
                  Mean reward/step: 3.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 9.58s
                        Total time: 16505.96s
                               ETA: 1019660.1s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.377s, learning 0.161s)
               Value function loss: 96.4366
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: 466.49
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 9.54s
                        Total time: 16515.50s
                               ETA: 1019599.0s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.465s, learning 0.166s)
               Value function loss: 112.4987
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 465.89
               Mean episode length: 149.51
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 9.63s
                        Total time: 16525.13s
                               ETA: 1019543.6s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.466s, learning 0.175s)
               Value function loss: 112.7086
                    Surrogate loss: -0.0233
             Mean action noise std: 0.76
                       Mean reward: 453.41
               Mean episode length: 150.00
                  Mean reward/step: 3.17
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 9.64s
                        Total time: 16534.77s
                               ETA: 1019488.8s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.105s, learning 0.163s)
               Value function loss: 108.1033
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 483.17
               Mean episode length: 150.00
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 9.27s
                        Total time: 16544.04s
                               ETA: 1019411.2s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.727s, learning 0.160s)
               Value function loss: 119.5701
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 448.10
               Mean episode length: 149.22
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 9.89s
                        Total time: 16553.93s
                               ETA: 1019371.8s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.442s, learning 0.162s)
               Value function loss: 119.2019
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 442.96
               Mean episode length: 149.37
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 9.60s
                        Total time: 16563.53s
                               ETA: 1019315.0s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.156s, learning 0.159s)
               Value function loss: 137.0905
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 467.85
               Mean episode length: 150.00
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 9.32s
                        Total time: 16572.85s
                               ETA: 1019240.4s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.333s, learning 0.160s)
               Value function loss: 122.6227
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 481.48
               Mean episode length: 149.48
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 9.49s
                        Total time: 16582.34s
                               ETA: 1019176.9s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1623 steps/s (collection: 9.932s, learning 0.160s)
               Value function loss: 132.6184
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 454.01
               Mean episode length: 149.24
                  Mean reward/step: 3.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 10.09s
                        Total time: 16592.43s
                               ETA: 1019150.2s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.614s, learning 0.159s)
               Value function loss: 147.6619
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 460.33
               Mean episode length: 148.39
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 9.77s
                        Total time: 16602.20s
                               ETA: 1019104.0s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.595s, learning 0.159s)
               Value function loss: 145.0444
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 488.47
               Mean episode length: 149.28
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 9.75s
                        Total time: 16611.96s
                               ETA: 1019056.7s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.504s, learning 0.173s)
               Value function loss: 122.1744
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 430.28
               Mean episode length: 148.63
                  Mean reward/step: 2.84
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 9.68s
                        Total time: 16621.64s
                               ETA: 1019004.7s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.192s, learning 0.162s)
               Value function loss: 107.6684
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 429.84
               Mean episode length: 148.79
                  Mean reward/step: 2.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 9.35s
                        Total time: 16630.99s
                               ETA: 1018932.9s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.412s, learning 0.166s)
               Value function loss: 116.0267
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 418.20
               Mean episode length: 149.31
                  Mean reward/step: 2.88
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 9.58s
                        Total time: 16640.57s
                               ETA: 1018874.9s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.106s, learning 0.163s)
               Value function loss: 105.1536
                    Surrogate loss: -0.0196
             Mean action noise std: 0.76
                       Mean reward: 454.92
               Mean episode length: 149.28
                  Mean reward/step: 2.87
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 9.27s
                        Total time: 16649.84s
                               ETA: 1018798.1s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.438s, learning 0.167s)
               Value function loss: 91.6840
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 448.44
               Mean episode length: 149.15
                  Mean reward/step: 2.94
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 9.60s
                        Total time: 16659.44s
                               ETA: 1018741.9s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.537s, learning 0.158s)
               Value function loss: 101.7735
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 442.17
               Mean episode length: 148.60
                  Mean reward/step: 2.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 9.69s
                        Total time: 16669.14s
                               ETA: 1018691.2s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.468s, learning 0.166s)
               Value function loss: 108.1135
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 454.42
               Mean episode length: 149.30
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 9.63s
                        Total time: 16678.77s
                               ETA: 1018636.9s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.421s, learning 0.161s)
               Value function loss: 99.0084
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 438.86
               Mean episode length: 147.76
                  Mean reward/step: 3.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 9.58s
                        Total time: 16688.35s
                               ETA: 1018579.5s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.339s, learning 0.165s)
               Value function loss: 91.5541
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 441.27
               Mean episode length: 147.73
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 9.50s
                        Total time: 16697.85s
                               ETA: 1018517.4s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.907s, learning 0.161s)
               Value function loss: 98.4381
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 436.92
               Mean episode length: 149.30
                  Mean reward/step: 3.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 9.07s
                        Total time: 16706.92s
                               ETA: 1018428.7s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.262s, learning 0.164s)
               Value function loss: 100.3891
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 432.25
               Mean episode length: 148.57
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 9.43s
                        Total time: 16716.35s
                               ETA: 1018362.0s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.396s, learning 0.161s)
               Value function loss: 101.1655
                    Surrogate loss: -0.0118
             Mean action noise std: 0.76
                       Mean reward: 448.57
               Mean episode length: 148.89
                  Mean reward/step: 3.20
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 9.56s
                        Total time: 16725.91s
                               ETA: 1018303.3s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.468s, learning 0.160s)
               Value function loss: 119.2189
                    Surrogate loss: -0.0203
             Mean action noise std: 0.76
                       Mean reward: 466.21
               Mean episode length: 149.67
                  Mean reward/step: 3.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 9.63s
                        Total time: 16735.53s
                               ETA: 1018249.1s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.325s, learning 0.161s)
               Value function loss: 115.3121
                    Surrogate loss: -0.0184
             Mean action noise std: 0.76
                       Mean reward: 457.82
               Mean episode length: 149.00
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 9.49s
                        Total time: 16745.02s
                               ETA: 1018186.2s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.757s, learning 0.165s)
               Value function loss: 121.1637
                    Surrogate loss: -0.0176
             Mean action noise std: 0.76
                       Mean reward: 471.69
               Mean episode length: 147.41
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 9.92s
                        Total time: 16754.94s
                               ETA: 1018149.9s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.389s, learning 0.161s)
               Value function loss: 106.4811
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 451.90
               Mean episode length: 149.10
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 9.55s
                        Total time: 16764.49s
                               ETA: 1018091.0s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.783s, learning 0.162s)
               Value function loss: 105.1526
                    Surrogate loss: -0.0163
             Mean action noise std: 0.76
                       Mean reward: 450.48
               Mean episode length: 149.95
                  Mean reward/step: 3.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 9.94s
                        Total time: 16774.44s
                               ETA: 1018056.1s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.365s, learning 0.176s)
               Value function loss: 111.7156
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 431.93
               Mean episode length: 149.73
                  Mean reward/step: 2.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 9.54s
                        Total time: 16783.98s
                               ETA: 1017996.8s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.713s, learning 0.166s)
               Value function loss: 119.7546
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 446.58
               Mean episode length: 150.00
                  Mean reward/step: 2.95
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 9.88s
                        Total time: 16793.86s
                               ETA: 1017958.1s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.512s, learning 0.167s)
               Value function loss: 105.6861
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 456.25
               Mean episode length: 148.09
                  Mean reward/step: 2.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 9.68s
                        Total time: 16803.53s
                               ETA: 1017907.2s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.216s, learning 0.173s)
               Value function loss: 108.5640
                    Surrogate loss: -0.0186
             Mean action noise std: 0.76
                       Mean reward: 460.43
               Mean episode length: 149.66
                  Mean reward/step: 2.91
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 9.39s
                        Total time: 16812.92s
                               ETA: 1017838.9s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.567s, learning 0.160s)
               Value function loss: 109.5473
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 467.28
               Mean episode length: 149.29
                  Mean reward/step: 2.86
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 9.73s
                        Total time: 16822.65s
                               ETA: 1017791.1s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.256s, learning 0.163s)
               Value function loss: 100.6186
                    Surrogate loss: -0.0229
             Mean action noise std: 0.76
                       Mean reward: 450.19
               Mean episode length: 149.30
                  Mean reward/step: 2.80
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 9.42s
                        Total time: 16832.07s
                               ETA: 1017724.7s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.369s, learning 0.165s)
               Value function loss: 101.0447
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: 477.43
               Mean episode length: 149.94
                  Mean reward/step: 2.79
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 9.53s
                        Total time: 16841.60s
                               ETA: 1017665.3s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.739s, learning 0.158s)
               Value function loss: 116.3595
                    Surrogate loss: -0.0219
             Mean action noise std: 0.76
                       Mean reward: 467.25
               Mean episode length: 149.73
                  Mean reward/step: 2.80
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 9.90s
                        Total time: 16851.50s
                               ETA: 1017627.9s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.348s, learning 0.158s)
               Value function loss: 109.1457
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 452.27
               Mean episode length: 149.61
                  Mean reward/step: 2.80
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 9.51s
                        Total time: 16861.01s
                               ETA: 1017567.0s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.413s, learning 0.160s)
               Value function loss: 95.7558
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 432.85
               Mean episode length: 149.91
                  Mean reward/step: 2.85
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 9.57s
                        Total time: 16870.58s
                               ETA: 1017510.1s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.385s, learning 0.165s)
               Value function loss: 98.1825
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 444.45
               Mean episode length: 150.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 9.55s
                        Total time: 16880.13s
                               ETA: 1017452.0s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.217s, learning 0.161s)
               Value function loss: 95.6124
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 440.57
               Mean episode length: 149.69
                  Mean reward/step: 2.94
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 9.38s
                        Total time: 16889.51s
                               ETA: 1017383.4s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.615s, learning 0.160s)
               Value function loss: 98.3348
                    Surrogate loss: -0.0248
             Mean action noise std: 0.76
                       Mean reward: 439.67
               Mean episode length: 150.00
                  Mean reward/step: 2.95
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 9.77s
                        Total time: 16899.28s
                               ETA: 1017338.9s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.318s, learning 0.178s)
               Value function loss: 103.6256
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 414.33
               Mean episode length: 149.39
                  Mean reward/step: 3.00
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 9.50s
                        Total time: 16908.78s
                               ETA: 1017277.6s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.424s, learning 0.159s)
               Value function loss: 105.8185
                    Surrogate loss: -0.0216
             Mean action noise std: 0.76
                       Mean reward: 447.61
               Mean episode length: 150.00
                  Mean reward/step: 2.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 9.58s
                        Total time: 16918.36s
                               ETA: 1017221.6s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.716s, learning 0.169s)
               Value function loss: 101.1046
                    Surrogate loss: -0.0223
             Mean action noise std: 0.76
                       Mean reward: 390.94
               Mean episode length: 149.13
                  Mean reward/step: 2.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 9.89s
                        Total time: 16928.25s
                               ETA: 1017183.9s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.409s, learning 0.176s)
               Value function loss: 105.6729
                    Surrogate loss: -0.0228
             Mean action noise std: 0.76
                       Mean reward: 402.77
               Mean episode length: 149.46
                  Mean reward/step: 2.94
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 9.58s
                        Total time: 16937.83s
                               ETA: 1017128.2s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.696s, learning 0.161s)
               Value function loss: 95.0558
                    Surrogate loss: -0.0257
             Mean action noise std: 0.76
                       Mean reward: 434.74
               Mean episode length: 149.80
                  Mean reward/step: 2.89
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 9.86s
                        Total time: 16947.69s
                               ETA: 1017088.7s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.385s, learning 0.164s)
               Value function loss: 106.9525
                    Surrogate loss: -0.0173
             Mean action noise std: 0.76
                       Mean reward: 402.35
               Mean episode length: 150.00
                  Mean reward/step: 2.89
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 9.55s
                        Total time: 16957.24s
                               ETA: 1017031.0s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.519s, learning 0.161s)
               Value function loss: 113.3204
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 434.68
               Mean episode length: 150.00
                  Mean reward/step: 2.87
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 9.68s
                        Total time: 16966.92s
                               ETA: 1016981.1s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.996s, learning 0.159s)
               Value function loss: 120.5933
                    Surrogate loss: -0.0269
             Mean action noise std: 0.76
                       Mean reward: 433.32
               Mean episode length: 148.99
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 9.16s
                        Total time: 16976.07s
                               ETA: 1016899.9s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.742s, learning 0.165s)
               Value function loss: 108.1076
                    Surrogate loss: -0.0142
             Mean action noise std: 0.76
                       Mean reward: 437.04
               Mean episode length: 148.92
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 9.91s
                        Total time: 16985.98s
                               ETA: 1016863.7s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.632s, learning 0.166s)
               Value function loss: 105.4916
                    Surrogate loss: -0.0233
             Mean action noise std: 0.76
                       Mean reward: 443.93
               Mean episode length: 149.36
                  Mean reward/step: 2.84
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 9.80s
                        Total time: 16995.78s
                               ETA: 1016821.0s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.588s, learning 0.162s)
               Value function loss: 119.2371
                    Surrogate loss: -0.0206
             Mean action noise std: 0.76
                       Mean reward: 430.77
               Mean episode length: 150.00
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 9.75s
                        Total time: 17005.53s
                               ETA: 1016775.4s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.740s, learning 0.170s)
               Value function loss: 115.9889
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 434.42
               Mean episode length: 149.37
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 9.91s
                        Total time: 17015.44s
                               ETA: 1016739.6s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.589s, learning 0.166s)
               Value function loss: 99.0800
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 419.34
               Mean episode length: 150.00
                  Mean reward/step: 2.97
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 9.75s
                        Total time: 17025.19s
                               ETA: 1016694.5s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.645s, learning 0.171s)
               Value function loss: 110.5894
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 452.89
               Mean episode length: 150.00
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 9.82s
                        Total time: 17035.01s
                               ETA: 1016653.0s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.635s, learning 0.163s)
               Value function loss: 107.1996
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 430.11
               Mean episode length: 149.53
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 9.80s
                        Total time: 17044.81s
                               ETA: 1016610.5s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.599s, learning 0.172s)
               Value function loss: 98.2009
                    Surrogate loss: -0.0259
             Mean action noise std: 0.76
                       Mean reward: 442.71
               Mean episode length: 150.00
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 9.77s
                        Total time: 17054.58s
                               ETA: 1016566.5s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.990s, learning 0.167s)
               Value function loss: 93.6893
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 418.81
               Mean episode length: 150.00
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 9.16s
                        Total time: 17063.73s
                               ETA: 1016486.0s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.609s, learning 0.162s)
               Value function loss: 107.3837
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 440.52
               Mean episode length: 150.00
                  Mean reward/step: 3.27
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 9.77s
                        Total time: 17073.51s
                               ETA: 1016442.1s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.174s, learning 0.168s)
               Value function loss: 94.5951
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 440.16
               Mean episode length: 149.83
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 9.34s
                        Total time: 17082.85s
                               ETA: 1016372.7s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.141s, learning 0.168s)
               Value function loss: 103.8538
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 462.30
               Mean episode length: 148.28
                  Mean reward/step: 3.37
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 9.31s
                        Total time: 17092.16s
                               ETA: 1016301.3s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.696s, learning 0.167s)
               Value function loss: 120.0901
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 489.00
               Mean episode length: 149.94
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 9.86s
                        Total time: 17102.02s
                               ETA: 1016263.0s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.724s, learning 0.161s)
               Value function loss: 120.6022
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 442.03
               Mean episode length: 148.56
                  Mean reward/step: 3.32
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 9.89s
                        Total time: 17111.91s
                               ETA: 1016226.1s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.437s, learning 0.163s)
               Value function loss: 108.7567
                    Surrogate loss: -0.0221
             Mean action noise std: 0.76
                       Mean reward: 463.98
               Mean episode length: 150.00
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 9.60s
                        Total time: 17121.51s
                               ETA: 1016172.2s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.716s, learning 0.161s)
               Value function loss: 119.4184
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 472.28
               Mean episode length: 150.00
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 9.88s
                        Total time: 17131.38s
                               ETA: 1016134.9s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.278s, learning 0.161s)
               Value function loss: 116.9655
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 477.01
               Mean episode length: 150.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 9.44s
                        Total time: 17140.82s
                               ETA: 1016071.5s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.784s, learning 0.160s)
               Value function loss: 135.2159
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 478.67
               Mean episode length: 149.14
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 9.94s
                        Total time: 17150.77s
                               ETA: 1016038.2s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.570s, learning 0.167s)
               Value function loss: 132.2843
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 473.96
               Mean episode length: 149.43
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 9.74s
                        Total time: 17160.50s
                               ETA: 1015992.6s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.672s, learning 0.163s)
               Value function loss: 115.0540
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 489.87
               Mean episode length: 149.62
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 9.83s
                        Total time: 17170.34s
                               ETA: 1015952.9s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.435s, learning 0.166s)
               Value function loss: 118.9741
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 495.75
               Mean episode length: 148.96
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 9.60s
                        Total time: 17179.94s
                               ETA: 1015899.4s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.385s, learning 0.174s)
               Value function loss: 130.4704
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 486.59
               Mean episode length: 149.20
                  Mean reward/step: 3.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 9.56s
                        Total time: 17189.50s
                               ETA: 1015843.5s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.338s, learning 0.170s)
               Value function loss: 124.8884
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 498.38
               Mean episode length: 150.00
                  Mean reward/step: 3.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 9.51s
                        Total time: 17199.00s
                               ETA: 1015784.6s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.605s, learning 0.163s)
               Value function loss: 116.2327
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 474.86
               Mean episode length: 148.36
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 9.77s
                        Total time: 17208.77s
                               ETA: 1015741.1s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.358s, learning 0.159s)
               Value function loss: 114.5238
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 495.25
               Mean episode length: 149.32
                  Mean reward/step: 3.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 9.52s
                        Total time: 17218.29s
                               ETA: 1015682.8s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.850s, learning 0.166s)
               Value function loss: 111.3344
                    Surrogate loss: -0.0250
             Mean action noise std: 0.76
                       Mean reward: 505.83
               Mean episode length: 148.92
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 10.02s
                        Total time: 17228.31s
                               ETA: 1015654.1s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.539s, learning 0.162s)
               Value function loss: 104.7034
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 485.75
               Mean episode length: 149.51
                  Mean reward/step: 3.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 9.70s
                        Total time: 17238.01s
                               ETA: 1015606.8s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.652s, learning 0.175s)
               Value function loss: 105.2684
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 478.23
               Mean episode length: 149.32
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 9.83s
                        Total time: 17247.83s
                               ETA: 1015567.0s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.465s, learning 0.166s)
               Value function loss: 96.2271
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 452.40
               Mean episode length: 148.76
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 16.63s
                        Total time: 17264.47s
                               ETA: 1015927.5s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 870 steps/s (collection: 18.651s, learning 0.172s)
               Value function loss: 99.2254
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 427.40
               Mean episode length: 148.97
                  Mean reward/step: 3.40
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 18.82s
                        Total time: 17283.29s
                               ETA: 1016416.6s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 879 steps/s (collection: 18.471s, learning 0.164s)
               Value function loss: 121.0563
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 453.06
               Mean episode length: 149.00
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 18.64s
                        Total time: 17301.92s
                               ETA: 1016894.0s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.501s, learning 0.175s)
               Value function loss: 121.4326
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 463.97
               Mean episode length: 149.14
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 18.68s
                        Total time: 17320.60s
                               ETA: 1017373.1s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 894 steps/s (collection: 18.141s, learning 0.168s)
               Value function loss: 131.5906
                    Surrogate loss: -0.0211
             Mean action noise std: 0.76
                       Mean reward: 463.76
               Mean episode length: 149.52
                  Mean reward/step: 3.43
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 18.31s
                        Total time: 17338.91s
                               ETA: 1017830.2s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.518s, learning 0.167s)
               Value function loss: 123.7759
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 496.79
               Mean episode length: 149.28
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 18.68s
                        Total time: 17357.59s
                               ETA: 1018308.7s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.510s, learning 0.160s)
               Value function loss: 125.2753
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 518.84
               Mean episode length: 148.98
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 18.67s
                        Total time: 17376.26s
                               ETA: 1018785.8s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 864 steps/s (collection: 18.791s, learning 0.162s)
               Value function loss: 135.6337
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 499.46
               Mean episode length: 150.00
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 18.95s
                        Total time: 17395.22s
                               ETA: 1019278.8s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 859 steps/s (collection: 18.907s, learning 0.164s)
               Value function loss: 117.7646
                    Surrogate loss: -0.0232
             Mean action noise std: 0.76
                       Mean reward: 486.15
               Mean episode length: 150.00
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 19.07s
                        Total time: 17414.29s
                               ETA: 1019778.2s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 855 steps/s (collection: 18.982s, learning 0.170s)
               Value function loss: 118.8449
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 484.22
               Mean episode length: 149.42
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 19.15s
                        Total time: 17433.44s
                               ETA: 1020281.7s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 858 steps/s (collection: 18.921s, learning 0.164s)
               Value function loss: 113.6960
                    Surrogate loss: -0.0180
             Mean action noise std: 0.76
                       Mean reward: 491.87
               Mean episode length: 149.77
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 19.08s
                        Total time: 17452.52s
                               ETA: 1020780.6s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.875s, learning 0.159s)
               Value function loss: 130.3410
                    Surrogate loss: -0.0148
             Mean action noise std: 0.76
                       Mean reward: 502.37
               Mean episode length: 150.00
                  Mean reward/step: 3.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 19.03s
                        Total time: 17471.56s
                               ETA: 1021275.9s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.577s, learning 0.164s)
               Value function loss: 125.6544
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 514.06
               Mean episode length: 150.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 18.74s
                        Total time: 17490.30s
                               ETA: 1021753.5s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.875s, learning 0.162s)
               Value function loss: 102.5495
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 470.79
               Mean episode length: 148.06
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 19.04s
                        Total time: 17509.34s
                               ETA: 1022247.9s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 871 steps/s (collection: 18.636s, learning 0.160s)
               Value function loss: 109.4256
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 507.29
               Mean episode length: 147.54
                  Mean reward/step: 3.16
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 18.80s
                        Total time: 17528.13s
                               ETA: 1022727.5s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.537s, learning 0.162s)
               Value function loss: 125.9032
                    Surrogate loss: -0.0172
             Mean action noise std: 0.76
                       Mean reward: 525.31
               Mean episode length: 150.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 18.70s
                        Total time: 17546.83s
                               ETA: 1023200.9s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 898 steps/s (collection: 18.057s, learning 0.169s)
               Value function loss: 107.4161
                    Surrogate loss: -0.0238
             Mean action noise std: 0.76
                       Mean reward: 458.91
               Mean episode length: 147.94
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 18.23s
                        Total time: 17565.06s
                               ETA: 1023646.2s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 883 steps/s (collection: 18.380s, learning 0.161s)
               Value function loss: 102.2766
                    Surrogate loss: -0.0233
             Mean action noise std: 0.76
                       Mean reward: 491.12
               Mean episode length: 149.35
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 18.54s
                        Total time: 17583.60s
                               ETA: 1024109.2s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 863 steps/s (collection: 18.820s, learning 0.164s)
               Value function loss: 107.8492
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 485.56
               Mean episode length: 149.22
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 18.98s
                        Total time: 17602.58s
                               ETA: 1024597.5s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 855 steps/s (collection: 18.984s, learning 0.164s)
               Value function loss: 130.5227
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 482.71
               Mean episode length: 148.23
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 19.15s
                        Total time: 17621.73s
                               ETA: 1025094.7s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 862 steps/s (collection: 18.829s, learning 0.162s)
               Value function loss: 124.4226
                    Surrogate loss: -0.0170
             Mean action noise std: 0.76
                       Mean reward: 458.83
               Mean episode length: 150.00
                  Mean reward/step: 3.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 18.99s
                        Total time: 17640.72s
                               ETA: 1025582.1s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 888 steps/s (collection: 18.261s, learning 0.172s)
               Value function loss: 121.7725
                    Surrogate loss: -0.0200
             Mean action noise std: 0.76
                       Mean reward: 450.44
               Mean episode length: 148.30
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 18.43s
                        Total time: 17659.15s
                               ETA: 1026036.5s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 892 steps/s (collection: 18.194s, learning 0.168s)
               Value function loss: 128.0493
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 478.32
               Mean episode length: 149.04
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 18.36s
                        Total time: 17677.52s
                               ETA: 1026486.2s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 889 steps/s (collection: 18.251s, learning 0.162s)
               Value function loss: 148.9252
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 496.49
               Mean episode length: 149.02
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 18.41s
                        Total time: 17695.93s
                               ETA: 1026938.4s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 875 steps/s (collection: 18.565s, learning 0.159s)
               Value function loss: 120.4133
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 465.29
               Mean episode length: 149.68
                  Mean reward/step: 3.20
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 18.72s
                        Total time: 17714.65s
                               ETA: 1027408.1s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 879 steps/s (collection: 18.457s, learning 0.166s)
               Value function loss: 123.8689
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 467.40
               Mean episode length: 149.84
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 18.62s
                        Total time: 17733.28s
                               ETA: 1027871.3s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 831 steps/s (collection: 19.536s, learning 0.160s)
               Value function loss: 131.5689
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 475.75
               Mean episode length: 148.94
                  Mean reward/step: 3.19
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 19.70s
                        Total time: 17752.97s
                               ETA: 1028396.1s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 884 steps/s (collection: 18.347s, learning 0.170s)
               Value function loss: 120.5433
                    Surrogate loss: -0.0157
             Mean action noise std: 0.76
                       Mean reward: 458.37
               Mean episode length: 149.33
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 18.52s
                        Total time: 17771.49s
                               ETA: 1028852.0s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 925 steps/s (collection: 17.536s, learning 0.161s)
               Value function loss: 123.9743
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 499.88
               Mean episode length: 148.94
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 17.70s
                        Total time: 17789.19s
                               ETA: 1029259.9s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 868 steps/s (collection: 18.704s, learning 0.166s)
               Value function loss: 109.7489
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 464.28
               Mean episode length: 150.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 18.87s
                        Total time: 17808.06s
                               ETA: 1029735.1s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.881s, learning 0.164s)
               Value function loss: 127.1231
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 490.24
               Mean episode length: 150.00
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 19.05s
                        Total time: 17827.10s
                               ETA: 1030219.9s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 893 steps/s (collection: 18.166s, learning 0.164s)
               Value function loss: 111.7755
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 473.21
               Mean episode length: 150.00
                  Mean reward/step: 3.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 18.33s
                        Total time: 17845.43s
                               ETA: 1030662.8s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 895 steps/s (collection: 18.137s, learning 0.158s)
               Value function loss: 118.8580
                    Surrogate loss: -0.0162
             Mean action noise std: 0.76
                       Mean reward: 444.75
               Mean episode length: 148.43
                  Mean reward/step: 3.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 18.30s
                        Total time: 17863.73s
                               ETA: 1031103.1s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 880 steps/s (collection: 18.442s, learning 0.161s)
               Value function loss: 116.6054
                    Surrogate loss: -0.0189
             Mean action noise std: 0.76
                       Mean reward: 507.60
               Mean episode length: 150.00
                  Mean reward/step: 3.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 18.60s
                        Total time: 17882.33s
                               ETA: 1031560.6s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 879 steps/s (collection: 18.467s, learning 0.165s)
               Value function loss: 120.5133
                    Surrogate loss: -0.0152
             Mean action noise std: 0.76
                       Mean reward: 493.83
               Mean episode length: 149.71
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 18.63s
                        Total time: 17900.96s
                               ETA: 1032019.3s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.527s, learning 0.173s)
               Value function loss: 113.1629
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 500.30
               Mean episode length: 150.00
                  Mean reward/step: 3.32
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 18.70s
                        Total time: 17919.66s
                               ETA: 1032481.4s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.411s, learning 0.163s)
               Value function loss: 96.3446
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 495.99
               Mean episode length: 149.31
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 18.57s
                        Total time: 17938.24s
                               ETA: 1032935.6s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 930 steps/s (collection: 17.440s, learning 0.158s)
               Value function loss: 109.1159
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 499.16
               Mean episode length: 150.00
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 17.60s
                        Total time: 17955.83s
                               ETA: 1033333.1s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.598s, learning 0.165s)
               Value function loss: 105.3600
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 482.77
               Mean episode length: 149.21
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 9.76s
                        Total time: 17965.60s
                               ETA: 1033279.5s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.406s, learning 0.159s)
               Value function loss: 117.4468
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 479.86
               Mean episode length: 148.99
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 9.56s
                        Total time: 17975.16s
                               ETA: 1033214.5s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.564s, learning 0.164s)
               Value function loss: 112.6290
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 474.30
               Mean episode length: 149.35
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 9.73s
                        Total time: 17984.89s
                               ETA: 1033159.0s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.348s, learning 0.174s)
               Value function loss: 118.4031
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 480.47
               Mean episode length: 148.51
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 9.52s
                        Total time: 17994.41s
                               ETA: 1033091.7s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.246s, learning 0.166s)
               Value function loss: 124.7757
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 485.65
               Mean episode length: 148.88
                  Mean reward/step: 3.41
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 9.41s
                        Total time: 18003.83s
                               ETA: 1033018.1s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.396s, learning 0.161s)
               Value function loss: 117.0272
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 505.09
               Mean episode length: 147.75
                  Mean reward/step: 3.38
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 9.56s
                        Total time: 18013.38s
                               ETA: 1032952.9s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.543s, learning 0.173s)
               Value function loss: 118.1501
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 493.06
               Mean episode length: 148.88
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 9.72s
                        Total time: 18023.10s
                               ETA: 1032896.9s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1756 steps/s (collection: 9.161s, learning 0.166s)
               Value function loss: 124.6807
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 497.30
               Mean episode length: 149.85
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 9.33s
                        Total time: 18032.42s
                               ETA: 1032818.7s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.293s, learning 0.158s)
               Value function loss: 145.1554
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 507.22
               Mean episode length: 149.16
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 9.45s
                        Total time: 18041.88s
                               ETA: 1032747.6s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.731s, learning 0.157s)
               Value function loss: 123.2402
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 478.26
               Mean episode length: 148.13
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 9.89s
                        Total time: 18051.76s
                               ETA: 1032701.7s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.065s, learning 0.162s)
               Value function loss: 132.4041
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 486.98
               Mean episode length: 148.57
                  Mean reward/step: 3.27
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 9.23s
                        Total time: 18060.99s
                               ETA: 1032618.0s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.472s, learning 0.166s)
               Value function loss: 139.1481
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 474.17
               Mean episode length: 148.68
                  Mean reward/step: 3.28
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 9.64s
                        Total time: 18070.63s
                               ETA: 1032557.8s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.457s, learning 0.164s)
               Value function loss: 122.7893
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 468.66
               Mean episode length: 148.15
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 9.62s
                        Total time: 18080.25s
                               ETA: 1032496.7s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.692s, learning 0.157s)
               Value function loss: 112.4165
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 499.33
               Mean episode length: 148.71
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 9.85s
                        Total time: 18090.10s
                               ETA: 1032448.7s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.516s, learning 0.162s)
               Value function loss: 123.7810
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 517.66
               Mean episode length: 150.00
                  Mean reward/step: 3.38
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 9.68s
                        Total time: 18099.78s
                               ETA: 1032391.1s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.493s, learning 0.161s)
               Value function loss: 115.5152
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 517.57
               Mean episode length: 149.95
                  Mean reward/step: 3.41
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 9.65s
                        Total time: 18109.43s
                               ETA: 1032332.0s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.237s, learning 0.166s)
               Value function loss: 100.6327
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 494.38
               Mean episode length: 149.48
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 9.40s
                        Total time: 18118.83s
                               ETA: 1032258.8s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.334s, learning 0.159s)
               Value function loss: 99.4397
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 511.75
               Mean episode length: 149.05
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 9.49s
                        Total time: 18128.33s
                               ETA: 1032190.8s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.562s, learning 0.161s)
               Value function loss: 100.3968
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 516.39
               Mean episode length: 149.38
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 9.72s
                        Total time: 18138.05s
                               ETA: 1032135.8s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.333s, learning 0.159s)
               Value function loss: 99.9115
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 480.94
               Mean episode length: 148.18
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 9.49s
                        Total time: 18147.54s
                               ETA: 1032067.8s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.331s, learning 0.169s)
               Value function loss: 109.1156
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 514.93
               Mean episode length: 149.56
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 9.50s
                        Total time: 18157.04s
                               ETA: 1032000.4s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.363s, learning 0.161s)
               Value function loss: 130.8038
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 533.88
               Mean episode length: 149.95
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 9.52s
                        Total time: 18166.56s
                               ETA: 1031934.4s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.595s, learning 0.172s)
               Value function loss: 125.6905
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 508.35
               Mean episode length: 149.01
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 9.77s
                        Total time: 18176.33s
                               ETA: 1031882.2s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.664s, learning 0.168s)
               Value function loss: 127.6462
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 520.26
               Mean episode length: 149.85
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 9.83s
                        Total time: 18186.16s
                               ETA: 1031833.8s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.540s, learning 0.164s)
               Value function loss: 125.4047
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 530.76
               Mean episode length: 150.00
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 9.70s
                        Total time: 18195.87s
                               ETA: 1031778.1s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.651s, learning 0.158s)
               Value function loss: 128.8186
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 530.94
               Mean episode length: 149.91
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 9.81s
                        Total time: 18205.68s
                               ETA: 1031728.5s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.635s, learning 0.164s)
               Value function loss: 129.3431
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 517.66
               Mean episode length: 150.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 9.80s
                        Total time: 18215.48s
                               ETA: 1031678.3s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.537s, learning 0.163s)
               Value function loss: 143.6507
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 540.98
               Mean episode length: 149.77
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 9.70s
                        Total time: 18225.18s
                               ETA: 1031622.6s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.582s, learning 0.163s)
               Value function loss: 136.7443
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 534.59
               Mean episode length: 148.60
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 9.74s
                        Total time: 18234.92s
                               ETA: 1031569.5s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1771 steps/s (collection: 9.088s, learning 0.160s)
               Value function loss: 129.0528
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 553.22
               Mean episode length: 149.98
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 9.25s
                        Total time: 18244.17s
                               ETA: 1031488.4s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.829s, learning 0.175s)
               Value function loss: 143.1714
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 544.85
               Mean episode length: 150.00
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 10.00s
                        Total time: 18254.17s
                               ETA: 1031450.0s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.346s, learning 0.177s)
               Value function loss: 128.1266
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 527.42
               Mean episode length: 150.00
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 9.52s
                        Total time: 18263.70s
                               ETA: 1031384.5s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.451s, learning 0.167s)
               Value function loss: 130.7105
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 541.11
               Mean episode length: 149.69
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 9.62s
                        Total time: 18273.31s
                               ETA: 1031324.5s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.572s, learning 0.180s)
               Value function loss: 128.1811
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 538.79
               Mean episode length: 149.13
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 9.75s
                        Total time: 18283.07s
                               ETA: 1031272.0s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.430s, learning 0.176s)
               Value function loss: 124.9118
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 544.85
               Mean episode length: 149.48
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 9.61s
                        Total time: 18292.67s
                               ETA: 1031211.4s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.079s, learning 0.189s)
               Value function loss: 118.1802
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 545.84
               Mean episode length: 148.44
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 9.27s
                        Total time: 18301.94s
                               ETA: 1031131.8s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.461s, learning 0.174s)
               Value function loss: 113.6603
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 555.44
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 9.64s
                        Total time: 18311.58s
                               ETA: 1031072.9s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.573s, learning 0.186s)
               Value function loss: 134.0016
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 554.39
               Mean episode length: 149.87
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 9.76s
                        Total time: 18321.34s
                               ETA: 1031021.1s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.103s, learning 0.173s)
               Value function loss: 105.2933
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 512.15
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 9.28s
                        Total time: 18330.61s
                               ETA: 1030942.1s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.450s, learning 0.162s)
               Value function loss: 141.4950
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 550.39
               Mean episode length: 149.80
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 9.61s
                        Total time: 18340.22s
                               ETA: 1030882.1s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.092s, learning 0.179s)
               Value function loss: 137.5235
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 557.63
               Mean episode length: 149.46
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 9.27s
                        Total time: 18349.49s
                               ETA: 1030803.0s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.667s, learning 0.179s)
               Value function loss: 157.5223
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 532.87
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 9.85s
                        Total time: 18359.34s
                               ETA: 1030756.3s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.316s, learning 0.169s)
               Value function loss: 162.1187
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 562.20
               Mean episode length: 148.92
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 9.48s
                        Total time: 18368.83s
                               ETA: 1030689.4s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.424s, learning 0.169s)
               Value function loss: 143.4807
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 535.39
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 9.59s
                        Total time: 18378.42s
                               ETA: 1030628.6s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.493s, learning 0.164s)
               Value function loss: 169.9353
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 566.51
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 9.66s
                        Total time: 18388.08s
                               ETA: 1030571.4s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.747s, learning 0.171s)
               Value function loss: 142.9581
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 536.35
               Mean episode length: 149.96
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 9.92s
                        Total time: 18397.99s
                               ETA: 1030528.9s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.521s, learning 0.158s)
               Value function loss: 140.7161
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 560.32
               Mean episode length: 150.00
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 9.68s
                        Total time: 18407.67s
                               ETA: 1030473.1s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.305s, learning 0.164s)
               Value function loss: 146.3427
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 552.60
               Mean episode length: 149.64
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 9.47s
                        Total time: 18417.14s
                               ETA: 1030405.5s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.308s, learning 0.174s)
               Value function loss: 144.0362
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 529.75
               Mean episode length: 148.26
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 9.48s
                        Total time: 18426.62s
                               ETA: 1030338.7s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.446s, learning 0.165s)
               Value function loss: 140.4920
                    Surrogate loss: -0.0240
             Mean action noise std: 0.75
                       Mean reward: 541.64
               Mean episode length: 148.98
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 9.61s
                        Total time: 18436.23s
                               ETA: 1030279.3s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.169s, learning 0.166s)
               Value function loss: 126.0004
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 548.17
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 9.34s
                        Total time: 18445.57s
                               ETA: 1030204.5s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.670s, learning 0.167s)
               Value function loss: 132.7073
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 572.36
               Mean episode length: 150.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 9.84s
                        Total time: 18455.41s
                               ETA: 1030157.7s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.601s, learning 0.165s)
               Value function loss: 125.0936
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 553.09
               Mean episode length: 149.94
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 9.77s
                        Total time: 18465.17s
                               ETA: 1030107.1s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.014s, learning 0.159s)
               Value function loss: 118.8038
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 543.55
               Mean episode length: 149.31
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 9.17s
                        Total time: 18474.35s
                               ETA: 1030023.4s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.763s, learning 0.158s)
               Value function loss: 105.4128
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 538.27
               Mean episode length: 149.91
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 9.92s
                        Total time: 18484.27s
                               ETA: 1029981.5s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.484s, learning 0.162s)
               Value function loss: 104.1706
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 562.04
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 9.65s
                        Total time: 18493.91s
                               ETA: 1029924.3s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.608s, learning 0.166s)
               Value function loss: 112.8500
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 531.57
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 9.77s
                        Total time: 18503.69s
                               ETA: 1029874.4s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.359s, learning 0.161s)
               Value function loss: 125.1343
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 510.52
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 9.52s
                        Total time: 18513.21s
                               ETA: 1029810.3s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.357s, learning 0.165s)
               Value function loss: 142.0230
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 526.88
               Mean episode length: 149.71
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 9.52s
                        Total time: 18522.73s
                               ETA: 1029746.3s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.522s, learning 0.184s)
               Value function loss: 145.5573
                    Surrogate loss: 0.0041
             Mean action noise std: 0.75
                       Mean reward: 552.43
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 9.71s
                        Total time: 18532.43s
                               ETA: 1029692.7s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.395s, learning 0.164s)
               Value function loss: 156.6596
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 557.81
               Mean episode length: 148.73
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 9.56s
                        Total time: 18541.99s
                               ETA: 1029630.9s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.667s, learning 0.166s)
               Value function loss: 142.6358
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 560.51
               Mean episode length: 150.00
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 9.83s
                        Total time: 18551.83s
                               ETA: 1029584.4s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.259s, learning 0.176s)
               Value function loss: 126.8231
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 541.33
               Mean episode length: 149.86
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 9.43s
                        Total time: 18561.26s
                               ETA: 1029515.9s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.548s, learning 0.165s)
               Value function loss: 143.1850
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 531.63
               Mean episode length: 149.54
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 9.71s
                        Total time: 18570.97s
                               ETA: 1029462.9s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.518s, learning 0.194s)
               Value function loss: 163.4323
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 551.48
               Mean episode length: 150.00
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 9.71s
                        Total time: 18580.69s
                               ETA: 1029409.8s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.433s, learning 0.170s)
               Value function loss: 128.2273
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 543.01
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 9.60s
                        Total time: 18590.29s
                               ETA: 1029350.8s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.652s, learning 0.160s)
               Value function loss: 146.2007
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 551.90
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 9.81s
                        Total time: 18600.10s
                               ETA: 1029303.4s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.257s, learning 0.165s)
               Value function loss: 146.1194
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 534.28
               Mean episode length: 150.00
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 9.42s
                        Total time: 18609.52s
                               ETA: 1029234.5s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.275s, learning 0.168s)
               Value function loss: 127.9440
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 541.86
               Mean episode length: 148.97
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 9.44s
                        Total time: 18618.97s
                               ETA: 1029166.8s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.462s, learning 0.167s)
               Value function loss: 136.0375
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 559.42
               Mean episode length: 150.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 9.63s
                        Total time: 18628.60s
                               ETA: 1029109.5s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.449s, learning 0.172s)
               Value function loss: 119.7774
                    Surrogate loss: -0.0233
             Mean action noise std: 0.75
                       Mean reward: 502.82
               Mean episode length: 148.88
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 9.62s
                        Total time: 18638.22s
                               ETA: 1029051.7s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.367s, learning 0.160s)
               Value function loss: 130.1428
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 538.78
               Mean episode length: 150.00
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 9.53s
                        Total time: 18647.74s
                               ETA: 1028988.8s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.805s, learning 0.167s)
               Value function loss: 126.8979
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 545.27
               Mean episode length: 150.00
                  Mean reward/step: 3.52
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 9.97s
                        Total time: 18657.72s
                               ETA: 1028950.5s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.503s, learning 0.180s)
               Value function loss: 105.0987
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 527.99
               Mean episode length: 150.00
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 9.68s
                        Total time: 18667.40s
                               ETA: 1028896.4s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.392s, learning 0.173s)
               Value function loss: 116.1567
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 519.48
               Mean episode length: 148.17
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 9.57s
                        Total time: 18676.97s
                               ETA: 1028835.8s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.549s, learning 0.160s)
               Value function loss: 130.2608
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 552.60
               Mean episode length: 149.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 9.71s
                        Total time: 18686.67s
                               ETA: 1028783.1s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.495s, learning 0.163s)
               Value function loss: 116.9251
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 525.50
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 9.66s
                        Total time: 18696.33s
                               ETA: 1028727.7s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.287s, learning 0.160s)
               Value function loss: 131.4800
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 521.53
               Mean episode length: 149.67
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 9.45s
                        Total time: 18705.78s
                               ETA: 1028660.8s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.460s, learning 0.171s)
               Value function loss: 137.0206
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 522.82
               Mean episode length: 149.44
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 9.63s
                        Total time: 18715.41s
                               ETA: 1028604.0s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.756s, learning 0.165s)
               Value function loss: 160.7132
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 545.15
               Mean episode length: 149.25
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 9.92s
                        Total time: 18725.33s
                               ETA: 1028563.2s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.711s, learning 0.184s)
               Value function loss: 162.8625
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 524.97
               Mean episode length: 149.83
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 9.89s
                        Total time: 18735.23s
                               ETA: 1028521.0s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.305s, learning 0.161s)
               Value function loss: 139.1268
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 537.47
               Mean episode length: 150.00
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 9.47s
                        Total time: 18744.69s
                               ETA: 1028455.3s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.578s, learning 0.181s)
               Value function loss: 136.8765
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 523.67
               Mean episode length: 150.00
                  Mean reward/step: 3.40
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 9.76s
                        Total time: 18754.45s
                               ETA: 1028405.7s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.385s, learning 0.185s)
               Value function loss: 146.1814
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 531.48
               Mean episode length: 148.82
                  Mean reward/step: 3.37
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 9.57s
                        Total time: 18764.02s
                               ETA: 1028345.8s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.568s, learning 0.168s)
               Value function loss: 131.2063
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 522.12
               Mean episode length: 150.00
                  Mean reward/step: 3.32
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 9.74s
                        Total time: 18773.76s
                               ETA: 1028295.1s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.224s, learning 0.167s)
               Value function loss: 132.1802
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 502.05
               Mean episode length: 149.11
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 9.39s
                        Total time: 18783.15s
                               ETA: 1028225.5s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.648s, learning 0.168s)
               Value function loss: 137.7929
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 531.36
               Mean episode length: 150.00
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 9.82s
                        Total time: 18792.96s
                               ETA: 1028179.2s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.164s, learning 0.172s)
               Value function loss: 144.0499
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 515.48
               Mean episode length: 148.33
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 9.34s
                        Total time: 18802.30s
                               ETA: 1028106.8s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.500s, learning 0.169s)
               Value function loss: 121.5226
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 505.28
               Mean episode length: 149.37
                  Mean reward/step: 3.37
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 9.67s
                        Total time: 18811.97s
                               ETA: 1028052.6s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.342s, learning 0.175s)
               Value function loss: 132.3494
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 528.94
               Mean episode length: 150.00
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 9.52s
                        Total time: 18821.48s
                               ETA: 1027990.1s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.518s, learning 0.175s)
               Value function loss: 125.1393
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 518.14
               Mean episode length: 150.00
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 9.69s
                        Total time: 18831.18s
                               ETA: 1027937.4s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.682s, learning 0.167s)
               Value function loss: 113.3105
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 485.37
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 9.85s
                        Total time: 18841.03s
                               ETA: 1027893.1s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.545s, learning 0.174s)
               Value function loss: 106.2294
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 533.13
               Mean episode length: 149.59
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 9.72s
                        Total time: 18850.75s
                               ETA: 1027841.9s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1631 steps/s (collection: 9.854s, learning 0.186s)
               Value function loss: 115.2064
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 506.31
               Mean episode length: 148.58
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 10.04s
                        Total time: 18860.79s
                               ETA: 1027808.2s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.199s, learning 0.183s)
               Value function loss: 116.3483
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 486.55
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 9.38s
                        Total time: 18870.17s
                               ETA: 1027738.6s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.322s, learning 0.162s)
               Value function loss: 131.7989
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 507.83
               Mean episode length: 148.82
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 9.48s
                        Total time: 18879.65s
                               ETA: 1027674.7s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.391s, learning 0.179s)
               Value function loss: 136.1667
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 528.34
               Mean episode length: 149.31
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 9.57s
                        Total time: 18889.22s
                               ETA: 1027615.6s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.435s, learning 0.173s)
               Value function loss: 153.1001
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 553.46
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 9.61s
                        Total time: 18898.83s
                               ETA: 1027558.5s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.919s, learning 0.164s)
               Value function loss: 134.2357
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 510.29
               Mean episode length: 149.64
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 9.08s
                        Total time: 18907.91s
                               ETA: 1027473.0s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.833s, learning 0.173s)
               Value function loss: 126.1687
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 532.36
               Mean episode length: 150.00
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 10.01s
                        Total time: 18917.92s
                               ETA: 1027437.7s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.188s, learning 0.166s)
               Value function loss: 131.2854
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 542.06
               Mean episode length: 150.00
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 9.35s
                        Total time: 18927.27s
                               ETA: 1027367.0s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.286s, learning 0.163s)
               Value function loss: 141.1620
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 540.74
               Mean episode length: 150.00
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 9.45s
                        Total time: 18936.72s
                               ETA: 1027301.6s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.534s, learning 0.177s)
               Value function loss: 148.1151
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 535.15
               Mean episode length: 150.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 9.71s
                        Total time: 18946.43s
                               ETA: 1027250.3s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.772s, learning 0.171s)
               Value function loss: 137.1826
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 531.92
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 9.94s
                        Total time: 18956.38s
                               ETA: 1027211.8s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.322s, learning 0.165s)
               Value function loss: 152.7240
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 552.73
               Mean episode length: 150.00
                  Mean reward/step: 3.52
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 9.49s
                        Total time: 18965.86s
                               ETA: 1027148.5s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.660s, learning 0.182s)
               Value function loss: 167.9789
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 548.57
               Mean episode length: 150.00
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 9.84s
                        Total time: 18975.71s
                               ETA: 1027104.5s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.333s, learning 0.172s)
               Value function loss: 142.8075
                    Surrogate loss: 0.0182
             Mean action noise std: 0.75
                       Mean reward: 541.01
               Mean episode length: 150.00
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 9.50s
                        Total time: 18985.21s
                               ETA: 1027042.4s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.743s, learning 0.162s)
               Value function loss: 121.0696
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 519.48
               Mean episode length: 149.78
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 9.91s
                        Total time: 18995.12s
                               ETA: 1027001.9s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.391s, learning 0.171s)
               Value function loss: 128.1162
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 538.56
               Mean episode length: 150.00
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 9.56s
                        Total time: 19004.68s
                               ETA: 1026942.9s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.594s, learning 0.164s)
               Value function loss: 124.9421
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 554.78
               Mean episode length: 149.52
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 9.76s
                        Total time: 19014.44s
                               ETA: 1026894.6s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.481s, learning 0.160s)
               Value function loss: 102.9485
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 549.35
               Mean episode length: 149.59
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 9.64s
                        Total time: 19024.08s
                               ETA: 1026840.0s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.473s, learning 0.161s)
               Value function loss: 114.4381
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 526.60
               Mean episode length: 149.59
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 9.63s
                        Total time: 19033.71s
                               ETA: 1026785.1s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.563s, learning 0.166s)
               Value function loss: 133.4656
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 548.50
               Mean episode length: 148.82
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 9.73s
                        Total time: 19043.44s
                               ETA: 1026735.3s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.475s, learning 0.161s)
               Value function loss: 123.3372
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 515.60
               Mean episode length: 149.70
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 9.64s
                        Total time: 19053.08s
                               ETA: 1026680.6s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.692s, learning 0.158s)
               Value function loss: 159.5114
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 521.83
               Mean episode length: 148.32
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 9.85s
                        Total time: 19062.93s
                               ETA: 1026637.4s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.496s, learning 0.164s)
               Value function loss: 144.2735
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 554.01
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 9.66s
                        Total time: 19072.59s
                               ETA: 1026584.1s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.515s, learning 0.170s)
               Value function loss: 156.7801
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 528.66
               Mean episode length: 149.20
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 9.68s
                        Total time: 19082.27s
                               ETA: 1026532.1s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.399s, learning 0.165s)
               Value function loss: 149.4398
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 531.72
               Mean episode length: 148.56
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 9.56s
                        Total time: 19091.84s
                               ETA: 1026473.7s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.680s, learning 0.170s)
               Value function loss: 134.6184
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 552.05
               Mean episode length: 150.00
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 9.85s
                        Total time: 19101.69s
                               ETA: 1026430.8s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.013s, learning 0.158s)
               Value function loss: 158.3307
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 550.23
               Mean episode length: 150.00
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 9.17s
                        Total time: 19110.86s
                               ETA: 1026351.4s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.357s, learning 0.161s)
               Value function loss: 149.8718
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 522.61
               Mean episode length: 149.56
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 9.52s
                        Total time: 19120.38s
                               ETA: 1026290.6s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.454s, learning 0.159s)
               Value function loss: 154.2389
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 539.41
               Mean episode length: 149.30
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 9.61s
                        Total time: 19129.99s
                               ETA: 1026235.0s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.683s, learning 0.169s)
               Value function loss: 147.4475
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 530.90
               Mean episode length: 150.00
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 9.85s
                        Total time: 19139.84s
                               ETA: 1026192.3s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.308s, learning 0.177s)
               Value function loss: 159.4894
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 556.38
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 9.49s
                        Total time: 19149.33s
                               ETA: 1026130.0s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.638s, learning 0.168s)
               Value function loss: 168.0014
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 560.47
               Mean episode length: 149.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 9.81s
                        Total time: 19159.13s
                               ETA: 1026084.9s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.428s, learning 0.160s)
               Value function loss: 139.9626
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 555.78
               Mean episode length: 149.47
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 9.59s
                        Total time: 19168.72s
                               ETA: 1026028.2s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.346s, learning 0.159s)
               Value function loss: 148.3516
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 546.15
               Mean episode length: 149.47
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 9.51s
                        Total time: 19178.22s
                               ETA: 1025967.1s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.182s, learning 0.165s)
               Value function loss: 166.5397
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 541.79
               Mean episode length: 148.93
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 9.35s
                        Total time: 19187.57s
                               ETA: 1025897.6s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.654s, learning 0.159s)
               Value function loss: 139.2381
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 545.45
               Mean episode length: 149.91
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 9.81s
                        Total time: 19197.38s
                               ETA: 1025853.1s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.588s, learning 0.158s)
               Value function loss: 114.0808
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 533.88
               Mean episode length: 149.91
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 9.75s
                        Total time: 19207.13s
                               ETA: 1025805.0s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.404s, learning 0.168s)
               Value function loss: 115.5811
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 530.98
               Mean episode length: 149.31
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 9.57s
                        Total time: 19216.70s
                               ETA: 1025747.6s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.806s, learning 0.172s)
               Value function loss: 135.2690
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 529.02
               Mean episode length: 149.68
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 9.98s
                        Total time: 19226.68s
                               ETA: 1025712.0s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.717s, learning 0.162s)
               Value function loss: 135.8925
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 544.96
               Mean episode length: 148.31
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 9.88s
                        Total time: 19236.56s
                               ETA: 1025671.2s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.673s, learning 0.159s)
               Value function loss: 125.5946
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 497.04
               Mean episode length: 149.29
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 9.83s
                        Total time: 19246.39s
                               ETA: 1025627.9s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.559s, learning 0.159s)
               Value function loss: 128.2085
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 523.14
               Mean episode length: 150.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 9.72s
                        Total time: 19256.11s
                               ETA: 1025578.5s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.740s, learning 0.164s)
               Value function loss: 151.8706
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 547.40
               Mean episode length: 150.00
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 9.90s
                        Total time: 19266.01s
                               ETA: 1025539.0s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.479s, learning 0.160s)
               Value function loss: 133.7725
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 499.10
               Mean episode length: 149.31
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 9.64s
                        Total time: 19275.65s
                               ETA: 1025485.6s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.484s, learning 0.162s)
               Value function loss: 133.9803
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 535.60
               Mean episode length: 149.31
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 9.65s
                        Total time: 19285.30s
                               ETA: 1025432.5s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.729s, learning 0.160s)
               Value function loss: 140.0694
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 527.03
               Mean episode length: 149.29
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 9.89s
                        Total time: 19295.19s
                               ETA: 1025392.4s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.252s, learning 0.164s)
               Value function loss: 156.5896
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 511.01
               Mean episode length: 149.60
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 9.42s
                        Total time: 19304.60s
                               ETA: 1025327.2s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.650s, learning 0.163s)
               Value function loss: 126.5893
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 494.28
               Mean episode length: 149.37
                  Mean reward/step: 3.40
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 9.81s
                        Total time: 19314.42s
                               ETA: 1025283.1s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.173s, learning 0.160s)
               Value function loss: 147.3823
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 509.70
               Mean episode length: 149.69
                  Mean reward/step: 3.40
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 9.33s
                        Total time: 19323.75s
                               ETA: 1025213.7s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.825s, learning 0.174s)
               Value function loss: 146.8180
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 541.11
               Mean episode length: 149.62
                  Mean reward/step: 3.37
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 10.00s
                        Total time: 19333.75s
                               ETA: 1025179.6s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.383s, learning 0.165s)
               Value function loss: 139.5088
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 521.73
               Mean episode length: 150.00
                  Mean reward/step: 3.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 9.55s
                        Total time: 19343.30s
                               ETA: 1025121.6s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1624 steps/s (collection: 9.915s, learning 0.168s)
               Value function loss: 139.5617
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 530.31
               Mean episode length: 150.00
                  Mean reward/step: 3.30
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 10.08s
                        Total time: 19353.38s
                               ETA: 1025092.1s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.307s, learning 0.163s)
               Value function loss: 137.4579
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 518.53
               Mean episode length: 148.52
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 9.47s
                        Total time: 19362.85s
                               ETA: 1025030.0s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.352s, learning 0.158s)
               Value function loss: 158.0100
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 514.15
               Mean episode length: 148.67
                  Mean reward/step: 3.31
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 9.51s
                        Total time: 19372.36s
                               ETA: 1024970.2s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.476s, learning 0.161s)
               Value function loss: 130.6054
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 521.41
               Mean episode length: 146.38
                  Mean reward/step: 3.33
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 9.64s
                        Total time: 19382.00s
                               ETA: 1024917.1s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.402s, learning 0.174s)
               Value function loss: 98.5446
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 493.92
               Mean episode length: 147.77
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 9.58s
                        Total time: 19391.57s
                               ETA: 1024860.9s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.383s, learning 0.161s)
               Value function loss: 109.7917
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 474.78
               Mean episode length: 148.60
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 9.54s
                        Total time: 19401.12s
                               ETA: 1024803.0s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.763s, learning 0.163s)
               Value function loss: 125.3896
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 498.08
               Mean episode length: 149.29
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 9.93s
                        Total time: 19411.04s
                               ETA: 1024765.3s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.466s, learning 0.162s)
               Value function loss: 133.9323
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 538.76
               Mean episode length: 149.05
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 9.63s
                        Total time: 19420.67s
                               ETA: 1024711.9s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.860s, learning 0.172s)
               Value function loss: 124.6663
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 490.92
               Mean episode length: 149.54
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 10.03s
                        Total time: 19430.70s
                               ETA: 1024679.9s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.690s, learning 0.162s)
               Value function loss: 121.4173
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 485.72
               Mean episode length: 148.46
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 9.85s
                        Total time: 19440.56s
                               ETA: 1024638.5s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.893s, learning 0.164s)
               Value function loss: 138.7053
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 472.93
               Mean episode length: 148.16
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 10.06s
                        Total time: 19450.61s
                               ETA: 1024607.8s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.259s, learning 0.172s)
               Value function loss: 116.5096
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 478.92
               Mean episode length: 149.22
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 9.43s
                        Total time: 19460.04s
                               ETA: 1024544.2s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.378s, learning 0.159s)
               Value function loss: 119.9209
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 481.36
               Mean episode length: 148.99
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 9.54s
                        Total time: 19469.58s
                               ETA: 1024486.3s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1611 steps/s (collection: 10.004s, learning 0.161s)
               Value function loss: 116.7184
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 505.04
               Mean episode length: 150.00
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 10.17s
                        Total time: 19479.75s
                               ETA: 1024461.4s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.425s, learning 0.165s)
               Value function loss: 138.0248
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 508.04
               Mean episode length: 150.00
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 9.59s
                        Total time: 19489.34s
                               ETA: 1024406.3s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.833s, learning 0.161s)
               Value function loss: 126.0913
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 488.44
               Mean episode length: 149.64
                  Mean reward/step: 3.52
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 9.99s
                        Total time: 19499.33s
                               ETA: 1024372.5s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.695s, learning 0.167s)
               Value function loss: 141.6429
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 499.09
               Mean episode length: 147.61
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 9.86s
                        Total time: 19509.19s
                               ETA: 1024331.8s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1624 steps/s (collection: 9.926s, learning 0.158s)
               Value function loss: 159.1914
                    Surrogate loss: -0.0003
             Mean action noise std: 0.75
                       Mean reward: 514.63
               Mean episode length: 149.98
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 10.08s
                        Total time: 19519.28s
                               ETA: 1024302.8s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.549s, learning 0.161s)
               Value function loss: 156.8359
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 544.96
               Mean episode length: 148.63
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 9.71s
                        Total time: 19528.99s
                               ETA: 1024254.1s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.507s, learning 0.173s)
               Value function loss: 133.3269
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 534.32
               Mean episode length: 148.52
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 9.68s
                        Total time: 19538.67s
                               ETA: 1024204.0s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.503s, learning 0.174s)
               Value function loss: 136.9715
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 543.82
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 9.68s
                        Total time: 19548.34s
                               ETA: 1024153.7s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.501s, learning 0.176s)
               Value function loss: 145.9350
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 528.70
               Mean episode length: 149.19
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 9.68s
                        Total time: 19558.02s
                               ETA: 1024103.5s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.374s, learning 0.173s)
               Value function loss: 136.5175
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 551.58
               Mean episode length: 149.41
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 9.55s
                        Total time: 19567.57s
                               ETA: 1024046.5s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.229s, learning 0.181s)
               Value function loss: 135.7938
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 548.73
               Mean episode length: 149.48
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 9.41s
                        Total time: 19576.98s
                               ETA: 1023982.4s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.202s, learning 0.179s)
               Value function loss: 136.9524
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 543.54
               Mean episode length: 149.38
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 9.38s
                        Total time: 19586.36s
                               ETA: 1023916.8s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.437s, learning 0.169s)
               Value function loss: 137.5958
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 523.45
               Mean episode length: 147.39
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 9.61s
                        Total time: 19595.96s
                               ETA: 1023863.1s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.557s, learning 0.181s)
               Value function loss: 147.2066
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 531.18
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 9.74s
                        Total time: 19605.70s
                               ETA: 1023816.2s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.617s, learning 0.171s)
               Value function loss: 140.5498
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 532.04
               Mean episode length: 149.27
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 9.79s
                        Total time: 19615.49s
                               ETA: 1023772.0s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.832s, learning 0.165s)
               Value function loss: 149.2216
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 524.51
               Mean episode length: 148.54
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 10.00s
                        Total time: 19625.49s
                               ETA: 1023738.8s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.439s, learning 0.166s)
               Value function loss: 160.0796
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 535.03
               Mean episode length: 150.00
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 9.61s
                        Total time: 19635.09s
                               ETA: 1023685.2s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.683s, learning 0.168s)
               Value function loss: 136.9397
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 521.09
               Mean episode length: 149.23
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 9.85s
                        Total time: 19644.94s
                               ETA: 1023644.4s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.476s, learning 0.165s)
               Value function loss: 130.8971
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 541.91
               Mean episode length: 149.92
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 9.64s
                        Total time: 19654.58s
                               ETA: 1023592.7s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.501s, learning 0.162s)
               Value function loss: 143.0651
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 547.67
               Mean episode length: 149.41
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 9.66s
                        Total time: 19664.25s
                               ETA: 1023542.3s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.343s, learning 0.160s)
               Value function loss: 143.3036
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 546.47
               Mean episode length: 150.00
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 9.50s
                        Total time: 19673.75s
                               ETA: 1023483.5s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.783s, learning 0.159s)
               Value function loss: 133.9902
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 531.27
               Mean episode length: 148.73
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 9.94s
                        Total time: 19683.69s
                               ETA: 1023447.7s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.395s, learning 0.171s)
               Value function loss: 161.4827
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 544.07
               Mean episode length: 150.00
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 9.57s
                        Total time: 19693.26s
                               ETA: 1023392.3s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.619s, learning 0.170s)
               Value function loss: 158.2536
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 539.59
               Mean episode length: 149.43
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 9.79s
                        Total time: 19703.05s
                               ETA: 1023348.5s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.007s, learning 0.163s)
               Value function loss: 149.7724
                    Surrogate loss: 0.0035
             Mean action noise std: 0.75
                       Mean reward: 538.70
               Mean episode length: 148.51
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 9.17s
                        Total time: 19712.22s
                               ETA: 1023272.6s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.225s, learning 0.162s)
               Value function loss: 126.3021
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 543.84
               Mean episode length: 149.42
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 9.39s
                        Total time: 19721.60s
                               ETA: 1023208.1s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.612s, learning 0.162s)
               Value function loss: 135.2082
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 522.56
               Mean episode length: 149.25
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 9.77s
                        Total time: 19731.38s
                               ETA: 1023163.6s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1614 steps/s (collection: 9.982s, learning 0.164s)
               Value function loss: 152.3273
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 565.19
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 10.15s
                        Total time: 19741.52s
                               ETA: 1023138.5s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.104s, learning 0.161s)
               Value function loss: 133.6629
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 573.04
               Mean episode length: 149.74
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 9.27s
                        Total time: 19750.79s
                               ETA: 1023067.8s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.560s, learning 0.161s)
               Value function loss: 134.5468
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 547.78
               Mean episode length: 149.18
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 9.72s
                        Total time: 19760.51s
                               ETA: 1023020.8s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.631s, learning 0.167s)
               Value function loss: 162.1023
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 549.65
               Mean episode length: 148.64
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 9.80s
                        Total time: 19770.31s
                               ETA: 1022977.7s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.744s, learning 0.163s)
               Value function loss: 113.9009
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 527.96
               Mean episode length: 149.37
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 9.91s
                        Total time: 19780.21s
                               ETA: 1022940.4s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.524s, learning 0.161s)
               Value function loss: 146.8939
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 565.87
               Mean episode length: 147.91
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 9.68s
                        Total time: 19789.90s
                               ETA: 1022891.6s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.457s, learning 0.164s)
               Value function loss: 145.7557
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 564.27
               Mean episode length: 149.62
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 9.62s
                        Total time: 19799.52s
                               ETA: 1022839.6s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.687s, learning 0.158s)
               Value function loss: 176.6562
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 562.61
               Mean episode length: 149.47
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 9.84s
                        Total time: 19809.36s
                               ETA: 1022799.1s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.367s, learning 0.157s)
               Value function loss: 142.4110
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 580.76
               Mean episode length: 148.79
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 9.52s
                        Total time: 19818.89s
                               ETA: 1022742.2s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.584s, learning 0.169s)
               Value function loss: 140.4347
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 556.41
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 9.75s
                        Total time: 19828.64s
                               ETA: 1022697.1s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.273s, learning 0.169s)
               Value function loss: 158.7171
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 559.14
               Mean episode length: 149.46
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 9.44s
                        Total time: 19838.08s
                               ETA: 1022635.9s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.399s, learning 0.161s)
               Value function loss: 144.7947
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 566.56
               Mean episode length: 149.78
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 9.56s
                        Total time: 19847.64s
                               ETA: 1022580.9s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.517s, learning 0.160s)
               Value function loss: 127.7163
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 555.48
               Mean episode length: 149.39
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 9.68s
                        Total time: 19857.32s
                               ETA: 1022532.1s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.292s, learning 0.163s)
               Value function loss: 152.3431
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 569.77
               Mean episode length: 149.65
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 9.45s
                        Total time: 19866.77s
                               ETA: 1022471.7s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.746s, learning 0.168s)
               Value function loss: 147.7941
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 578.28
               Mean episode length: 149.71
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 9.91s
                        Total time: 19876.69s
                               ETA: 1022435.1s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.790s, learning 0.168s)
               Value function loss: 154.3994
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 553.22
               Mean episode length: 149.67
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 9.96s
                        Total time: 19886.65s
                               ETA: 1022400.8s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.850s, learning 0.161s)
               Value function loss: 136.5540
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 533.29
               Mean episode length: 149.63
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 10.01s
                        Total time: 19896.66s
                               ETA: 1022369.2s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.626s, learning 0.168s)
               Value function loss: 149.7454
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 572.54
               Mean episode length: 149.62
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 9.79s
                        Total time: 19906.45s
                               ETA: 1022326.5s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.441s, learning 0.169s)
               Value function loss: 162.3487
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 548.90
               Mean episode length: 149.02
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 9.61s
                        Total time: 19916.06s
                               ETA: 1022274.4s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.588s, learning 0.168s)
               Value function loss: 146.7819
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 577.42
               Mean episode length: 149.32
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 9.76s
                        Total time: 19925.82s
                               ETA: 1022229.8s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.517s, learning 0.158s)
               Value function loss: 137.9097
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 577.11
               Mean episode length: 148.65
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 9.67s
                        Total time: 19935.49s
                               ETA: 1022181.1s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.286s, learning 0.159s)
               Value function loss: 135.1205
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 565.97
               Mean episode length: 149.87
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 9.44s
                        Total time: 19944.94s
                               ETA: 1022120.6s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.589s, learning 0.159s)
               Value function loss: 157.3451
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 542.66
               Mean episode length: 148.94
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 9.75s
                        Total time: 19954.68s
                               ETA: 1022075.7s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.671s, learning 0.166s)
               Value function loss: 164.1889
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 595.46
               Mean episode length: 149.38
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 9.84s
                        Total time: 19964.52s
                               ETA: 1022035.5s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.429s, learning 0.168s)
               Value function loss: 171.1160
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 571.04
               Mean episode length: 149.22
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 9.60s
                        Total time: 19974.12s
                               ETA: 1021983.0s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.280s, learning 0.173s)
               Value function loss: 178.6241
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 577.90
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 9.45s
                        Total time: 19983.57s
                               ETA: 1021923.1s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.587s, learning 0.158s)
               Value function loss: 183.6330
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 544.53
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 9.74s
                        Total time: 19993.31s
                               ETA: 1021878.2s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.539s, learning 0.168s)
               Value function loss: 174.5470
                    Surrogate loss: 0.0031
             Mean action noise std: 0.75
                       Mean reward: 510.17
               Mean episode length: 149.20
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 9.71s
                        Total time: 20003.02s
                               ETA: 1021831.5s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.558s, learning 0.164s)
               Value function loss: 150.9084
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 576.72
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 9.72s
                        Total time: 20012.74s
                               ETA: 1021785.5s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.498s, learning 0.162s)
               Value function loss: 166.0781
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 561.86
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 9.66s
                        Total time: 20022.40s
                               ETA: 1021736.4s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.578s, learning 0.163s)
               Value function loss: 173.1013
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 545.95
               Mean episode length: 149.17
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 9.74s
                        Total time: 20032.14s
                               ETA: 1021691.5s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.388s, learning 0.168s)
               Value function loss: 142.5052
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 541.89
               Mean episode length: 149.78
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 9.56s
                        Total time: 20041.70s
                               ETA: 1021637.1s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.577s, learning 0.164s)
               Value function loss: 170.6151
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 575.69
               Mean episode length: 149.48
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 8.74s
                        Total time: 20050.44s
                               ETA: 1021541.4s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.786s, learning 0.160s)
               Value function loss: 184.3570
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 540.50
               Mean episode length: 149.65
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 9.95s
                        Total time: 20060.39s
                               ETA: 1021507.0s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1773 steps/s (collection: 9.081s, learning 0.160s)
               Value function loss: 178.2987
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 560.94
               Mean episode length: 149.03
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 9.24s
                        Total time: 20069.63s
                               ETA: 1021436.8s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.443s, learning 0.160s)
               Value function loss: 165.2219
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 570.40
               Mean episode length: 149.42
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 9.60s
                        Total time: 20079.23s
                               ETA: 1021385.1s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.417s, learning 0.170s)
               Value function loss: 145.7401
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 534.78
               Mean episode length: 149.54
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 9.59s
                        Total time: 20088.82s
                               ETA: 1021332.6s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.661s, learning 0.159s)
               Value function loss: 155.5598
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 541.70
               Mean episode length: 148.33
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 9.82s
                        Total time: 20098.64s
                               ETA: 1021292.0s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.107s, learning 0.158s)
               Value function loss: 185.3690
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 578.46
               Mean episode length: 148.96
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 9.26s
                        Total time: 20107.90s
                               ETA: 1021223.2s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.351s, learning 0.165s)
               Value function loss: 138.4174
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 551.15
               Mean episode length: 149.25
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 9.52s
                        Total time: 20117.42s
                               ETA: 1021167.3s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.284s, learning 0.165s)
               Value function loss: 148.7667
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 545.69
               Mean episode length: 148.07
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 9.45s
                        Total time: 20126.87s
                               ETA: 1021108.0s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.729s, learning 0.161s)
               Value function loss: 153.4159
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 582.60
               Mean episode length: 148.82
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 9.89s
                        Total time: 20136.76s
                               ETA: 1021071.0s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.327s, learning 0.175s)
               Value function loss: 138.3944
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 534.71
               Mean episode length: 148.01
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 9.50s
                        Total time: 20146.26s
                               ETA: 1021014.5s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.452s, learning 0.162s)
               Value function loss: 161.7483
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 548.87
               Mean episode length: 147.71
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 9.61s
                        Total time: 20155.87s
                               ETA: 1020963.7s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.774s, learning 0.160s)
               Value function loss: 147.1541
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 538.97
               Mean episode length: 149.31
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 9.93s
                        Total time: 20165.81s
                               ETA: 1020929.2s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.404s, learning 0.164s)
               Value function loss: 175.6841
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 577.39
               Mean episode length: 149.43
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 9.57s
                        Total time: 20175.38s
                               ETA: 1020876.1s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.199s, learning 0.161s)
               Value function loss: 161.9974
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 543.93
               Mean episode length: 149.11
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 9.36s
                        Total time: 20184.74s
                               ETA: 1020812.6s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.593s, learning 0.162s)
               Value function loss: 163.4998
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 530.28
               Mean episode length: 146.93
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 9.75s
                        Total time: 20194.49s
                               ETA: 1020769.1s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.524s, learning 0.161s)
               Value function loss: 175.6649
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 573.19
               Mean episode length: 149.80
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 9.69s
                        Total time: 20204.18s
                               ETA: 1020722.1s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.442s, learning 0.159s)
               Value function loss: 169.5132
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 557.99
               Mean episode length: 149.41
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 9.60s
                        Total time: 20213.78s
                               ETA: 1020670.8s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.489s, learning 0.170s)
               Value function loss: 153.1808
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 546.44
               Mean episode length: 149.36
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 9.66s
                        Total time: 20223.44s
                               ETA: 1020622.6s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.547s, learning 0.166s)
               Value function loss: 171.5137
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 554.31
               Mean episode length: 148.62
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 9.71s
                        Total time: 20233.15s
                               ETA: 1020577.1s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1613 steps/s (collection: 9.974s, learning 0.178s)
               Value function loss: 193.6894
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 543.84
               Mean episode length: 149.56
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 10.15s
                        Total time: 20243.30s
                               ETA: 1020553.8s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.278s, learning 0.171s)
               Value function loss: 191.2746
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 571.94
               Mean episode length: 148.34
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 9.45s
                        Total time: 20252.75s
                               ETA: 1020495.1s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.583s, learning 0.175s)
               Value function loss: 148.6789
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 560.96
               Mean episode length: 149.84
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 9.76s
                        Total time: 20262.51s
                               ETA: 1020452.0s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.511s, learning 0.175s)
               Value function loss: 149.0347
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 551.76
               Mean episode length: 148.29
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 9.69s
                        Total time: 20272.19s
                               ETA: 1020405.3s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.660s, learning 0.180s)
               Value function loss: 153.0483
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 550.49
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 9.84s
                        Total time: 20282.03s
                               ETA: 1020366.4s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.544s, learning 0.170s)
               Value function loss: 159.2622
                    Surrogate loss: 0.0097
             Mean action noise std: 0.75
                       Mean reward: 557.14
               Mean episode length: 148.69
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 9.71s
                        Total time: 20291.75s
                               ETA: 1020321.1s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.057s, learning 0.173s)
               Value function loss: 144.8050
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 542.54
               Mean episode length: 148.82
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 9.23s
                        Total time: 20300.98s
                               ETA: 1020251.6s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.741s, learning 0.179s)
               Value function loss: 172.0970
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 557.51
               Mean episode length: 148.24
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 9.92s
                        Total time: 20310.90s
                               ETA: 1020216.9s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.446s, learning 0.168s)
               Value function loss: 151.7632
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 531.45
               Mean episode length: 146.59
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 9.61s
                        Total time: 20320.51s
                               ETA: 1020166.7s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.624s, learning 0.168s)
               Value function loss: 153.9058
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 562.11
               Mean episode length: 148.61
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 9.79s
                        Total time: 20330.30s
                               ETA: 1020125.6s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.576s, learning 0.170s)
               Value function loss: 165.4878
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 546.53
               Mean episode length: 149.65
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 9.75s
                        Total time: 20340.05s
                               ETA: 1020082.2s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.385s, learning 0.161s)
               Value function loss: 179.2363
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 559.88
               Mean episode length: 149.04
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 9.55s
                        Total time: 20349.60s
                               ETA: 1020028.7s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.375s, learning 0.162s)
               Value function loss: 170.5821
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 563.34
               Mean episode length: 148.76
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 9.54s
                        Total time: 20359.13s
                               ETA: 1019974.9s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.763s, learning 0.167s)
               Value function loss: 154.0950
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 565.22
               Mean episode length: 149.40
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 9.93s
                        Total time: 20369.06s
                               ETA: 1019940.8s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.405s, learning 0.192s)
               Value function loss: 148.9901
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 554.12
               Mean episode length: 149.35
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 9.60s
                        Total time: 20378.66s
                               ETA: 1019890.1s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.381s, learning 0.170s)
               Value function loss: 154.1043
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 535.99
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 9.55s
                        Total time: 20388.21s
                               ETA: 1019837.1s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1606 steps/s (collection: 10.016s, learning 0.180s)
               Value function loss: 157.8174
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 561.50
               Mean episode length: 149.31
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 10.20s
                        Total time: 20398.41s
                               ETA: 1019816.4s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.495s, learning 0.184s)
               Value function loss: 156.0128
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 575.00
               Mean episode length: 149.71
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 9.68s
                        Total time: 20408.09s
                               ETA: 1019769.9s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.616s, learning 0.165s)
               Value function loss: 168.9640
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 556.69
               Mean episode length: 146.91
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 9.78s
                        Total time: 20417.87s
                               ETA: 1019728.4s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.328s, learning 0.169s)
               Value function loss: 166.0250
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 552.98
               Mean episode length: 149.11
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 9.50s
                        Total time: 20427.37s
                               ETA: 1019672.9s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.457s, learning 0.164s)
               Value function loss: 158.9372
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 555.12
               Mean episode length: 147.71
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 9.62s
                        Total time: 20436.99s
                               ETA: 1019623.6s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.640s, learning 0.189s)
               Value function loss: 148.9205
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 565.62
               Mean episode length: 149.01
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 9.83s
                        Total time: 20446.81s
                               ETA: 1019584.7s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.456s, learning 0.185s)
               Value function loss: 149.2485
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 552.60
               Mean episode length: 147.75
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 9.64s
                        Total time: 20456.46s
                               ETA: 1019536.4s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.332s, learning 0.189s)
               Value function loss: 144.9340
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 547.42
               Mean episode length: 148.74
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 9.52s
                        Total time: 20465.98s
                               ETA: 1019482.2s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.597s, learning 0.199s)
               Value function loss: 137.4748
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 559.62
               Mean episode length: 149.68
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 9.80s
                        Total time: 20475.77s
                               ETA: 1019441.8s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.833s, learning 0.176s)
               Value function loss: 143.1441
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 545.26
               Mean episode length: 149.24
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 10.01s
                        Total time: 20485.78s
                               ETA: 1019412.0s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.667s, learning 0.188s)
               Value function loss: 152.6278
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 525.40
               Mean episode length: 147.85
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 9.85s
                        Total time: 20495.64s
                               ETA: 1019374.5s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.439s, learning 0.164s)
               Value function loss: 143.1531
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 538.11
               Mean episode length: 149.57
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 9.60s
                        Total time: 20505.24s
                               ETA: 1019324.6s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.414s, learning 0.159s)
               Value function loss: 174.0093
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 559.38
               Mean episode length: 147.76
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 9.57s
                        Total time: 20514.81s
                               ETA: 1019273.2s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.103s, learning 0.175s)
               Value function loss: 163.0246
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 566.71
               Mean episode length: 148.94
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 9.28s
                        Total time: 20524.09s
                               ETA: 1019207.1s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.798s, learning 0.169s)
               Value function loss: 193.3229
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 550.35
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 9.97s
                        Total time: 20534.06s
                               ETA: 1019175.4s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.617s, learning 0.164s)
               Value function loss: 225.9695
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 557.91
               Mean episode length: 148.49
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 9.78s
                        Total time: 20543.84s
                               ETA: 1019134.4s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.557s, learning 0.164s)
               Value function loss: 258.5828
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 568.93
               Mean episode length: 149.56
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 9.72s
                        Total time: 20553.56s
                               ETA: 1019090.5s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.472s, learning 0.165s)
               Value function loss: 329.2947
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 551.02
               Mean episode length: 148.88
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 9.64s
                        Total time: 20563.19s
                               ETA: 1019042.5s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.296s, learning 0.161s)
               Value function loss: 358.8947
                    Surrogate loss: 0.0031
             Mean action noise std: 0.75
                       Mean reward: 557.87
               Mean episode length: 149.13
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 9.46s
                        Total time: 20572.65s
                               ETA: 1018985.6s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.324s, learning 0.163s)
               Value function loss: 248.1261
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 566.43
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 9.49s
                        Total time: 20582.14s
                               ETA: 1018930.2s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.580s, learning 0.160s)
               Value function loss: 248.0714
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 541.25
               Mean episode length: 148.66
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 9.74s
                        Total time: 20591.88s
                               ETA: 1018887.4s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.578s, learning 0.165s)
               Value function loss: 207.9377
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 542.87
               Mean episode length: 149.51
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 9.74s
                        Total time: 20601.62s
                               ETA: 1018844.8s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.491s, learning 0.167s)
               Value function loss: 206.9086
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 582.04
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 9.66s
                        Total time: 20611.28s
                               ETA: 1018798.0s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.378s, learning 0.159s)
               Value function loss: 157.8439
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 540.56
               Mean episode length: 149.47
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 9.54s
                        Total time: 20620.82s
                               ETA: 1018745.3s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.395s, learning 0.169s)
               Value function loss: 153.1935
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 551.82
               Mean episode length: 150.00
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 9.56s
                        Total time: 20630.38s
                               ETA: 1018693.9s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.578s, learning 0.160s)
               Value function loss: 180.4090
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 561.81
               Mean episode length: 149.22
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 9.74s
                        Total time: 20640.12s
                               ETA: 1018651.2s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.565s, learning 0.166s)
               Value function loss: 165.8178
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 542.26
               Mean episode length: 149.48
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 9.73s
                        Total time: 20649.85s
                               ETA: 1018608.2s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.263s, learning 0.166s)
               Value function loss: 137.2593
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 556.78
               Mean episode length: 149.28
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 9.43s
                        Total time: 20659.28s
                               ETA: 1018550.2s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.182s, learning 0.172s)
               Value function loss: 132.9906
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 560.56
               Mean episode length: 149.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 9.35s
                        Total time: 20668.63s
                               ETA: 1018488.7s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.522s, learning 0.164s)
               Value function loss: 139.3859
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 542.37
               Mean episode length: 148.77
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 9.69s
                        Total time: 20678.32s
                               ETA: 1018443.6s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.448s, learning 0.189s)
               Value function loss: 149.7409
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 561.56
               Mean episode length: 149.89
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 9.64s
                        Total time: 20687.96s
                               ETA: 1018396.1s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.035s, learning 0.183s)
               Value function loss: 158.1844
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 548.27
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 9.22s
                        Total time: 20697.17s
                               ETA: 1018328.0s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.246s, learning 0.184s)
               Value function loss: 166.8814
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 549.79
               Mean episode length: 149.22
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 9.43s
                        Total time: 20706.60s
                               ETA: 1018270.4s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.231s, learning 0.180s)
               Value function loss: 159.3895
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 533.55
               Mean episode length: 150.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 9.41s
                        Total time: 20716.02s
                               ETA: 1018211.9s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.535s, learning 0.167s)
               Value function loss: 164.3022
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 564.29
               Mean episode length: 149.75
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 9.70s
                        Total time: 20725.72s
                               ETA: 1018167.8s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.463s, learning 0.182s)
               Value function loss: 136.9815
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 554.91
               Mean episode length: 149.65
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 9.64s
                        Total time: 20735.36s
                               ETA: 1018120.8s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.137s, learning 0.171s)
               Value function loss: 164.1783
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 565.42
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 9.31s
                        Total time: 20744.67s
                               ETA: 1018057.4s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.204s, learning 0.171s)
               Value function loss: 152.6343
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 562.29
               Mean episode length: 149.45
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 9.37s
                        Total time: 20754.04s
                               ETA: 1017997.3s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.604s, learning 0.163s)
               Value function loss: 152.0987
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 581.81
               Mean episode length: 149.52
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 9.77s
                        Total time: 20763.81s
                               ETA: 1017956.5s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.353s, learning 0.171s)
               Value function loss: 184.6465
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 553.88
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 9.52s
                        Total time: 20773.34s
                               ETA: 1017903.8s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.375s, learning 0.166s)
               Value function loss: 182.9944
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 570.06
               Mean episode length: 148.90
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 9.54s
                        Total time: 20782.88s
                               ETA: 1017852.1s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.101s, learning 0.187s)
               Value function loss: 167.3674
                    Surrogate loss: 0.0021
             Mean action noise std: 0.75
                       Mean reward: 548.12
               Mean episode length: 149.20
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 9.29s
                        Total time: 20792.17s
                               ETA: 1017787.9s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.106s, learning 0.174s)
               Value function loss: 152.5486
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 551.86
               Mean episode length: 149.12
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 9.28s
                        Total time: 20801.44s
                               ETA: 1017723.4s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.172s, learning 0.179s)
               Value function loss: 142.7477
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 530.99
               Mean episode length: 149.84
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 9.35s
                        Total time: 20810.80s
                               ETA: 1017662.4s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.406s, learning 0.164s)
               Value function loss: 142.4141
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 558.72
               Mean episode length: 148.38
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 9.57s
                        Total time: 20820.37s
                               ETA: 1017612.2s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.218s, learning 0.158s)
               Value function loss: 151.7478
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 535.58
               Mean episode length: 148.55
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 9.38s
                        Total time: 20829.74s
                               ETA: 1017552.6s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.556s, learning 0.161s)
               Value function loss: 127.8219
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 546.23
               Mean episode length: 148.90
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 9.72s
                        Total time: 20839.46s
                               ETA: 1017509.7s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.279s, learning 0.167s)
               Value function loss: 157.5244
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 556.09
               Mean episode length: 148.95
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 9.45s
                        Total time: 20848.90s
                               ETA: 1017453.5s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.458s, learning 0.185s)
               Value function loss: 153.0505
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 586.84
               Mean episode length: 149.44
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 9.64s
                        Total time: 20858.55s
                               ETA: 1017407.1s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.364s, learning 0.191s)
               Value function loss: 145.4509
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 534.84
               Mean episode length: 148.72
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 9.55s
                        Total time: 20868.10s
                               ETA: 1017356.3s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.321s, learning 0.189s)
               Value function loss: 169.9239
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 562.37
               Mean episode length: 149.18
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 9.51s
                        Total time: 20877.61s
                               ETA: 1017303.5s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.252s, learning 0.162s)
               Value function loss: 166.7580
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 576.62
               Mean episode length: 149.09
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 9.41s
                        Total time: 20887.03s
                               ETA: 1017245.9s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.550s, learning 0.177s)
               Value function loss: 186.9277
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 570.00
               Mean episode length: 149.37
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 9.73s
                        Total time: 20896.75s
                               ETA: 1017203.7s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.475s, learning 0.180s)
               Value function loss: 172.2809
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 558.17
               Mean episode length: 149.28
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 9.66s
                        Total time: 20906.41s
                               ETA: 1017158.0s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.673s, learning 0.172s)
               Value function loss: 149.8317
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 577.21
               Mean episode length: 149.07
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 9.85s
                        Total time: 20916.25s
                               ETA: 1017121.6s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1598 steps/s (collection: 10.074s, learning 0.178s)
               Value function loss: 164.1029
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 554.96
               Mean episode length: 147.96
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 10.25s
                        Total time: 20926.51s
                               ETA: 1017105.0s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.496s, learning 0.164s)
               Value function loss: 188.1215
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 586.03
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 9.66s
                        Total time: 20936.17s
                               ETA: 1017059.6s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.478s, learning 0.167s)
               Value function loss: 157.9584
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 571.66
               Mean episode length: 148.16
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 9.64s
                        Total time: 20945.81s
                               ETA: 1017013.5s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.628s, learning 0.159s)
               Value function loss: 183.4194
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 565.69
               Mean episode length: 149.51
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 9.79s
                        Total time: 20955.60s
                               ETA: 1016974.3s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.637s, learning 0.163s)
               Value function loss: 187.6474
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 582.38
               Mean episode length: 149.22
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 9.80s
                        Total time: 20965.40s
                               ETA: 1016935.9s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.641s, learning 0.167s)
               Value function loss: 181.8069
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 544.95
               Mean episode length: 148.89
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 9.81s
                        Total time: 20975.20s
                               ETA: 1016897.8s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.577s, learning 0.162s)
               Value function loss: 157.6260
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 555.42
               Mean episode length: 149.04
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 9.74s
                        Total time: 20984.94s
                               ETA: 1016856.4s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.408s, learning 0.161s)
               Value function loss: 144.5927
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 575.55
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 9.57s
                        Total time: 20994.51s
                               ETA: 1016806.8s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.344s, learning 0.163s)
               Value function loss: 147.2396
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 518.91
               Mean episode length: 149.23
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 9.51s
                        Total time: 21004.02s
                               ETA: 1016754.3s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.312s, learning 0.165s)
               Value function loss: 157.0859
                    Surrogate loss: 0.0016
             Mean action noise std: 0.75
                       Mean reward: 557.01
               Mean episode length: 147.90
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 9.48s
                        Total time: 21013.50s
                               ETA: 1016700.4s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.629s, learning 0.168s)
               Value function loss: 143.2245
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 558.48
               Mean episode length: 149.48
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 9.80s
                        Total time: 21023.29s
                               ETA: 1016662.0s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.405s, learning 0.159s)
               Value function loss: 145.7923
                    Surrogate loss: 0.0055
             Mean action noise std: 0.75
                       Mean reward: 533.28
               Mean episode length: 149.04
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 9.56s
                        Total time: 21032.86s
                               ETA: 1016612.3s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.565s, learning 0.169s)
               Value function loss: 151.8498
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 535.83
               Mean episode length: 149.53
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 9.73s
                        Total time: 21042.59s
                               ETA: 1016570.9s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.264s, learning 0.163s)
               Value function loss: 140.5713
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 533.95
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 9.43s
                        Total time: 21052.02s
                               ETA: 1016514.6s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.469s, learning 0.161s)
               Value function loss: 154.7281
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 548.47
               Mean episode length: 149.54
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 9.63s
                        Total time: 21061.65s
                               ETA: 1016468.3s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.535s, learning 0.157s)
               Value function loss: 178.6648
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 587.81
               Mean episode length: 149.27
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 9.69s
                        Total time: 21071.34s
                               ETA: 1016424.9s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.213s, learning 0.185s)
               Value function loss: 173.8216
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 550.61
               Mean episode length: 149.76
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 9.40s
                        Total time: 21080.74s
                               ETA: 1016367.4s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.401s, learning 0.164s)
               Value function loss: 154.8835
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 550.63
               Mean episode length: 149.70
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 9.57s
                        Total time: 21090.30s
                               ETA: 1016318.1s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.564s, learning 0.182s)
               Value function loss: 154.7356
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 565.63
               Mean episode length: 148.77
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 9.75s
                        Total time: 21100.05s
                               ETA: 1016277.4s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.541s, learning 0.187s)
               Value function loss: 163.0383
                    Surrogate loss: 0.0021
             Mean action noise std: 0.75
                       Mean reward: 530.62
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 9.73s
                        Total time: 21109.78s
                               ETA: 1016236.0s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.532s, learning 0.165s)
               Value function loss: 152.8872
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 547.48
               Mean episode length: 149.13
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 9.70s
                        Total time: 21119.47s
                               ETA: 1016193.0s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1598 steps/s (collection: 10.088s, learning 0.160s)
               Value function loss: 158.9583
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 549.27
               Mean episode length: 149.10
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 10.25s
                        Total time: 21129.72s
                               ETA: 1016176.7s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.514s, learning 0.170s)
               Value function loss: 170.8271
                    Surrogate loss: -0.0001
             Mean action noise std: 0.75
                       Mean reward: 547.37
               Mean episode length: 149.73
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 9.68s
                        Total time: 21139.40s
                               ETA: 1016133.1s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.506s, learning 0.164s)
               Value function loss: 177.8638
                    Surrogate loss: 0.0068
             Mean action noise std: 0.75
                       Mean reward: 545.70
               Mean episode length: 148.62
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 9.67s
                        Total time: 21149.07s
                               ETA: 1016089.0s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.418s, learning 0.162s)
               Value function loss: 170.2650
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 546.86
               Mean episode length: 149.11
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 9.58s
                        Total time: 21158.65s
                               ETA: 1016040.6s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.640s, learning 0.167s)
               Value function loss: 160.8904
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 556.85
               Mean episode length: 149.80
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 9.81s
                        Total time: 21168.46s
                               ETA: 1016003.0s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.381s, learning 0.164s)
               Value function loss: 148.2640
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 550.92
               Mean episode length: 149.89
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 9.55s
                        Total time: 21178.00s
                               ETA: 1015953.0s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1769 steps/s (collection: 9.095s, learning 0.163s)
               Value function loss: 144.4565
                    Surrogate loss: 0.0052
             Mean action noise std: 0.75
                       Mean reward: 547.74
               Mean episode length: 148.65
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 9.26s
                        Total time: 21187.26s
                               ETA: 1015889.3s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.406s, learning 0.170s)
               Value function loss: 135.3050
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 540.83
               Mean episode length: 149.14
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 9.58s
                        Total time: 21196.84s
                               ETA: 1015840.8s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.151s, learning 0.170s)
               Value function loss: 130.6788
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 532.43
               Mean episode length: 150.00
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 9.32s
                        Total time: 21206.16s
                               ETA: 1015780.2s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.212s, learning 0.163s)
               Value function loss: 163.2632
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 553.26
               Mean episode length: 149.69
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 9.37s
                        Total time: 21215.53s
                               ETA: 1015722.2s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.343s, learning 0.163s)
               Value function loss: 132.8884
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 511.00
               Mean episode length: 147.94
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 9.51s
                        Total time: 21225.04s
                               ETA: 1015670.5s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.320s, learning 0.162s)
               Value function loss: 157.9986
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 500.67
               Mean episode length: 149.31
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 9.48s
                        Total time: 21234.52s
                               ETA: 1015617.7s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.472s, learning 0.165s)
               Value function loss: 164.6105
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 531.97
               Mean episode length: 148.90
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 9.64s
                        Total time: 21244.16s
                               ETA: 1015572.4s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.699s, learning 0.162s)
               Value function loss: 183.8196
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 541.58
               Mean episode length: 150.00
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 9.86s
                        Total time: 21254.02s
                               ETA: 1015537.8s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.518s, learning 0.163s)
               Value function loss: 144.7890
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 543.18
               Mean episode length: 149.21
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 9.68s
                        Total time: 21263.70s
                               ETA: 1015494.6s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.502s, learning 0.162s)
               Value function loss: 114.6373
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 531.62
               Mean episode length: 150.00
                  Mean reward/step: 3.40
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 9.66s
                        Total time: 21273.36s
                               ETA: 1015450.6s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.337s, learning 0.166s)
               Value function loss: 133.1925
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 510.21
               Mean episode length: 150.00
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 9.50s
                        Total time: 21282.87s
                               ETA: 1015399.1s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.399s, learning 0.158s)
               Value function loss: 141.7076
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 523.09
               Mean episode length: 150.00
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 9.56s
                        Total time: 21292.42s
                               ETA: 1015350.1s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.248s, learning 0.158s)
               Value function loss: 124.8040
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 500.91
               Mean episode length: 148.51
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 9.41s
                        Total time: 21301.83s
                               ETA: 1015293.9s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.257s, learning 0.163s)
               Value function loss: 140.0551
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 514.46
               Mean episode length: 148.81
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 9.42s
                        Total time: 21311.25s
                               ETA: 1015238.5s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.433s, learning 0.165s)
               Value function loss: 161.6490
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 523.11
               Mean episode length: 150.00
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 9.60s
                        Total time: 21320.85s
                               ETA: 1015191.6s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.017s, learning 0.160s)
               Value function loss: 145.6594
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 502.17
               Mean episode length: 149.07
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 9.18s
                        Total time: 21330.03s
                               ETA: 1015124.7s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.387s, learning 0.161s)
               Value function loss: 142.8458
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 518.29
               Mean episode length: 149.08
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 9.55s
                        Total time: 21339.57s
                               ETA: 1015075.5s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.136s, learning 0.175s)
               Value function loss: 135.0535
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 505.38
               Mean episode length: 149.00
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 9.31s
                        Total time: 21348.88s
                               ETA: 1015015.1s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1771 steps/s (collection: 9.084s, learning 0.166s)
               Value function loss: 149.1919
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 504.10
               Mean episode length: 148.53
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 9.25s
                        Total time: 21358.13s
                               ETA: 1014951.8s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.528s, learning 0.165s)
               Value function loss: 135.5625
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 502.95
               Mean episode length: 148.78
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 9.69s
                        Total time: 21367.83s
                               ETA: 1014909.7s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.621s, learning 0.160s)
               Value function loss: 109.4862
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 555.96
               Mean episode length: 149.95
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 9.78s
                        Total time: 21377.61s
                               ETA: 1014871.7s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.108s, learning 0.162s)
               Value function loss: 122.1914
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 547.31
               Mean episode length: 149.94
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 9.27s
                        Total time: 21386.88s
                               ETA: 1014809.5s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.425s, learning 0.176s)
               Value function loss: 158.9808
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 544.60
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 9.60s
                        Total time: 21396.48s
                               ETA: 1014763.0s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.263s, learning 0.162s)
               Value function loss: 138.0396
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 530.90
               Mean episode length: 148.16
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 9.43s
                        Total time: 21405.90s
                               ETA: 1014708.3s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.852s, learning 0.171s)
               Value function loss: 159.4680
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 546.58
               Mean episode length: 149.73
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 10.02s
                        Total time: 21415.93s
                               ETA: 1014681.9s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.238s, learning 0.163s)
               Value function loss: 129.1212
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 502.17
               Mean episode length: 149.98
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 9.40s
                        Total time: 21425.33s
                               ETA: 1014626.1s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.393s, learning 0.167s)
               Value function loss: 160.6279
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 540.30
               Mean episode length: 149.47
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 9.56s
                        Total time: 21434.89s
                               ETA: 1014577.9s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.492s, learning 0.161s)
               Value function loss: 158.7667
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 538.02
               Mean episode length: 149.86
                  Mean reward/step: 3.52
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 9.65s
                        Total time: 21444.54s
                               ETA: 1014534.1s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.073s, learning 0.162s)
               Value function loss: 125.5797
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 530.80
               Mean episode length: 148.40
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 9.24s
                        Total time: 21453.78s
                               ETA: 1014470.6s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.523s, learning 0.173s)
               Value function loss: 151.3642
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 551.46
               Mean episode length: 148.20
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 9.70s
                        Total time: 21463.47s
                               ETA: 1014428.9s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.446s, learning 0.162s)
               Value function loss: 129.3637
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 537.36
               Mean episode length: 150.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 9.61s
                        Total time: 21473.08s
                               ETA: 1014383.0s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.439s, learning 0.167s)
               Value function loss: 143.9761
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 527.92
               Mean episode length: 148.35
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 9.61s
                        Total time: 21482.69s
                               ETA: 1014337.1s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.165s)
               Value function loss: 174.0033
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 530.86
               Mean episode length: 149.41
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 9.59s
                        Total time: 21492.28s
                               ETA: 1014290.6s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.063s, learning 0.165s)
               Value function loss: 151.9399
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 516.82
               Mean episode length: 150.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 9.23s
                        Total time: 21501.51s
                               ETA: 1014226.9s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.401s, learning 0.164s)
               Value function loss: 139.5326
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 507.41
               Mean episode length: 149.25
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 9.56s
                        Total time: 21511.07s
                               ETA: 1014179.2s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.844s, learning 0.160s)
               Value function loss: 142.0536
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 522.66
               Mean episode length: 148.82
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 10.00s
                        Total time: 21521.08s
                               ETA: 1014152.2s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.310s, learning 0.165s)
               Value function loss: 144.1621
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 557.23
               Mean episode length: 147.96
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 9.47s
                        Total time: 21530.55s
                               ETA: 1014100.4s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.628s, learning 0.161s)
               Value function loss: 159.5663
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 541.35
               Mean episode length: 149.17
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 9.79s
                        Total time: 21540.34s
                               ETA: 1014063.3s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.562s, learning 0.167s)
               Value function loss: 151.9897
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 569.74
               Mean episode length: 149.83
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 9.73s
                        Total time: 21550.07s
                               ETA: 1014023.4s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.846s, learning 0.164s)
               Value function loss: 130.7264
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 542.94
               Mean episode length: 148.09
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 10.01s
                        Total time: 21560.08s
                               ETA: 1013996.8s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1771 steps/s (collection: 9.084s, learning 0.168s)
               Value function loss: 145.0612
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 562.10
               Mean episode length: 149.54
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 9.25s
                        Total time: 21569.33s
                               ETA: 1013934.5s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.228s, learning 0.169s)
               Value function loss: 143.4010
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 548.12
               Mean episode length: 149.60
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 9.40s
                        Total time: 21578.73s
                               ETA: 1013879.2s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.496s, learning 0.163s)
               Value function loss: 138.7209
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 538.90
               Mean episode length: 147.70
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 9.66s
                        Total time: 21588.39s
                               ETA: 1013836.2s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.632s, learning 0.162s)
               Value function loss: 139.4305
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 531.17
               Mean episode length: 149.27
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 9.79s
                        Total time: 21598.18s
                               ETA: 1013799.5s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.246s, learning 0.184s)
               Value function loss: 140.0450
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 530.62
               Mean episode length: 148.59
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 9.43s
                        Total time: 21607.61s
                               ETA: 1013745.8s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.450s, learning 0.169s)
               Value function loss: 164.8727
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 535.81
               Mean episode length: 149.47
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 9.62s
                        Total time: 21617.23s
                               ETA: 1013701.0s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.855s, learning 0.166s)
               Value function loss: 146.6998
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 549.91
               Mean episode length: 149.02
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 9.02s
                        Total time: 21626.25s
                               ETA: 1013628.2s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.348s, learning 0.162s)
               Value function loss: 128.3026
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 589.87
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 9.51s
                        Total time: 21635.76s
                               ETA: 1013578.4s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.735s, learning 0.163s)
               Value function loss: 137.4385
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 540.87
               Mean episode length: 149.25
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 9.90s
                        Total time: 21645.66s
                               ETA: 1013546.8s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.236s, learning 0.176s)
               Value function loss: 128.8823
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 553.43
               Mean episode length: 149.59
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 9.41s
                        Total time: 21655.07s
                               ETA: 1013492.4s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.099s, learning 0.169s)
               Value function loss: 131.7621
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 557.51
               Mean episode length: 148.96
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 9.27s
                        Total time: 21664.34s
                               ETA: 1013431.4s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.194s, learning 0.169s)
               Value function loss: 138.2141
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 565.65
               Mean episode length: 149.65
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 9.36s
                        Total time: 21673.70s
                               ETA: 1013374.8s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.433s, learning 0.170s)
               Value function loss: 145.3781
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 594.81
               Mean episode length: 149.82
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 9.60s
                        Total time: 21683.30s
                               ETA: 1013329.6s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.880s, learning 0.164s)
               Value function loss: 148.2665
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 526.26
               Mean episode length: 148.43
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 9.04s
                        Total time: 21692.35s
                               ETA: 1013258.2s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.506s, learning 0.158s)
               Value function loss: 131.0929
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 515.45
               Mean episode length: 147.40
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 9.66s
                        Total time: 21702.01s
                               ETA: 1013215.9s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.459s, learning 0.159s)
               Value function loss: 125.0741
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 536.17
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 9.62s
                        Total time: 21711.63s
                               ETA: 1013171.4s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.754s, learning 0.158s)
               Value function loss: 136.5437
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 559.09
               Mean episode length: 148.85
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 9.91s
                        Total time: 21721.54s
                               ETA: 1013140.7s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.178s, learning 0.171s)
               Value function loss: 155.8826
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 563.13
               Mean episode length: 148.38
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 9.35s
                        Total time: 21730.89s
                               ETA: 1013083.8s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.389s, learning 0.169s)
               Value function loss: 157.1546
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 547.61
               Mean episode length: 148.32
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 9.56s
                        Total time: 21740.45s
                               ETA: 1013036.6s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.293s, learning 0.168s)
               Value function loss: 160.8666
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 527.86
               Mean episode length: 147.74
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 9.46s
                        Total time: 21749.91s
                               ETA: 1012985.0s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.244s, learning 0.161s)
               Value function loss: 160.5198
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 555.73
               Mean episode length: 148.80
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 9.40s
                        Total time: 21759.31s
                               ETA: 1012930.8s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.291s, learning 0.166s)
               Value function loss: 145.9941
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 553.86
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 9.46s
                        Total time: 21768.77s
                               ETA: 1012879.0s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.516s, learning 0.163s)
               Value function loss: 159.5610
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 548.75
               Mean episode length: 148.84
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 9.68s
                        Total time: 21778.45s
                               ETA: 1012837.6s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.489s, learning 0.158s)
               Value function loss: 179.1616
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 550.92
               Mean episode length: 148.69
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 9.65s
                        Total time: 21788.10s
                               ETA: 1012794.8s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.017s, learning 0.162s)
               Value function loss: 190.5268
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 573.42
               Mean episode length: 148.29
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 9.18s
                        Total time: 21797.28s
                               ETA: 1012730.2s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.235s, learning 0.159s)
               Value function loss: 123.1569
                    Surrogate loss: -0.0213
             Mean action noise std: 0.75
                       Mean reward: 554.71
               Mean episode length: 149.63
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 9.39s
                        Total time: 21806.67s
                               ETA: 1012675.7s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.265s, learning 0.174s)
               Value function loss: 128.0323
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 543.76
               Mean episode length: 148.62
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 9.44s
                        Total time: 21816.11s
                               ETA: 1012623.3s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.729s, learning 0.163s)
               Value function loss: 129.2328
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 551.56
               Mean episode length: 149.23
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 9.89s
                        Total time: 21826.00s
                               ETA: 1012591.9s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.861s, learning 0.162s)
               Value function loss: 123.2291
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 510.10
               Mean episode length: 148.23
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 9.02s
                        Total time: 21835.02s
                               ETA: 1012520.3s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.336s, learning 0.164s)
               Value function loss: 136.2924
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 563.23
               Mean episode length: 149.63
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 9.50s
                        Total time: 21844.52s
                               ETA: 1012470.9s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.625s, learning 0.174s)
               Value function loss: 145.9692
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 508.48
               Mean episode length: 149.10
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 9.80s
                        Total time: 21854.32s
                               ETA: 1012435.3s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.460s, learning 0.177s)
               Value function loss: 153.7609
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 528.23
               Mean episode length: 149.94
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 9.64s
                        Total time: 21863.96s
                               ETA: 1012392.3s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.830s, learning 0.171s)
               Value function loss: 149.9920
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 535.65
               Mean episode length: 149.61
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 10.00s
                        Total time: 21873.96s
                               ETA: 1012366.1s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.935s, learning 0.168s)
               Value function loss: 135.4021
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 519.79
               Mean episode length: 150.00
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 9.10s
                        Total time: 21883.06s
                               ETA: 1012298.5s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.706s, learning 0.162s)
               Value function loss: 128.5473
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 511.70
               Mean episode length: 150.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 9.87s
                        Total time: 21892.93s
                               ETA: 1012266.2s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.151s, learning 0.159s)
               Value function loss: 136.4176
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 558.40
               Mean episode length: 148.57
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 9.31s
                        Total time: 21902.24s
                               ETA: 1012208.1s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.691s, learning 0.169s)
               Value function loss: 125.2252
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 526.60
               Mean episode length: 149.44
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 9.86s
                        Total time: 21912.10s
                               ETA: 1012175.6s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.581s, learning 0.162s)
               Value function loss: 128.7676
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 532.15
               Mean episode length: 149.31
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 9.74s
                        Total time: 21921.84s
                               ETA: 1012137.6s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.386s, learning 0.162s)
               Value function loss: 141.8244
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 541.11
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 9.55s
                        Total time: 21931.39s
                               ETA: 1012090.7s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.068s, learning 0.159s)
               Value function loss: 128.7818
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 524.50
               Mean episode length: 149.43
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 9.23s
                        Total time: 21940.62s
                               ETA: 1012029.0s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.749s, learning 0.165s)
               Value function loss: 145.8781
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 515.94
               Mean episode length: 149.33
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 9.91s
                        Total time: 21950.53s
                               ETA: 1011999.0s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.527s, learning 0.160s)
               Value function loss: 144.8631
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 544.87
               Mean episode length: 148.99
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 9.69s
                        Total time: 21960.22s
                               ETA: 1011958.6s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.501s, learning 0.174s)
               Value function loss: 183.4315
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 593.37
               Mean episode length: 149.23
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 9.67s
                        Total time: 21969.89s
                               ETA: 1011917.7s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.532s, learning 0.161s)
               Value function loss: 155.4010
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 525.75
               Mean episode length: 149.28
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 9.69s
                        Total time: 21979.58s
                               ETA: 1011877.6s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.763s, learning 0.166s)
               Value function loss: 148.5926
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 594.33
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 9.93s
                        Total time: 21989.51s
                               ETA: 1011848.4s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.557s, learning 0.166s)
               Value function loss: 158.8302
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 563.69
               Mean episode length: 148.54
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 9.72s
                        Total time: 21999.24s
                               ETA: 1011809.8s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.376s, learning 0.169s)
               Value function loss: 149.8766
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 587.67
               Mean episode length: 148.22
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 9.54s
                        Total time: 22008.78s
                               ETA: 1011763.0s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.343s, learning 0.175s)
               Value function loss: 147.4691
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 556.85
               Mean episode length: 149.09
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 9.52s
                        Total time: 22018.30s
                               ETA: 1011715.0s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.488s, learning 0.168s)
               Value function loss: 169.5952
                    Surrogate loss: -0.0219
             Mean action noise std: 0.74
                       Mean reward: 554.21
               Mean episode length: 148.26
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 9.66s
                        Total time: 22027.96s
                               ETA: 1011673.4s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.529s, learning 0.162s)
               Value function loss: 160.9867
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 585.23
               Mean episode length: 149.27
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 9.69s
                        Total time: 22037.65s
                               ETA: 1011633.4s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.604s, learning 0.163s)
               Value function loss: 169.1925
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 556.54
               Mean episode length: 149.11
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 9.77s
                        Total time: 22047.41s
                               ETA: 1011597.0s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.654s, learning 0.168s)
               Value function loss: 156.5454
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 584.00
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 9.82s
                        Total time: 22057.24s
                               ETA: 1011563.1s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.637s, learning 0.166s)
               Value function loss: 163.1733
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 596.20
               Mean episode length: 149.83
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 9.80s
                        Total time: 22067.04s
                               ETA: 1011528.2s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.673s, learning 0.164s)
               Value function loss: 162.4155
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 595.53
               Mean episode length: 149.03
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 9.84s
                        Total time: 22076.88s
                               ETA: 1011495.0s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.404s, learning 0.159s)
               Value function loss: 158.4183
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 598.84
               Mean episode length: 149.78
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 9.56s
                        Total time: 22086.44s
                               ETA: 1011449.3s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.258s, learning 0.172s)
               Value function loss: 128.6377
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 587.36
               Mean episode length: 149.19
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 9.43s
                        Total time: 22095.87s
                               ETA: 1011397.6s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.158s, learning 0.180s)
               Value function loss: 140.1408
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 594.26
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 9.34s
                        Total time: 22105.21s
                               ETA: 1011341.6s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.222s, learning 0.159s)
               Value function loss: 172.4553
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 579.40
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 9.38s
                        Total time: 22114.59s
                               ETA: 1011287.6s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.481s, learning 0.166s)
               Value function loss: 155.6755
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 592.53
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 9.65s
                        Total time: 22124.23s
                               ETA: 1011245.9s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.690s, learning 0.178s)
               Value function loss: 167.6241
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 582.53
               Mean episode length: 149.03
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 9.87s
                        Total time: 22134.10s
                               ETA: 1011214.3s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.381s, learning 0.162s)
               Value function loss: 161.6028
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 592.40
               Mean episode length: 150.00
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 9.54s
                        Total time: 22143.64s
                               ETA: 1011167.8s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.387s, learning 0.165s)
               Value function loss: 169.4461
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 569.30
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 9.55s
                        Total time: 22153.20s
                               ETA: 1011121.9s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.336s, learning 0.165s)
               Value function loss: 157.9651
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 586.44
               Mean episode length: 149.60
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 9.50s
                        Total time: 22162.70s
                               ETA: 1011073.6s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.475s, learning 0.167s)
               Value function loss: 157.3156
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 618.52
               Mean episode length: 149.70
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 9.64s
                        Total time: 22172.34s
                               ETA: 1011031.8s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.337s, learning 0.168s)
               Value function loss: 161.6818
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 593.74
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 9.50s
                        Total time: 22181.84s
                               ETA: 1010983.7s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.673s, learning 0.159s)
               Value function loss: 158.1919
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 601.12
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 9.83s
                        Total time: 22191.67s
                               ETA: 1010950.6s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.524s, learning 0.167s)
               Value function loss: 172.7574
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 576.16
               Mean episode length: 149.64
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 9.69s
                        Total time: 22201.37s
                               ETA: 1010911.2s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.528s, learning 0.171s)
               Value function loss: 189.4426
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 585.20
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 9.70s
                        Total time: 22211.07s
                               ETA: 1010872.1s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.450s, learning 0.168s)
               Value function loss: 177.6632
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 574.24
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 9.62s
                        Total time: 22220.68s
                               ETA: 1010829.3s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.209s, learning 0.161s)
               Value function loss: 167.5700
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 581.42
               Mean episode length: 149.89
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 9.37s
                        Total time: 22230.05s
                               ETA: 1010775.3s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.767s, learning 0.169s)
               Value function loss: 162.1962
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 584.09
               Mean episode length: 149.15
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 9.94s
                        Total time: 22239.99s
                               ETA: 1010747.1s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.424s, learning 0.161s)
               Value function loss: 139.6542
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 579.03
               Mean episode length: 150.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 9.59s
                        Total time: 22249.58s
                               ETA: 1010703.0s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.420s, learning 0.160s)
               Value function loss: 172.8904
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 578.00
               Mean episode length: 148.66
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 9.58s
                        Total time: 22259.15s
                               ETA: 1010658.6s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.463s, learning 0.172s)
               Value function loss: 164.6461
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 579.31
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 9.64s
                        Total time: 22268.79s
                               ETA: 1010616.8s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.833s, learning 0.170s)
               Value function loss: 130.7754
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: 587.93
               Mean episode length: 150.00
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 10.00s
                        Total time: 22278.79s
                               ETA: 1010591.7s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.852s, learning 0.160s)
               Value function loss: 147.9204
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 619.61
               Mean episode length: 149.99
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 9.01s
                        Total time: 22287.81s
                               ETA: 1010521.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.731s, learning 0.173s)
               Value function loss: 160.8795
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 590.14
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 9.90s
                        Total time: 22297.71s
                               ETA: 1010492.1s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.176s, learning 0.170s)
               Value function loss: 147.0867
                    Surrogate loss: -0.0185
             Mean action noise std: 0.74
                       Mean reward: 589.27
               Mean episode length: 148.85
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 9.35s
                        Total time: 22307.05s
                               ETA: 1010437.3s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.457s, learning 0.160s)
               Value function loss: 167.8373
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 592.98
               Mean episode length: 149.38
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 9.62s
                        Total time: 22316.67s
                               ETA: 1010394.8s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.559s, learning 0.169s)
               Value function loss: 148.0837
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 590.03
               Mean episode length: 150.00
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 9.73s
                        Total time: 22326.40s
                               ETA: 1010357.3s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.561s, learning 0.159s)
               Value function loss: 166.8522
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 579.44
               Mean episode length: 149.25
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 9.72s
                        Total time: 22336.12s
                               ETA: 1010319.6s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.192s, learning 0.162s)
               Value function loss: 173.3164
                    Surrogate loss: -0.0199
             Mean action noise std: 0.74
                       Mean reward: 568.20
               Mean episode length: 148.97
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 9.35s
                        Total time: 22345.47s
                               ETA: 1010265.3s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.560s, learning 0.181s)
               Value function loss: 146.0586
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 587.71
               Mean episode length: 149.46
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 9.74s
                        Total time: 22355.21s
                               ETA: 1010228.5s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.310s, learning 0.176s)
               Value function loss: 140.5777
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 574.40
               Mean episode length: 149.75
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 9.49s
                        Total time: 22364.70s
                               ETA: 1010180.3s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.363s, learning 0.179s)
               Value function loss: 141.0930
                    Surrogate loss: -0.0221
             Mean action noise std: 0.74
                       Mean reward: 569.82
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 9.54s
                        Total time: 22374.24s
                               ETA: 1010134.6s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.510s, learning 0.158s)
               Value function loss: 146.3774
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 579.70
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 9.67s
                        Total time: 22383.91s
                               ETA: 1010094.6s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.191s, learning 0.169s)
               Value function loss: 162.4309
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 617.84
               Mean episode length: 148.59
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 9.36s
                        Total time: 22393.27s
                               ETA: 1010040.8s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.396s, learning 0.163s)
               Value function loss: 584.9967
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 588.66
               Mean episode length: 148.27
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 9.56s
                        Total time: 22402.83s
                               ETA: 1009996.0s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.200s, learning 0.161s)
               Value function loss: 166.4050
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 612.95
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 9.36s
                        Total time: 22412.19s
                               ETA: 1009942.3s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.634s, learning 0.163s)
               Value function loss: 129.4375
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 580.09
               Mean episode length: 148.55
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 9.80s
                        Total time: 22421.99s
                               ETA: 1009908.3s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.521s, learning 0.159s)
               Value function loss: 113.1619
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 593.60
               Mean episode length: 149.20
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 9.68s
                        Total time: 22431.67s
                               ETA: 1009869.0s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.541s, learning 0.161s)
               Value function loss: 145.3902
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 607.39
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 9.70s
                        Total time: 22441.37s
                               ETA: 1009830.8s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.717s, learning 0.175s)
               Value function loss: 143.9571
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 591.22
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 9.89s
                        Total time: 22451.26s
                               ETA: 1009801.1s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.382s, learning 0.171s)
               Value function loss: 149.9915
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 608.26
               Mean episode length: 149.79
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 9.55s
                        Total time: 22460.82s
                               ETA: 1009756.2s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.723s, learning 0.159s)
               Value function loss: 150.5077
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 601.14
               Mean episode length: 149.06
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 9.88s
                        Total time: 22470.70s
                               ETA: 1009726.1s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.367s, learning 0.160s)
               Value function loss: 162.0167
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 572.78
               Mean episode length: 149.92
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 9.53s
                        Total time: 22480.23s
                               ETA: 1009680.1s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1614 steps/s (collection: 9.990s, learning 0.160s)
               Value function loss: 136.5620
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 578.64
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 10.15s
                        Total time: 22490.38s
                               ETA: 1009662.0s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.497s, learning 0.158s)
               Value function loss: 155.6976
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 581.78
               Mean episode length: 149.01
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 9.65s
                        Total time: 22500.03s
                               ETA: 1009621.8s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.442s, learning 0.158s)
               Value function loss: 158.3926
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 576.74
               Mean episode length: 148.66
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 9.60s
                        Total time: 22509.63s
                               ETA: 1009579.1s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.340s, learning 0.158s)
               Value function loss: 175.9110
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 588.79
               Mean episode length: 149.13
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 9.50s
                        Total time: 22519.13s
                               ETA: 1009531.9s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.470s, learning 0.160s)
               Value function loss: 159.9109
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 588.57
               Mean episode length: 148.62
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 9.63s
                        Total time: 22528.76s
                               ETA: 1009490.6s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.571s, learning 0.175s)
               Value function loss: 159.1611
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 576.00
               Mean episode length: 149.76
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 9.75s
                        Total time: 22538.50s
                               ETA: 1009454.6s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.411s, learning 0.160s)
               Value function loss: 186.2906
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 624.67
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 9.57s
                        Total time: 22548.07s
                               ETA: 1009410.7s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.629s, learning 0.161s)
               Value function loss: 179.4528
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 590.07
               Mean episode length: 149.72
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 9.79s
                        Total time: 22557.86s
                               ETA: 1009376.7s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.552s, learning 0.171s)
               Value function loss: 184.5007
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 604.74
               Mean episode length: 148.74
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 9.72s
                        Total time: 22567.59s
                               ETA: 1009339.7s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.681s, learning 0.169s)
               Value function loss: 191.2607
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 602.34
               Mean episode length: 149.47
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 9.85s
                        Total time: 22577.44s
                               ETA: 1009308.5s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.462s, learning 0.170s)
               Value function loss: 185.7198
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 601.54
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 9.63s
                        Total time: 22587.07s
                               ETA: 1009267.5s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.267s, learning 0.171s)
               Value function loss: 156.7603
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 603.67
               Mean episode length: 149.69
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 9.44s
                        Total time: 22596.51s
                               ETA: 1009217.9s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.675s, learning 0.170s)
               Value function loss: 158.2918
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 621.36
               Mean episode length: 149.76
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 9.84s
                        Total time: 22606.35s
                               ETA: 1009186.4s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.416s, learning 0.166s)
               Value function loss: 135.7939
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 582.41
               Mean episode length: 149.19
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 9.58s
                        Total time: 22615.94s
                               ETA: 1009143.3s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.641s, learning 0.184s)
               Value function loss: 146.8407
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 610.33
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 9.83s
                        Total time: 22625.76s
                               ETA: 1009111.0s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.633s, learning 0.173s)
               Value function loss: 123.9127
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 580.09
               Mean episode length: 149.06
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 9.81s
                        Total time: 22635.57s
                               ETA: 1009077.9s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.648s, learning 0.171s)
               Value function loss: 143.2365
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 615.82
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 9.82s
                        Total time: 22645.39s
                               ETA: 1009045.4s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.449s, learning 0.164s)
               Value function loss: 150.7258
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 577.81
               Mean episode length: 148.55
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 9.61s
                        Total time: 22655.00s
                               ETA: 1009003.7s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.261s, learning 0.165s)
               Value function loss: 141.6384
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 612.15
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 9.43s
                        Total time: 22664.43s
                               ETA: 1008953.8s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.351s, learning 0.158s)
               Value function loss: 147.1479
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 610.11
               Mean episode length: 149.20
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 9.51s
                        Total time: 22673.93s
                               ETA: 1008907.6s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.945s, learning 0.161s)
               Value function loss: 166.5236
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 591.70
               Mean episode length: 149.42
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 9.11s
                        Total time: 22683.04s
                               ETA: 1008843.5s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.691s, learning 0.162s)
               Value function loss: 173.5153
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 582.00
               Mean episode length: 149.25
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 9.85s
                        Total time: 22692.89s
                               ETA: 1008812.6s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.262s, learning 0.158s)
               Value function loss: 167.4863
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 587.05
               Mean episode length: 149.83
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 9.42s
                        Total time: 22702.31s
                               ETA: 1008762.5s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.561s, learning 0.168s)
               Value function loss: 153.8994
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 576.74
               Mean episode length: 148.31
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 9.73s
                        Total time: 22712.04s
                               ETA: 1008726.2s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.261s, learning 0.159s)
               Value function loss: 161.6479
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 598.10
               Mean episode length: 149.26
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 9.42s
                        Total time: 22721.46s
                               ETA: 1008676.2s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.654s, learning 0.158s)
               Value function loss: 160.3266
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 586.11
               Mean episode length: 149.58
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 9.81s
                        Total time: 22731.27s
                               ETA: 1008643.6s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.394s, learning 0.162s)
               Value function loss: 143.2562
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 591.49
               Mean episode length: 149.76
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 9.56s
                        Total time: 22740.83s
                               ETA: 1008599.6s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.706s, learning 0.158s)
               Value function loss: 166.7853
                    Surrogate loss: 0.0020
             Mean action noise std: 0.74
                       Mean reward: 598.59
               Mean episode length: 149.29
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 9.86s
                        Total time: 22750.69s
                               ETA: 1008569.4s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.516s, learning 0.163s)
               Value function loss: 183.7112
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 597.06
               Mean episode length: 148.33
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 9.68s
                        Total time: 22760.37s
                               ETA: 1008531.1s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.230s, learning 0.174s)
               Value function loss: 187.8817
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 605.74
               Mean episode length: 149.66
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 9.40s
                        Total time: 22769.78s
                               ETA: 1008480.5s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.494s, learning 0.158s)
               Value function loss: 175.4791
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 583.66
               Mean episode length: 148.88
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 9.65s
                        Total time: 22779.43s
                               ETA: 1008440.9s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.877s, learning 0.170s)
               Value function loss: 129.6676
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 556.26
               Mean episode length: 149.15
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 9.05s
                        Total time: 22788.48s
                               ETA: 1008374.6s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.833s, learning 0.171s)
               Value function loss: 156.7815
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 598.47
               Mean episode length: 149.21
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 10.00s
                        Total time: 22798.48s
                               ETA: 1008350.7s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.428s, learning 0.165s)
               Value function loss: 150.6262
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 582.59
               Mean episode length: 149.36
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 9.59s
                        Total time: 22808.07s
                               ETA: 1008308.6s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.576s, learning 0.163s)
               Value function loss: 114.6433
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 595.96
               Mean episode length: 148.65
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 9.74s
                        Total time: 22817.81s
                               ETA: 1008273.1s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.421s, learning 0.167s)
               Value function loss: 130.9995
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 598.07
               Mean episode length: 149.19
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 9.59s
                        Total time: 22827.40s
                               ETA: 1008230.8s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.636s, learning 0.163s)
               Value function loss: 149.7321
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 579.75
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 9.80s
                        Total time: 22837.20s
                               ETA: 1008197.9s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.452s, learning 0.179s)
               Value function loss: 124.1527
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 556.86
               Mean episode length: 149.64
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 9.63s
                        Total time: 22846.83s
                               ETA: 1008157.6s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1617 steps/s (collection: 9.972s, learning 0.159s)
               Value function loss: 150.4559
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 602.27
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 10.13s
                        Total time: 22856.96s
                               ETA: 1008139.4s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.338s, learning 0.166s)
               Value function loss: 146.3512
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 575.43
               Mean episode length: 149.50
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 9.50s
                        Total time: 22866.47s
                               ETA: 1008093.6s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.703s, learning 0.177s)
               Value function loss: 172.2422
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 586.63
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 9.88s
                        Total time: 22876.34s
                               ETA: 1008064.3s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.640s, learning 0.167s)
               Value function loss: 158.3820
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 604.93
               Mean episode length: 149.19
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 9.81s
                        Total time: 22886.15s
                               ETA: 1008031.9s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.640s, learning 0.175s)
               Value function loss: 123.5552
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 602.31
               Mean episode length: 149.93
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 9.82s
                        Total time: 22895.97s
                               ETA: 1007999.8s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.677s, learning 0.162s)
               Value function loss: 157.5140
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 598.97
               Mean episode length: 148.91
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 9.84s
                        Total time: 22905.81s
                               ETA: 1007968.8s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1761 steps/s (collection: 9.129s, learning 0.172s)
               Value function loss: 140.4953
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 576.43
               Mean episode length: 148.98
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 9.30s
                        Total time: 22915.11s
                               ETA: 1007914.2s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.586s, learning 0.180s)
               Value function loss: 142.7972
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 603.94
               Mean episode length: 149.38
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 9.77s
                        Total time: 22924.87s
                               ETA: 1007880.1s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.057s, learning 0.171s)
               Value function loss: 157.8303
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 585.53
               Mean episode length: 149.65
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 9.23s
                        Total time: 22934.10s
                               ETA: 1007822.3s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.529s, learning 0.161s)
               Value function loss: 150.8492
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 588.27
               Mean episode length: 149.40
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 9.69s
                        Total time: 22943.79s
                               ETA: 1007784.8s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.209s, learning 0.175s)
               Value function loss: 164.7540
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 596.46
               Mean episode length: 148.09
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 9.38s
                        Total time: 22953.17s
                               ETA: 1007734.0s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.807s, learning 0.181s)
               Value function loss: 127.5919
                    Surrogate loss: -0.0227
             Mean action noise std: 0.74
                       Mean reward: 570.87
               Mean episode length: 149.74
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 9.99s
                        Total time: 22963.16s
                               ETA: 1007709.7s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.313s, learning 0.173s)
               Value function loss: 99.6716
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 575.67
               Mean episode length: 149.31
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 9.49s
                        Total time: 22972.65s
                               ETA: 1007663.4s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.856s, learning 0.161s)
               Value function loss: 130.0360
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 585.21
               Mean episode length: 148.10
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 10.02s
                        Total time: 22982.67s
                               ETA: 1007640.4s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1764 steps/s (collection: 9.111s, learning 0.176s)
               Value function loss: 125.0876
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 594.10
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 9.29s
                        Total time: 22991.95s
                               ETA: 1007585.5s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.700s, learning 0.169s)
               Value function loss: 107.3908
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 580.09
               Mean episode length: 149.12
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 9.87s
                        Total time: 23001.82s
                               ETA: 1007556.0s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.787s, learning 0.178s)
               Value function loss: 108.9071
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 564.92
               Mean episode length: 149.68
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 11.97s
                        Total time: 23013.79s
                               ETA: 1007618.4s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.502s, learning 0.196s)
               Value function loss: 114.5487
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 572.13
               Mean episode length: 149.55
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 18.70s
                        Total time: 23032.48s
                               ETA: 1007975.3s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 893 steps/s (collection: 18.169s, learning 0.178s)
               Value function loss: 107.0764
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 544.39
               Mean episode length: 148.82
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 18.35s
                        Total time: 23050.83s
                               ETA: 1008316.6s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 873 steps/s (collection: 18.577s, learning 0.170s)
               Value function loss: 128.2098
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 556.42
               Mean episode length: 148.93
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 18.75s
                        Total time: 23069.58s
                               ETA: 1008675.0s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 890 steps/s (collection: 18.215s, learning 0.185s)
               Value function loss: 110.1180
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 592.41
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 18.40s
                        Total time: 23087.98s
                               ETA: 1009017.9s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 875 steps/s (collection: 18.556s, learning 0.165s)
               Value function loss: 120.7608
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 566.41
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 18.72s
                        Total time: 23106.70s
                               ETA: 1009374.6s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 901 steps/s (collection: 17.999s, learning 0.170s)
               Value function loss: 127.5096
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 569.67
               Mean episode length: 148.82
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 18.17s
                        Total time: 23124.87s
                               ETA: 1009706.7s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 880 steps/s (collection: 18.439s, learning 0.163s)
               Value function loss: 113.8729
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 597.77
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 18.60s
                        Total time: 23143.47s
                               ETA: 1010057.5s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 862 steps/s (collection: 18.822s, learning 0.168s)
               Value function loss: 119.1299
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 543.76
               Mean episode length: 148.14
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 18.99s
                        Total time: 23162.46s
                               ETA: 1010424.9s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 884 steps/s (collection: 18.363s, learning 0.162s)
               Value function loss: 127.1753
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 576.13
               Mean episode length: 149.38
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 18.53s
                        Total time: 23180.99s
                               ETA: 1010771.6s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 885 steps/s (collection: 18.325s, learning 0.171s)
               Value function loss: 118.9650
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 587.61
               Mean episode length: 149.67
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 18.50s
                        Total time: 23199.48s
                               ETA: 1011116.8s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.579s, learning 0.162s)
               Value function loss: 151.8024
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 590.02
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 18.74s
                        Total time: 23218.22s
                               ETA: 1011472.3s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 873 steps/s (collection: 18.597s, learning 0.170s)
               Value function loss: 144.0073
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 606.12
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 18.77s
                        Total time: 23236.99s
                               ETA: 1011828.6s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 896 steps/s (collection: 18.098s, learning 0.171s)
               Value function loss: 132.3845
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 562.73
               Mean episode length: 149.85
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 18.27s
                        Total time: 23255.26s
                               ETA: 1012162.9s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 859 steps/s (collection: 18.901s, learning 0.165s)
               Value function loss: 108.6661
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 582.44
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 19.07s
                        Total time: 23274.32s
                               ETA: 1012531.5s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 903 steps/s (collection: 17.962s, learning 0.163s)
               Value function loss: 96.7335
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 568.10
               Mean episode length: 148.87
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 18.12s
                        Total time: 23292.45s
                               ETA: 1012858.9s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 898 steps/s (collection: 18.075s, learning 0.166s)
               Value function loss: 94.1994
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 565.08
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 18.24s
                        Total time: 23310.69s
                               ETA: 1013191.0s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 881 steps/s (collection: 18.425s, learning 0.172s)
               Value function loss: 106.6175
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 584.43
               Mean episode length: 149.20
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 18.60s
                        Total time: 23329.29s
                               ETA: 1013538.3s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 872 steps/s (collection: 18.599s, learning 0.172s)
               Value function loss: 95.3240
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 575.17
               Mean episode length: 149.96
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 18.77s
                        Total time: 23348.06s
                               ETA: 1013892.8s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 857 steps/s (collection: 18.937s, learning 0.171s)
               Value function loss: 119.6337
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: 568.71
               Mean episode length: 149.04
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 19.11s
                        Total time: 23367.17s
                               ETA: 1014261.6s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 883 steps/s (collection: 18.372s, learning 0.165s)
               Value function loss: 127.4862
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 595.08
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 18.54s
                        Total time: 23385.70s
                               ETA: 1014605.3s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 892 steps/s (collection: 18.188s, learning 0.165s)
               Value function loss: 107.5517
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 585.72
               Mean episode length: 149.40
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 18.35s
                        Total time: 23404.06s
                               ETA: 1014940.7s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 873 steps/s (collection: 18.591s, learning 0.171s)
               Value function loss: 121.5337
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 579.98
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 18.76s
                        Total time: 23422.82s
                               ETA: 1015293.5s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.500s, learning 0.174s)
               Value function loss: 131.6043
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 598.65
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 18.67s
                        Total time: 23441.49s
                               ETA: 1015642.2s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 862 steps/s (collection: 18.842s, learning 0.162s)
               Value function loss: 130.5414
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 599.43
               Mean episode length: 149.79
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 19.00s
                        Total time: 23460.50s
                               ETA: 1016004.8s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.572s, learning 0.164s)
               Value function loss: 112.0147
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 568.16
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 18.74s
                        Total time: 23479.23s
                               ETA: 1016355.5s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 872 steps/s (collection: 18.605s, learning 0.167s)
               Value function loss: 118.0993
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 579.84
               Mean episode length: 148.66
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 18.77s
                        Total time: 23498.01s
                               ETA: 1016707.4s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 888 steps/s (collection: 18.262s, learning 0.175s)
               Value function loss: 110.9851
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 592.05
               Mean episode length: 149.25
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 18.44s
                        Total time: 23516.44s
                               ETA: 1017044.5s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 902 steps/s (collection: 17.985s, learning 0.161s)
               Value function loss: 111.5413
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 611.10
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 18.15s
                        Total time: 23534.59s
                               ETA: 1017368.7s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 884 steps/s (collection: 18.362s, learning 0.160s)
               Value function loss: 124.7397
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 578.22
               Mean episode length: 148.54
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 18.52s
                        Total time: 23553.11s
                               ETA: 1017708.8s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 875 steps/s (collection: 18.551s, learning 0.158s)
               Value function loss: 122.3307
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 584.71
               Mean episode length: 149.27
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 18.71s
                        Total time: 23571.82s
                               ETA: 1018056.7s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 899 steps/s (collection: 18.053s, learning 0.166s)
               Value function loss: 128.0191
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 578.49
               Mean episode length: 150.00
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 18.22s
                        Total time: 23590.04s
                               ETA: 1018383.2s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 871 steps/s (collection: 18.634s, learning 0.165s)
               Value function loss: 130.3976
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 600.49
               Mean episode length: 149.31
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 18.80s
                        Total time: 23608.84s
                               ETA: 1018734.3s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 914 steps/s (collection: 17.747s, learning 0.161s)
               Value function loss: 104.1690
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 597.88
               Mean episode length: 149.05
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 17.91s
                        Total time: 23626.75s
                               ETA: 1019046.8s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 868 steps/s (collection: 18.706s, learning 0.163s)
               Value function loss: 103.6326
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 610.34
               Mean episode length: 149.96
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 18.87s
                        Total time: 23645.61s
                               ETA: 1019400.3s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 885 steps/s (collection: 18.335s, learning 0.174s)
               Value function loss: 100.5840
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 581.14
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 18.51s
                        Total time: 23664.12s
                               ETA: 1019738.0s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 880 steps/s (collection: 18.436s, learning 0.167s)
               Value function loss: 96.3053
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 590.79
               Mean episode length: 149.83
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 18.60s
                        Total time: 23682.73s
                               ETA: 1020079.4s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 885 steps/s (collection: 18.325s, learning 0.168s)
               Value function loss: 110.0842
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 576.82
               Mean episode length: 149.34
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 18.49s
                        Total time: 23701.22s
                               ETA: 1020415.8s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.742s, learning 0.162s)
               Value function loss: 113.0009
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 572.81
               Mean episode length: 149.89
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 12.90s
                        Total time: 23714.12s
                               ETA: 1020511.3s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.250s, learning 0.165s)
               Value function loss: 101.3928
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 590.38
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 9.42s
                        Total time: 23723.54s
                               ETA: 1020456.7s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.497s, learning 0.160s)
               Value function loss: 101.8163
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 554.43
               Mean episode length: 149.36
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 9.66s
                        Total time: 23733.19s
                               ETA: 1020412.5s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.591s, learning 0.163s)
               Value function loss: 102.7067
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 547.60
               Mean episode length: 148.97
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 8.75s
                        Total time: 23741.95s
                               ETA: 1020329.6s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.405s, learning 0.165s)
               Value function loss: 119.5025
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 586.93
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 9.57s
                        Total time: 23751.52s
                               ETA: 1020281.8s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.061s, learning 0.165s)
               Value function loss: 118.3050
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 576.09
               Mean episode length: 150.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 9.23s
                        Total time: 23760.75s
                               ETA: 1020219.2s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.259s, learning 0.162s)
               Value function loss: 106.8883
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 571.13
               Mean episode length: 149.28
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 9.42s
                        Total time: 23770.17s
                               ETA: 1020165.0s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.406s, learning 0.162s)
               Value function loss: 132.8323
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 603.20
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 9.57s
                        Total time: 23779.74s
                               ETA: 1020117.2s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.581s, learning 0.164s)
               Value function loss: 120.8706
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 574.75
               Mean episode length: 150.00
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 9.75s
                        Total time: 23789.48s
                               ETA: 1020077.1s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.547s, learning 0.168s)
               Value function loss: 122.7667
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 584.99
               Mean episode length: 149.23
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 9.71s
                        Total time: 23799.20s
                               ETA: 1020035.6s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.395s, learning 0.160s)
               Value function loss: 160.0450
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 579.09
               Mean episode length: 148.89
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 9.56s
                        Total time: 23808.75s
                               ETA: 1019987.3s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.288s, learning 0.158s)
               Value function loss: 141.3306
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 602.13
               Mean episode length: 148.80
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 9.45s
                        Total time: 23818.20s
                               ETA: 1019934.4s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.586s, learning 0.165s)
               Value function loss: 155.0399
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 580.41
               Mean episode length: 148.78
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 9.75s
                        Total time: 23827.95s
                               ETA: 1019894.5s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.988s, learning 0.162s)
               Value function loss: 145.3037
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 568.24
               Mean episode length: 148.19
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 9.15s
                        Total time: 23837.10s
                               ETA: 1019829.0s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.403s, learning 0.175s)
               Value function loss: 120.3524
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 588.02
               Mean episode length: 148.59
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 9.58s
                        Total time: 23846.68s
                               ETA: 1019781.9s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.576s, learning 0.176s)
               Value function loss: 132.2395
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 582.75
               Mean episode length: 149.28
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 9.75s
                        Total time: 23856.43s
                               ETA: 1019742.2s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.425s, learning 0.193s)
               Value function loss: 135.7484
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 597.24
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 9.62s
                        Total time: 23866.04s
                               ETA: 1019696.8s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1756 steps/s (collection: 9.163s, learning 0.163s)
               Value function loss: 95.7404
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 593.79
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 9.33s
                        Total time: 23875.37s
                               ETA: 1019639.0s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.797s, learning 0.161s)
               Value function loss: 105.1584
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 598.03
               Mean episode length: 149.41
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 9.96s
                        Total time: 23885.33s
                               ETA: 1019608.2s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1618 steps/s (collection: 9.940s, learning 0.179s)
               Value function loss: 132.9293
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 588.94
               Mean episode length: 148.56
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 10.12s
                        Total time: 23895.45s
                               ETA: 1019584.3s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.678s, learning 0.169s)
               Value function loss: 124.6141
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 563.06
               Mean episode length: 149.01
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 9.85s
                        Total time: 23905.29s
                               ETA: 1019548.8s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.592s, learning 0.158s)
               Value function loss: 136.3895
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 583.42
               Mean episode length: 149.32
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 9.75s
                        Total time: 23915.04s
                               ETA: 1019509.2s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.463s, learning 0.177s)
               Value function loss: 132.8526
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 609.89
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 9.64s
                        Total time: 23924.68s
                               ETA: 1019464.9s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1625 steps/s (collection: 9.911s, learning 0.168s)
               Value function loss: 120.5180
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 544.90
               Mean episode length: 148.79
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 10.08s
                        Total time: 23934.76s
                               ETA: 1019439.3s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.554s, learning 0.169s)
               Value function loss: 119.7073
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 590.61
               Mean episode length: 149.62
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 9.72s
                        Total time: 23944.48s
                               ETA: 1019398.6s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.354s, learning 0.164s)
               Value function loss: 107.8998
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 560.11
               Mean episode length: 149.40
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 9.52s
                        Total time: 23954.00s
                               ETA: 1019349.2s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1756 steps/s (collection: 9.158s, learning 0.171s)
               Value function loss: 128.6079
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 577.94
               Mean episode length: 148.82
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 9.33s
                        Total time: 23963.33s
                               ETA: 1019291.8s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.219s, learning 0.165s)
               Value function loss: 127.2009
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 567.89
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 9.38s
                        Total time: 23972.72s
                               ETA: 1019236.8s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.400s, learning 0.168s)
               Value function loss: 125.2090
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 563.46
               Mean episode length: 149.91
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 9.57s
                        Total time: 23982.28s
                               ETA: 1019189.6s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.312s, learning 0.160s)
               Value function loss: 127.0669
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 583.08
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 9.47s
                        Total time: 23991.76s
                               ETA: 1019138.5s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.658s, learning 0.163s)
               Value function loss: 136.3569
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 600.05
               Mean episode length: 149.63
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 9.82s
                        Total time: 24001.58s
                               ETA: 1019102.1s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.590s, learning 0.161s)
               Value function loss: 136.6294
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 582.17
               Mean episode length: 149.73
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 9.75s
                        Total time: 24011.33s
                               ETA: 1019062.9s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.319s, learning 0.162s)
               Value function loss: 119.5551
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 585.37
               Mean episode length: 147.65
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 9.48s
                        Total time: 24020.81s
                               ETA: 1019012.1s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.237s, learning 0.162s)
               Value function loss: 134.5405
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 595.45
               Mean episode length: 149.86
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 9.40s
                        Total time: 24030.21s
                               ETA: 1018958.0s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.568s, learning 0.166s)
               Value function loss: 118.9053
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 579.08
               Mean episode length: 149.75
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 9.73s
                        Total time: 24039.94s
                               ETA: 1018918.1s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.427s, learning 0.159s)
               Value function loss: 135.9391
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 585.13
               Mean episode length: 149.47
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 9.59s
                        Total time: 24049.53s
                               ETA: 1018871.9s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.523s, learning 0.171s)
               Value function loss: 106.6266
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 580.33
               Mean episode length: 148.92
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 9.69s
                        Total time: 24059.22s
                               ETA: 1018830.3s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.553s, learning 0.164s)
               Value function loss: 115.9443
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 556.11
               Mean episode length: 148.39
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 9.72s
                        Total time: 24068.94s
                               ETA: 1018789.8s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.251s, learning 0.158s)
               Value function loss: 124.0090
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 566.33
               Mean episode length: 149.01
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 9.41s
                        Total time: 24078.35s
                               ETA: 1018736.2s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.687s, learning 0.161s)
               Value function loss: 110.4004
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 568.28
               Mean episode length: 149.83
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 9.85s
                        Total time: 24088.20s
                               ETA: 1018701.3s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.759s, learning 0.164s)
               Value function loss: 141.7144
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 579.63
               Mean episode length: 149.73
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 9.92s
                        Total time: 24098.12s
                               ETA: 1018669.5s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.342s, learning 0.160s)
               Value function loss: 121.9012
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 600.34
               Mean episode length: 149.24
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 9.50s
                        Total time: 24107.62s
                               ETA: 1018620.0s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.577s, learning 0.171s)
               Value function loss: 141.2499
                    Surrogate loss: 0.0141
             Mean action noise std: 0.74
                       Mean reward: 586.70
               Mean episode length: 149.53
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 9.75s
                        Total time: 24117.37s
                               ETA: 1018580.9s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.293s, learning 0.158s)
               Value function loss: 113.2548
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 556.08
               Mean episode length: 149.61
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 9.45s
                        Total time: 24126.82s
                               ETA: 1018529.3s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.233s, learning 0.161s)
               Value function loss: 107.4540
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 574.11
               Mean episode length: 149.27
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 9.39s
                        Total time: 24136.21s
                               ETA: 1018475.3s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.792s, learning 0.166s)
               Value function loss: 108.4426
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 559.18
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 9.96s
                        Total time: 24146.17s
                               ETA: 1018445.1s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.397s, learning 0.161s)
               Value function loss: 125.6114
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 557.50
               Mean episode length: 148.46
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 9.56s
                        Total time: 24155.73s
                               ETA: 1018398.1s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.645s, learning 0.165s)
               Value function loss: 119.4558
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 578.51
               Mean episode length: 150.00
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 9.81s
                        Total time: 24165.54s
                               ETA: 1018361.7s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.448s, learning 0.163s)
               Value function loss: 126.2397
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 557.02
               Mean episode length: 148.53
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 9.61s
                        Total time: 24175.15s
                               ETA: 1018317.0s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.567s, learning 0.159s)
               Value function loss: 120.4297
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 588.34
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 9.73s
                        Total time: 24184.88s
                               ETA: 1018277.2s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.588s, learning 0.162s)
               Value function loss: 121.2668
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 540.51
               Mean episode length: 148.16
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 9.75s
                        Total time: 24194.63s
                               ETA: 1018238.3s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1762 steps/s (collection: 9.137s, learning 0.158s)
               Value function loss: 129.2908
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 576.21
               Mean episode length: 149.46
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 9.30s
                        Total time: 24203.92s
                               ETA: 1018180.4s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.297s, learning 0.164s)
               Value function loss: 125.1815
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 568.21
               Mean episode length: 148.75
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 9.46s
                        Total time: 24213.38s
                               ETA: 1018129.5s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.644s, learning 0.165s)
               Value function loss: 119.3527
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 574.04
               Mean episode length: 148.64
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 9.81s
                        Total time: 24223.19s
                               ETA: 1018093.2s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.292s, learning 0.161s)
               Value function loss: 110.1086
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 562.94
               Mean episode length: 149.45
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 9.45s
                        Total time: 24232.65s
                               ETA: 1018042.1s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.310s, learning 0.172s)
               Value function loss: 105.7575
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 561.12
               Mean episode length: 147.71
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 9.48s
                        Total time: 24242.13s
                               ETA: 1017992.2s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.678s, learning 0.164s)
               Value function loss: 109.4737
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 579.71
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 9.84s
                        Total time: 24251.97s
                               ETA: 1017957.4s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.541s, learning 0.159s)
               Value function loss: 118.1684
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 584.53
               Mean episode length: 149.66
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 9.70s
                        Total time: 24261.67s
                               ETA: 1017916.7s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.414s, learning 0.162s)
               Value function loss: 126.2968
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 545.36
               Mean episode length: 148.43
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 9.58s
                        Total time: 24271.25s
                               ETA: 1017870.9s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.477s, learning 0.164s)
               Value function loss: 128.5169
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 567.10
               Mean episode length: 149.43
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 9.64s
                        Total time: 24280.89s
                               ETA: 1017827.7s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.733s, learning 0.166s)
               Value function loss: 115.5940
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 579.67
               Mean episode length: 149.77
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 9.90s
                        Total time: 24290.79s
                               ETA: 1017795.5s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.639s, learning 0.160s)
               Value function loss: 150.8176
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 578.79
               Mean episode length: 147.10
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 9.80s
                        Total time: 24300.59s
                               ETA: 1017759.0s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.437s, learning 0.158s)
               Value function loss: 115.5077
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 562.91
               Mean episode length: 149.63
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 9.60s
                        Total time: 24310.18s
                               ETA: 1017714.0s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.587s, learning 0.158s)
               Value function loss: 112.9525
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 571.48
               Mean episode length: 149.39
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 9.74s
                        Total time: 24319.93s
                               ETA: 1017675.3s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.621s, learning 0.166s)
               Value function loss: 130.5611
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 589.19
               Mean episode length: 148.43
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 9.79s
                        Total time: 24329.71s
                               ETA: 1017638.5s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.686s, learning 0.161s)
               Value function loss: 138.3508
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 599.44
               Mean episode length: 149.03
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 9.85s
                        Total time: 24339.56s
                               ETA: 1017604.1s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.674s, learning 0.165s)
               Value function loss: 130.5550
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 590.40
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 9.84s
                        Total time: 24349.40s
                               ETA: 1017569.4s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.584s, learning 0.159s)
               Value function loss: 106.4197
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: 599.00
               Mean episode length: 149.37
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 9.74s
                        Total time: 24359.14s
                               ETA: 1017530.8s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.839s, learning 0.166s)
               Value function loss: 120.2761
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 569.58
               Mean episode length: 149.77
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 10.01s
                        Total time: 24369.15s
                               ETA: 1017503.1s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1613 steps/s (collection: 9.988s, learning 0.165s)
               Value function loss: 122.4252
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 544.91
               Mean episode length: 148.90
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 10.15s
                        Total time: 24379.30s
                               ETA: 1017481.6s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.653s, learning 0.161s)
               Value function loss: 116.0476
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 557.94
               Mean episode length: 148.73
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 9.81s
                        Total time: 24389.12s
                               ETA: 1017446.0s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.736s, learning 0.163s)
               Value function loss: 114.5307
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 587.49
               Mean episode length: 150.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 9.90s
                        Total time: 24399.01s
                               ETA: 1017413.9s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.664s, learning 0.164s)
               Value function loss: 103.1953
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 575.47
               Mean episode length: 150.00
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 9.83s
                        Total time: 24408.84s
                               ETA: 1017378.9s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.825s, learning 0.159s)
               Value function loss: 102.5849
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 550.77
               Mean episode length: 148.83
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 9.98s
                        Total time: 24418.83s
                               ETA: 1017350.4s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.335s, learning 0.160s)
               Value function loss: 113.0764
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 583.49
               Mean episode length: 149.37
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 9.49s
                        Total time: 24428.32s
                               ETA: 1017301.5s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.647s, learning 0.163s)
               Value function loss: 126.9236
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 556.74
               Mean episode length: 149.18
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 9.81s
                        Total time: 24438.13s
                               ETA: 1017265.8s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.258s, learning 0.159s)
               Value function loss: 103.6204
                    Surrogate loss: -0.0222
             Mean action noise std: 0.74
                       Mean reward: 551.26
               Mean episode length: 148.63
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 9.42s
                        Total time: 24447.55s
                               ETA: 1017213.8s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.558s, learning 0.165s)
               Value function loss: 148.0301
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 562.81
               Mean episode length: 148.83
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 9.72s
                        Total time: 24457.27s
                               ETA: 1017174.5s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.597s, learning 0.166s)
               Value function loss: 141.7888
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 599.16
               Mean episode length: 149.35
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 9.76s
                        Total time: 24467.03s
                               ETA: 1017137.0s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.207s, learning 0.168s)
               Value function loss: 160.0954
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 588.69
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 9.38s
                        Total time: 24476.41s
                               ETA: 1017083.3s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.571s, learning 0.161s)
               Value function loss: 154.1746
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 574.70
               Mean episode length: 147.40
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 9.73s
                        Total time: 24486.14s
                               ETA: 1017044.5s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.552s, learning 0.163s)
               Value function loss: 134.6175
                    Surrogate loss: -0.0211
             Mean action noise std: 0.74
                       Mean reward: 601.47
               Mean episode length: 149.62
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 9.71s
                        Total time: 24495.86s
                               ETA: 1017005.0s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.809s, learning 0.166s)
               Value function loss: 159.0962
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 602.42
               Mean episode length: 149.49
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 9.98s
                        Total time: 24505.83s
                               ETA: 1016976.4s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.492s, learning 0.159s)
               Value function loss: 149.0236
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 605.65
               Mean episode length: 150.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 9.65s
                        Total time: 24515.48s
                               ETA: 1016934.2s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.418s, learning 0.161s)
               Value function loss: 133.4977
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 615.10
               Mean episode length: 149.52
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 9.58s
                        Total time: 24525.06s
                               ETA: 1016889.2s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.481s, learning 0.164s)
               Value function loss: 160.8580
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 598.18
               Mean episode length: 149.90
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 9.64s
                        Total time: 24534.70s
                               ETA: 1016846.9s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.489s, learning 0.163s)
               Value function loss: 158.8288
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 620.32
               Mean episode length: 149.95
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 9.65s
                        Total time: 24544.36s
                               ETA: 1016804.9s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.445s, learning 0.157s)
               Value function loss: 158.0410
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 606.92
               Mean episode length: 149.30
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 9.60s
                        Total time: 24553.96s
                               ETA: 1016760.9s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.658s, learning 0.160s)
               Value function loss: 146.0399
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 615.25
               Mean episode length: 149.91
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 9.82s
                        Total time: 24563.78s
                               ETA: 1016725.8s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.844s, learning 0.162s)
               Value function loss: 132.7714
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 575.11
               Mean episode length: 148.87
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 9.01s
                        Total time: 24572.78s
                               ETA: 1016657.2s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1612 steps/s (collection: 9.998s, learning 0.161s)
               Value function loss: 144.1006
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 608.55
               Mean episode length: 149.15
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 10.16s
                        Total time: 24582.94s
                               ETA: 1016636.4s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.118s, learning 0.175s)
               Value function loss: 144.7381
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 604.39
               Mean episode length: 149.22
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 9.29s
                        Total time: 24592.24s
                               ETA: 1016579.7s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.578s, learning 0.162s)
               Value function loss: 118.1744
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 602.34
               Mean episode length: 148.47
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 9.74s
                        Total time: 24601.98s
                               ETA: 1016541.5s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.530s, learning 0.165s)
               Value function loss: 129.4719
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 583.59
               Mean episode length: 145.85
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 9.69s
                        Total time: 24611.67s
                               ETA: 1016501.5s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.328s, learning 0.161s)
               Value function loss: 140.6618
                    Surrogate loss: -0.0215
             Mean action noise std: 0.74
                       Mean reward: 599.28
               Mean episode length: 149.26
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 9.49s
                        Total time: 24621.16s
                               ETA: 1016453.1s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.535s, learning 0.166s)
               Value function loss: 133.9094
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 602.63
               Mean episode length: 149.83
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 9.70s
                        Total time: 24630.86s
                               ETA: 1016413.4s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.869s, learning 0.162s)
               Value function loss: 144.8658
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 596.10
               Mean episode length: 148.97
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 10.03s
                        Total time: 24640.89s
                               ETA: 1016387.4s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.447s, learning 0.169s)
               Value function loss: 140.8272
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 621.93
               Mean episode length: 148.97
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 9.62s
                        Total time: 24650.51s
                               ETA: 1016344.2s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.379s, learning 0.162s)
               Value function loss: 156.3694
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 606.00
               Mean episode length: 148.27
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 9.54s
                        Total time: 24660.05s
                               ETA: 1016298.0s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.374s, learning 0.167s)
               Value function loss: 134.6665
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 616.16
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 9.54s
                        Total time: 24669.59s
                               ETA: 1016251.8s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.585s, learning 0.161s)
               Value function loss: 117.8013
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 616.89
               Mean episode length: 148.38
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 9.75s
                        Total time: 24679.33s
                               ETA: 1016214.0s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.818s, learning 0.161s)
               Value function loss: 139.1157
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 592.69
               Mean episode length: 149.11
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 9.98s
                        Total time: 24689.31s
                               ETA: 1016185.9s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.437s, learning 0.160s)
               Value function loss: 120.3201
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 578.24
               Mean episode length: 149.54
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 9.60s
                        Total time: 24698.91s
                               ETA: 1016142.1s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.536s, learning 0.162s)
               Value function loss: 127.9562
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 584.05
               Mean episode length: 148.71
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 9.70s
                        Total time: 24708.61s
                               ETA: 1016102.5s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.649s, learning 0.160s)
               Value function loss: 132.1987
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 591.54
               Mean episode length: 148.91
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 9.81s
                        Total time: 24718.42s
                               ETA: 1016067.5s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.737s, learning 0.173s)
               Value function loss: 128.3695
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 600.48
               Mean episode length: 149.72
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 9.91s
                        Total time: 24728.33s
                               ETA: 1016036.7s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.536s, learning 0.164s)
               Value function loss: 131.5607
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 595.17
               Mean episode length: 149.07
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 9.70s
                        Total time: 24738.03s
                               ETA: 1015997.2s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1621 steps/s (collection: 9.947s, learning 0.159s)
               Value function loss: 135.5157
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 596.78
               Mean episode length: 149.13
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 10.11s
                        Total time: 24748.13s
                               ETA: 1015974.4s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1776 steps/s (collection: 9.063s, learning 0.159s)
               Value function loss: 112.1915
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 590.39
               Mean episode length: 149.85
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 9.22s
                        Total time: 24757.36s
                               ETA: 1015915.4s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.821s, learning 0.161s)
               Value function loss: 125.9986
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 596.68
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 9.98s
                        Total time: 24767.34s
                               ETA: 1015887.6s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.357s, learning 0.171s)
               Value function loss: 125.4685
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 598.54
               Mean episode length: 150.00
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 9.53s
                        Total time: 24776.87s
                               ETA: 1015841.2s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.366s, learning 0.165s)
               Value function loss: 102.7163
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 586.45
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 9.53s
                        Total time: 24786.40s
                               ETA: 1015794.9s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.638s, learning 0.163s)
               Value function loss: 121.0712
                    Surrogate loss: -0.0014
             Mean action noise std: 0.74
                       Mean reward: 567.98
               Mean episode length: 148.82
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 9.80s
                        Total time: 24796.20s
                               ETA: 1015759.7s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.614s, learning 0.180s)
               Value function loss: 115.6999
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 591.26
               Mean episode length: 148.44
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 9.79s
                        Total time: 24805.99s
                               ETA: 1015724.2s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.520s, learning 0.173s)
               Value function loss: 112.4439
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 582.84
               Mean episode length: 149.13
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 9.69s
                        Total time: 24815.68s
                               ETA: 1015684.7s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.564s, learning 0.164s)
               Value function loss: 138.5523
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 591.55
               Mean episode length: 149.81
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 9.73s
                        Total time: 24825.41s
                               ETA: 1015646.6s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.749s, learning 0.158s)
               Value function loss: 110.9819
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 572.86
               Mean episode length: 149.44
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 9.91s
                        Total time: 24835.32s
                               ETA: 1015615.8s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.451s, learning 0.159s)
               Value function loss: 147.2308
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 595.41
               Mean episode length: 150.00
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 9.61s
                        Total time: 24844.93s
                               ETA: 1015572.9s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.559s, learning 0.159s)
               Value function loss: 130.3341
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 584.03
               Mean episode length: 149.57
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 9.72s
                        Total time: 24854.65s
                               ETA: 1015534.5s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1776 steps/s (collection: 9.063s, learning 0.162s)
               Value function loss: 114.6324
                    Surrogate loss: -0.0192
             Mean action noise std: 0.74
                       Mean reward: 582.30
               Mean episode length: 149.07
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 9.22s
                        Total time: 24863.87s
                               ETA: 1015475.9s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.552s, learning 0.158s)
               Value function loss: 143.7402
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 565.91
               Mean episode length: 148.88
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 9.71s
                        Total time: 24873.58s
                               ETA: 1015437.2s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.322s, learning 0.161s)
               Value function loss: 154.4735
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 575.77
               Mean episode length: 149.27
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 9.48s
                        Total time: 24883.07s
                               ETA: 1015389.3s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.460s, learning 0.158s)
               Value function loss: 150.6707
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: 571.22
               Mean episode length: 149.04
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 9.62s
                        Total time: 24892.68s
                               ETA: 1015346.9s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.492s, learning 0.158s)
               Value function loss: 147.4606
                    Surrogate loss: 0.0061
             Mean action noise std: 0.74
                       Mean reward: 586.47
               Mean episode length: 149.81
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 9.65s
                        Total time: 24902.33s
                               ETA: 1015305.8s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.348s, learning 0.159s)
               Value function loss: 143.9534
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 577.76
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 9.51s
                        Total time: 24911.84s
                               ETA: 1015259.0s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.782s, learning 0.158s)
               Value function loss: 161.0384
                    Surrogate loss: 0.0057
             Mean action noise std: 0.74
                       Mean reward: 571.45
               Mean episode length: 149.31
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 9.94s
                        Total time: 24921.78s
                               ETA: 1015229.7s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1620 steps/s (collection: 9.953s, learning 0.161s)
               Value function loss: 130.0951
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 588.31
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 10.11s
                        Total time: 24931.89s
                               ETA: 1015207.6s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.591s, learning 0.161s)
               Value function loss: 129.1952
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 565.18
               Mean episode length: 149.50
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 9.75s
                        Total time: 24941.65s
                               ETA: 1015170.8s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.876s, learning 0.160s)
               Value function loss: 128.4411
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 600.82
               Mean episode length: 148.99
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 10.04s
                        Total time: 24951.68s
                               ETA: 1015145.5s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.231s, learning 0.160s)
               Value function loss: 125.3786
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 586.54
               Mean episode length: 149.77
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 9.39s
                        Total time: 24961.07s
                               ETA: 1015094.1s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.369s, learning 0.160s)
               Value function loss: 116.3539
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 576.59
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 9.53s
                        Total time: 24970.60s
                               ETA: 1015048.2s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.694s, learning 0.160s)
               Value function loss: 127.6698
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 559.25
               Mean episode length: 149.32
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 9.85s
                        Total time: 24980.46s
                               ETA: 1015015.6s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.603s, learning 0.161s)
               Value function loss: 141.4504
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 553.57
               Mean episode length: 148.18
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 9.76s
                        Total time: 24990.22s
                               ETA: 1014979.4s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.504s, learning 0.159s)
               Value function loss: 139.7385
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 568.42
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 9.66s
                        Total time: 24999.88s
                               ETA: 1014939.1s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.594s, learning 0.160s)
               Value function loss: 153.4441
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 557.05
               Mean episode length: 149.39
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 9.75s
                        Total time: 25009.64s
                               ETA: 1014902.5s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1627 steps/s (collection: 9.896s, learning 0.173s)
               Value function loss: 152.9002
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 563.72
               Mean episode length: 149.41
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 10.07s
                        Total time: 25019.71s
                               ETA: 1014878.7s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.956s, learning 0.160s)
               Value function loss: 150.8855
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 558.49
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 9.12s
                        Total time: 25028.82s
                               ETA: 1014816.3s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.436s, learning 0.159s)
               Value function loss: 131.4940
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 549.78
               Mean episode length: 147.77
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 9.60s
                        Total time: 25038.42s
                               ETA: 1014773.4s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.359s, learning 0.169s)
               Value function loss: 147.2821
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 552.51
               Mean episode length: 148.52
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 9.53s
                        Total time: 25047.95s
                               ETA: 1014727.7s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.794s, learning 0.177s)
               Value function loss: 152.5265
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 564.16
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 9.97s
                        Total time: 25057.92s
                               ETA: 1014700.1s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.089s, learning 0.177s)
               Value function loss: 144.1242
                    Surrogate loss: 0.0016
             Mean action noise std: 0.74
                       Mean reward: 541.72
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 9.27s
                        Total time: 25067.18s
                               ETA: 1014643.9s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.659s, learning 0.172s)
               Value function loss: 167.9747
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 557.91
               Mean episode length: 149.35
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 9.83s
                        Total time: 25077.01s
                               ETA: 1014610.5s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1780 steps/s (collection: 9.028s, learning 0.176s)
               Value function loss: 147.5485
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 556.63
               Mean episode length: 149.91
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 9.20s
                        Total time: 25086.22s
                               ETA: 1014551.9s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.323s, learning 0.178s)
               Value function loss: 182.0734
                    Surrogate loss: 0.0029
             Mean action noise std: 0.74
                       Mean reward: 573.79
               Mean episode length: 150.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 9.50s
                        Total time: 25095.72s
                               ETA: 1014505.3s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.505s, learning 0.168s)
               Value function loss: 175.1047
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 523.67
               Mean episode length: 148.39
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 9.67s
                        Total time: 25105.39s
                               ETA: 1014465.7s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.291s, learning 0.171s)
               Value function loss: 152.9808
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: 551.31
               Mean episode length: 149.33
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 9.46s
                        Total time: 25114.85s
                               ETA: 1014417.6s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.326s, learning 0.178s)
               Value function loss: 151.1216
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 535.85
               Mean episode length: 149.24
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 9.50s
                        Total time: 25124.36s
                               ETA: 1014371.2s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.369s, learning 0.179s)
               Value function loss: 145.1331
                    Surrogate loss: 0.0015
             Mean action noise std: 0.74
                       Mean reward: 544.19
               Mean episode length: 148.86
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 9.55s
                        Total time: 25133.90s
                               ETA: 1014326.6s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.387s, learning 0.167s)
               Value function loss: 134.1425
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 550.12
               Mean episode length: 150.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 9.55s
                        Total time: 25143.46s
                               ETA: 1014282.3s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.623s, learning 0.160s)
               Value function loss: 147.8824
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 526.19
               Mean episode length: 149.97
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 9.78s
                        Total time: 25153.24s
                               ETA: 1014247.3s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.476s, learning 0.163s)
               Value function loss: 154.8020
                    Surrogate loss: 0.0035
             Mean action noise std: 0.74
                       Mean reward: 555.41
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 9.64s
                        Total time: 25162.88s
                               ETA: 1014206.4s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.458s, learning 0.171s)
               Value function loss: 122.6830
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 535.02
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 9.63s
                        Total time: 25172.51s
                               ETA: 1014165.2s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.309s, learning 0.166s)
               Value function loss: 153.3470
                    Surrogate loss: -0.0014
             Mean action noise std: 0.74
                       Mean reward: 542.82
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 9.48s
                        Total time: 25181.98s
                               ETA: 1014117.9s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.644s, learning 0.163s)
               Value function loss: 116.9656
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 536.60
               Mean episode length: 150.00
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 9.81s
                        Total time: 25191.79s
                               ETA: 1014083.9s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.690s, learning 0.164s)
               Value function loss: 124.6912
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 550.50
               Mean episode length: 150.00
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 9.85s
                        Total time: 25201.65s
                               ETA: 1014051.9s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.746s, learning 0.169s)
               Value function loss: 123.6656
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 526.27
               Mean episode length: 150.00
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 9.91s
                        Total time: 25211.56s
                               ETA: 1014022.3s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.210s, learning 0.161s)
               Value function loss: 92.8627
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 548.84
               Mean episode length: 149.44
                  Mean reward/step: 3.58
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 9.37s
                        Total time: 25220.93s
                               ETA: 1013970.8s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.442s, learning 0.161s)
               Value function loss: 118.0935
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 513.77
               Mean episode length: 150.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 9.60s
                        Total time: 25230.53s
                               ETA: 1013928.7s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.568s, learning 0.161s)
               Value function loss: 122.8057
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 528.31
               Mean episode length: 148.92
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 9.73s
                        Total time: 25240.26s
                               ETA: 1013891.7s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.715s, learning 0.157s)
               Value function loss: 113.8351
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 545.79
               Mean episode length: 150.00
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 9.87s
                        Total time: 25250.13s
                               ETA: 1013860.5s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.813s, learning 0.170s)
               Value function loss: 123.2543
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 551.65
               Mean episode length: 150.00
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 9.98s
                        Total time: 25260.12s
                               ETA: 1013833.7s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.801s, learning 0.177s)
               Value function loss: 115.6948
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 522.53
               Mean episode length: 148.22
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 9.98s
                        Total time: 25270.10s
                               ETA: 1013806.7s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.498s, learning 0.165s)
               Value function loss: 121.2017
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 545.05
               Mean episode length: 149.64
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 9.66s
                        Total time: 25279.76s
                               ETA: 1013767.2s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.455s, learning 0.162s)
               Value function loss: 112.3090
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 541.89
               Mean episode length: 149.23
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 9.62s
                        Total time: 25289.38s
                               ETA: 1013725.8s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.288s, learning 0.165s)
               Value function loss: 107.0639
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 517.89
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 9.45s
                        Total time: 25298.83s
                               ETA: 1013677.8s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.392s, learning 0.162s)
               Value function loss: 97.8595
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 546.85
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 9.55s
                        Total time: 25308.38s
                               ETA: 1013634.0s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.972s, learning 0.160s)
               Value function loss: 99.4392
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 558.37
               Mean episode length: 148.95
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 9.13s
                        Total time: 25317.52s
                               ETA: 1013573.3s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.700s, learning 0.170s)
               Value function loss: 83.8394
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 553.72
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 9.87s
                        Total time: 25327.39s
                               ETA: 1013542.1s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.491s, learning 0.165s)
               Value function loss: 99.7390
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 559.98
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 9.66s
                        Total time: 25337.04s
                               ETA: 1013502.4s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.555s, learning 0.159s)
               Value function loss: 108.8512
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 551.62
               Mean episode length: 148.92
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 9.71s
                        Total time: 25346.76s
                               ETA: 1013465.1s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.481s, learning 0.172s)
               Value function loss: 105.7741
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 539.25
               Mean episode length: 148.98
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 9.65s
                        Total time: 25356.41s
                               ETA: 1013425.3s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.318s, learning 0.166s)
               Value function loss: 120.6610
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 564.80
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 9.48s
                        Total time: 25365.89s
                               ETA: 1013378.8s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.600s, learning 0.167s)
               Value function loss: 110.4214
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 569.48
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 9.77s
                        Total time: 25375.66s
                               ETA: 1013343.7s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.363s, learning 0.169s)
               Value function loss: 121.9720
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 556.06
               Mean episode length: 149.21
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 9.53s
                        Total time: 25385.19s
                               ETA: 1013299.2s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.863s, learning 0.175s)
               Value function loss: 125.2025
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 570.59
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 10.04s
                        Total time: 25395.23s
                               ETA: 1013274.9s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.465s, learning 0.176s)
               Value function loss: 109.3106
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 570.64
               Mean episode length: 149.33
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 9.64s
                        Total time: 25404.87s
                               ETA: 1013234.8s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.804s, learning 0.169s)
               Value function loss: 138.0091
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 559.43
               Mean episode length: 149.04
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 9.97s
                        Total time: 25414.84s
                               ETA: 1013207.9s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1761 steps/s (collection: 9.128s, learning 0.172s)
               Value function loss: 109.5903
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 572.61
               Mean episode length: 148.97
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 9.30s
                        Total time: 25424.14s
                               ETA: 1013154.2s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.482s, learning 0.173s)
               Value function loss: 109.8355
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 557.60
               Mean episode length: 147.39
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 9.66s
                        Total time: 25433.80s
                               ETA: 1013114.7s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1773 steps/s (collection: 9.064s, learning 0.173s)
               Value function loss: 110.6688
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 590.02
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 9.24s
                        Total time: 25443.04s
                               ETA: 1013058.6s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.698s, learning 0.179s)
               Value function loss: 119.1998
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 578.57
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 9.88s
                        Total time: 25452.91s
                               ETA: 1013028.0s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.162s, learning 0.183s)
               Value function loss: 110.3197
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 566.19
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 9.35s
                        Total time: 25462.26s
                               ETA: 1012976.3s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.211s, learning 0.170s)
               Value function loss: 112.2920
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 575.49
               Mean episode length: 149.23
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 9.38s
                        Total time: 25471.64s
                               ETA: 1012926.0s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.035s, learning 0.170s)
               Value function loss: 99.9219
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 562.31
               Mean episode length: 148.30
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 9.21s
                        Total time: 25480.85s
                               ETA: 1012868.8s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.454s, learning 0.184s)
               Value function loss: 95.5513
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 551.75
               Mean episode length: 149.81
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 9.64s
                        Total time: 25490.48s
                               ETA: 1012828.8s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.364s, learning 0.163s)
               Value function loss: 102.8909
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 587.60
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 9.53s
                        Total time: 25500.01s
                               ETA: 1012784.4s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.332s, learning 0.179s)
               Value function loss: 90.9534
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 558.65
               Mean episode length: 149.33
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 9.51s
                        Total time: 25509.52s
                               ETA: 1012739.4s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.505s, learning 0.183s)
               Value function loss: 113.0626
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 567.88
               Mean episode length: 148.91
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 9.69s
                        Total time: 25519.21s
                               ETA: 1012701.5s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.335s, learning 0.168s)
               Value function loss: 101.8181
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 565.91
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 9.50s
                        Total time: 25528.71s
                               ETA: 1012656.2s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.435s, learning 0.180s)
               Value function loss: 91.4400
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 542.38
               Mean episode length: 149.14
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 9.61s
                        Total time: 25538.33s
                               ETA: 1012615.4s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.328s, learning 0.179s)
               Value function loss: 109.3053
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 560.98
               Mean episode length: 149.30
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 9.51s
                        Total time: 25547.83s
                               ETA: 1012570.4s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.558s, learning 0.174s)
               Value function loss: 104.7042
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 578.70
               Mean episode length: 150.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 9.73s
                        Total time: 25557.57s
                               ETA: 1012534.3s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.297s, learning 0.170s)
               Value function loss: 110.1995
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 553.70
               Mean episode length: 149.02
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 9.47s
                        Total time: 25567.03s
                               ETA: 1012487.7s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.316s, learning 0.166s)
               Value function loss: 99.9209
                    Surrogate loss: -0.0001
             Mean action noise std: 0.74
                       Mean reward: 566.29
               Mean episode length: 150.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 9.48s
                        Total time: 25576.52s
                               ETA: 1012441.8s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.483s, learning 0.173s)
               Value function loss: 85.2689
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 557.82
               Mean episode length: 149.60
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 9.66s
                        Total time: 25586.17s
                               ETA: 1012402.8s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.632s, learning 0.163s)
               Value function loss: 98.5030
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 563.97
               Mean episode length: 149.36
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 9.79s
                        Total time: 25595.97s
                               ETA: 1012369.3s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.519s, learning 0.161s)
               Value function loss: 94.6339
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 542.97
               Mean episode length: 149.26
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 9.68s
                        Total time: 25605.65s
                               ETA: 1012331.2s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.348s, learning 0.159s)
               Value function loss: 97.8331
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 550.21
               Mean episode length: 150.00
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 9.51s
                        Total time: 25615.15s
                               ETA: 1012286.4s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.363s, learning 0.165s)
               Value function loss: 97.2852
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 553.54
               Mean episode length: 149.72
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 9.53s
                        Total time: 25624.68s
                               ETA: 1012242.4s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.129s, learning 0.163s)
               Value function loss: 97.2414
                    Surrogate loss: -0.0206
             Mean action noise std: 0.74
                       Mean reward: 516.60
               Mean episode length: 148.43
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 9.29s
                        Total time: 25633.97s
                               ETA: 1012189.1s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.257s, learning 0.159s)
               Value function loss: 104.4408
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 549.20
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 9.42s
                        Total time: 25643.39s
                               ETA: 1012140.8s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.650s, learning 0.164s)
               Value function loss: 91.8080
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 525.36
               Mean episode length: 150.00
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 9.81s
                        Total time: 25653.20s
                               ETA: 1012108.1s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.276s, learning 0.166s)
               Value function loss: 79.7750
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 519.26
               Mean episode length: 148.91
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 9.44s
                        Total time: 25662.65s
                               ETA: 1012060.9s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.458s, learning 0.165s)
               Value function loss: 79.6409
                    Surrogate loss: 0.0041
             Mean action noise std: 0.74
                       Mean reward: 548.90
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 9.62s
                        Total time: 25672.27s
                               ETA: 1012020.8s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.763s, learning 0.163s)
               Value function loss: 83.1875
                    Surrogate loss: -0.0176
             Mean action noise std: 0.74
                       Mean reward: 537.35
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 9.93s
                        Total time: 25682.20s
                               ETA: 1011992.6s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.383s, learning 0.168s)
               Value function loss: 77.2558
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 514.61
               Mean episode length: 149.80
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 9.55s
                        Total time: 25691.75s
                               ETA: 1011949.7s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.253s, learning 0.174s)
               Value function loss: 98.0889
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 542.31
               Mean episode length: 149.05
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 9.43s
                        Total time: 25701.17s
                               ETA: 1011902.0s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.284s, learning 0.167s)
               Value function loss: 104.0686
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 557.97
               Mean episode length: 149.04
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 9.45s
                        Total time: 25710.62s
                               ETA: 1011855.2s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.748s, learning 0.164s)
               Value function loss: 98.2732
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 571.95
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 9.91s
                        Total time: 25720.54s
                               ETA: 1011826.6s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.484s, learning 0.168s)
               Value function loss: 104.7083
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 577.29
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 9.65s
                        Total time: 25730.19s
                               ETA: 1011787.8s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.421s, learning 0.163s)
               Value function loss: 107.7188
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 562.86
               Mean episode length: 149.24
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 9.58s
                        Total time: 25739.77s
                               ETA: 1011746.3s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.369s, learning 0.161s)
               Value function loss: 106.9203
                    Surrogate loss: -0.0196
             Mean action noise std: 0.74
                       Mean reward: 558.38
               Mean episode length: 148.26
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 9.53s
                        Total time: 25749.30s
                               ETA: 1011702.7s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.328s, learning 0.171s)
               Value function loss: 95.7980
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 587.70
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 9.50s
                        Total time: 25758.80s
                               ETA: 1011657.9s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.429s, learning 0.167s)
               Value function loss: 95.9378
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 579.17
               Mean episode length: 150.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 9.60s
                        Total time: 25768.40s
                               ETA: 1011617.0s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.487s, learning 0.165s)
               Value function loss: 99.0200
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 558.97
               Mean episode length: 148.91
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 9.65s
                        Total time: 25778.05s
                               ETA: 1011578.3s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.332s, learning 0.166s)
               Value function loss: 98.7403
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 571.33
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 9.50s
                        Total time: 25787.55s
                               ETA: 1011533.6s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.330s, learning 0.159s)
               Value function loss: 101.7652
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 538.86
               Mean episode length: 148.24
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 9.49s
                        Total time: 25797.03s
                               ETA: 1011488.5s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.388s, learning 0.162s)
               Value function loss: 96.0476
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 565.04
               Mean episode length: 148.96
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 9.55s
                        Total time: 25806.58s
                               ETA: 1011445.9s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.831s, learning 0.159s)
               Value function loss: 105.2800
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 571.26
               Mean episode length: 150.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 9.99s
                        Total time: 25816.57s
                               ETA: 1011420.5s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.162s)
               Value function loss: 98.0426
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 550.88
               Mean episode length: 149.86
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 9.70s
                        Total time: 25826.27s
                               ETA: 1011383.8s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.453s, learning 0.158s)
               Value function loss: 103.6803
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 554.77
               Mean episode length: 149.01
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 9.61s
                        Total time: 25835.88s
                               ETA: 1011343.7s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.447s, learning 0.159s)
               Value function loss: 100.2440
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 545.77
               Mean episode length: 148.43
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 9.61s
                        Total time: 25845.49s
                               ETA: 1011303.4s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.613s, learning 0.163s)
               Value function loss: 97.4329
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 564.64
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 9.78s
                        Total time: 25855.27s
                               ETA: 1011269.7s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.229s, learning 0.168s)
               Value function loss: 84.7531
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 583.46
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 9.40s
                        Total time: 25864.66s
                               ETA: 1011221.3s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.347s, learning 0.165s)
               Value function loss: 94.4405
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 576.33
               Mean episode length: 148.95
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 9.51s
                        Total time: 25874.18s
                               ETA: 1011177.3s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.816s, learning 0.158s)
               Value function loss: 90.9357
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 544.11
               Mean episode length: 150.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 9.97s
                        Total time: 25884.15s
                               ETA: 1011151.5s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.656s, learning 0.168s)
               Value function loss: 102.6793
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 554.73
               Mean episode length: 147.93
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 9.82s
                        Total time: 25893.97s
                               ETA: 1011119.8s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.747s, learning 0.162s)
               Value function loss: 123.2304
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 594.89
               Mean episode length: 149.12
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 9.91s
                        Total time: 25903.88s
                               ETA: 1011091.4s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.461s, learning 0.165s)
               Value function loss: 96.4629
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 551.50
               Mean episode length: 149.25
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 9.63s
                        Total time: 25913.51s
                               ETA: 1011052.0s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.233s, learning 0.170s)
               Value function loss: 115.3421
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 584.77
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 9.40s
                        Total time: 25922.91s
                               ETA: 1011003.9s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.462s, learning 0.164s)
               Value function loss: 107.8380
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 574.98
               Mean episode length: 149.13
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 9.63s
                        Total time: 25932.54s
                               ETA: 1010964.6s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.374s, learning 0.165s)
               Value function loss: 94.2218
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 572.79
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 9.54s
                        Total time: 25942.08s
                               ETA: 1010921.9s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.365s, learning 0.161s)
               Value function loss: 105.3875
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 580.88
               Mean episode length: 148.83
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 9.53s
                        Total time: 25951.60s
                               ETA: 1010878.7s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.437s, learning 0.179s)
               Value function loss: 110.3927
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 594.66
               Mean episode length: 148.94
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 9.62s
                        Total time: 25961.22s
                               ETA: 1010839.0s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.393s, learning 0.160s)
               Value function loss: 98.8386
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 588.53
               Mean episode length: 148.64
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 9.55s
                        Total time: 25970.77s
                               ETA: 1010797.0s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.383s, learning 0.187s)
               Value function loss: 106.3519
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 576.01
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 9.57s
                        Total time: 25980.34s
                               ETA: 1010755.6s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.665s, learning 0.184s)
               Value function loss: 102.6218
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 588.67
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 9.85s
                        Total time: 25990.19s
                               ETA: 1010725.1s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.394s, learning 0.169s)
               Value function loss: 95.6801
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 547.66
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 9.56s
                        Total time: 25999.75s
                               ETA: 1010683.4s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.549s, learning 0.176s)
               Value function loss: 108.7654
                    Surrogate loss: -0.0001
             Mean action noise std: 0.74
                       Mean reward: 599.94
               Mean episode length: 148.77
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 9.72s
                        Total time: 26009.48s
                               ETA: 1010648.1s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.350s, learning 0.173s)
               Value function loss: 93.4488
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 572.32
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 9.52s
                        Total time: 26019.00s
                               ETA: 1010605.0s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.274s, learning 0.174s)
               Value function loss: 101.2910
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 575.40
               Mean episode length: 148.78
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 9.45s
                        Total time: 26028.45s
                               ETA: 1010559.0s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.311s, learning 0.168s)
               Value function loss: 92.8252
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 566.12
               Mean episode length: 149.05
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 9.48s
                        Total time: 26037.93s
                               ETA: 1010514.2s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.520s, learning 0.165s)
               Value function loss: 81.4491
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: 554.13
               Mean episode length: 148.72
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 9.68s
                        Total time: 26047.61s
                               ETA: 1010477.4s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.715s, learning 0.158s)
               Value function loss: 93.8500
                    Surrogate loss: 0.0027
             Mean action noise std: 0.74
                       Mean reward: 583.73
               Mean episode length: 148.87
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 9.87s
                        Total time: 26057.49s
                               ETA: 1010448.0s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.718s, learning 0.163s)
               Value function loss: 104.5460
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 578.45
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 9.88s
                        Total time: 26067.37s
                               ETA: 1010418.8s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.369s, learning 0.165s)
               Value function loss: 97.5705
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 583.76
               Mean episode length: 149.34
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 9.53s
                        Total time: 26076.90s
                               ETA: 1010376.3s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.543s, learning 0.165s)
               Value function loss: 91.1460
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 569.64
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 9.71s
                        Total time: 26086.61s
                               ETA: 1010340.5s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.671s, learning 0.160s)
               Value function loss: 95.7892
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 556.81
               Mean episode length: 148.92
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 9.83s
                        Total time: 26096.44s
                               ETA: 1010309.5s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.733s, learning 0.163s)
               Value function loss: 1989.4667
                    Surrogate loss: 0.0050
             Mean action noise std: 0.74
                       Mean reward: 562.16
               Mean episode length: 148.75
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 9.90s
                        Total time: 26106.34s
                               ETA: 1010281.0s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.529s, learning 0.159s)
               Value function loss: 104.8409
                    Surrogate loss: -0.0020
             Mean action noise std: 0.74
                       Mean reward: 580.87
               Mean episode length: 147.54
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 9.69s
                        Total time: 26116.02s
                               ETA: 1010244.5s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.577s, learning 0.169s)
               Value function loss: 85.4867
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 592.57
               Mean episode length: 148.74
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 9.75s
                        Total time: 26125.77s
                               ETA: 1010210.3s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.541s, learning 0.163s)
               Value function loss: 106.1300
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 604.33
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 9.70s
                        Total time: 26135.47s
                               ETA: 1010174.4s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.274s, learning 0.183s)
               Value function loss: 91.5002
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 579.94
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 9.46s
                        Total time: 26144.93s
                               ETA: 1010129.1s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.600s, learning 0.175s)
               Value function loss: 101.3837
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 584.81
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 9.78s
                        Total time: 26154.71s
                               ETA: 1010096.0s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.273s, learning 0.163s)
               Value function loss: 104.4916
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 580.85
               Mean episode length: 149.50
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 9.44s
                        Total time: 26164.14s
                               ETA: 1010049.9s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1616 steps/s (collection: 9.974s, learning 0.162s)
               Value function loss: 101.0375
                    Surrogate loss: 0.0144
             Mean action noise std: 0.74
                       Mean reward: 594.06
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 10.14s
                        Total time: 26174.28s
                               ETA: 1010030.8s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.676s, learning 0.163s)
               Value function loss: 121.2452
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: 581.73
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 9.84s
                        Total time: 26184.12s
                               ETA: 1010000.3s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.872s, learning 0.161s)
               Value function loss: 107.7140
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 586.39
               Mean episode length: 148.04
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 10.03s
                        Total time: 26194.15s
                               ETA: 1009977.2s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.252s, learning 0.163s)
               Value function loss: 79.1691
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 584.75
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 9.42s
                        Total time: 26203.57s
                               ETA: 1009930.4s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.621s, learning 0.159s)
               Value function loss: 90.1634
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 597.25
               Mean episode length: 149.67
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 9.78s
                        Total time: 26213.35s
                               ETA: 1009897.7s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.313s, learning 0.161s)
               Value function loss: 86.7856
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 594.48
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 9.47s
                        Total time: 26222.82s
                               ETA: 1009853.1s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.687s, learning 0.166s)
               Value function loss: 76.9906
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 578.11
               Mean episode length: 149.28
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 9.85s
                        Total time: 26232.67s
                               ETA: 1009823.2s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.164s)
               Value function loss: 92.0430
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 570.54
               Mean episode length: 149.13
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 9.69s
                        Total time: 26242.36s
                               ETA: 1009786.9s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.423s, learning 0.161s)
               Value function loss: 97.5424
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 593.20
               Mean episode length: 149.19
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 9.58s
                        Total time: 26251.94s
                               ETA: 1009746.7s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.412s, learning 0.172s)
               Value function loss: 87.0138
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 590.33
               Mean episode length: 148.30
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 9.58s
                        Total time: 26261.53s
                               ETA: 1009706.5s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1761 steps/s (collection: 9.138s, learning 0.165s)
               Value function loss: 96.1991
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 578.21
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 9.30s
                        Total time: 26270.83s
                               ETA: 1009655.5s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.353s, learning 0.164s)
               Value function loss: 87.1046
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 586.52
               Mean episode length: 148.79
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 9.52s
                        Total time: 26280.35s
                               ETA: 1009612.8s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.671s, learning 0.171s)
               Value function loss: 101.8775
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 585.54
               Mean episode length: 149.91
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 9.84s
                        Total time: 26290.19s
                               ETA: 1009582.5s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.463s, learning 0.160s)
               Value function loss: 97.2162
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 581.61
               Mean episode length: 148.73
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 9.62s
                        Total time: 26299.81s
                               ETA: 1009544.0s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.625s, learning 0.174s)
               Value function loss: 86.9390
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 583.16
               Mean episode length: 147.80
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 9.80s
                        Total time: 26309.61s
                               ETA: 1009512.1s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.394s, learning 0.164s)
               Value function loss: 103.1766
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 581.34
               Mean episode length: 148.50
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 9.56s
                        Total time: 26319.17s
                               ETA: 1009471.1s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.372s, learning 0.165s)
               Value function loss: 102.9402
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 562.46
               Mean episode length: 147.08
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 9.54s
                        Total time: 26328.70s
                               ETA: 1009429.3s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.795s, learning 0.163s)
               Value function loss: 89.4007
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 584.89
               Mean episode length: 149.81
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 9.96s
                        Total time: 26338.66s
                               ETA: 1009403.6s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.195s, learning 0.161s)
               Value function loss: 96.0455
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 569.71
               Mean episode length: 148.03
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 9.36s
                        Total time: 26348.02s
                               ETA: 1009354.9s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.496s, learning 0.169s)
               Value function loss: 99.4130
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 573.47
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 9.67s
                        Total time: 26357.68s
                               ETA: 1009318.1s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.694s, learning 0.167s)
               Value function loss: 94.5306
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 599.15
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 9.86s
                        Total time: 26367.55s
                               ETA: 1009288.8s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.126s, learning 0.166s)
               Value function loss: 95.8633
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 582.55
               Mean episode length: 149.94
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 9.29s
                        Total time: 26376.84s
                               ETA: 1009237.7s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.255s, learning 0.158s)
               Value function loss: 90.1633
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 574.45
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 9.41s
                        Total time: 26386.25s
                               ETA: 1009191.3s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.871s, learning 0.162s)
               Value function loss: 88.9707
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 590.54
               Mean episode length: 149.51
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 10.03s
                        Total time: 26396.28s
                               ETA: 1009168.6s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.429s, learning 0.158s)
               Value function loss: 91.7255
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 569.27
               Mean episode length: 148.57
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 9.59s
                        Total time: 26405.87s
                               ETA: 1009128.9s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.038s, learning 0.173s)
               Value function loss: 91.8316
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 569.21
               Mean episode length: 148.49
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 9.21s
                        Total time: 26415.08s
                               ETA: 1009074.8s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.337s, learning 0.160s)
               Value function loss: 99.2487
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 561.74
               Mean episode length: 147.66
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 9.50s
                        Total time: 26424.58s
                               ETA: 1009031.7s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.370s, learning 0.164s)
               Value function loss: 115.0416
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 571.77
               Mean episode length: 148.61
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 9.53s
                        Total time: 26434.11s
                               ETA: 1008990.0s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.587s, learning 0.165s)
               Value function loss: 111.5352
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 591.40
               Mean episode length: 148.90
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 9.75s
                        Total time: 26443.87s
                               ETA: 1008956.7s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.162s, learning 0.158s)
               Value function loss: 127.2984
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 591.56
               Mean episode length: 149.94
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 9.32s
                        Total time: 26453.19s
                               ETA: 1008906.9s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.306s, learning 0.162s)
               Value function loss: 119.5271
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 587.26
               Mean episode length: 149.31
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 9.47s
                        Total time: 26462.65s
                               ETA: 1008862.8s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.561s, learning 0.162s)
               Value function loss: 137.1045
                    Surrogate loss: 0.0007
             Mean action noise std: 0.74
                       Mean reward: 602.69
               Mean episode length: 149.24
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 9.72s
                        Total time: 26472.38s
                               ETA: 1008828.4s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.542s, learning 0.162s)
               Value function loss: 107.4268
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 596.17
               Mean episode length: 149.33
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 9.70s
                        Total time: 26482.08s
                               ETA: 1008793.4s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.697s, learning 0.166s)
               Value function loss: 106.9433
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 582.40
               Mean episode length: 149.09
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 9.86s
                        Total time: 26491.94s
                               ETA: 1008764.4s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.347s, learning 0.176s)
               Value function loss: 113.2103
                    Surrogate loss: 0.0006
             Mean action noise std: 0.74
                       Mean reward: 562.85
               Mean episode length: 148.07
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 9.52s
                        Total time: 26501.47s
                               ETA: 1008722.4s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.353s, learning 0.163s)
               Value function loss: 108.3355
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 594.06
               Mean episode length: 149.52
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 9.52s
                        Total time: 26510.98s
                               ETA: 1008680.3s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.446s, learning 0.169s)
               Value function loss: 111.0757
                    Surrogate loss: 0.0002
             Mean action noise std: 0.74
                       Mean reward: 604.39
               Mean episode length: 148.91
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 9.61s
                        Total time: 26520.60s
                               ETA: 1008641.9s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.293s, learning 0.163s)
               Value function loss: 108.3194
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 588.13
               Mean episode length: 149.20
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 9.46s
                        Total time: 26530.05s
                               ETA: 1008597.5s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.869s, learning 0.162s)
               Value function loss: 121.9756
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 577.39
               Mean episode length: 148.90
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 10.03s
                        Total time: 26540.09s
                               ETA: 1008575.0s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.143s, learning 0.164s)
               Value function loss: 126.0887
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 591.82
               Mean episode length: 148.51
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 9.31s
                        Total time: 26549.39s
                               ETA: 1008525.0s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.620s, learning 0.167s)
               Value function loss: 107.4073
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 605.17
               Mean episode length: 149.73
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 9.79s
                        Total time: 26559.18s
                               ETA: 1008493.2s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.598s, learning 0.167s)
               Value function loss: 103.8512
                    Surrogate loss: -0.0021
             Mean action noise std: 0.74
                       Mean reward: 574.93
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 9.77s
                        Total time: 26568.94s
                               ETA: 1008460.7s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.291s, learning 0.164s)
               Value function loss: 103.6524
                    Surrogate loss: 0.0009
             Mean action noise std: 0.74
                       Mean reward: 573.25
               Mean episode length: 149.10
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 9.46s
                        Total time: 26578.40s
                               ETA: 1008416.4s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.575s, learning 0.170s)
               Value function loss: 113.4690
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 586.76
               Mean episode length: 149.42
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 9.75s
                        Total time: 26588.15s
                               ETA: 1008383.1s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1784 steps/s (collection: 9.010s, learning 0.173s)
               Value function loss: 118.0262
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 599.31
               Mean episode length: 149.26
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 9.18s
                        Total time: 26597.33s
                               ETA: 1008328.5s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1630 steps/s (collection: 9.876s, learning 0.170s)
               Value function loss: 118.7627
                    Surrogate loss: 0.0004
             Mean action noise std: 0.74
                       Mean reward: 575.52
               Mean episode length: 147.53
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 10.05s
                        Total time: 26607.37s
                               ETA: 1008306.7s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.418s, learning 0.164s)
               Value function loss: 121.9203
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 564.08
               Mean episode length: 148.31
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 9.58s
                        Total time: 26616.96s
                               ETA: 1008267.3s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.550s, learning 0.169s)
               Value function loss: 11362.9501
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 507.14
               Mean episode length: 149.39
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 9.72s
                        Total time: 26626.67s
                               ETA: 1008233.1s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.902s, learning 0.158s)
               Value function loss: 131.9696
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 593.92
               Mean episode length: 149.37
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 9.06s
                        Total time: 26635.74s
                               ETA: 1008174.0s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.574s, learning 0.173s)
               Value function loss: 135.1716
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 586.06
               Mean episode length: 149.30
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 9.75s
                        Total time: 26645.48s
                               ETA: 1008140.9s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.415s, learning 0.168s)
               Value function loss: 137.2770
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 586.22
               Mean episode length: 149.06
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 9.58s
                        Total time: 26655.07s
                               ETA: 1008101.6s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.434s, learning 0.161s)
               Value function loss: 114.1918
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 559.96
               Mean episode length: 147.14
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 9.59s
                        Total time: 26664.66s
                               ETA: 1008062.8s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.533s, learning 0.158s)
               Value function loss: 122.5629
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 578.76
               Mean episode length: 146.80
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 9.69s
                        Total time: 26674.35s
                               ETA: 1008027.7s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.207s, learning 0.163s)
               Value function loss: 124.0021
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 552.81
               Mean episode length: 148.63
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 9.37s
                        Total time: 26683.72s
                               ETA: 1007980.4s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.314s, learning 0.166s)
               Value function loss: 107.9367
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 600.96
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 9.48s
                        Total time: 26693.20s
                               ETA: 1007937.3s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.394s, learning 0.168s)
               Value function loss: 131.7092
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: 576.85
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 9.56s
                        Total time: 26702.76s
                               ETA: 1007897.4s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.331s, learning 0.164s)
               Value function loss: 125.5978
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 599.64
               Mean episode length: 149.12
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 9.49s
                        Total time: 26712.26s
                               ETA: 1007854.9s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.527s, learning 0.163s)
               Value function loss: 143.5595
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 570.59
               Mean episode length: 146.41
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 9.69s
                        Total time: 26721.95s
                               ETA: 1007819.8s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.163s)
               Value function loss: 128.8998
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 609.87
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 9.59s
                        Total time: 26731.54s
                               ETA: 1007781.0s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.525s, learning 0.166s)
               Value function loss: 113.0128
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 566.93
               Mean episode length: 148.76
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 9.69s
                        Total time: 26741.23s
                               ETA: 1007746.0s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.159s)
               Value function loss: 135.4792
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 582.53
               Mean episode length: 149.26
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 9.70s
                        Total time: 26750.92s
                               ETA: 1007711.2s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.346s, learning 0.175s)
               Value function loss: 116.6160
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 578.86
               Mean episode length: 149.52
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 9.52s
                        Total time: 26760.44s
                               ETA: 1007669.9s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.836s, learning 0.163s)
               Value function loss: 106.1437
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 582.40
               Mean episode length: 149.41
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 10.00s
                        Total time: 26770.44s
                               ETA: 1007646.5s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.339s, learning 0.166s)
               Value function loss: 115.4357
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 585.01
               Mean episode length: 149.03
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 9.50s
                        Total time: 26779.95s
                               ETA: 1007604.6s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.628s, learning 0.172s)
               Value function loss: 123.5177
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 566.84
               Mean episode length: 148.78
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 9.80s
                        Total time: 26789.75s
                               ETA: 1007573.8s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.207s, learning 0.159s)
               Value function loss: 118.8948
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 569.34
               Mean episode length: 149.04
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 9.37s
                        Total time: 26799.11s
                               ETA: 1007526.7s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.474s, learning 0.171s)
               Value function loss: 128.6024
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 571.12
               Mean episode length: 149.32
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 9.65s
                        Total time: 26808.76s
                               ETA: 1007490.1s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.617s, learning 0.172s)
               Value function loss: 134.7881
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 587.78
               Mean episode length: 149.03
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 9.79s
                        Total time: 26818.55s
                               ETA: 1007459.0s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.634s, learning 0.184s)
               Value function loss: 146.7914
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 581.47
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 9.82s
                        Total time: 26828.37s
                               ETA: 1007428.9s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.409s, learning 0.173s)
               Value function loss: 136.2540
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 600.02
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 9.58s
                        Total time: 26837.95s
                               ETA: 1007390.0s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.451s, learning 0.159s)
               Value function loss: 99.1350
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 561.70
               Mean episode length: 149.17
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 9.61s
                        Total time: 26847.56s
                               ETA: 1007352.2s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.225s, learning 0.180s)
               Value function loss: 122.5311
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 561.31
               Mean episode length: 148.71
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 9.40s
                        Total time: 26856.96s
                               ETA: 1007306.7s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1784 steps/s (collection: 9.020s, learning 0.163s)
               Value function loss: 126.9889
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 564.59
               Mean episode length: 149.02
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 9.18s
                        Total time: 26866.15s
                               ETA: 1007253.0s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.627s, learning 0.168s)
               Value function loss: 109.4773
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 544.17
               Mean episode length: 149.51
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 9.79s
                        Total time: 26875.94s
                               ETA: 1007222.2s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.568s, learning 0.178s)
               Value function loss: 128.0878
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 557.09
               Mean episode length: 149.34
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 9.75s
                        Total time: 26885.69s
                               ETA: 1007189.5s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.471s, learning 0.161s)
               Value function loss: 124.0462
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 528.46
               Mean episode length: 148.83
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 9.63s
                        Total time: 26895.32s
                               ETA: 1007152.7s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.242s, learning 0.164s)
               Value function loss: 122.4051
                    Surrogate loss: -0.0189
             Mean action noise std: 0.74
                       Mean reward: 552.18
               Mean episode length: 148.86
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 9.41s
                        Total time: 26904.72s
                               ETA: 1007107.3s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.527s, learning 0.160s)
               Value function loss: 115.7887
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 551.50
               Mean episode length: 149.87
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 9.69s
                        Total time: 26914.41s
                               ETA: 1007072.5s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.382s, learning 0.161s)
               Value function loss: 106.6466
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 566.52
               Mean episode length: 149.30
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 9.54s
                        Total time: 26923.95s
                               ETA: 1007032.4s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1621 steps/s (collection: 9.942s, learning 0.165s)
               Value function loss: 109.0443
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 575.90
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 10.11s
                        Total time: 26934.06s
                               ETA: 1007013.3s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.274s, learning 0.158s)
               Value function loss: 93.9073
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 566.71
               Mean episode length: 149.87
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 9.43s
                        Total time: 26943.49s
                               ETA: 1006969.1s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.775s, learning 0.163s)
               Value function loss: 101.0546
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 568.68
               Mean episode length: 149.34
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 9.94s
                        Total time: 26953.43s
                               ETA: 1006943.8s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.336s, learning 0.158s)
               Value function loss: 116.9795
                    Surrogate loss: 0.0013
             Mean action noise std: 0.74
                       Mean reward: 570.68
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 9.49s
                        Total time: 26962.93s
                               ETA: 1006901.9s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.587s, learning 0.164s)
               Value function loss: 110.8646
                    Surrogate loss: 0.0022
             Mean action noise std: 0.74
                       Mean reward: 560.85
               Mean episode length: 149.66
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 9.75s
                        Total time: 26972.68s
                               ETA: 1006869.6s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.539s, learning 0.164s)
               Value function loss: 113.1926
                    Surrogate loss: 0.0047
             Mean action noise std: 0.74
                       Mean reward: 562.16
               Mean episode length: 149.79
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 9.70s
                        Total time: 26982.38s
                               ETA: 1006835.6s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.474s, learning 0.162s)
               Value function loss: 127.3338
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 563.83
               Mean episode length: 148.93
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 9.64s
                        Total time: 26992.02s
                               ETA: 1006799.1s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.550s, learning 0.168s)
               Value function loss: 105.2888
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 570.32
               Mean episode length: 148.81
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 9.72s
                        Total time: 27001.73s
                               ETA: 1006765.6s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.480s, learning 0.164s)
               Value function loss: 139.0708
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 576.05
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 9.64s
                        Total time: 27011.38s
                               ETA: 1006729.4s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.322s, learning 0.166s)
               Value function loss: 125.1916
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 581.84
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 9.49s
                        Total time: 27020.87s
                               ETA: 1006687.5s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.196s, learning 0.161s)
               Value function loss: 98.7054
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 545.39
               Mean episode length: 148.33
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 9.36s
                        Total time: 27030.22s
                               ETA: 1006640.7s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.692s, learning 0.167s)
               Value function loss: 119.4567
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 563.59
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 9.86s
                        Total time: 27040.08s
                               ETA: 1006612.6s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.414s, learning 0.160s)
               Value function loss: 115.2843
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 563.22
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 9.57s
                        Total time: 27049.66s
                               ETA: 1006573.9s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1756 steps/s (collection: 9.165s, learning 0.161s)
               Value function loss: 111.2599
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 541.13
               Mean episode length: 150.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 9.33s
                        Total time: 27058.98s
                               ETA: 1006526.0s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.304s, learning 0.163s)
               Value function loss: 117.1024
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 552.75
               Mean episode length: 150.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 9.47s
                        Total time: 27068.45s
                               ETA: 1006483.3s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.494s, learning 0.161s)
               Value function loss: 127.7592
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 570.53
               Mean episode length: 149.47
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 9.65s
                        Total time: 27078.10s
                               ETA: 1006447.7s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.239s, learning 0.165s)
               Value function loss: 127.3869
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 568.46
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 9.40s
                        Total time: 27087.51s
                               ETA: 1006402.8s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.572s, learning 0.164s)
               Value function loss: 107.6105
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 552.70
               Mean episode length: 148.69
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 9.74s
                        Total time: 27097.24s
                               ETA: 1006370.2s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.511s, learning 0.164s)
               Value function loss: 95.3444
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 530.43
               Mean episode length: 150.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 9.68s
                        Total time: 27106.92s
                               ETA: 1006335.4s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.435s, learning 0.161s)
               Value function loss: 100.1679
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 579.29
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 9.60s
                        Total time: 27116.52s
                               ETA: 1006297.6s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.232s, learning 0.162s)
               Value function loss: 91.3229
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 558.62
               Mean episode length: 149.44
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 9.39s
                        Total time: 27125.91s
                               ETA: 1006252.4s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.661s, learning 0.159s)
               Value function loss: 95.8928
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 585.81
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 9.82s
                        Total time: 27135.73s
                               ETA: 1006223.0s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.423s, learning 0.160s)
               Value function loss: 103.5760
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 568.63
               Mean episode length: 149.84
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 9.58s
                        Total time: 27145.31s
                               ETA: 1006184.9s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.458s, learning 0.158s)
               Value function loss: 109.0011
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 541.63
               Mean episode length: 149.30
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 9.62s
                        Total time: 27154.93s
                               ETA: 1006148.0s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.855s, learning 0.161s)
               Value function loss: 107.2840
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 554.07
               Mean episode length: 147.22
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 10.02s
                        Total time: 27164.94s
                               ETA: 1006125.9s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.309s, learning 0.162s)
               Value function loss: 126.5881
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 570.48
               Mean episode length: 148.57
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 9.47s
                        Total time: 27174.42s
                               ETA: 1006083.6s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.477s, learning 0.164s)
               Value function loss: 112.4046
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 575.88
               Mean episode length: 149.62
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 9.64s
                        Total time: 27184.06s
                               ETA: 1006047.7s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.452s, learning 0.160s)
               Value function loss: 135.4188
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 579.95
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 9.61s
                        Total time: 27193.67s
                               ETA: 1006010.8s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.673s, learning 0.158s)
               Value function loss: 117.4531
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 564.97
               Mean episode length: 148.47
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 9.83s
                        Total time: 27203.50s
                               ETA: 1005981.9s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.368s, learning 0.159s)
               Value function loss: 114.1556
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 580.79
               Mean episode length: 149.41
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 9.53s
                        Total time: 27213.03s
                               ETA: 1005941.8s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.398s, learning 0.161s)
               Value function loss: 121.1678
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 587.88
               Mean episode length: 149.25
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 9.56s
                        Total time: 27222.59s
                               ETA: 1005903.0s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.178s, learning 0.168s)
               Value function loss: 114.2475
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 579.17
               Mean episode length: 149.28
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 9.35s
                        Total time: 27231.93s
                               ETA: 1005856.3s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.362s, learning 0.179s)
               Value function loss: 119.3448
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 594.17
               Mean episode length: 150.00
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 9.54s
                        Total time: 27241.47s
                               ETA: 1005816.7s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.653s, learning 0.172s)
               Value function loss: 110.5566
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 581.73
               Mean episode length: 149.58
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 9.83s
                        Total time: 27251.30s
                               ETA: 1005787.8s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.660s, learning 0.160s)
               Value function loss: 117.2421
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 583.70
               Mean episode length: 149.36
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 9.82s
                        Total time: 27261.12s
                               ETA: 1005758.6s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.455s, learning 0.160s)
               Value function loss: 106.5542
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 575.60
               Mean episode length: 149.72
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 9.62s
                        Total time: 27270.73s
                               ETA: 1005721.9s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.454s, learning 0.163s)
               Value function loss: 105.2393
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 582.56
               Mean episode length: 149.35
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 9.62s
                        Total time: 27280.35s
                               ETA: 1005685.3s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.437s, learning 0.160s)
               Value function loss: 97.7304
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 582.45
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 9.60s
                        Total time: 27289.95s
                               ETA: 1005648.0s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.492s, learning 0.178s)
               Value function loss: 92.0811
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 576.75
               Mean episode length: 148.15
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 9.67s
                        Total time: 27299.62s
                               ETA: 1005613.4s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.530s, learning 0.161s)
               Value function loss: 86.7445
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 593.27
               Mean episode length: 149.43
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 9.69s
                        Total time: 27309.31s
                               ETA: 1005579.5s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.226s, learning 0.158s)
               Value function loss: 92.8237
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 579.93
               Mean episode length: 149.35
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 9.38s
                        Total time: 27318.69s
                               ETA: 1005534.4s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.870s, learning 0.161s)
               Value function loss: 106.2682
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 589.15
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 10.03s
                        Total time: 27328.72s
                               ETA: 1005513.1s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.317s, learning 0.161s)
               Value function loss: 107.2978
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 569.94
               Mean episode length: 148.90
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 9.48s
                        Total time: 27338.20s
                               ETA: 1005471.5s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.804s, learning 0.164s)
               Value function loss: 117.6831
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 572.70
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 9.97s
                        Total time: 27348.17s
                               ETA: 1005448.0s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.417s, learning 0.163s)
               Value function loss: 98.1296
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 583.73
               Mean episode length: 148.85
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 9.58s
                        Total time: 27357.75s
                               ETA: 1005410.2s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.275s, learning 0.157s)
               Value function loss: 101.4308
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 568.72
               Mean episode length: 149.42
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 9.43s
                        Total time: 27367.18s
                               ETA: 1005367.0s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.529s, learning 0.158s)
               Value function loss: 113.6869
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 569.29
               Mean episode length: 148.58
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 9.69s
                        Total time: 27376.87s
                               ETA: 1005333.1s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.516s, learning 0.157s)
               Value function loss: 102.3233
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 589.32
               Mean episode length: 149.18
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 9.67s
                        Total time: 27386.54s
                               ETA: 1005298.8s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.346s, learning 0.159s)
               Value function loss: 106.0803
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 601.09
               Mean episode length: 149.93
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 9.50s
                        Total time: 27396.05s
                               ETA: 1005258.3s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.263s, learning 0.161s)
               Value function loss: 134.2231
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 592.22
               Mean episode length: 149.46
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 9.42s
                        Total time: 27405.47s
                               ETA: 1005214.9s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.283s, learning 0.159s)
               Value function loss: 113.5123
                    Surrogate loss: -0.0002
             Mean action noise std: 0.74
                       Mean reward: 587.00
               Mean episode length: 147.04
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 9.44s
                        Total time: 27414.91s
                               ETA: 1005172.2s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.372s, learning 0.163s)
               Value function loss: 1041.8990
                    Surrogate loss: 0.0040
             Mean action noise std: 0.74
                       Mean reward: 553.66
               Mean episode length: 148.83
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 9.54s
                        Total time: 27424.45s
                               ETA: 1005132.9s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.577s, learning 0.169s)
               Value function loss: 98.7006
                    Surrogate loss: -0.0209
             Mean action noise std: 0.74
                       Mean reward: 588.23
               Mean episode length: 149.77
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 9.75s
                        Total time: 27434.19s
                               ETA: 1005101.3s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.309s, learning 0.161s)
               Value function loss: 111.4698
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 582.31
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 9.47s
                        Total time: 27443.66s
                               ETA: 1005059.7s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.691s, learning 0.162s)
               Value function loss: 104.3884
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 547.66
               Mean episode length: 149.30
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 9.85s
                        Total time: 27453.52s
                               ETA: 1005032.0s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.465s, learning 0.163s)
               Value function loss: 91.0997
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 551.59
               Mean episode length: 149.40
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 9.63s
                        Total time: 27463.14s
                               ETA: 1004996.2s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1644 steps/s (collection: 9.801s, learning 0.159s)
               Value function loss: 101.9022
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 579.54
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 9.96s
                        Total time: 27473.10s
                               ETA: 1004972.6s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.337s, learning 0.171s)
               Value function loss: 80.3015
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 561.36
               Mean episode length: 148.36
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 9.51s
                        Total time: 27482.61s
                               ETA: 1004932.4s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.860s, learning 0.177s)
               Value function loss: 81.9937
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 566.75
               Mean episode length: 148.02
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 10.04s
                        Total time: 27492.65s
                               ETA: 1004911.6s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.136s, learning 0.195s)
               Value function loss: 94.3828
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 573.13
               Mean episode length: 148.73
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 9.33s
                        Total time: 27501.98s
                               ETA: 1004865.0s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.685s, learning 0.170s)
               Value function loss: 114.1586
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 568.65
               Mean episode length: 148.45
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 9.85s
                        Total time: 27511.84s
                               ETA: 1004837.6s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.300s, learning 0.173s)
               Value function loss: 112.6608
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 550.71
               Mean episode length: 148.24
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 9.47s
                        Total time: 27521.31s
                               ETA: 1004796.2s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.467s, learning 0.161s)
               Value function loss: 115.7149
                    Surrogate loss: 0.0034
             Mean action noise std: 0.74
                       Mean reward: 559.33
               Mean episode length: 148.54
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 9.63s
                        Total time: 27530.94s
                               ETA: 1004760.5s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.244s, learning 0.160s)
               Value function loss: 109.4065
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 564.63
               Mean episode length: 148.28
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 9.40s
                        Total time: 27540.34s
                               ETA: 1004716.7s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.210s, learning 0.161s)
               Value function loss: 133.5569
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 567.25
               Mean episode length: 150.00
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 9.37s
                        Total time: 27549.71s
                               ETA: 1004671.7s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.225s, learning 0.161s)
               Value function loss: 111.5511
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 570.70
               Mean episode length: 150.00
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 9.39s
                        Total time: 27559.10s
                               ETA: 1004627.2s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.677s, learning 0.160s)
               Value function loss: 113.7354
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 566.56
               Mean episode length: 148.88
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 9.84s
                        Total time: 27568.94s
                               ETA: 1004599.3s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.774s, learning 0.160s)
               Value function loss: 135.4109
                    Surrogate loss: -0.0190
             Mean action noise std: 0.74
                       Mean reward: 550.45
               Mean episode length: 149.19
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 9.93s
                        Total time: 27578.87s
                               ETA: 1004574.8s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.746s, learning 0.170s)
               Value function loss: 138.9252
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 552.34
               Mean episode length: 149.72
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 9.92s
                        Total time: 27588.79s
                               ETA: 1004549.7s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.440s, learning 0.168s)
               Value function loss: 138.7406
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 578.69
               Mean episode length: 149.24
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 9.61s
                        Total time: 27598.39s
                               ETA: 1004513.5s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.346s, learning 0.165s)
               Value function loss: 136.2940
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 553.42
               Mean episode length: 150.00
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 9.51s
                        Total time: 27607.91s
                               ETA: 1004473.7s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.555s, learning 0.164s)
               Value function loss: 98.9157
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 546.39
               Mean episode length: 148.06
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 9.72s
                        Total time: 27617.62s
                               ETA: 1004441.4s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.211s, learning 0.159s)
               Value function loss: 97.8378
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 487.84
               Mean episode length: 147.77
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 9.37s
                        Total time: 27626.99s
                               ETA: 1004396.6s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1611 steps/s (collection: 10.004s, learning 0.163s)
               Value function loss: 97.8665
                    Surrogate loss: -0.0193
             Mean action noise std: 0.74
                       Mean reward: 507.86
               Mean episode length: 148.56
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 10.17s
                        Total time: 27637.16s
                               ETA: 1004380.7s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.109s, learning 0.168s)
               Value function loss: 90.6242
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 518.51
               Mean episode length: 149.02
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 9.28s
                        Total time: 27646.44s
                               ETA: 1004332.5s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.450s, learning 0.159s)
               Value function loss: 101.8915
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 532.29
               Mean episode length: 150.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 9.61s
                        Total time: 27656.05s
                               ETA: 1004296.3s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.213s, learning 0.161s)
               Value function loss: 102.7531
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 527.45
               Mean episode length: 149.22
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 9.37s
                        Total time: 27665.42s
                               ETA: 1004251.7s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.670s, learning 0.166s)
               Value function loss: 86.4775
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 541.92
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 9.84s
                        Total time: 27675.26s
                               ETA: 1004223.8s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.586s, learning 0.169s)
               Value function loss: 112.4419
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 539.44
               Mean episode length: 149.29
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 9.76s
                        Total time: 27685.01s
                               ETA: 1004193.1s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.515s, learning 0.165s)
               Value function loss: 111.2669
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 519.83
               Mean episode length: 148.84
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 9.68s
                        Total time: 27694.69s
                               ETA: 1004159.6s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.343s, learning 0.159s)
               Value function loss: 109.7758
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 538.97
               Mean episode length: 149.28
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 9.50s
                        Total time: 27704.19s
                               ETA: 1004119.7s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.237s, learning 0.163s)
               Value function loss: 106.6566
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 544.08
               Mean episode length: 149.03
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 9.40s
                        Total time: 27713.59s
                               ETA: 1004076.1s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.332s, learning 0.166s)
               Value function loss: 105.8892
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 532.28
               Mean episode length: 149.22
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 9.50s
                        Total time: 27723.09s
                               ETA: 1004036.1s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.558s, learning 0.163s)
               Value function loss: 138.6887
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 556.51
               Mean episode length: 149.79
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 9.72s
                        Total time: 27732.81s
                               ETA: 1004004.2s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.821s, learning 0.164s)
               Value function loss: 131.2100
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 563.81
               Mean episode length: 148.98
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 9.99s
                        Total time: 27742.80s
                               ETA: 1003981.9s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.642s, learning 0.173s)
               Value function loss: 116.4242
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 561.15
               Mean episode length: 149.43
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 9.81s
                        Total time: 27752.61s
                               ETA: 1003953.4s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.699s, learning 0.167s)
               Value function loss: 133.3846
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 582.09
               Mean episode length: 150.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 9.87s
                        Total time: 27762.48s
                               ETA: 1003926.8s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.037s, learning 0.177s)
               Value function loss: 139.4135
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 545.84
               Mean episode length: 149.62
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 9.21s
                        Total time: 27771.69s
                               ETA: 1003876.6s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.470s, learning 0.158s)
               Value function loss: 123.6700
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 569.91
               Mean episode length: 148.59
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 9.63s
                        Total time: 27781.32s
                               ETA: 1003841.4s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.857s, learning 0.165s)
               Value function loss: 123.9438
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 573.49
               Mean episode length: 150.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 9.02s
                        Total time: 27790.34s
                               ETA: 1003784.3s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.578s, learning 0.158s)
               Value function loss: 135.1452
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 573.60
               Mean episode length: 149.12
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 9.74s
                        Total time: 27800.08s
                               ETA: 1003753.1s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.583s, learning 0.169s)
               Value function loss: 146.6380
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 560.81
               Mean episode length: 149.84
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 9.75s
                        Total time: 27809.83s
                               ETA: 1003722.5s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.737s, learning 0.183s)
               Value function loss: 123.8899
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 560.84
               Mean episode length: 149.26
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 9.92s
                        Total time: 27819.75s
                               ETA: 1003697.9s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.043s, learning 0.167s)
               Value function loss: 125.5581
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 569.72
               Mean episode length: 147.72
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 9.21s
                        Total time: 27828.96s
                               ETA: 1003647.8s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.756s, learning 0.163s)
               Value function loss: 112.5460
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 553.40
               Mean episode length: 146.84
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 9.92s
                        Total time: 27838.88s
                               ETA: 1003623.2s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.322s, learning 0.159s)
               Value function loss: 115.9161
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 568.78
               Mean episode length: 149.17
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 9.48s
                        Total time: 27848.36s
                               ETA: 1003582.8s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.255s, learning 0.160s)
               Value function loss: 120.6404
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 578.19
               Mean episode length: 149.17
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 9.42s
                        Total time: 27857.78s
                               ETA: 1003540.2s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.422s, learning 0.169s)
               Value function loss: 127.4702
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 555.53
               Mean episode length: 149.19
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 9.59s
                        Total time: 27867.37s
                               ETA: 1003503.8s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.494s, learning 0.165s)
               Value function loss: 151.6032
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 579.91
               Mean episode length: 148.99
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 9.66s
                        Total time: 27877.03s
                               ETA: 1003469.9s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.455s, learning 0.165s)
               Value function loss: 125.9532
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 567.98
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 9.62s
                        Total time: 27886.65s
                               ETA: 1003434.7s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.403s, learning 0.166s)
               Value function loss: 134.1845
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: 550.03
               Mean episode length: 149.35
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 9.57s
                        Total time: 27896.22s
                               ETA: 1003397.6s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.748s, learning 0.163s)
               Value function loss: 121.3591
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 566.18
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 9.91s
                        Total time: 27906.13s
                               ETA: 1003372.8s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1786 steps/s (collection: 9.004s, learning 0.165s)
               Value function loss: 145.6852
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 558.35
               Mean episode length: 149.45
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 9.17s
                        Total time: 27915.30s
                               ETA: 1003321.4s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.756s, learning 0.167s)
               Value function loss: 131.2394
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 573.17
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 9.92s
                        Total time: 27925.22s
                               ETA: 1003297.1s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.299s, learning 0.162s)
               Value function loss: 121.3019
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 574.49
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 9.46s
                        Total time: 27934.68s
                               ETA: 1003256.3s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.488s, learning 0.170s)
               Value function loss: 125.9091
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 543.37
               Mean episode length: 149.46
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 9.66s
                        Total time: 27944.34s
                               ETA: 1003222.5s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.295s, learning 0.161s)
               Value function loss: 127.6637
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 561.83
               Mean episode length: 149.53
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 9.46s
                        Total time: 27953.80s
                               ETA: 1003181.5s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.429s, learning 0.164s)
               Value function loss: 132.2712
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 578.06
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 9.59s
                        Total time: 27963.39s
                               ETA: 1003145.4s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.732s, learning 0.167s)
               Value function loss: 113.5131
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 576.73
               Mean episode length: 148.51
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 9.90s
                        Total time: 27973.29s
                               ETA: 1003120.3s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.701s, learning 0.167s)
               Value function loss: 127.2386
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 560.09
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 9.87s
                        Total time: 27983.16s
                               ETA: 1003094.1s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.726s, learning 0.161s)
               Value function loss: 146.6144
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 559.97
               Mean episode length: 148.10
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 9.89s
                        Total time: 27993.05s
                               ETA: 1003068.7s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.712s, learning 0.164s)
               Value function loss: 123.1062
                    Surrogate loss: 0.0003
             Mean action noise std: 0.74
                       Mean reward: 570.11
               Mean episode length: 149.42
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 9.88s
                        Total time: 28002.92s
                               ETA: 1003042.8s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.678s, learning 0.163s)
               Value function loss: 130.4218
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 585.52
               Mean episode length: 149.42
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 9.84s
                        Total time: 28012.76s
                               ETA: 1003015.7s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.515s, learning 0.173s)
               Value function loss: 128.5532
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 571.78
               Mean episode length: 149.12
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 9.69s
                        Total time: 28022.45s
                               ETA: 1002983.1s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.625s, learning 0.164s)
               Value function loss: 119.8815
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 546.74
               Mean episode length: 148.12
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 9.79s
                        Total time: 28032.24s
                               ETA: 1002954.2s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.432s, learning 0.167s)
               Value function loss: 146.1394
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 603.78
               Mean episode length: 149.38
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 9.60s
                        Total time: 28041.84s
                               ETA: 1002918.4s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.735s, learning 0.191s)
               Value function loss: 142.8111
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 569.19
               Mean episode length: 149.19
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 9.93s
                        Total time: 28051.76s
                               ETA: 1002894.4s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.363s, learning 0.160s)
               Value function loss: 148.0575
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 556.33
               Mean episode length: 149.20
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 9.52s
                        Total time: 28061.29s
                               ETA: 1002856.0s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.383s, learning 0.165s)
               Value function loss: 176.6086
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 594.20
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 9.55s
                        Total time: 28070.84s
                               ETA: 1002818.5s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.473s, learning 0.162s)
               Value function loss: 154.1581
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 591.84
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 9.64s
                        Total time: 28080.47s
                               ETA: 1002784.1s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.663s, learning 0.159s)
               Value function loss: 138.7272
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 568.16
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 9.82s
                        Total time: 28090.29s
                               ETA: 1002756.4s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.452s, learning 0.163s)
               Value function loss: 147.5279
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 563.29
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 9.62s
                        Total time: 28099.91s
                               ETA: 1002721.4s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.664s, learning 0.180s)
               Value function loss: 125.2804
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 584.60
               Mean episode length: 149.19
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 9.84s
                        Total time: 28109.75s
                               ETA: 1002694.6s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.863s, learning 0.167s)
               Value function loss: 122.5911
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 567.70
               Mean episode length: 150.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 10.03s
                        Total time: 28119.78s
                               ETA: 1002674.4s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.478s, learning 0.158s)
               Value function loss: 147.0200
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 582.50
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 9.64s
                        Total time: 28129.42s
                               ETA: 1002640.1s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.222s, learning 0.168s)
               Value function loss: 132.6241
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 582.85
               Mean episode length: 150.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 9.39s
                        Total time: 28138.81s
                               ETA: 1002597.1s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.167s)
               Value function loss: 142.3543
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 558.00
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 9.69s
                        Total time: 28148.50s
                               ETA: 1002564.7s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.391s, learning 0.169s)
               Value function loss: 136.8089
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 597.39
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 9.56s
                        Total time: 28158.06s
                               ETA: 1002527.8s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.490s, learning 0.164s)
               Value function loss: 154.2003
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 568.65
               Mean episode length: 149.52
                  Mean reward/step: 3.75
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 9.65s
                        Total time: 28167.71s
                               ETA: 1002494.2s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.701s, learning 0.162s)
               Value function loss: 142.2318
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 565.96
               Mean episode length: 149.40
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 9.86s
                        Total time: 28177.57s
                               ETA: 1002468.2s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.389s, learning 0.175s)
               Value function loss: 123.6031
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 535.79
               Mean episode length: 147.72
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 9.56s
                        Total time: 28187.14s
                               ETA: 1002431.4s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.569s, learning 0.165s)
               Value function loss: 145.0836
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 557.95
               Mean episode length: 148.54
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 9.73s
                        Total time: 28196.87s
                               ETA: 1002400.8s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.487s, learning 0.170s)
               Value function loss: 130.4735
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 549.72
               Mean episode length: 148.23
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 9.66s
                        Total time: 28206.53s
                               ETA: 1002367.5s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.550s, learning 0.167s)
               Value function loss: 119.9575
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: 553.49
               Mean episode length: 148.40
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 9.72s
                        Total time: 28216.24s
                               ETA: 1002336.2s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.606s, learning 0.162s)
               Value function loss: 134.1514
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 571.23
               Mean episode length: 148.19
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 9.77s
                        Total time: 28226.01s
                               ETA: 1002306.8s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.749s, learning 0.169s)
               Value function loss: 138.0398
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 567.51
               Mean episode length: 148.05
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 9.92s
                        Total time: 28235.93s
                               ETA: 1002282.8s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.189s, learning 0.171s)
               Value function loss: 143.0512
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 546.30
               Mean episode length: 149.87
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 9.36s
                        Total time: 28245.29s
                               ETA: 1002239.0s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.516s, learning 0.161s)
               Value function loss: 155.2144
                    Surrogate loss: 0.0013
             Mean action noise std: 0.74
                       Mean reward: 537.81
               Mean episode length: 149.19
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 9.68s
                        Total time: 28254.97s
                               ETA: 1002206.4s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.310s, learning 0.162s)
               Value function loss: 163.9296
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 584.46
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 9.47s
                        Total time: 28264.44s
                               ETA: 1002166.5s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.335s, learning 0.162s)
               Value function loss: 160.2611
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 547.35
               Mean episode length: 149.70
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 9.50s
                        Total time: 28273.94s
                               ETA: 1002127.6s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.658s, learning 0.177s)
               Value function loss: 147.7150
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 573.99
               Mean episode length: 149.22
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 9.83s
                        Total time: 28283.77s
                               ETA: 1002100.7s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.511s, learning 0.171s)
               Value function loss: 114.1617
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 555.84
               Mean episode length: 149.22
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 9.68s
                        Total time: 28293.45s
                               ETA: 1002068.4s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.539s, learning 0.157s)
               Value function loss: 134.1699
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 567.34
               Mean episode length: 149.44
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 9.70s
                        Total time: 28303.15s
                               ETA: 1002036.6s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.170s, learning 0.164s)
               Value function loss: 137.7389
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 551.82
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 9.33s
                        Total time: 28312.48s
                               ETA: 1001992.0s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.409s, learning 0.160s)
               Value function loss: 138.5002
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 529.93
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 9.57s
                        Total time: 28322.05s
                               ETA: 1001955.8s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.188s, learning 0.165s)
               Value function loss: 139.6610
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 531.17
               Mean episode length: 148.41
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 9.35s
                        Total time: 28331.41s
                               ETA: 1001911.8s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.386s, learning 0.162s)
               Value function loss: 133.0333
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 561.02
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 9.55s
                        Total time: 28340.95s
                               ETA: 1001874.9s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.363s, learning 0.170s)
               Value function loss: 149.8476
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 577.77
               Mean episode length: 150.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 9.53s
                        Total time: 28350.49s
                               ETA: 1001837.4s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.590s, learning 0.170s)
               Value function loss: 135.5381
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 533.76
               Mean episode length: 148.39
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 9.76s
                        Total time: 28360.25s
                               ETA: 1001808.0s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.249s, learning 0.162s)
               Value function loss: 116.1266
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 549.86
               Mean episode length: 147.16
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 9.41s
                        Total time: 28369.66s
                               ETA: 1001766.2s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.434s, learning 0.167s)
               Value function loss: 127.7248
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 523.19
               Mean episode length: 149.26
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 9.60s
                        Total time: 28379.26s
                               ETA: 1001731.2s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.416s, learning 0.166s)
               Value function loss: 147.8077
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 551.33
               Mean episode length: 148.78
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 9.58s
                        Total time: 28388.84s
                               ETA: 1001695.5s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.447s, learning 0.164s)
               Value function loss: 139.9632
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 545.27
               Mean episode length: 149.21
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 9.61s
                        Total time: 28398.45s
                               ETA: 1001660.9s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1772 steps/s (collection: 9.084s, learning 0.158s)
               Value function loss: 137.1907
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 536.72
               Mean episode length: 149.29
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 9.24s
                        Total time: 28407.69s
                               ETA: 1001613.3s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.299s, learning 0.159s)
               Value function loss: 111.4696
                    Surrogate loss: 0.0059
             Mean action noise std: 0.74
                       Mean reward: 521.09
               Mean episode length: 147.75
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 9.46s
                        Total time: 28417.15s
                               ETA: 1001573.3s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.464s, learning 0.159s)
               Value function loss: 114.3883
                    Surrogate loss: -0.0002
             Mean action noise std: 0.74
                       Mean reward: 561.38
               Mean episode length: 150.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 9.62s
                        Total time: 28426.77s
                               ETA: 1001539.1s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.668s, learning 0.158s)
               Value function loss: 126.9558
                    Surrogate loss: 0.0071
             Mean action noise std: 0.74
                       Mean reward: 567.43
               Mean episode length: 150.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 9.83s
                        Total time: 28436.60s
                               ETA: 1001512.2s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.725s, learning 0.164s)
               Value function loss: 100.8909
                    Surrogate loss: -0.0197
             Mean action noise std: 0.74
                       Mean reward: 541.69
               Mean episode length: 148.73
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 9.89s
                        Total time: 28446.49s
                               ETA: 1001487.4s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.385s, learning 0.166s)
               Value function loss: 115.1864
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 523.44
               Mean episode length: 150.00
                  Mean reward/step: 3.54
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 9.55s
                        Total time: 28456.04s
                               ETA: 1001450.8s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.542s, learning 0.166s)
               Value function loss: 114.6501
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 550.26
               Mean episode length: 150.00
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 9.71s
                        Total time: 28465.75s
                               ETA: 1001419.7s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.479s, learning 0.159s)
               Value function loss: 95.2922
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 559.45
               Mean episode length: 149.51
                  Mean reward/step: 3.55
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 9.64s
                        Total time: 28475.39s
                               ETA: 1001386.2s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.159s, learning 0.163s)
               Value function loss: 126.3490
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 553.01
               Mean episode length: 149.54
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 9.32s
                        Total time: 28484.71s
                               ETA: 1001341.5s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.298s, learning 0.173s)
               Value function loss: 128.3350
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 526.60
               Mean episode length: 148.06
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 9.47s
                        Total time: 28494.18s
                               ETA: 1001302.2s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.711s, learning 0.170s)
               Value function loss: 103.9353
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 539.08
               Mean episode length: 150.00
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 9.88s
                        Total time: 28504.06s
                               ETA: 1001277.3s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.380s, learning 0.165s)
               Value function loss: 104.5058
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 549.01
               Mean episode length: 149.18
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 9.55s
                        Total time: 28513.61s
                               ETA: 1001240.5s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.552s, learning 0.164s)
               Value function loss: 113.5162
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 535.42
               Mean episode length: 149.52
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 9.72s
                        Total time: 28523.32s
                               ETA: 1001209.9s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.980s, learning 0.180s)
               Value function loss: 130.6833
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 549.96
               Mean episode length: 149.18
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 9.16s
                        Total time: 28532.48s
                               ETA: 1001159.7s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.360s, learning 0.186s)
               Value function loss: 117.7834
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 511.24
               Mean episode length: 148.96
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 9.55s
                        Total time: 28542.03s
                               ETA: 1001123.0s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.161s)
               Value function loss: 112.9182
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 550.10
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 9.59s
                        Total time: 28551.62s
                               ETA: 1001087.9s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.510s, learning 0.161s)
               Value function loss: 107.1255
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 543.59
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 9.67s
                        Total time: 28561.29s
                               ETA: 1001055.6s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.485s, learning 0.169s)
               Value function loss: 106.6013
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 552.92
               Mean episode length: 149.43
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 9.65s
                        Total time: 28570.94s
                               ETA: 1001022.8s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.744s, learning 0.165s)
               Value function loss: 124.5998
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 542.85
               Mean episode length: 150.00
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 9.91s
                        Total time: 28580.85s
                               ETA: 1000999.0s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.633s, learning 0.157s)
               Value function loss: 149.4074
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 534.91
               Mean episode length: 147.65
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 9.79s
                        Total time: 28590.64s
                               ETA: 1000971.0s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.602s, learning 0.163s)
               Value function loss: 159.6475
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 569.49
               Mean episode length: 149.44
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 9.76s
                        Total time: 28600.41s
                               ETA: 1000942.1s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.656s, learning 0.166s)
               Value function loss: 158.5009
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 579.86
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 9.82s
                        Total time: 28610.23s
                               ETA: 1000915.3s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.547s, learning 0.169s)
               Value function loss: 161.4430
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 560.00
               Mean episode length: 149.02
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 9.72s
                        Total time: 28619.94s
                               ETA: 1000884.7s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.391s, learning 0.158s)
               Value function loss: 142.7623
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 548.21
               Mean episode length: 148.33
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 9.55s
                        Total time: 28629.49s
                               ETA: 1000848.3s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.351s, learning 0.158s)
               Value function loss: 159.1572
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 576.34
               Mean episode length: 149.36
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 9.51s
                        Total time: 28639.00s
                               ETA: 1000810.6s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.816s, learning 0.163s)
               Value function loss: 130.9628
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 533.71
               Mean episode length: 150.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 9.98s
                        Total time: 28648.98s
                               ETA: 1000789.3s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.567s, learning 0.163s)
               Value function loss: 126.6266
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 570.45
               Mean episode length: 150.00
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 9.73s
                        Total time: 28658.71s
                               ETA: 1000759.3s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.221s, learning 0.179s)
               Value function loss: 143.6071
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 543.39
               Mean episode length: 149.34
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 9.40s
                        Total time: 28668.11s
                               ETA: 1000717.8s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.395s, learning 0.161s)
               Value function loss: 146.0229
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 549.19
               Mean episode length: 150.00
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 9.56s
                        Total time: 28677.67s
                               ETA: 1000681.7s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.241s, learning 0.159s)
               Value function loss: 124.1850
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 549.59
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 9.40s
                        Total time: 28687.07s
                               ETA: 1000640.3s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.539s, learning 0.169s)
               Value function loss: 121.0372
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 564.53
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 9.71s
                        Total time: 28696.77s
                               ETA: 1000609.5s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.751s, learning 0.168s)
               Value function loss: 147.0873
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 549.17
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 9.92s
                        Total time: 28706.69s
                               ETA: 1000586.2s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.775s, learning 0.160s)
               Value function loss: 136.5186
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 514.38
               Mean episode length: 149.30
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 9.94s
                        Total time: 28716.63s
                               ETA: 1000563.5s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.655s, learning 0.172s)
               Value function loss: 135.9450
                    Surrogate loss: -0.0195
             Mean action noise std: 0.74
                       Mean reward: 551.55
               Mean episode length: 149.11
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 9.83s
                        Total time: 28726.45s
                               ETA: 1000536.9s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.584s, learning 0.161s)
               Value function loss: 126.2682
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 519.22
               Mean episode length: 149.97
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 9.74s
                        Total time: 28736.20s
                               ETA: 1000507.6s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.587s, learning 0.170s)
               Value function loss: 125.9945
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 523.43
               Mean episode length: 149.71
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 9.76s
                        Total time: 28745.96s
                               ETA: 1000478.6s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.632s, learning 0.161s)
               Value function loss: 130.0554
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 550.70
               Mean episode length: 149.03
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 9.79s
                        Total time: 28755.75s
                               ETA: 1000451.0s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1621 steps/s (collection: 9.941s, learning 0.162s)
               Value function loss: 138.0143
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 547.45
               Mean episode length: 148.31
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 10.10s
                        Total time: 28765.85s
                               ETA: 1000434.1s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.621s, learning 0.173s)
               Value function loss: 142.2092
                    Surrogate loss: -0.0220
             Mean action noise std: 0.74
                       Mean reward: 552.78
               Mean episode length: 148.95
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 16.79s
                        Total time: 28782.65s
                               ETA: 1000649.9s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 873 steps/s (collection: 18.594s, learning 0.165s)
               Value function loss: 141.6703
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 567.65
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 18.76s
                        Total time: 28801.40s
                               ETA: 1000933.7s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 869 steps/s (collection: 18.682s, learning 0.169s)
               Value function loss: 162.1865
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 567.89
               Mean episode length: 149.06
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 18.85s
                        Total time: 28820.25s
                               ETA: 1001220.6s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.507s, learning 0.166s)
               Value function loss: 143.9723
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 579.26
               Mean episode length: 149.82
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 18.67s
                        Total time: 28838.93s
                               ETA: 1001501.1s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 886 steps/s (collection: 18.310s, learning 0.182s)
               Value function loss: 158.0193
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 571.67
               Mean episode length: 150.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 18.49s
                        Total time: 28857.42s
                               ETA: 1001775.0s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.509s, learning 0.165s)
               Value function loss: 194.7473
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 571.49
               Mean episode length: 148.76
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 18.67s
                        Total time: 28876.09s
                               ETA: 1002055.1s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 868 steps/s (collection: 18.698s, learning 0.169s)
               Value function loss: 155.3547
                    Surrogate loss: 0.0098
             Mean action noise std: 0.74
                       Mean reward: 544.42
               Mean episode length: 148.96
                  Mean reward/step: 3.60
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 18.87s
                        Total time: 28894.96s
                               ETA: 1002341.6s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 853 steps/s (collection: 19.026s, learning 0.172s)
               Value function loss: 154.5699
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 527.55
               Mean episode length: 149.24
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 19.20s
                        Total time: 28914.16s
                               ETA: 1002639.4s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 875 steps/s (collection: 18.549s, learning 0.163s)
               Value function loss: 170.0495
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 552.59
               Mean episode length: 148.33
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 18.71s
                        Total time: 28932.87s
                               ETA: 1002920.2s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 865 steps/s (collection: 18.753s, learning 0.167s)
               Value function loss: 127.5186
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 568.18
               Mean episode length: 149.63
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 18.92s
                        Total time: 28951.79s
                               ETA: 1003207.9s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.881s, learning 0.162s)
               Value function loss: 142.2046
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 551.47
               Mean episode length: 149.85
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 19.04s
                        Total time: 28970.83s
                               ETA: 1003499.7s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 881 steps/s (collection: 18.418s, learning 0.165s)
               Value function loss: 127.6627
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 537.67
               Mean episode length: 149.30
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 18.58s
                        Total time: 28989.42s
                               ETA: 1003775.3s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 861 steps/s (collection: 18.841s, learning 0.172s)
               Value function loss: 141.1390
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 540.11
               Mean episode length: 149.25
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 19.01s
                        Total time: 29008.43s
                               ETA: 1004065.6s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 883 steps/s (collection: 18.394s, learning 0.159s)
               Value function loss: 133.4918
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 549.25
               Mean episode length: 147.52
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 18.55s
                        Total time: 29026.98s
                               ETA: 1004339.8s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 866 steps/s (collection: 18.729s, learning 0.171s)
               Value function loss: 119.5330
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 549.65
               Mean episode length: 149.18
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 18.90s
                        Total time: 29045.88s
                               ETA: 1004625.8s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.559s, learning 0.167s)
               Value function loss: 126.6806
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 567.90
               Mean episode length: 149.28
                  Mean reward/step: 3.63
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 18.73s
                        Total time: 29064.61s
                               ETA: 1004905.5s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 857 steps/s (collection: 18.928s, learning 0.170s)
               Value function loss: 127.8941
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 542.73
               Mean episode length: 149.61
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 19.10s
                        Total time: 29083.71s
                               ETA: 1005197.9s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.524s, learning 0.166s)
               Value function loss: 119.1061
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 532.28
               Mean episode length: 148.04
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 18.69s
                        Total time: 29102.40s
                               ETA: 1005475.9s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 852 steps/s (collection: 19.015s, learning 0.195s)
               Value function loss: 123.2885
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 534.38
               Mean episode length: 149.22
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 19.21s
                        Total time: 29121.61s
                               ETA: 1005771.7s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 864 steps/s (collection: 18.796s, learning 0.163s)
               Value function loss: 130.4538
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 529.96
               Mean episode length: 149.53
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 18.96s
                        Total time: 29140.56s
                               ETA: 1006058.6s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 862 steps/s (collection: 18.823s, learning 0.162s)
               Value function loss: 137.8104
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 512.71
               Mean episode length: 148.25
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 18.99s
                        Total time: 29159.55s
                               ETA: 1006346.2s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.403s, learning 0.163s)
               Value function loss: 136.3306
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 542.46
               Mean episode length: 147.68
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 18.57s
                        Total time: 29178.12s
                               ETA: 1006619.1s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 878 steps/s (collection: 18.483s, learning 0.168s)
               Value function loss: 133.7825
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 532.53
               Mean episode length: 149.10
                  Mean reward/step: 3.67
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 18.65s
                        Total time: 29196.77s
                               ETA: 1006894.8s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 878 steps/s (collection: 18.482s, learning 0.162s)
               Value function loss: 162.6745
                    Surrogate loss: 0.0241
             Mean action noise std: 0.74
                       Mean reward: 514.84
               Mean episode length: 149.76
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 18.64s
                        Total time: 29215.41s
                               ETA: 1007170.0s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 856 steps/s (collection: 18.958s, learning 0.161s)
               Value function loss: 143.1160
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 581.31
               Mean episode length: 150.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 19.12s
                        Total time: 29234.53s
                               ETA: 1007461.3s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.881s, learning 0.162s)
               Value function loss: 122.1274
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 568.79
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 19.04s
                        Total time: 29253.57s
                               ETA: 1007749.8s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 871 steps/s (collection: 18.633s, learning 0.163s)
               Value function loss: 154.8641
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 581.27
               Mean episode length: 150.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 18.80s
                        Total time: 29272.37s
                               ETA: 1008029.6s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 872 steps/s (collection: 18.598s, learning 0.173s)
               Value function loss: 137.8072
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 505.19
               Mean episode length: 148.48
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 18.77s
                        Total time: 29291.14s
                               ETA: 1008308.3s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.869s, learning 0.179s)
               Value function loss: 148.9053
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 554.85
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 19.05s
                        Total time: 29310.19s
                               ETA: 1008596.3s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 864 steps/s (collection: 18.772s, learning 0.175s)
               Value function loss: 154.3259
                    Surrogate loss: 0.0015
             Mean action noise std: 0.74
                       Mean reward: 566.81
               Mean episode length: 148.65
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 18.95s
                        Total time: 29329.13s
                               ETA: 1008880.7s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 884 steps/s (collection: 18.343s, learning 0.180s)
               Value function loss: 147.4268
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 547.88
               Mean episode length: 150.00
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 18.52s
                        Total time: 29347.66s
                               ETA: 1009150.2s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 866 steps/s (collection: 18.738s, learning 0.177s)
               Value function loss: 153.0336
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 548.26
               Mean episode length: 149.84
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 18.91s
                        Total time: 29366.57s
                               ETA: 1009433.0s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 875 steps/s (collection: 18.534s, learning 0.177s)
               Value function loss: 163.7319
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 568.97
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 18.71s
                        Total time: 29385.28s
                               ETA: 1009708.6s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 880 steps/s (collection: 18.440s, learning 0.166s)
               Value function loss: 155.8209
                    Surrogate loss: 0.0037
             Mean action noise std: 0.74
                       Mean reward: 557.05
               Mean episode length: 147.76
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 18.61s
                        Total time: 29403.89s
                               ETA: 1009980.4s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 863 steps/s (collection: 18.812s, learning 0.165s)
               Value function loss: 152.5157
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 575.28
               Mean episode length: 149.79
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 18.98s
                        Total time: 29422.86s
                               ETA: 1010264.7s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.521s, learning 0.166s)
               Value function loss: 164.4329
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 562.08
               Mean episode length: 149.08
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 18.69s
                        Total time: 29441.55s
                               ETA: 1010538.9s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 878 steps/s (collection: 18.472s, learning 0.172s)
               Value function loss: 174.1838
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 572.83
               Mean episode length: 147.74
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 18.64s
                        Total time: 29460.19s
                               ETA: 1010811.3s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 931 steps/s (collection: 17.431s, learning 0.166s)
               Value function loss: 170.3615
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 609.85
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 17.60s
                        Total time: 29477.79s
                               ETA: 1011047.7s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.664s, learning 0.183s)
               Value function loss: 165.1684
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 549.24
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 9.85s
                        Total time: 29487.64s
                               ETA: 1011018.1s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.410s, learning 0.182s)
               Value function loss: 154.6210
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 576.94
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 9.59s
                        Total time: 29497.23s
                               ETA: 1010979.8s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.816s, learning 0.171s)
               Value function loss: 201.5275
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 598.21
               Mean episode length: 149.17
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 9.99s
                        Total time: 29507.22s
                               ETA: 1010955.1s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.669s, learning 0.180s)
               Value function loss: 173.7647
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 562.53
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 9.85s
                        Total time: 29517.07s
                               ETA: 1010925.7s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.469s, learning 0.188s)
               Value function loss: 216.3696
                    Surrogate loss: 0.0024
             Mean action noise std: 0.74
                       Mean reward: 591.92
               Mean episode length: 149.29
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 9.66s
                        Total time: 29526.72s
                               ETA: 1010889.7s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.451s, learning 0.189s)
               Value function loss: 157.0904
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 560.36
               Mean episode length: 149.38
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 9.64s
                        Total time: 29536.36s
                               ETA: 1010853.1s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.292s, learning 0.161s)
               Value function loss: 125.7152
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 541.52
               Mean episode length: 148.95
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 9.45s
                        Total time: 29545.82s
                               ETA: 1010810.2s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.555s, learning 0.171s)
               Value function loss: 153.6208
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 575.78
               Mean episode length: 149.17
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 9.73s
                        Total time: 29555.54s
                               ETA: 1010776.6s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1626 steps/s (collection: 9.902s, learning 0.173s)
               Value function loss: 174.5607
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 585.72
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 10.08s
                        Total time: 29565.62s
                               ETA: 1010755.0s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.473s, learning 0.171s)
               Value function loss: 155.4232
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 577.31
               Mean episode length: 148.92
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 9.64s
                        Total time: 29575.26s
                               ETA: 1010718.7s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.331s, learning 0.175s)
               Value function loss: 141.7308
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 557.50
               Mean episode length: 149.35
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 9.51s
                        Total time: 29584.77s
                               ETA: 1010677.6s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.485s, learning 0.157s)
               Value function loss: 152.8836
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 584.64
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 9.64s
                        Total time: 29594.41s
                               ETA: 1010641.2s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.708s, learning 0.160s)
               Value function loss: 169.3470
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 570.01
               Mean episode length: 149.13
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 9.87s
                        Total time: 29604.28s
                               ETA: 1010612.6s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.211s, learning 0.171s)
               Value function loss: 176.0355
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 584.36
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 9.38s
                        Total time: 29613.66s
                               ETA: 1010567.4s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.640s, learning 0.171s)
               Value function loss: 154.6739
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 590.05
               Mean episode length: 149.29
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 9.81s
                        Total time: 29623.47s
                               ETA: 1010536.8s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.338s, learning 0.162s)
               Value function loss: 135.6860
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 564.91
               Mean episode length: 149.45
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 9.50s
                        Total time: 29632.97s
                               ETA: 1010495.7s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.858s, learning 0.171s)
               Value function loss: 134.0763
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 584.60
               Mean episode length: 149.27
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 10.03s
                        Total time: 29643.00s
                               ETA: 1010472.6s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.349s, learning 0.167s)
               Value function loss: 144.0262
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 563.18
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 9.52s
                        Total time: 29652.51s
                               ETA: 1010432.1s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.783s, learning 0.164s)
               Value function loss: 139.9037
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 559.16
               Mean episode length: 148.11
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 9.95s
                        Total time: 29662.46s
                               ETA: 1010406.2s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.532s, learning 0.164s)
               Value function loss: 152.5128
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 576.45
               Mean episode length: 149.19
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 9.70s
                        Total time: 29672.16s
                               ETA: 1010371.8s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.468s, learning 0.164s)
               Value function loss: 182.0369
                    Surrogate loss: 0.0003
             Mean action noise std: 0.74
                       Mean reward: 612.09
               Mean episode length: 150.00
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 9.63s
                        Total time: 29681.79s
                               ETA: 1010335.2s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.866s, learning 0.159s)
               Value function loss: 189.9866
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 585.71
               Mean episode length: 149.05
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 10.03s
                        Total time: 29691.81s
                               ETA: 1010312.1s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.730s, learning 0.171s)
               Value function loss: 160.0194
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 574.94
               Mean episode length: 147.61
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 9.90s
                        Total time: 29701.71s
                               ETA: 1010284.7s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.389s, learning 0.157s)
               Value function loss: 191.7280
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 587.72
               Mean episode length: 148.36
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 9.55s
                        Total time: 29711.26s
                               ETA: 1010245.3s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.623s, learning 0.173s)
               Value function loss: 176.8833
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 578.33
               Mean episode length: 147.44
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 9.80s
                        Total time: 29721.06s
                               ETA: 1010214.3s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1781 steps/s (collection: 9.034s, learning 0.160s)
               Value function loss: 137.5325
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 580.96
               Mean episode length: 147.08
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 9.19s
                        Total time: 29730.25s
                               ETA: 1010163.0s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.352s, learning 0.163s)
               Value function loss: 143.4976
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 629.78
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 9.52s
                        Total time: 29739.77s
                               ETA: 1010122.6s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1644 steps/s (collection: 9.795s, learning 0.167s)
               Value function loss: 134.5826
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 568.86
               Mean episode length: 148.53
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 9.96s
                        Total time: 29749.73s
                               ETA: 1010097.4s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.381s, learning 0.158s)
               Value function loss: 142.6970
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 574.17
               Mean episode length: 149.01
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 9.54s
                        Total time: 29759.27s
                               ETA: 1010057.8s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.474s, learning 0.169s)
               Value function loss: 136.0744
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 585.03
               Mean episode length: 149.15
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 9.64s
                        Total time: 29768.91s
                               ETA: 1010021.8s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.545s, learning 0.168s)
               Value function loss: 142.8857
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 583.37
               Mean episode length: 149.93
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 9.71s
                        Total time: 29778.62s
                               ETA: 1009988.1s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.606s, learning 0.161s)
               Value function loss: 157.2801
                    Surrogate loss: -0.0012
             Mean action noise std: 0.74
                       Mean reward: 553.00
               Mean episode length: 149.39
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 9.77s
                        Total time: 29788.39s
                               ETA: 1009956.3s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.315s, learning 0.164s)
               Value function loss: 160.6248
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 620.71
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 9.48s
                        Total time: 29797.87s
                               ETA: 1009914.8s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.667s, learning 0.163s)
               Value function loss: 161.5548
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 600.37
               Mean episode length: 148.20
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 9.83s
                        Total time: 29807.70s
                               ETA: 1009885.2s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.424s, learning 0.161s)
               Value function loss: 137.2531
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 590.02
               Mean episode length: 148.90
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 9.58s
                        Total time: 29817.28s
                               ETA: 1009847.3s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.485s, learning 0.167s)
               Value function loss: 124.3445
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 593.05
               Mean episode length: 148.47
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 9.65s
                        Total time: 29826.94s
                               ETA: 1009811.7s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.248s, learning 0.166s)
               Value function loss: 130.6248
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 579.03
               Mean episode length: 148.62
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 9.41s
                        Total time: 29836.35s
                               ETA: 1009768.1s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.570s, learning 0.165s)
               Value function loss: 151.0945
                    Surrogate loss: -0.0002
             Mean action noise std: 0.74
                       Mean reward: 587.62
               Mean episode length: 148.52
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 9.73s
                        Total time: 29846.08s
                               ETA: 1009735.3s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.488s, learning 0.162s)
               Value function loss: 139.0440
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 614.52
               Mean episode length: 149.69
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 9.65s
                        Total time: 29855.73s
                               ETA: 1009699.7s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.522s, learning 0.171s)
               Value function loss: 129.9046
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 583.37
               Mean episode length: 149.35
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 9.69s
                        Total time: 29865.43s
                               ETA: 1009665.5s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.246s, learning 0.158s)
               Value function loss: 137.1088
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 598.25
               Mean episode length: 148.20
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 9.40s
                        Total time: 29874.83s
                               ETA: 1009621.6s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.419s, learning 0.169s)
               Value function loss: 146.0408
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 576.33
               Mean episode length: 149.19
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 9.59s
                        Total time: 29884.42s
                               ETA: 1009584.0s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.327s, learning 0.164s)
               Value function loss: 140.7146
                    Surrogate loss: 0.0014
             Mean action noise std: 0.74
                       Mean reward: 597.61
               Mean episode length: 149.91
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 9.49s
                        Total time: 29893.91s
                               ETA: 1009543.1s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.412s, learning 0.158s)
               Value function loss: 128.9346
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 630.52
               Mean episode length: 149.54
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 9.57s
                        Total time: 29903.48s
                               ETA: 1009504.9s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.342s, learning 0.160s)
               Value function loss: 129.0800
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 604.18
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 9.50s
                        Total time: 29912.98s
                               ETA: 1009464.4s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.248s, learning 0.162s)
               Value function loss: 120.4943
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 585.38
               Mean episode length: 148.12
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 9.41s
                        Total time: 29922.39s
                               ETA: 1009420.8s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.846s, learning 0.171s)
               Value function loss: 118.1310
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 608.05
               Mean episode length: 149.52
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 10.02s
                        Total time: 29932.41s
                               ETA: 1009397.7s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.433s, learning 0.167s)
               Value function loss: 133.3752
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 587.44
               Mean episode length: 149.68
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 9.60s
                        Total time: 29942.01s
                               ETA: 1009360.5s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.446s, learning 0.164s)
               Value function loss: 134.4072
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 598.59
               Mean episode length: 149.85
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 9.61s
                        Total time: 29951.62s
                               ETA: 1009323.8s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.478s, learning 0.175s)
               Value function loss: 127.4410
                    Surrogate loss: 0.0018
             Mean action noise std: 0.74
                       Mean reward: 613.66
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 9.65s
                        Total time: 29961.27s
                               ETA: 1009288.4s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.527s, learning 0.160s)
               Value function loss: 104.2028
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 600.38
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 9.69s
                        Total time: 29970.96s
                               ETA: 1009254.3s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.560s, learning 0.173s)
               Value function loss: 98.8966
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 570.95
               Mean episode length: 149.22
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 9.73s
                        Total time: 29980.69s
                               ETA: 1009221.7s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.385s, learning 0.171s)
               Value function loss: 114.0219
                    Surrogate loss: -0.0162
             Mean action noise std: 0.74
                       Mean reward: 579.12
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 9.56s
                        Total time: 29990.24s
                               ETA: 1009183.1s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.684s, learning 0.168s)
               Value function loss: 118.5822
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 593.78
               Mean episode length: 149.50
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 9.85s
                        Total time: 30000.10s
                               ETA: 1009154.6s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.562s, learning 0.182s)
               Value function loss: 112.4430
                    Surrogate loss: 0.0030
             Mean action noise std: 0.74
                       Mean reward: 594.81
               Mean episode length: 149.90
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 9.74s
                        Total time: 30009.84s
                               ETA: 1009122.4s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.428s, learning 0.162s)
               Value function loss: 114.7181
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 549.09
               Mean episode length: 148.56
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 9.59s
                        Total time: 30019.43s
                               ETA: 1009085.1s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.031s, learning 0.176s)
               Value function loss: 141.9970
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: 580.73
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 9.21s
                        Total time: 30028.64s
                               ETA: 1009034.9s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.112s, learning 0.161s)
               Value function loss: 143.7233
                    Surrogate loss: -0.0012
             Mean action noise std: 0.74
                       Mean reward: 569.86
               Mean episode length: 149.05
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 9.27s
                        Total time: 30037.91s
                               ETA: 1008987.0s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.445s, learning 0.169s)
               Value function loss: 156.7190
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: 594.38
               Mean episode length: 149.67
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 9.61s
                        Total time: 30047.52s
                               ETA: 1008950.6s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.302s, learning 0.172s)
               Value function loss: 162.6254
                    Surrogate loss: 0.0057
             Mean action noise std: 0.74
                       Mean reward: 574.19
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 9.47s
                        Total time: 30057.00s
                               ETA: 1008909.4s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.133s, learning 0.171s)
               Value function loss: 185.4762
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 572.76
               Mean episode length: 149.69
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 9.30s
                        Total time: 30066.30s
                               ETA: 1008862.6s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.234s, learning 0.169s)
               Value function loss: 185.9153
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: 601.71
               Mean episode length: 149.55
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 9.40s
                        Total time: 30075.71s
                               ETA: 1008819.2s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.492s, learning 0.168s)
               Value function loss: 145.4437
                    Surrogate loss: 0.0039
             Mean action noise std: 0.74
                       Mean reward: 601.49
               Mean episode length: 150.00
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 9.66s
                        Total time: 30085.37s
                               ETA: 1008784.4s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.354s, learning 0.160s)
               Value function loss: 149.6170
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 603.34
               Mean episode length: 149.53
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 9.51s
                        Total time: 30094.88s
                               ETA: 1008744.7s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1770 steps/s (collection: 9.090s, learning 0.164s)
               Value function loss: 157.0329
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 595.67
               Mean episode length: 150.00
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 9.25s
                        Total time: 30104.14s
                               ETA: 1008696.3s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.501s, learning 0.166s)
               Value function loss: 153.5769
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 592.86
               Mean episode length: 150.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 9.67s
                        Total time: 30113.80s
                               ETA: 1008661.7s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1636 steps/s (collection: 9.850s, learning 0.161s)
               Value function loss: 171.4312
                    Surrogate loss: 0.0141
             Mean action noise std: 0.74
                       Mean reward: 574.69
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 10.01s
                        Total time: 30123.81s
                               ETA: 1008638.7s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.413s, learning 0.161s)
               Value function loss: 147.4253
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 573.48
               Mean episode length: 149.30
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 9.57s
                        Total time: 30133.39s
                               ETA: 1008601.1s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.448s, learning 0.171s)
               Value function loss: 148.6472
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 576.20
               Mean episode length: 150.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 9.62s
                        Total time: 30143.01s
                               ETA: 1008565.0s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.405s, learning 0.175s)
               Value function loss: 150.3062
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 586.24
               Mean episode length: 149.20
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 9.58s
                        Total time: 30152.58s
                               ETA: 1008527.6s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.313s, learning 0.163s)
               Value function loss: 166.0200
                    Surrogate loss: 0.0020
             Mean action noise std: 0.74
                       Mean reward: 576.16
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 9.48s
                        Total time: 30162.06s
                               ETA: 1008486.8s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.045s, learning 0.163s)
               Value function loss: 145.7460
                    Surrogate loss: 0.0016
             Mean action noise std: 0.74
                       Mean reward: 561.70
               Mean episode length: 149.47
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 9.21s
                        Total time: 30171.27s
                               ETA: 1008437.0s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.390s, learning 0.166s)
               Value function loss: 150.6655
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 554.60
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 9.56s
                        Total time: 30180.82s
                               ETA: 1008398.9s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.375s, learning 0.162s)
               Value function loss: 156.0105
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 562.49
               Mean episode length: 150.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 9.54s
                        Total time: 30190.36s
                               ETA: 1008360.2s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.416s, learning 0.165s)
               Value function loss: 145.2860
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 559.77
               Mean episode length: 149.63
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 9.58s
                        Total time: 30199.94s
                               ETA: 1008322.9s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.450s, learning 0.175s)
               Value function loss: 153.3435
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 562.04
               Mean episode length: 148.93
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 9.63s
                        Total time: 30209.57s
                               ETA: 1008287.2s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.518s, learning 0.165s)
               Value function loss: 136.4668
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 526.99
               Mean episode length: 149.57
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 9.68s
                        Total time: 30219.25s
                               ETA: 1008253.4s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.655s, learning 0.158s)
               Value function loss: 167.1434
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 563.83
               Mean episode length: 150.00
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 9.81s
                        Total time: 30229.07s
                               ETA: 1008224.0s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.553s, learning 0.161s)
               Value function loss: 145.0267
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 551.69
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 9.71s
                        Total time: 30238.78s
                               ETA: 1008191.2s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.266s, learning 0.160s)
               Value function loss: 181.5054
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 558.21
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 9.43s
                        Total time: 30248.21s
                               ETA: 1008148.9s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.468s, learning 0.160s)
               Value function loss: 162.8718
                    Surrogate loss: 0.0162
             Mean action noise std: 0.74
                       Mean reward: 562.86
               Mean episode length: 148.94
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 9.63s
                        Total time: 30257.83s
                               ETA: 1008113.3s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.175s, learning 0.168s)
               Value function loss: 114.7478
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 562.90
               Mean episode length: 149.01
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 9.34s
                        Total time: 30267.18s
                               ETA: 1008068.3s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.658s, learning 0.164s)
               Value function loss: 139.8386
                    Surrogate loss: -0.0016
             Mean action noise std: 0.74
                       Mean reward: 562.79
               Mean episode length: 150.00
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 9.82s
                        Total time: 30277.00s
                               ETA: 1008039.2s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1783 steps/s (collection: 9.020s, learning 0.166s)
               Value function loss: 143.6548
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 566.50
               Mean episode length: 149.78
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 9.19s
                        Total time: 30286.18s
                               ETA: 1007989.0s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.710s, learning 0.163s)
               Value function loss: 146.3569
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 595.50
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 9.87s
                        Total time: 30296.06s
                               ETA: 1007961.6s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.163s, learning 0.161s)
               Value function loss: 140.6655
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 577.64
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 9.32s
                        Total time: 30305.38s
                               ETA: 1007916.1s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.222s, learning 0.161s)
               Value function loss: 149.1171
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 593.21
               Mean episode length: 150.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 9.38s
                        Total time: 30314.76s
                               ETA: 1007872.5s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.319s, learning 0.162s)
               Value function loss: 151.7769
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 591.06
               Mean episode length: 149.67
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 9.48s
                        Total time: 30324.24s
                               ETA: 1007832.1s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.317s, learning 0.170s)
               Value function loss: 121.2677
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 571.73
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 9.49s
                        Total time: 30333.73s
                               ETA: 1007792.0s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.514s, learning 0.159s)
               Value function loss: 127.1724
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 566.38
               Mean episode length: 149.28
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 9.67s
                        Total time: 30343.40s
                               ETA: 1007758.1s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.720s, learning 0.157s)
               Value function loss: 126.6036
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 596.99
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 9.88s
                        Total time: 30353.28s
                               ETA: 1007731.0s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.758s, learning 0.165s)
               Value function loss: 119.3363
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 588.55
               Mean episode length: 149.24
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 9.92s
                        Total time: 30363.21s
                               ETA: 1007705.5s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.701s, learning 0.166s)
               Value function loss: 134.7180
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 584.45
               Mean episode length: 149.78
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 9.87s
                        Total time: 30373.07s
                               ETA: 1007678.1s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.498s, learning 0.186s)
               Value function loss: 133.7166
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 578.88
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 9.68s
                        Total time: 30382.76s
                               ETA: 1007644.6s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.874s, learning 0.165s)
               Value function loss: 128.4917
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 596.40
               Mean episode length: 149.43
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 9.04s
                        Total time: 30391.79s
                               ETA: 1007589.7s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.509s, learning 0.161s)
               Value function loss: 148.5081
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 604.99
               Mean episode length: 149.04
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 9.67s
                        Total time: 30401.46s
                               ETA: 1007555.8s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.211s, learning 0.162s)
               Value function loss: 144.7690
                    Surrogate loss: 0.0012
             Mean action noise std: 0.74
                       Mean reward: 559.88
               Mean episode length: 149.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 9.37s
                        Total time: 30410.84s
                               ETA: 1007512.1s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.723s, learning 0.173s)
               Value function loss: 112.2668
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 586.29
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 9.90s
                        Total time: 30420.73s
                               ETA: 1007485.7s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.294s, learning 0.166s)
               Value function loss: 147.1995
                    Surrogate loss: 0.0017
             Mean action noise std: 0.74
                       Mean reward: 622.48
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 9.46s
                        Total time: 30430.19s
                               ETA: 1007444.9s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.530s, learning 0.173s)
               Value function loss: 109.3525
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 565.77
               Mean episode length: 149.56
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 9.70s
                        Total time: 30439.90s
                               ETA: 1007412.2s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.324s, learning 0.176s)
               Value function loss: 108.5519
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 592.77
               Mean episode length: 149.56
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 9.50s
                        Total time: 30449.40s
                               ETA: 1007372.7s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.141s, learning 0.178s)
               Value function loss: 113.7150
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 618.17
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 9.32s
                        Total time: 30458.72s
                               ETA: 1007327.3s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.392s, learning 0.175s)
               Value function loss: 104.0175
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 591.96
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 9.57s
                        Total time: 30468.28s
                               ETA: 1007290.2s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.444s, learning 0.164s)
               Value function loss: 111.8758
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 628.39
               Mean episode length: 150.00
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 9.61s
                        Total time: 30477.89s
                               ETA: 1007254.4s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.228s, learning 0.172s)
               Value function loss: 104.5733
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 568.56
               Mean episode length: 149.21
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 9.40s
                        Total time: 30487.29s
                               ETA: 1007211.7s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.467s, learning 0.170s)
               Value function loss: 101.8160
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 615.61
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 9.64s
                        Total time: 30496.93s
                               ETA: 1007176.9s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.262s, learning 0.182s)
               Value function loss: 126.5294
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 606.58
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 9.44s
                        Total time: 30506.37s
                               ETA: 1007135.7s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.527s, learning 0.163s)
               Value function loss: 107.1051
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 561.25
               Mean episode length: 149.24
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 9.69s
                        Total time: 30516.06s
                               ETA: 1007102.7s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.434s, learning 0.160s)
               Value function loss: 119.1866
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 565.84
               Mean episode length: 149.17
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 9.59s
                        Total time: 30525.66s
                               ETA: 1007066.5s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.798s, learning 0.181s)
               Value function loss: 104.2261
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 588.88
               Mean episode length: 149.54
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 8.98s
                        Total time: 30534.64s
                               ETA: 1007010.1s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.005s, learning 0.172s)
               Value function loss: 107.9743
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 576.00
               Mean episode length: 149.75
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 9.18s
                        Total time: 30543.81s
                               ETA: 1006960.2s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1783 steps/s (collection: 9.018s, learning 0.170s)
               Value function loss: 117.0215
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 612.31
               Mean episode length: 149.58
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 9.19s
                        Total time: 30553.00s
                               ETA: 1006910.7s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.540s, learning 0.167s)
               Value function loss: 115.9375
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 581.85
               Mean episode length: 149.49
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 9.71s
                        Total time: 30562.71s
                               ETA: 1006878.3s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.459s, learning 0.162s)
               Value function loss: 123.3245
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 602.86
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 9.62s
                        Total time: 30572.33s
                               ETA: 1006843.1s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.294s, learning 0.163s)
               Value function loss: 129.9279
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 618.00
               Mean episode length: 149.71
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 9.46s
                        Total time: 30581.79s
                               ETA: 1006802.6s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.997s, learning 0.163s)
               Value function loss: 105.3163
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 581.64
               Mean episode length: 149.16
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 9.16s
                        Total time: 30590.95s
                               ETA: 1006752.3s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.862s, learning 0.170s)
               Value function loss: 110.7708
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 602.07
               Mean episode length: 149.93
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 10.03s
                        Total time: 30600.98s
                               ETA: 1006730.7s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.261s, learning 0.170s)
               Value function loss: 128.6816
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 610.69
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 9.43s
                        Total time: 30610.41s
                               ETA: 1006689.3s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.465s, learning 0.183s)
               Value function loss: 99.9705
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 600.40
               Mean episode length: 149.49
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 9.65s
                        Total time: 30620.06s
                               ETA: 1006655.1s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.524s, learning 0.171s)
               Value function loss: 111.9424
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 605.34
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 9.69s
                        Total time: 30629.75s
                               ETA: 1006622.4s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.518s, learning 0.173s)
               Value function loss: 119.1101
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 603.41
               Mean episode length: 149.33
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 9.69s
                        Total time: 30639.44s
                               ETA: 1006589.7s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.385s, learning 0.171s)
               Value function loss: 111.0825
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 592.77
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 9.56s
                        Total time: 30649.00s
                               ETA: 1006552.5s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.239s, learning 0.161s)
               Value function loss: 131.3815
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 614.49
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 9.40s
                        Total time: 30658.40s
                               ETA: 1006510.2s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.521s, learning 0.159s)
               Value function loss: 129.9960
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 615.33
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 9.68s
                        Total time: 30668.08s
                               ETA: 1006477.1s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.198s, learning 0.159s)
               Value function loss: 115.8354
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 612.70
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 9.36s
                        Total time: 30677.43s
                               ETA: 1006433.5s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.611s, learning 0.163s)
               Value function loss: 124.0124
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 586.34
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 9.77s
                        Total time: 30687.21s
                               ETA: 1006403.5s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1788 steps/s (collection: 9.001s, learning 0.160s)
               Value function loss: 105.9141
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 591.70
               Mean episode length: 149.46
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 9.16s
                        Total time: 30696.37s
                               ETA: 1006353.5s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.396s, learning 0.159s)
               Value function loss: 125.9402
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 606.94
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 9.55s
                        Total time: 30705.92s
                               ETA: 1006316.4s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.694s, learning 0.159s)
               Value function loss: 120.9686
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 605.11
               Mean episode length: 149.73
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 9.85s
                        Total time: 30715.78s
                               ETA: 1006289.1s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.557s, learning 0.159s)
               Value function loss: 108.8769
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 599.11
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 9.72s
                        Total time: 30725.49s
                               ETA: 1006257.3s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.643s, learning 0.162s)
               Value function loss: 136.0401
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 612.82
               Mean episode length: 148.62
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 9.80s
                        Total time: 30735.30s
                               ETA: 1006228.4s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1618 steps/s (collection: 9.947s, learning 0.173s)
               Value function loss: 132.4272
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 604.64
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 10.12s
                        Total time: 30745.42s
                               ETA: 1006209.9s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.100s, learning 0.167s)
               Value function loss: 144.1492
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 603.42
               Mean episode length: 148.97
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 9.27s
                        Total time: 30754.68s
                               ETA: 1006163.5s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.629s, learning 0.175s)
               Value function loss: 146.8061
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 625.03
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 9.80s
                        Total time: 30764.49s
                               ETA: 1006134.6s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1756 steps/s (collection: 9.169s, learning 0.160s)
               Value function loss: 136.5491
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 620.60
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 9.33s
                        Total time: 30773.82s
                               ETA: 1006090.2s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.319s, learning 0.171s)
               Value function loss: 150.4608
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 599.57
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 9.49s
                        Total time: 30783.31s
                               ETA: 1006051.1s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.617s, learning 0.161s)
               Value function loss: 127.4721
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 611.54
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 9.78s
                        Total time: 30793.09s
                               ETA: 1006021.5s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.615s, learning 0.159s)
               Value function loss: 119.9326
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 630.16
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 9.77s
                        Total time: 30802.86s
                               ETA: 1005991.7s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.627s, learning 0.158s)
               Value function loss: 142.5191
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 612.59
               Mean episode length: 149.66
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 9.79s
                        Total time: 30812.64s
                               ETA: 1005962.3s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.665s, learning 0.162s)
               Value function loss: 137.6832
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 625.96
               Mean episode length: 149.41
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 9.83s
                        Total time: 30822.47s
                               ETA: 1005934.3s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.589s, learning 0.161s)
               Value function loss: 139.2500
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 634.14
               Mean episode length: 148.57
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 9.75s
                        Total time: 30832.22s
                               ETA: 1005903.8s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.248s, learning 0.166s)
               Value function loss: 138.5464
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 631.84
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 9.41s
                        Total time: 30841.63s
                               ETA: 1005862.3s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.700s, learning 0.174s)
               Value function loss: 145.2613
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 621.86
               Mean episode length: 148.53
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 9.87s
                        Total time: 30851.51s
                               ETA: 1005835.9s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1771 steps/s (collection: 9.092s, learning 0.158s)
               Value function loss: 148.6936
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 621.98
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 9.25s
                        Total time: 30860.76s
                               ETA: 1005789.1s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.595s, learning 0.159s)
               Value function loss: 135.3563
                    Surrogate loss: 0.0052
             Mean action noise std: 0.74
                       Mean reward: 614.17
               Mean episode length: 149.54
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 9.75s
                        Total time: 30870.51s
                               ETA: 1005758.8s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.494s, learning 0.167s)
               Value function loss: 136.3422
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 621.75
               Mean episode length: 148.66
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 9.66s
                        Total time: 30880.17s
                               ETA: 1005725.5s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.369s, learning 0.158s)
               Value function loss: 116.5409
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 592.88
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 9.53s
                        Total time: 30889.70s
                               ETA: 1005687.8s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.829s, learning 0.165s)
               Value function loss: 109.2434
                    Surrogate loss: 0.0044
             Mean action noise std: 0.74
                       Mean reward: 612.23
               Mean episode length: 149.09
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 9.99s
                        Total time: 30899.69s
                               ETA: 1005665.3s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.442s, learning 0.170s)
               Value function loss: 119.6058
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 628.14
               Mean episode length: 149.15
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 9.61s
                        Total time: 30909.31s
                               ETA: 1005630.5s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.307s, learning 0.166s)
               Value function loss: 119.6652
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 614.02
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 9.47s
                        Total time: 30918.78s
                               ETA: 1005591.1s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.510s, learning 0.169s)
               Value function loss: 126.1844
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 616.02
               Mean episode length: 149.06
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 9.68s
                        Total time: 30928.46s
                               ETA: 1005558.4s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1614 steps/s (collection: 9.970s, learning 0.176s)
               Value function loss: 112.3082
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 600.58
               Mean episode length: 148.96
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 10.15s
                        Total time: 30938.60s
                               ETA: 1005540.9s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.989s, learning 0.165s)
               Value function loss: 143.1766
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 628.00
               Mean episode length: 149.12
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 9.15s
                        Total time: 30947.76s
                               ETA: 1005491.2s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.690s, learning 0.160s)
               Value function loss: 108.3994
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 616.84
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 9.85s
                        Total time: 30957.61s
                               ETA: 1005464.2s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.427s, learning 0.167s)
               Value function loss: 124.1092
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 600.71
               Mean episode length: 149.46
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 9.59s
                        Total time: 30967.20s
                               ETA: 1005428.8s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.377s, learning 0.166s)
               Value function loss: 119.8847
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 610.73
               Mean episode length: 149.52
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 9.54s
                        Total time: 30976.75s
                               ETA: 1005391.8s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.560s, learning 0.176s)
               Value function loss: 103.5306
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 630.08
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 9.74s
                        Total time: 30986.48s
                               ETA: 1005361.1s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.550s, learning 0.173s)
               Value function loss: 121.8143
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 614.03
               Mean episode length: 149.13
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 9.72s
                        Total time: 30996.20s
                               ETA: 1005329.9s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.632s, learning 0.168s)
               Value function loss: 125.9522
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 601.04
               Mean episode length: 148.89
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 9.80s
                        Total time: 31006.00s
                               ETA: 1005301.3s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.374s, learning 0.175s)
               Value function loss: 149.4405
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 635.00
               Mean episode length: 148.65
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 9.55s
                        Total time: 31015.55s
                               ETA: 1005264.5s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.533s, learning 0.174s)
               Value function loss: 120.6433
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 619.30
               Mean episode length: 149.60
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 9.71s
                        Total time: 31025.26s
                               ETA: 1005232.9s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.305s, learning 0.164s)
               Value function loss: 135.7540
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 620.57
               Mean episode length: 147.73
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 9.47s
                        Total time: 31034.73s
                               ETA: 1005193.6s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.355s, learning 0.173s)
               Value function loss: 145.8294
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 639.87
               Mean episode length: 149.84
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 9.53s
                        Total time: 31044.26s
                               ETA: 1005156.3s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.500s, learning 0.168s)
               Value function loss: 140.2015
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 623.98
               Mean episode length: 149.59
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 9.67s
                        Total time: 31053.92s
                               ETA: 1005123.4s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.335s, learning 0.182s)
               Value function loss: 116.3237
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 614.22
               Mean episode length: 149.35
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 9.52s
                        Total time: 31063.44s
                               ETA: 1005085.7s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.763s, learning 0.173s)
               Value function loss: 119.4859
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 626.59
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 9.94s
                        Total time: 31073.38s
                               ETA: 1005061.6s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.145s, learning 0.174s)
               Value function loss: 123.9900
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 618.27
               Mean episode length: 149.10
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 9.32s
                        Total time: 31082.70s
                               ETA: 1005017.5s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.313s, learning 0.178s)
               Value function loss: 137.0213
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 624.11
               Mean episode length: 149.49
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 9.49s
                        Total time: 31092.19s
                               ETA: 1004979.0s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.626s, learning 0.180s)
               Value function loss: 130.4048
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 600.37
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 9.81s
                        Total time: 31101.99s
                               ETA: 1004950.7s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.572s, learning 0.189s)
               Value function loss: 130.3385
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 598.04
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 9.76s
                        Total time: 31111.75s
                               ETA: 1004921.0s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.751s, learning 0.167s)
               Value function loss: 136.3281
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 620.09
               Mean episode length: 148.76
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 9.92s
                        Total time: 31121.67s
                               ETA: 1004896.4s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.539s, learning 0.171s)
               Value function loss: 143.2932
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 630.99
               Mean episode length: 149.10
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 9.71s
                        Total time: 31131.38s
                               ETA: 1004865.0s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.664s, learning 0.165s)
               Value function loss: 138.9591
                    Surrogate loss: -0.0004
             Mean action noise std: 0.74
                       Mean reward: 610.87
               Mean episode length: 147.55
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 9.83s
                        Total time: 31141.21s
                               ETA: 1004837.5s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.629s, learning 0.168s)
               Value function loss: 155.4234
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 627.99
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 9.80s
                        Total time: 31151.01s
                               ETA: 1004809.0s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.499s, learning 0.168s)
               Value function loss: 136.5009
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 602.99
               Mean episode length: 149.20
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 9.67s
                        Total time: 31160.67s
                               ETA: 1004776.3s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.133s, learning 0.184s)
               Value function loss: 119.7225
                    Surrogate loss: -0.0165
             Mean action noise std: 0.74
                       Mean reward: 618.32
               Mean episode length: 149.42
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 9.32s
                        Total time: 31169.99s
                               ETA: 1004732.4s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.574s, learning 0.177s)
               Value function loss: 112.1442
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 613.12
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 9.75s
                        Total time: 31179.74s
                               ETA: 1004702.4s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.038s, learning 0.179s)
               Value function loss: 136.3018
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 634.60
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 9.22s
                        Total time: 31188.96s
                               ETA: 1004655.3s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.542s, learning 0.169s)
               Value function loss: 138.4943
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 613.48
               Mean episode length: 149.63
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 9.71s
                        Total time: 31198.67s
                               ETA: 1004624.1s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.256s, learning 0.193s)
               Value function loss: 118.2304
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 616.09
               Mean episode length: 149.96
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 9.45s
                        Total time: 31208.12s
                               ETA: 1004584.5s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.168s, learning 0.177s)
               Value function loss: 127.1549
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 631.69
               Mean episode length: 149.34
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 9.34s
                        Total time: 31217.46s
                               ETA: 1004541.5s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.503s, learning 0.164s)
               Value function loss: 141.3214
                    Surrogate loss: 0.0004
             Mean action noise std: 0.74
                       Mean reward: 625.02
               Mean episode length: 149.14
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 9.67s
                        Total time: 31227.13s
                               ETA: 1004508.9s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.680s, learning 0.164s)
               Value function loss: 142.0132
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 612.19
               Mean episode length: 149.49
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 9.84s
                        Total time: 31236.97s
                               ETA: 1004482.0s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.441s, learning 0.167s)
               Value function loss: 130.4424
                    Surrogate loss: -0.0188
             Mean action noise std: 0.74
                       Mean reward: 639.45
               Mean episode length: 149.64
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 9.61s
                        Total time: 31246.58s
                               ETA: 1004447.6s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.685s, learning 0.170s)
               Value function loss: 112.8996
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 621.89
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 9.85s
                        Total time: 31256.44s
                               ETA: 1004421.1s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.342s, learning 0.157s)
               Value function loss: 106.6267
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 610.15
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 9.50s
                        Total time: 31265.93s
                               ETA: 1004383.2s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1787 steps/s (collection: 9.006s, learning 0.160s)
               Value function loss: 133.4468
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 634.24
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 9.17s
                        Total time: 31275.10s
                               ETA: 1004334.6s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.362s, learning 0.163s)
               Value function loss: 134.9937
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 625.50
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 9.53s
                        Total time: 31284.63s
                               ETA: 1004297.6s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.666s, learning 0.161s)
               Value function loss: 128.7676
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 634.84
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 9.83s
                        Total time: 31294.45s
                               ETA: 1004270.3s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.537s, learning 0.164s)
               Value function loss: 145.1823
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 625.31
               Mean episode length: 149.54
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 9.70s
                        Total time: 31304.15s
                               ETA: 1004238.9s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.274s, learning 0.161s)
               Value function loss: 124.5582
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 594.69
               Mean episode length: 149.56
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 9.43s
                        Total time: 31313.59s
                               ETA: 1004199.1s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.684s, learning 0.162s)
               Value function loss: 138.9991
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 619.44
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 9.85s
                        Total time: 31323.44s
                               ETA: 1004172.4s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.416s, learning 0.162s)
               Value function loss: 127.7204
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 585.73
               Mean episode length: 148.83
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 9.58s
                        Total time: 31333.01s
                               ETA: 1004137.1s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.421s, learning 0.162s)
               Value function loss: 117.3621
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 619.25
               Mean episode length: 149.55
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 9.58s
                        Total time: 31342.60s
                               ETA: 1004102.1s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.871s, learning 0.167s)
               Value function loss: 125.3881
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 640.99
               Mean episode length: 149.54
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 10.04s
                        Total time: 31352.63s
                               ETA: 1004081.6s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.331s, learning 0.166s)
               Value function loss: 115.2054
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 602.07
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 9.50s
                        Total time: 31362.13s
                               ETA: 1004043.8s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.357s, learning 0.160s)
               Value function loss: 112.0241
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 604.05
               Mean episode length: 149.74
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 9.52s
                        Total time: 31371.65s
                               ETA: 1004006.6s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.289s, learning 0.159s)
               Value function loss: 134.3206
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 614.31
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 9.45s
                        Total time: 31381.10s
                               ETA: 1003967.3s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.160s)
               Value function loss: 107.1532
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 606.65
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 9.70s
                        Total time: 31390.79s
                               ETA: 1003936.0s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.407s, learning 0.164s)
               Value function loss: 119.9694
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 580.88
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 9.57s
                        Total time: 31400.36s
                               ETA: 1003900.6s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.447s, learning 0.165s)
               Value function loss: 117.0348
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 586.80
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 9.61s
                        Total time: 31409.98s
                               ETA: 1003866.6s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.411s, learning 0.159s)
               Value function loss: 106.5338
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 614.13
               Mean episode length: 150.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 9.57s
                        Total time: 31419.55s
                               ETA: 1003831.2s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.270s, learning 0.166s)
               Value function loss: 113.7154
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 596.55
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 9.44s
                        Total time: 31428.98s
                               ETA: 1003791.6s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.204s, learning 0.163s)
               Value function loss: 113.6853
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 601.85
               Mean episode length: 149.26
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 9.37s
                        Total time: 31438.35s
                               ETA: 1003749.8s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.769s, learning 0.158s)
               Value function loss: 120.3983
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 606.93
               Mean episode length: 149.13
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 9.93s
                        Total time: 31448.28s
                               ETA: 1003725.9s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.536s, learning 0.161s)
               Value function loss: 129.7183
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 602.76
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 9.70s
                        Total time: 31457.98s
                               ETA: 1003694.7s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.356s, learning 0.163s)
               Value function loss: 128.8828
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 601.28
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 9.52s
                        Total time: 31467.49s
                               ETA: 1003657.8s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.261s, learning 0.161s)
               Value function loss: 137.2573
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 583.91
               Mean episode length: 148.81
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 9.42s
                        Total time: 31476.92s
                               ETA: 1003617.8s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.422s, learning 0.167s)
               Value function loss: 145.8071
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: 623.91
               Mean episode length: 149.89
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 9.59s
                        Total time: 31486.51s
                               ETA: 1003583.2s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.200s, learning 0.173s)
               Value function loss: 149.9195
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 588.22
               Mean episode length: 149.09
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 9.37s
                        Total time: 31495.88s
                               ETA: 1003541.7s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.256s, learning 0.172s)
               Value function loss: 131.0507
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 590.03
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 9.43s
                        Total time: 31505.31s
                               ETA: 1003502.0s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.371s, learning 0.166s)
               Value function loss: 124.2317
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 599.41
               Mean episode length: 149.63
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 9.54s
                        Total time: 31514.84s
                               ETA: 1003465.7s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.452s, learning 0.161s)
               Value function loss: 106.4992
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 615.34
               Mean episode length: 149.63
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 9.61s
                        Total time: 31524.46s
                               ETA: 1003432.0s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.295s, learning 0.162s)
               Value function loss: 118.7842
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 559.19
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 9.46s
                        Total time: 31533.91s
                               ETA: 1003393.2s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.455s, learning 0.162s)
               Value function loss: 124.4894
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 583.32
               Mean episode length: 148.98
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 9.62s
                        Total time: 31543.53s
                               ETA: 1003359.6s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.380s, learning 0.164s)
               Value function loss: 117.0308
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 606.15
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 9.54s
                        Total time: 31553.08s
                               ETA: 1003323.6s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1603 steps/s (collection: 10.054s, learning 0.166s)
               Value function loss: 112.6652
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 569.86
               Mean episode length: 147.26
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 10.22s
                        Total time: 31563.29s
                               ETA: 1003309.2s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.361s, learning 0.181s)
               Value function loss: 115.0753
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 603.71
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 9.54s
                        Total time: 31572.84s
                               ETA: 1003273.2s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.498s, learning 0.173s)
               Value function loss: 126.9190
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 607.27
               Mean episode length: 148.05
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 9.67s
                        Total time: 31582.51s
                               ETA: 1003241.3s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.487s, learning 0.159s)
               Value function loss: 122.7086
                    Surrogate loss: 0.0060
             Mean action noise std: 0.74
                       Mean reward: 600.49
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 9.65s
                        Total time: 31592.15s
                               ETA: 1003208.7s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.285s, learning 0.163s)
               Value function loss: 112.4504
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 591.06
               Mean episode length: 149.57
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 9.45s
                        Total time: 31601.60s
                               ETA: 1003169.8s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1655 steps/s (collection: 9.732s, learning 0.167s)
               Value function loss: 115.9664
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 619.31
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 9.90s
                        Total time: 31611.50s
                               ETA: 1003145.2s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.320s, learning 0.166s)
               Value function loss: 110.6579
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 564.13
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 9.49s
                        Total time: 31620.99s
                               ETA: 1003107.5s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.759s, learning 0.162s)
               Value function loss: 113.1625
                    Surrogate loss: 0.0058
             Mean action noise std: 0.74
                       Mean reward: 584.43
               Mean episode length: 149.40
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 9.92s
                        Total time: 31630.91s
                               ETA: 1003083.7s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.498s, learning 0.181s)
               Value function loss: 123.4931
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 617.90
               Mean episode length: 149.62
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 9.68s
                        Total time: 31640.59s
                               ETA: 1003052.1s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.189s, learning 0.176s)
               Value function loss: 124.7533
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 599.26
               Mean episode length: 148.99
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 9.37s
                        Total time: 31649.95s
                               ETA: 1003010.7s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.707s, learning 0.163s)
               Value function loss: 102.4294
                    Surrogate loss: 0.0052
             Mean action noise std: 0.74
                       Mean reward: 599.90
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 9.87s
                        Total time: 31659.82s
                               ETA: 1002985.2s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.155s, learning 0.168s)
               Value function loss: 120.8292
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 610.01
               Mean episode length: 148.92
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 9.32s
                        Total time: 31669.14s
                               ETA: 1002942.5s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.530s, learning 0.161s)
               Value function loss: 114.2510
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: 588.60
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 9.69s
                        Total time: 31678.84s
                               ETA: 1002911.4s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.354s, learning 0.162s)
               Value function loss: 120.3195
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 620.82
               Mean episode length: 150.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 9.52s
                        Total time: 31688.35s
                               ETA: 1002874.8s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.519s, learning 0.161s)
               Value function loss: 125.5064
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 615.10
               Mean episode length: 149.49
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 9.68s
                        Total time: 31698.03s
                               ETA: 1002843.4s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.425s, learning 0.162s)
               Value function loss: 96.2737
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 608.11
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 9.59s
                        Total time: 31707.62s
                               ETA: 1002809.1s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1602 steps/s (collection: 10.066s, learning 0.157s)
               Value function loss: 104.7325
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 597.81
               Mean episode length: 149.15
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 10.22s
                        Total time: 31717.84s
                               ETA: 1002794.9s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.477s, learning 0.164s)
               Value function loss: 125.7736
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 606.82
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 9.64s
                        Total time: 31727.48s
                               ETA: 1002762.3s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.411s, learning 0.161s)
               Value function loss: 115.7713
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 605.60
               Mean episode length: 148.28
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 9.57s
                        Total time: 31737.06s
                               ETA: 1002727.5s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.276s, learning 0.163s)
               Value function loss: 102.9243
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 595.34
               Mean episode length: 148.50
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 9.44s
                        Total time: 31746.49s
                               ETA: 1002688.6s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1783 steps/s (collection: 9.018s, learning 0.170s)
               Value function loss: 106.3337
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 617.48
               Mean episode length: 149.85
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 9.19s
                        Total time: 31755.68s
                               ETA: 1002641.7s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.259s, learning 0.161s)
               Value function loss: 114.1058
                    Surrogate loss: -0.0198
             Mean action noise std: 0.74
                       Mean reward: 603.25
               Mean episode length: 149.75
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 9.42s
                        Total time: 31765.10s
                               ETA: 1002602.2s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.591s, learning 0.176s)
               Value function loss: 102.2219
                    Surrogate loss: -0.0175
             Mean action noise std: 0.74
                       Mean reward: 605.65
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 9.77s
                        Total time: 31774.87s
                               ETA: 1002573.7s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.159s)
               Value function loss: 97.7428
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 621.82
               Mean episode length: 149.31
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 9.70s
                        Total time: 31784.57s
                               ETA: 1002543.0s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1742 steps/s (collection: 9.234s, learning 0.170s)
               Value function loss: 102.5545
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 575.07
               Mean episode length: 147.54
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 9.40s
                        Total time: 31793.97s
                               ETA: 1002503.0s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.436s, learning 0.167s)
               Value function loss: 108.4194
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 596.88
               Mean episode length: 148.44
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 9.60s
                        Total time: 31803.58s
                               ETA: 1002469.4s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.555s, learning 0.161s)
               Value function loss: 112.6095
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 630.46
               Mean episode length: 149.99
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 9.72s
                        Total time: 31813.29s
                               ETA: 1002439.3s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.710s, learning 0.165s)
               Value function loss: 119.3003
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 592.95
               Mean episode length: 148.32
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 9.88s
                        Total time: 31823.17s
                               ETA: 1002414.2s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.446s, learning 0.173s)
               Value function loss: 138.0115
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 620.23
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 9.62s
                        Total time: 31832.79s
                               ETA: 1002381.1s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.386s, learning 0.167s)
               Value function loss: 126.2387
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 596.51
               Mean episode length: 150.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 9.55s
                        Total time: 31842.34s
                               ETA: 1002345.9s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.396s, learning 0.159s)
               Value function loss: 121.3104
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 604.09
               Mean episode length: 148.56
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 9.56s
                        Total time: 31851.89s
                               ETA: 1002310.9s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.861s, learning 0.163s)
               Value function loss: 111.1981
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 568.58
               Mean episode length: 148.79
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 10.02s
                        Total time: 31861.92s
                               ETA: 1002290.5s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.300s, learning 0.156s)
               Value function loss: 120.1015
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 606.25
               Mean episode length: 150.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 9.46s
                        Total time: 31871.37s
                               ETA: 1002252.3s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.612s, learning 0.169s)
               Value function loss: 114.9403
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 588.16
               Mean episode length: 148.26
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 9.78s
                        Total time: 31881.16s
                               ETA: 1002224.4s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.632s, learning 0.158s)
               Value function loss: 103.2565
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 598.51
               Mean episode length: 148.70
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 9.79s
                        Total time: 31890.95s
                               ETA: 1002196.8s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.821s, learning 0.160s)
               Value function loss: 112.6219
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 600.03
               Mean episode length: 150.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 9.98s
                        Total time: 31900.93s
                               ETA: 1002175.1s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.647s, learning 0.158s)
               Value function loss: 118.5882
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 586.87
               Mean episode length: 148.71
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 9.81s
                        Total time: 31910.73s
                               ETA: 1002147.9s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.493s, learning 0.158s)
               Value function loss: 112.4938
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 581.31
               Mean episode length: 149.75
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 9.65s
                        Total time: 31920.38s
                               ETA: 1002116.0s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.769s, learning 0.159s)
               Value function loss: 106.0723
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 606.48
               Mean episode length: 149.26
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 9.93s
                        Total time: 31930.31s
                               ETA: 1002092.7s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.615s, learning 0.169s)
               Value function loss: 99.3823
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 577.88
               Mean episode length: 149.22
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 9.78s
                        Total time: 31940.10s
                               ETA: 1002064.9s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.501s, learning 0.162s)
               Value function loss: 124.5978
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 582.69
               Mean episode length: 149.08
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 9.66s
                        Total time: 31949.76s
                               ETA: 1002033.4s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.245s, learning 0.169s)
               Value function loss: 105.7575
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 534.83
               Mean episode length: 149.22
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 9.41s
                        Total time: 31959.17s
                               ETA: 1001994.0s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.381s, learning 0.163s)
               Value function loss: 97.2415
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 587.86
               Mean episode length: 149.42
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 9.54s
                        Total time: 31968.72s
                               ETA: 1001958.7s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.582s, learning 0.164s)
               Value function loss: 94.7511
                    Surrogate loss: -0.0003
             Mean action noise std: 0.74
                       Mean reward: 554.65
               Mean episode length: 149.29
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 9.75s
                        Total time: 31978.46s
                               ETA: 1001929.8s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.602s, learning 0.161s)
               Value function loss: 92.4765
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 567.71
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 9.76s
                        Total time: 31988.23s
                               ETA: 1001901.4s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.381s, learning 0.165s)
               Value function loss: 96.2078
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 555.68
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 9.55s
                        Total time: 31997.77s
                               ETA: 1001866.3s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.448s, learning 0.167s)
               Value function loss: 109.1367
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 604.90
               Mean episode length: 149.72
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 9.62s
                        Total time: 32007.39s
                               ETA: 1001833.3s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.565s, learning 0.161s)
               Value function loss: 96.8553
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 576.90
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 9.73s
                        Total time: 32017.11s
                               ETA: 1001803.8s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.830s, learning 0.162s)
               Value function loss: 113.1826
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 559.46
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 9.99s
                        Total time: 32027.11s
                               ETA: 1001782.7s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.300s, learning 0.167s)
               Value function loss: 107.9342
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 595.68
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 9.47s
                        Total time: 32036.57s
                               ETA: 1001745.1s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.711s, learning 0.164s)
               Value function loss: 101.9496
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 570.31
               Mean episode length: 150.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 9.88s
                        Total time: 32046.45s
                               ETA: 1001720.3s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.193s, learning 0.165s)
               Value function loss: 109.0670
                    Surrogate loss: 0.0008
             Mean action noise std: 0.74
                       Mean reward: 603.83
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 9.36s
                        Total time: 32055.81s
                               ETA: 1001679.4s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.533s, learning 0.162s)
               Value function loss: 98.5290
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 597.03
               Mean episode length: 149.16
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 9.70s
                        Total time: 32065.50s
                               ETA: 1001649.0s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.447s, learning 0.166s)
               Value function loss: 105.6907
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 607.90
               Mean episode length: 149.41
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 9.61s
                        Total time: 32075.12s
                               ETA: 1001616.0s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.474s, learning 0.182s)
               Value function loss: 109.3355
                    Surrogate loss: -0.0208
             Mean action noise std: 0.74
                       Mean reward: 566.62
               Mean episode length: 149.06
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 9.66s
                        Total time: 32084.77s
                               ETA: 1001584.5s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.479s, learning 0.168s)
               Value function loss: 99.8300
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 599.50
               Mean episode length: 149.19
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 9.65s
                        Total time: 32094.42s
                               ETA: 1001552.6s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.047s, learning 0.160s)
               Value function loss: 112.3326
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 603.75
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 9.21s
                        Total time: 32103.63s
                               ETA: 1001507.0s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.601s, learning 0.161s)
               Value function loss: 97.3980
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 617.32
               Mean episode length: 149.31
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 9.76s
                        Total time: 32113.39s
                               ETA: 1001478.8s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.308s, learning 0.168s)
               Value function loss: 94.8376
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 599.93
               Mean episode length: 150.00
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 9.48s
                        Total time: 32122.86s
                               ETA: 1001441.7s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.407s, learning 0.171s)
               Value function loss: 108.0432
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 611.08
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 9.58s
                        Total time: 32132.44s
                               ETA: 1001407.7s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.470s, learning 0.168s)
               Value function loss: 102.7683
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 583.96
               Mean episode length: 148.39
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 9.64s
                        Total time: 32142.08s
                               ETA: 1001375.7s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.577s, learning 0.162s)
               Value function loss: 99.3642
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 615.36
               Mean episode length: 150.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 9.74s
                        Total time: 32151.82s
                               ETA: 1001346.8s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.427s, learning 0.170s)
               Value function loss: 104.4057
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 612.41
               Mean episode length: 150.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 9.60s
                        Total time: 32161.42s
                               ETA: 1001313.4s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.753s, learning 0.160s)
               Value function loss: 92.9888
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 578.54
               Mean episode length: 149.74
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 9.91s
                        Total time: 32171.33s
                               ETA: 1001290.0s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.790s, learning 0.165s)
               Value function loss: 107.3530
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 593.39
               Mean episode length: 149.85
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 9.96s
                        Total time: 32181.28s
                               ETA: 1001267.8s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.071s, learning 0.164s)
               Value function loss: 132.0064
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 577.98
               Mean episode length: 148.65
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 9.23s
                        Total time: 32190.52s
                               ETA: 1001223.3s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.637s, learning 0.169s)
               Value function loss: 128.1753
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 593.05
               Mean episode length: 147.16
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 9.81s
                        Total time: 32200.32s
                               ETA: 1001196.5s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.378s, learning 0.172s)
               Value function loss: 150.8543
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 578.59
               Mean episode length: 149.16
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 9.55s
                        Total time: 32209.87s
                               ETA: 1001161.8s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.599s, learning 0.162s)
               Value function loss: 158.0984
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: 595.94
               Mean episode length: 149.67
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 9.76s
                        Total time: 32219.64s
                               ETA: 1001133.7s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.506s, learning 0.175s)
               Value function loss: 150.2888
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 593.69
               Mean episode length: 149.20
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 9.68s
                        Total time: 32229.32s
                               ETA: 1001103.1s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.579s, learning 0.167s)
               Value function loss: 149.0568
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 596.08
               Mean episode length: 149.42
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 9.75s
                        Total time: 32239.06s
                               ETA: 1001074.6s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.877s, learning 0.165s)
               Value function loss: 126.5303
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: 592.70
               Mean episode length: 149.82
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 9.04s
                        Total time: 32248.10s
                               ETA: 1001024.1s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.730s, learning 0.162s)
               Value function loss: 131.2115
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 598.82
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 9.89s
                        Total time: 32258.00s
                               ETA: 1001000.1s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.600s, learning 0.174s)
               Value function loss: 94.4170
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 555.26
               Mean episode length: 150.00
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 9.77s
                        Total time: 32267.77s
                               ETA: 1000972.5s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.636s, learning 0.171s)
               Value function loss: 109.8384
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 580.77
               Mean episode length: 149.56
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 9.81s
                        Total time: 32277.58s
                               ETA: 1000945.9s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.387s, learning 0.162s)
               Value function loss: 105.0125
                    Surrogate loss: 0.0030
             Mean action noise std: 0.74
                       Mean reward: 567.60
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 9.55s
                        Total time: 32287.13s
                               ETA: 1000911.3s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.757s, learning 0.174s)
               Value function loss: 105.1656
                    Surrogate loss: 0.0045
             Mean action noise std: 0.74
                       Mean reward: 586.22
               Mean episode length: 149.05
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 9.93s
                        Total time: 32297.06s
                               ETA: 1000888.5s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.128s, learning 0.191s)
               Value function loss: 100.4622
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 583.75
               Mean episode length: 148.53
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 9.32s
                        Total time: 32306.38s
                               ETA: 1000846.8s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.482s, learning 0.183s)
               Value function loss: 99.0575
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 580.68
               Mean episode length: 149.62
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 9.67s
                        Total time: 32316.04s
                               ETA: 1000815.9s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1762 steps/s (collection: 9.131s, learning 0.165s)
               Value function loss: 149.1214
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 589.71
               Mean episode length: 148.08
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 9.30s
                        Total time: 32325.34s
                               ETA: 1000773.5s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.450s, learning 0.187s)
               Value function loss: 90.9460
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 567.07
               Mean episode length: 148.97
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 9.64s
                        Total time: 32334.98s
                               ETA: 1000741.7s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.158s, learning 0.175s)
               Value function loss: 113.0109
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 563.75
               Mean episode length: 147.75
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 9.33s
                        Total time: 32344.31s
                               ETA: 1000700.5s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.326s, learning 0.179s)
               Value function loss: 118.3463
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 575.39
               Mean episode length: 149.56
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 9.51s
                        Total time: 32353.81s
                               ETA: 1000664.6s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.694s, learning 0.180s)
               Value function loss: 149.1109
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 571.48
               Mean episode length: 149.88
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 9.87s
                        Total time: 32363.69s
                               ETA: 1000640.2s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.544s, learning 0.182s)
               Value function loss: 242.5136
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 598.06
               Mean episode length: 149.81
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 9.73s
                        Total time: 32373.41s
                               ETA: 1000611.2s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.720s, learning 0.161s)
               Value function loss: 173.8225
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 571.13
               Mean episode length: 149.23
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 9.88s
                        Total time: 32383.30s
                               ETA: 1000587.0s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.275s, learning 0.162s)
               Value function loss: 134.9018
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 599.11
               Mean episode length: 149.70
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 9.44s
                        Total time: 32392.73s
                               ETA: 1000549.1s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.745s, learning 0.158s)
               Value function loss: 115.0771
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 594.42
               Mean episode length: 149.38
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 9.90s
                        Total time: 32402.64s
                               ETA: 1000525.6s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.477s, learning 0.163s)
               Value function loss: 110.7668
                    Surrogate loss: 0.0084
             Mean action noise std: 0.73
                       Mean reward: 593.43
               Mean episode length: 149.39
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 9.64s
                        Total time: 32412.28s
                               ETA: 1000494.0s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.433s, learning 0.172s)
               Value function loss: 106.7566
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 587.12
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 9.60s
                        Total time: 32421.88s
                               ETA: 1000461.3s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.550s, learning 0.157s)
               Value function loss: 88.3584
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 607.92
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 9.71s
                        Total time: 32431.59s
                               ETA: 1000431.8s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.339s, learning 0.164s)
               Value function loss: 103.7470
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 596.39
               Mean episode length: 148.10
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 9.50s
                        Total time: 32441.09s
                               ETA: 1000396.0s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.763s, learning 0.165s)
               Value function loss: 105.9874
                    Surrogate loss: 0.0112
             Mean action noise std: 0.73
                       Mean reward: 562.46
               Mean episode length: 148.76
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 9.93s
                        Total time: 32451.02s
                               ETA: 1000373.3s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.390s, learning 0.176s)
               Value function loss: 98.0530
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: 594.96
               Mean episode length: 149.33
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 9.57s
                        Total time: 32460.58s
                               ETA: 1000339.5s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.514s, learning 0.184s)
               Value function loss: 86.7636
                    Surrogate loss: 0.0141
             Mean action noise std: 0.73
                       Mean reward: 598.72
               Mean episode length: 149.71
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 9.70s
                        Total time: 32470.28s
                               ETA: 1000309.8s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.668s, learning 0.168s)
               Value function loss: 104.3502
                    Surrogate loss: 0.0007
             Mean action noise std: 0.73
                       Mean reward: 571.93
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 9.84s
                        Total time: 32480.12s
                               ETA: 1000284.3s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.121s, learning 0.171s)
               Value function loss: 105.1765
                    Surrogate loss: -0.0018
             Mean action noise std: 0.73
                       Mean reward: 604.03
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 9.29s
                        Total time: 32489.41s
                               ETA: 1000242.1s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.273s, learning 0.167s)
               Value function loss: 94.3164
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 571.02
               Mean episode length: 149.34
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 9.44s
                        Total time: 32498.85s
                               ETA: 1000204.5s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.570s, learning 0.162s)
               Value function loss: 105.1906
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 597.11
               Mean episode length: 149.80
                  Mean reward/step: 3.76
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 9.73s
                        Total time: 32508.58s
                               ETA: 1000175.8s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.243s, learning 0.165s)
               Value function loss: 105.6537
                    Surrogate loss: 0.0088
             Mean action noise std: 0.73
                       Mean reward: 611.51
               Mean episode length: 150.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 9.41s
                        Total time: 32517.99s
                               ETA: 1000137.2s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.321s, learning 0.164s)
               Value function loss: 106.3193
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: 579.01
               Mean episode length: 149.13
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 9.48s
                        Total time: 32527.47s
                               ETA: 1000101.0s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.476s, learning 0.169s)
               Value function loss: 103.4955
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: 574.44
               Mean episode length: 149.90
                  Mean reward/step: 3.61
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 9.65s
                        Total time: 32537.12s
                               ETA: 1000069.8s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.661s, learning 0.167s)
               Value function loss: 110.1144
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 564.28
               Mean episode length: 149.53
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 9.83s
                        Total time: 32546.95s
                               ETA: 1000044.2s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.435s, learning 0.168s)
               Value function loss: 101.6305
                    Surrogate loss: 0.0083
             Mean action noise std: 0.73
                       Mean reward: 552.50
               Mean episode length: 149.17
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 9.60s
                        Total time: 32556.55s
                               ETA: 1000011.6s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.501s, learning 0.167s)
               Value function loss: 98.9154
                    Surrogate loss: 0.0240
             Mean action noise std: 0.73
                       Mean reward: 510.54
               Mean episode length: 150.00
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 9.67s
                        Total time: 32566.22s
                               ETA: 999981.1s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.532s, learning 0.171s)
               Value function loss: 91.2089
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 525.05
               Mean episode length: 149.75
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 9.70s
                        Total time: 32575.92s
                               ETA: 999951.7s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1766 steps/s (collection: 9.104s, learning 0.170s)
               Value function loss: 99.1469
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 517.01
               Mean episode length: 150.00
                  Mean reward/step: 3.38
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 9.27s
                        Total time: 32585.19s
                               ETA: 999909.1s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.287s, learning 0.162s)
               Value function loss: 103.7253
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 558.17
               Mean episode length: 149.57
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 9.45s
                        Total time: 32594.64s
                               ETA: 999871.9s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.676s, learning 0.161s)
               Value function loss: 87.2018
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 515.15
               Mean episode length: 149.42
                  Mean reward/step: 3.35
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 9.84s
                        Total time: 32604.48s
                               ETA: 999846.6s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.857s, learning 0.173s)
               Value function loss: 85.6978
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 520.77
               Mean episode length: 149.36
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 10.03s
                        Total time: 32614.51s
                               ETA: 999827.3s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.559s, learning 0.165s)
               Value function loss: 91.9135
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 499.98
               Mean episode length: 148.88
                  Mean reward/step: 3.43
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 9.72s
                        Total time: 32624.23s
                               ETA: 999798.6s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.203s, learning 0.172s)
               Value function loss: 95.1591
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 505.56
               Mean episode length: 149.36
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 9.38s
                        Total time: 32633.61s
                               ETA: 999759.2s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.464s, learning 0.172s)
               Value function loss: 87.4347
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 560.91
               Mean episode length: 150.00
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 9.64s
                        Total time: 32643.25s
                               ETA: 999727.8s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1658 steps/s (collection: 9.717s, learning 0.161s)
               Value function loss: 87.8092
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 461.62
               Mean episode length: 147.88
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 9.88s
                        Total time: 32653.12s
                               ETA: 999703.8s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.652s, learning 0.163s)
               Value function loss: 96.0491
                    Surrogate loss: 0.0043
             Mean action noise std: 0.73
                       Mean reward: 499.54
               Mean episode length: 148.75
                  Mean reward/step: 3.41
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 9.82s
                        Total time: 32662.94s
                               ETA: 999677.9s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.561s, learning 0.181s)
               Value function loss: 85.7820
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 526.61
               Mean episode length: 150.00
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 9.74s
                        Total time: 32672.68s
                               ETA: 999649.8s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.524s, learning 0.180s)
               Value function loss: 87.3539
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 501.93
               Mean episode length: 148.60
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 9.70s
                        Total time: 32682.39s
                               ETA: 999620.6s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.720s, learning 0.174s)
               Value function loss: 93.2031
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 524.49
               Mean episode length: 149.62
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 9.89s
                        Total time: 32692.28s
                               ETA: 999597.1s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.533s, learning 0.164s)
               Value function loss: 83.3447
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 485.39
               Mean episode length: 149.62
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 9.70s
                        Total time: 32701.98s
                               ETA: 999567.7s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.655s, learning 0.164s)
               Value function loss: 93.0557
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 546.15
               Mean episode length: 149.85
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 9.82s
                        Total time: 32711.80s
                               ETA: 999542.0s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.400s, learning 0.159s)
               Value function loss: 107.6798
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 524.85
               Mean episode length: 149.56
                  Mean reward/step: 3.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 9.56s
                        Total time: 32721.35s
                               ETA: 999508.3s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.399s, learning 0.166s)
               Value function loss: 119.6565
                    Surrogate loss: 0.0007
             Mean action noise std: 0.73
                       Mean reward: 513.65
               Mean episode length: 148.96
                  Mean reward/step: 3.44
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 9.57s
                        Total time: 32730.92s
                               ETA: 999474.9s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.157s, learning 0.166s)
               Value function loss: 112.9119
                    Surrogate loss: 0.0069
             Mean action noise std: 0.73
                       Mean reward: 480.21
               Mean episode length: 149.69
                  Mean reward/step: 3.43
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 9.32s
                        Total time: 32740.24s
                               ETA: 999434.1s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1622 steps/s (collection: 9.939s, learning 0.161s)
               Value function loss: 117.8416
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 519.13
               Mean episode length: 149.25
                  Mean reward/step: 3.45
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 10.10s
                        Total time: 32750.34s
                               ETA: 999417.0s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.426s, learning 0.177s)
               Value function loss: 104.8561
                    Surrogate loss: 0.0112
             Mean action noise std: 0.73
                       Mean reward: 472.13
               Mean episode length: 149.73
                  Mean reward/step: 3.46
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 9.60s
                        Total time: 32759.95s
                               ETA: 999384.8s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.583s, learning 0.169s)
               Value function loss: 94.2445
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 494.85
               Mean episode length: 147.66
                  Mean reward/step: 3.49
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 9.75s
                        Total time: 32769.70s
                               ETA: 999357.1s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.474s, learning 0.162s)
               Value function loss: 95.6111
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 533.04
               Mean episode length: 149.39
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 9.64s
                        Total time: 32779.33s
                               ETA: 999325.8s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1770 steps/s (collection: 9.075s, learning 0.178s)
               Value function loss: 108.0634
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 537.75
               Mean episode length: 150.00
                  Mean reward/step: 3.52
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 9.25s
                        Total time: 32788.59s
                               ETA: 999283.0s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.154s, learning 0.190s)
               Value function loss: 80.0819
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 527.54
               Mean episode length: 149.87
                  Mean reward/step: 3.62
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 9.34s
                        Total time: 32797.93s
                               ETA: 999242.9s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.870s, learning 0.164s)
               Value function loss: 86.8045
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 502.89
               Mean episode length: 148.96
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 10.03s
                        Total time: 32807.96s
                               ETA: 999223.9s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.317s, learning 0.166s)
               Value function loss: 85.9273
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 524.65
               Mean episode length: 149.80
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 9.48s
                        Total time: 32817.45s
                               ETA: 999188.1s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.383s, learning 0.164s)
               Value function loss: 83.9845
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 510.65
               Mean episode length: 148.71
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 9.55s
                        Total time: 32826.99s
                               ETA: 999154.2s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.565s, learning 0.171s)
               Value function loss: 83.6871
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 542.22
               Mean episode length: 149.70
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 9.74s
                        Total time: 32836.73s
                               ETA: 999126.1s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.514s, learning 0.166s)
               Value function loss: 90.8236
                    Surrogate loss: 0.0016
             Mean action noise std: 0.73
                       Mean reward: 574.65
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 9.68s
                        Total time: 32846.41s
                               ETA: 999096.3s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.522s, learning 0.172s)
               Value function loss: 87.7102
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 547.38
               Mean episode length: 150.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 9.69s
                        Total time: 32856.10s
                               ETA: 999067.0s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.175s, learning 0.170s)
               Value function loss: 82.6783
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 539.53
               Mean episode length: 149.13
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 9.35s
                        Total time: 32865.45s
                               ETA: 999027.1s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1657 steps/s (collection: 9.715s, learning 0.168s)
               Value function loss: 81.3046
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 553.06
               Mean episode length: 148.89
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 9.88s
                        Total time: 32875.33s
                               ETA: 999003.5s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.353s, learning 0.160s)
               Value function loss: 81.7142
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 552.14
               Mean episode length: 150.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 9.51s
                        Total time: 32884.84s
                               ETA: 998968.7s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.643s, learning 0.159s)
               Value function loss: 79.6254
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 545.76
               Mean episode length: 150.00
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 9.80s
                        Total time: 32894.65s
                               ETA: 998942.7s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.330s, learning 0.158s)
               Value function loss: 83.2230
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 561.67
               Mean episode length: 150.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 9.49s
                        Total time: 32904.13s
                               ETA: 998907.2s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.635s, learning 0.163s)
               Value function loss: 93.0395
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 569.56
               Mean episode length: 150.00
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 9.80s
                        Total time: 32913.93s
                               ETA: 998881.1s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.524s, learning 0.165s)
               Value function loss: 89.1636
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 538.54
               Mean episode length: 149.06
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 9.69s
                        Total time: 32923.62s
                               ETA: 998851.7s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.665s, learning 0.162s)
               Value function loss: 98.2508
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 551.03
               Mean episode length: 150.00
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 9.83s
                        Total time: 32933.45s
                               ETA: 998826.5s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.391s, learning 0.164s)
               Value function loss: 92.4651
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 572.81
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 9.56s
                        Total time: 32943.00s
                               ETA: 998793.1s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.628s, learning 0.159s)
               Value function loss: 91.0811
                    Surrogate loss: -0.0228
             Mean action noise std: 0.73
                       Mean reward: 555.47
               Mean episode length: 149.05
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 9.79s
                        Total time: 32952.79s
                               ETA: 998766.7s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1780 steps/s (collection: 9.042s, learning 0.159s)
               Value function loss: 105.5282
                    Surrogate loss: 0.0002
             Mean action noise std: 0.73
                       Mean reward: 575.07
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 9.20s
                        Total time: 32961.99s
                               ETA: 998722.6s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.301s, learning 0.160s)
               Value function loss: 81.6043
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 572.19
               Mean episode length: 149.58
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 9.46s
                        Total time: 32971.45s
                               ETA: 998686.3s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.509s, learning 0.163s)
               Value function loss: 93.7956
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 574.84
               Mean episode length: 150.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 9.67s
                        Total time: 32981.12s
                               ETA: 998656.5s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.648s, learning 0.162s)
               Value function loss: 102.9179
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 555.82
               Mean episode length: 149.79
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 9.81s
                        Total time: 32990.93s
                               ETA: 998630.8s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.344s, learning 0.173s)
               Value function loss: 100.7755
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 556.59
               Mean episode length: 150.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 9.52s
                        Total time: 33000.45s
                               ETA: 998596.4s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.703s, learning 0.158s)
               Value function loss: 93.0818
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 568.67
               Mean episode length: 149.68
                  Mean reward/step: 3.56
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 9.86s
                        Total time: 33010.31s
                               ETA: 998572.3s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.526s, learning 0.162s)
               Value function loss: 101.6140
                    Surrogate loss: 0.0059
             Mean action noise std: 0.73
                       Mean reward: 582.88
               Mean episode length: 149.37
                  Mean reward/step: 3.53
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 9.69s
                        Total time: 33020.00s
                               ETA: 998543.0s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.370s, learning 0.158s)
               Value function loss: 102.6027
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 508.39
               Mean episode length: 149.44
                  Mean reward/step: 3.57
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 9.53s
                        Total time: 33029.53s
                               ETA: 998508.8s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.520s, learning 0.164s)
               Value function loss: 102.3690
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 545.02
               Mean episode length: 148.91
                  Mean reward/step: 3.59
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 9.68s
                        Total time: 33039.21s
                               ETA: 998479.4s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.340s, learning 0.164s)
               Value function loss: 109.0047
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 516.21
               Mean episode length: 150.00
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 9.50s
                        Total time: 33048.72s
                               ETA: 998444.6s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.768s, learning 0.166s)
               Value function loss: 101.3721
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 544.32
               Mean episode length: 149.53
                  Mean reward/step: 3.66
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 9.93s
                        Total time: 33058.65s
                               ETA: 998422.8s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.854s, learning 0.170s)
               Value function loss: 96.4485
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 532.57
               Mean episode length: 149.15
                  Mean reward/step: 3.68
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 10.02s
                        Total time: 33068.67s
                               ETA: 998403.7s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.291s, learning 0.161s)
               Value function loss: 107.6071
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 568.52
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 9.45s
                        Total time: 33078.13s
                               ETA: 998367.4s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.247s, learning 0.163s)
               Value function loss: 122.0627
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 543.54
               Mean episode length: 146.16
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 9.41s
                        Total time: 33087.54s
                               ETA: 998329.8s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.357s, learning 0.175s)
               Value function loss: 134.9221
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 543.67
               Mean episode length: 150.00
                  Mean reward/step: 3.69
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 9.53s
                        Total time: 33097.07s
                               ETA: 998295.9s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.821s, learning 0.161s)
               Value function loss: 109.5915
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 539.57
               Mean episode length: 149.72
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 9.98s
                        Total time: 33107.05s
                               ETA: 998275.5s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.181s, learning 0.164s)
               Value function loss: 113.2575
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 581.36
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 9.35s
                        Total time: 33116.40s
                               ETA: 998236.0s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.249s, learning 0.183s)
               Value function loss: 101.9821
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 516.62
               Mean episode length: 150.00
                  Mean reward/step: 3.72
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 9.43s
                        Total time: 33125.83s
                               ETA: 998199.2s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.332s, learning 0.164s)
               Value function loss: 120.8679
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 576.95
               Mean episode length: 148.82
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 9.50s
                        Total time: 33135.32s
                               ETA: 998164.2s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.399s, learning 0.161s)
               Value function loss: 117.7925
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 542.46
               Mean episode length: 148.32
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 9.56s
                        Total time: 33144.88s
                               ETA: 998131.2s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.381s, learning 0.172s)
               Value function loss: 103.1312
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 557.35
               Mean episode length: 149.80
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 9.55s
                        Total time: 33154.44s
                               ETA: 998098.1s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.428s, learning 0.161s)
               Value function loss: 117.4246
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 561.98
               Mean episode length: 148.31
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 9.59s
                        Total time: 33164.03s
                               ETA: 998066.0s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1781 steps/s (collection: 9.035s, learning 0.163s)
               Value function loss: 137.9823
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 567.33
               Mean episode length: 147.93
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 9.20s
                        Total time: 33173.22s
                               ETA: 998022.2s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.298s, learning 0.168s)
               Value function loss: 124.0879
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 574.83
               Mean episode length: 149.79
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 9.47s
                        Total time: 33182.69s
                               ETA: 997986.4s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.517s, learning 0.170s)
               Value function loss: 124.8608
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 583.50
               Mean episode length: 149.77
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 9.69s
                        Total time: 33192.38s
                               ETA: 997957.3s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.705s, learning 0.170s)
               Value function loss: 121.8659
                    Surrogate loss: 0.0147
             Mean action noise std: 0.73
                       Mean reward: 579.25
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 9.88s
                        Total time: 33202.25s
                               ETA: 997933.9s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.371s, learning 0.162s)
               Value function loss: 137.4521
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 559.56
               Mean episode length: 147.74
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 9.53s
                        Total time: 33211.78s
                               ETA: 997900.2s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1783 steps/s (collection: 9.022s, learning 0.162s)
               Value function loss: 133.1101
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 612.48
               Mean episode length: 149.34
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 9.18s
                        Total time: 33220.97s
                               ETA: 997856.0s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.300s, learning 0.175s)
               Value function loss: 130.2304
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 583.11
               Mean episode length: 149.65
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 9.47s
                        Total time: 33230.44s
                               ETA: 997820.6s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.239s, learning 0.159s)
               Value function loss: 124.0791
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 600.75
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 9.40s
                        Total time: 33239.84s
                               ETA: 997782.9s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.214s, learning 0.173s)
               Value function loss: 120.0487
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 570.65
               Mean episode length: 149.15
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 9.39s
                        Total time: 33249.23s
                               ETA: 997744.9s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.307s, learning 0.175s)
               Value function loss: 147.3235
                    Surrogate loss: 0.0088
             Mean action noise std: 0.73
                       Mean reward: 603.04
               Mean episode length: 149.18
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 9.48s
                        Total time: 33258.71s
                               ETA: 997709.8s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.513s, learning 0.158s)
               Value function loss: 164.5831
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 638.68
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 9.67s
                        Total time: 33268.38s
                               ETA: 997680.3s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.207s, learning 0.161s)
               Value function loss: 154.6669
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 588.93
               Mean episode length: 149.17
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 9.37s
                        Total time: 33277.75s
                               ETA: 997641.8s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.655s, learning 0.160s)
               Value function loss: 147.9423
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 595.54
               Mean episode length: 148.74
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 9.81s
                        Total time: 33287.56s
                               ETA: 997616.6s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.388s, learning 0.176s)
               Value function loss: 149.0350
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 623.56
               Mean episode length: 149.77
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 9.56s
                        Total time: 33297.13s
                               ETA: 997584.0s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.531s, learning 0.158s)
               Value function loss: 163.5529
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 605.25
               Mean episode length: 149.54
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 9.69s
                        Total time: 33306.82s
                               ETA: 997555.2s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.432s, learning 0.175s)
               Value function loss: 155.5593
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 622.34
               Mean episode length: 149.67
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 9.61s
                        Total time: 33316.43s
                               ETA: 997523.9s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.852s, learning 0.164s)
               Value function loss: 161.2806
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 610.78
               Mean episode length: 149.33
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 10.02s
                        Total time: 33326.44s
                               ETA: 997504.8s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.332s, learning 0.158s)
               Value function loss: 152.3165
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 611.45
               Mean episode length: 149.21
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 9.49s
                        Total time: 33335.93s
                               ETA: 997470.1s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.233s, learning 0.162s)
               Value function loss: 165.6600
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 592.60
               Mean episode length: 148.02
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 9.39s
                        Total time: 33345.33s
                               ETA: 997432.4s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.459s, learning 0.166s)
               Value function loss: 163.4623
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 610.82
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 9.62s
                        Total time: 33354.95s
                               ETA: 997401.7s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.547s, learning 0.170s)
               Value function loss: 160.9281
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 617.71
               Mean episode length: 147.87
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 9.72s
                        Total time: 33364.67s
                               ETA: 997373.8s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.580s, learning 0.171s)
               Value function loss: 150.8757
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 606.76
               Mean episode length: 148.74
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 9.75s
                        Total time: 33374.42s
                               ETA: 997346.8s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.393s, learning 0.172s)
               Value function loss: 136.1647
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 622.24
               Mean episode length: 149.69
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 9.57s
                        Total time: 33383.99s
                               ETA: 997314.4s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.237s, learning 0.168s)
               Value function loss: 146.2502
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 600.33
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 9.41s
                        Total time: 33393.39s
                               ETA: 997277.1s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.258s, learning 0.163s)
               Value function loss: 147.5141
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 622.81
               Mean episode length: 149.45
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 9.42s
                        Total time: 33402.81s
                               ETA: 997240.4s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.679s, learning 0.158s)
               Value function loss: 142.9685
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 601.09
               Mean episode length: 148.23
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 9.84s
                        Total time: 33412.65s
                               ETA: 997216.1s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.144s, learning 0.162s)
               Value function loss: 115.8961
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 607.64
               Mean episode length: 148.59
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 9.31s
                        Total time: 33421.95s
                               ETA: 997175.9s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.604s, learning 0.169s)
               Value function loss: 134.3697
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 613.73
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 9.77s
                        Total time: 33431.73s
                               ETA: 997149.7s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.249s, learning 0.158s)
               Value function loss: 124.8070
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 639.62
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 9.41s
                        Total time: 33441.13s
                               ETA: 997112.6s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.773s, learning 0.167s)
               Value function loss: 138.4543
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 618.14
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 9.94s
                        Total time: 33451.08s
                               ETA: 997091.4s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.435s, learning 0.169s)
               Value function loss: 126.6686
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 616.32
               Mean episode length: 149.81
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 9.60s
                        Total time: 33460.68s
                               ETA: 997060.2s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.608s, learning 0.160s)
               Value function loss: 140.8984
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 604.81
               Mean episode length: 149.05
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 9.77s
                        Total time: 33470.45s
                               ETA: 997033.9s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.265s, learning 0.165s)
               Value function loss: 124.9404
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 613.30
               Mean episode length: 149.84
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 9.43s
                        Total time: 33479.88s
                               ETA: 996997.5s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.339s, learning 0.161s)
               Value function loss: 127.3609
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 621.02
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 9.50s
                        Total time: 33489.38s
                               ETA: 996963.3s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.240s, learning 0.160s)
               Value function loss: 135.3450
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 626.24
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 9.40s
                        Total time: 33498.78s
                               ETA: 996926.0s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.265s, learning 0.157s)
               Value function loss: 131.2619
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 612.66
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 9.42s
                        Total time: 33508.20s
                               ETA: 996889.5s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.649s, learning 0.162s)
               Value function loss: 112.8664
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 603.54
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 9.81s
                        Total time: 33518.01s
                               ETA: 996864.5s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.513s, learning 0.161s)
               Value function loss: 154.1418
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 610.58
               Mean episode length: 149.44
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 9.67s
                        Total time: 33527.68s
                               ETA: 996835.5s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.301s, learning 0.162s)
               Value function loss: 136.1549
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 629.42
               Mean episode length: 149.35
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 9.46s
                        Total time: 33537.15s
                               ETA: 996800.2s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.156s, learning 0.165s)
               Value function loss: 136.1993
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 619.81
               Mean episode length: 149.27
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 9.32s
                        Total time: 33546.47s
                               ETA: 996760.7s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.689s, learning 0.157s)
               Value function loss: 121.2115
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 628.94
               Mean episode length: 149.66
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 9.85s
                        Total time: 33556.31s
                               ETA: 996736.8s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.434s, learning 0.162s)
               Value function loss: 142.2098
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 618.45
               Mean episode length: 148.70
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 9.60s
                        Total time: 33565.91s
                               ETA: 996705.5s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.493s, learning 0.158s)
               Value function loss: 133.8846
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 600.89
               Mean episode length: 149.74
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 9.65s
                        Total time: 33575.56s
                               ETA: 996675.9s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1772 steps/s (collection: 9.080s, learning 0.165s)
               Value function loss: 127.3327
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 607.37
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 9.25s
                        Total time: 33584.80s
                               ETA: 996634.2s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.473s, learning 0.168s)
               Value function loss: 151.2495
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 644.20
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 9.64s
                        Total time: 33594.45s
                               ETA: 996604.3s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.261s, learning 0.160s)
               Value function loss: 133.8871
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 604.06
               Mean episode length: 149.09
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 9.42s
                        Total time: 33603.87s
                               ETA: 996567.9s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1589 steps/s (collection: 10.152s, learning 0.159s)
               Value function loss: 145.8730
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 605.01
               Mean episode length: 149.25
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 10.31s
                        Total time: 33614.18s
                               ETA: 996557.8s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.483s, learning 0.164s)
               Value function loss: 146.3799
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 619.65
               Mean episode length: 147.19
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 9.65s
                        Total time: 33623.82s
                               ETA: 996528.1s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.351s, learning 0.157s)
               Value function loss: 172.0268
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 627.45
               Mean episode length: 150.00
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 9.51s
                        Total time: 33633.33s
                               ETA: 996494.3s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.309s, learning 0.165s)
               Value function loss: 156.8663
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 627.62
               Mean episode length: 149.47
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 9.47s
                        Total time: 33642.81s
                               ETA: 996459.6s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.864s, learning 0.162s)
               Value function loss: 198.4455
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 642.97
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 10.03s
                        Total time: 33652.83s
                               ETA: 996441.1s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.544s, learning 0.159s)
               Value function loss: 145.8539
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 615.45
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 9.70s
                        Total time: 33662.53s
                               ETA: 996413.1s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.302s, learning 0.160s)
               Value function loss: 154.2261
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 603.86
               Mean episode length: 149.48
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 9.46s
                        Total time: 33672.00s
                               ETA: 996378.0s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.411s, learning 0.163s)
               Value function loss: 155.7208
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 614.75
               Mean episode length: 149.06
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 9.57s
                        Total time: 33681.57s
                               ETA: 996346.2s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.625s, learning 0.162s)
               Value function loss: 128.8806
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 619.10
               Mean episode length: 149.76
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 9.79s
                        Total time: 33691.36s
                               ETA: 996320.7s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.443s, learning 0.160s)
               Value function loss: 154.9393
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 639.09
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 9.60s
                        Total time: 33700.96s
                               ETA: 996289.8s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.421s, learning 0.161s)
               Value function loss: 158.2810
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 610.83
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 9.58s
                        Total time: 33710.54s
                               ETA: 996258.3s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.681s, learning 0.162s)
               Value function loss: 142.8531
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 641.81
               Mean episode length: 149.47
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 9.84s
                        Total time: 33720.39s
                               ETA: 996234.5s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.350s, learning 0.157s)
               Value function loss: 153.7725
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 638.21
               Mean episode length: 149.53
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 9.51s
                        Total time: 33729.89s
                               ETA: 996200.8s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.826s, learning 0.165s)
               Value function loss: 146.4672
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 618.83
               Mean episode length: 147.73
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 9.99s
                        Total time: 33739.89s
                               ETA: 996181.4s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.476s, learning 0.158s)
               Value function loss: 155.8669
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 631.17
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 9.63s
                        Total time: 33749.52s
                               ETA: 996151.5s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.592s, learning 0.166s)
               Value function loss: 141.4123
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 611.21
               Mean episode length: 149.29
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 9.76s
                        Total time: 33759.28s
                               ETA: 996125.2s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.334s, learning 0.160s)
               Value function loss: 142.2391
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 627.66
               Mean episode length: 149.65
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 9.49s
                        Total time: 33768.77s
                               ETA: 996091.2s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.707s, learning 0.163s)
               Value function loss: 123.0961
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 616.10
               Mean episode length: 148.99
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 9.87s
                        Total time: 33778.64s
                               ETA: 996068.3s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.695s, learning 0.163s)
               Value function loss: 140.3232
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: 610.92
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 9.86s
                        Total time: 33788.50s
                               ETA: 996045.0s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.458s, learning 0.164s)
               Value function loss: 131.6535
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 622.72
               Mean episode length: 149.46
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 9.62s
                        Total time: 33798.12s
                               ETA: 996014.7s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.474s, learning 0.162s)
               Value function loss: 145.6190
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 626.54
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 9.64s
                        Total time: 33807.76s
                               ETA: 995984.9s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.891s, learning 0.162s)
               Value function loss: 140.8908
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 611.56
               Mean episode length: 149.21
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 10.05s
                        Total time: 33817.81s
                               ETA: 995967.4s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.612s, learning 0.167s)
               Value function loss: 128.5545
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 615.75
               Mean episode length: 149.66
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 9.78s
                        Total time: 33827.59s
                               ETA: 995941.9s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.248s, learning 0.158s)
               Value function loss: 142.9169
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 615.46
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 9.41s
                        Total time: 33836.99s
                               ETA: 995905.3s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1633 steps/s (collection: 9.850s, learning 0.179s)
               Value function loss: 126.1767
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 614.42
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 10.03s
                        Total time: 33847.02s
                               ETA: 995887.2s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.635s, learning 0.163s)
               Value function loss: 149.4728
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 627.16
               Mean episode length: 149.60
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 9.80s
                        Total time: 33856.82s
                               ETA: 995862.2s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.463s, learning 0.163s)
               Value function loss: 148.9065
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 616.09
               Mean episode length: 149.48
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 9.63s
                        Total time: 33866.45s
                               ETA: 995832.2s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.100s, learning 0.163s)
               Value function loss: 138.2852
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 619.79
               Mean episode length: 148.94
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 9.26s
                        Total time: 33875.71s
                               ETA: 995791.5s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.648s, learning 0.160s)
               Value function loss: 155.4035
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 622.43
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 9.81s
                        Total time: 33885.52s
                               ETA: 995766.8s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.460s, learning 0.160s)
               Value function loss: 169.6792
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 611.12
               Mean episode length: 149.92
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 9.62s
                        Total time: 33895.14s
                               ETA: 995736.7s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.422s, learning 0.161s)
               Value function loss: 152.1880
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 613.84
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 9.58s
                        Total time: 33904.72s
                               ETA: 995705.4s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.335s, learning 0.163s)
               Value function loss: 145.0100
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 631.74
               Mean episode length: 148.79
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 9.50s
                        Total time: 33914.22s
                               ETA: 995671.7s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.888s, learning 0.166s)
               Value function loss: 130.1014
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 602.24
               Mean episode length: 148.46
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 10.05s
                        Total time: 33924.28s
                               ETA: 995654.3s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.205s, learning 0.163s)
               Value function loss: 163.8553
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 639.99
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 9.37s
                        Total time: 33933.64s
                               ETA: 995616.8s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.599s, learning 0.162s)
               Value function loss: 126.9641
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 610.58
               Mean episode length: 149.83
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 9.76s
                        Total time: 33943.40s
                               ETA: 995590.8s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.314s, learning 0.164s)
               Value function loss: 135.5982
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 617.76
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 9.48s
                        Total time: 33952.88s
                               ETA: 995556.6s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.168s, learning 0.170s)
               Value function loss: 126.5892
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 623.04
               Mean episode length: 149.19
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 9.34s
                        Total time: 33962.22s
                               ETA: 995518.3s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.633s, learning 0.160s)
               Value function loss: 134.9067
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 627.70
               Mean episode length: 148.48
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 9.79s
                        Total time: 33972.01s
                               ETA: 995493.2s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.415s, learning 0.164s)
               Value function loss: 130.9854
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 623.14
               Mean episode length: 148.54
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 9.58s
                        Total time: 33981.59s
                               ETA: 995462.0s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.565s, learning 0.161s)
               Value function loss: 152.0263
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 609.30
               Mean episode length: 148.09
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 9.73s
                        Total time: 33991.32s
                               ETA: 995435.1s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.697s, learning 0.176s)
               Value function loss: 150.4229
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 610.09
               Mean episode length: 149.92
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 9.87s
                        Total time: 34001.19s
                               ETA: 995412.5s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.558s, learning 0.171s)
               Value function loss: 144.2427
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 616.07
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 9.73s
                        Total time: 34010.92s
                               ETA: 995385.6s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.560s, learning 0.162s)
               Value function loss: 139.7570
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 626.75
               Mean episode length: 149.41
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 9.72s
                        Total time: 34020.64s
                               ETA: 995358.6s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.688s, learning 0.167s)
               Value function loss: 128.6667
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 623.56
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 9.86s
                        Total time: 34030.50s
                               ETA: 995335.5s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.463s, learning 0.162s)
               Value function loss: 133.6046
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 619.43
               Mean episode length: 149.98
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 9.63s
                        Total time: 34040.12s
                               ETA: 995305.7s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.806s, learning 0.181s)
               Value function loss: 140.7989
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 581.10
               Mean episode length: 148.68
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 9.99s
                        Total time: 34050.11s
                               ETA: 995286.4s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1751 steps/s (collection: 9.189s, learning 0.165s)
               Value function loss: 123.5674
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 603.77
               Mean episode length: 149.71
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 9.35s
                        Total time: 34059.47s
                               ETA: 995248.7s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.717s, learning 0.174s)
               Value function loss: 142.9038
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 616.69
               Mean episode length: 149.32
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 9.89s
                        Total time: 34069.36s
                               ETA: 995226.7s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.412s, learning 0.164s)
               Value function loss: 144.3844
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 627.05
               Mean episode length: 149.06
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 9.58s
                        Total time: 34078.93s
                               ETA: 995195.4s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.659s, learning 0.166s)
               Value function loss: 141.1832
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 622.56
               Mean episode length: 149.54
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 9.83s
                        Total time: 34088.76s
                               ETA: 995171.5s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.408s, learning 0.168s)
               Value function loss: 152.6268
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 642.15
               Mean episode length: 148.07
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 9.58s
                        Total time: 34098.33s
                               ETA: 995140.3s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.448s, learning 0.161s)
               Value function loss: 132.1447
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 630.23
               Mean episode length: 149.24
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 9.61s
                        Total time: 34107.94s
                               ETA: 995110.1s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.509s, learning 0.170s)
               Value function loss: 134.2843
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 613.30
               Mean episode length: 148.28
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 9.68s
                        Total time: 34117.62s
                               ETA: 995081.9s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.699s, learning 0.170s)
               Value function loss: 138.5281
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 610.10
               Mean episode length: 147.78
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 9.87s
                        Total time: 34127.49s
                               ETA: 995059.3s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.460s, learning 0.165s)
               Value function loss: 149.4820
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 625.04
               Mean episode length: 148.95
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 9.62s
                        Total time: 34137.12s
                               ETA: 995029.5s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1662 steps/s (collection: 9.682s, learning 0.175s)
               Value function loss: 126.3775
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 618.61
               Mean episode length: 149.25
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 9.86s
                        Total time: 34146.97s
                               ETA: 995006.6s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.645s, learning 0.167s)
               Value function loss: 131.3063
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 632.42
               Mean episode length: 149.38
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 9.81s
                        Total time: 34156.79s
                               ETA: 994982.3s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.259s, learning 0.158s)
               Value function loss: 122.4347
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 613.37
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 9.42s
                        Total time: 34166.20s
                               ETA: 994946.6s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.447s, learning 0.163s)
               Value function loss: 156.7995
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 624.01
               Mean episode length: 148.81
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 9.61s
                        Total time: 34175.81s
                               ETA: 994916.4s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.464s, learning 0.164s)
               Value function loss: 122.8659
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: 609.14
               Mean episode length: 148.82
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 9.63s
                        Total time: 34185.44s
                               ETA: 994886.9s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.381s, learning 0.178s)
               Value function loss: 158.5309
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 637.46
               Mean episode length: 148.89
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 9.56s
                        Total time: 34195.00s
                               ETA: 994855.3s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.824s, learning 0.168s)
               Value function loss: 136.9922
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 627.34
               Mean episode length: 148.92
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 9.99s
                        Total time: 34204.99s
                               ETA: 994836.3s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1656 steps/s (collection: 9.705s, learning 0.183s)
               Value function loss: 125.7808
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 625.74
               Mean episode length: 148.98
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 9.89s
                        Total time: 34214.88s
                               ETA: 994814.3s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.215s, learning 0.178s)
               Value function loss: 123.6457
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 608.97
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 9.39s
                        Total time: 34224.27s
                               ETA: 994778.0s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.857s, learning 0.181s)
               Value function loss: 138.8490
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 606.34
               Mean episode length: 149.41
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 10.04s
                        Total time: 34234.31s
                               ETA: 994760.3s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.440s, learning 0.168s)
               Value function loss: 117.4783
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 586.42
               Mean episode length: 147.24
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 9.61s
                        Total time: 34243.92s
                               ETA: 994730.3s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.587s, learning 0.167s)
               Value function loss: 143.2220
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 597.60
               Mean episode length: 147.92
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 9.75s
                        Total time: 34253.67s
                               ETA: 994704.4s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.271s, learning 0.159s)
               Value function loss: 120.8503
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 603.26
               Mean episode length: 148.95
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 9.43s
                        Total time: 34263.10s
                               ETA: 994669.1s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.830s, learning 0.168s)
               Value function loss: 121.9139
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 634.76
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 10.00s
                        Total time: 34273.10s
                               ETA: 994650.4s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.522s, learning 0.172s)
               Value function loss: 129.8266
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 613.42
               Mean episode length: 149.93
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 9.69s
                        Total time: 34282.79s
                               ETA: 994622.9s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.452s, learning 0.164s)
               Value function loss: 119.3504
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 606.58
               Mean episode length: 149.50
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 9.62s
                        Total time: 34292.41s
                               ETA: 994593.0s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.698s, learning 0.165s)
               Value function loss: 122.3306
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 632.34
               Mean episode length: 149.65
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 9.86s
                        Total time: 34302.27s
                               ETA: 994570.4s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.406s, learning 0.162s)
               Value function loss: 111.1105
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 628.11
               Mean episode length: 149.99
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 9.57s
                        Total time: 34311.84s
                               ETA: 994539.2s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.462s, learning 0.163s)
               Value function loss: 114.5453
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 639.12
               Mean episode length: 149.87
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 9.63s
                        Total time: 34321.47s
                               ETA: 994509.7s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.584s, learning 0.178s)
               Value function loss: 105.7067
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 624.34
               Mean episode length: 148.88
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 9.76s
                        Total time: 34331.23s
                               ETA: 994484.2s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.489s, learning 0.171s)
               Value function loss: 111.4618
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 622.75
               Mean episode length: 149.56
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 9.66s
                        Total time: 34340.89s
                               ETA: 994455.7s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.535s, learning 0.176s)
               Value function loss: 115.5605
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 606.27
               Mean episode length: 149.99
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 9.71s
                        Total time: 34350.60s
                               ETA: 994428.7s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.398s, learning 0.163s)
               Value function loss: 130.9708
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 601.11
               Mean episode length: 148.36
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 9.56s
                        Total time: 34360.16s
                               ETA: 994397.4s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.772s, learning 0.167s)
               Value function loss: 131.0948
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 617.82
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.94s
                        Total time: 34369.10s
                               ETA: 994348.1s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.573s, learning 0.177s)
               Value function loss: 120.7707
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 599.74
               Mean episode length: 148.05
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 9.75s
                        Total time: 34378.85s
                               ETA: 994322.2s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1628 steps/s (collection: 9.877s, learning 0.186s)
               Value function loss: 132.3779
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 593.74
               Mean episode length: 149.08
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 10.06s
                        Total time: 34388.91s
                               ETA: 994305.5s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.610s, learning 0.174s)
               Value function loss: 106.1981
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 623.94
               Mean episode length: 149.95
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 9.78s
                        Total time: 34398.69s
                               ETA: 994280.7s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.559s, learning 0.184s)
               Value function loss: 118.2155
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 604.79
               Mean episode length: 148.85
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 9.74s
                        Total time: 34408.44s
                               ETA: 994254.7s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.474s, learning 0.157s)
               Value function loss: 92.7124
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: 624.34
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 9.63s
                        Total time: 34418.07s
                               ETA: 994225.4s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.485s, learning 0.175s)
               Value function loss: 111.4610
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 601.36
               Mean episode length: 148.71
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 9.66s
                        Total time: 34427.73s
                               ETA: 994197.1s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.875s, learning 0.170s)
               Value function loss: 127.9249
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 613.95
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 9.05s
                        Total time: 34436.77s
                               ETA: 994151.0s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.776s, learning 0.163s)
               Value function loss: 122.6154
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 606.23
               Mean episode length: 148.23
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 9.94s
                        Total time: 34446.71s
                               ETA: 994130.7s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.340s, learning 0.171s)
               Value function loss: 116.4983
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 601.85
               Mean episode length: 148.04
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 9.51s
                        Total time: 34456.22s
                               ETA: 994098.0s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.515s, learning 0.169s)
               Value function loss: 126.1434
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 592.91
               Mean episode length: 148.62
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 9.68s
                        Total time: 34465.91s
                               ETA: 994070.4s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.341s, learning 0.180s)
               Value function loss: 119.4657
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 622.00
               Mean episode length: 149.84
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 9.52s
                        Total time: 34475.43s
                               ETA: 994038.1s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.648s, learning 0.160s)
               Value function loss: 93.9849
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 596.06
               Mean episode length: 148.32
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 9.81s
                        Total time: 34485.24s
                               ETA: 994014.0s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1753 steps/s (collection: 9.166s, learning 0.177s)
               Value function loss: 109.1268
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 616.64
               Mean episode length: 149.40
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 9.34s
                        Total time: 34494.58s
                               ETA: 993976.6s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.329s, learning 0.169s)
               Value function loss: 105.5097
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 631.06
               Mean episode length: 149.87
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 9.50s
                        Total time: 34504.08s
                               ETA: 993943.7s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.455s, learning 0.165s)
               Value function loss: 99.7968
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 618.94
               Mean episode length: 149.35
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 9.62s
                        Total time: 34513.70s
                               ETA: 993914.3s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.591s, learning 0.172s)
               Value function loss: 105.2747
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 616.73
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 9.76s
                        Total time: 34523.46s
                               ETA: 993889.0s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.111s, learning 0.187s)
               Value function loss: 112.4204
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 603.64
               Mean episode length: 148.82
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 12.30s
                        Total time: 34535.76s
                               ETA: 993936.6s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 896 steps/s (collection: 18.115s, learning 0.163s)
               Value function loss: 126.1392
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 608.41
               Mean episode length: 149.86
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 18.28s
                        Total time: 34554.04s
                               ETA: 994156.3s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 887 steps/s (collection: 18.295s, learning 0.163s)
               Value function loss: 106.2035
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 584.43
               Mean episode length: 149.82
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 18.46s
                        Total time: 34572.49s
                               ETA: 994381.1s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 866 steps/s (collection: 18.730s, learning 0.176s)
               Value function loss: 135.4115
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 585.49
               Mean episode length: 149.24
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 18.91s
                        Total time: 34591.40s
                               ETA: 994618.5s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 880 steps/s (collection: 18.438s, learning 0.167s)
               Value function loss: 107.6189
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 572.36
               Mean episode length: 148.96
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 18.61s
                        Total time: 34610.00s
                               ETA: 994847.2s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 880 steps/s (collection: 18.408s, learning 0.198s)
               Value function loss: 118.8715
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 591.33
               Mean episode length: 150.00
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 18.61s
                        Total time: 34628.61s
                               ETA: 995075.7s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 894 steps/s (collection: 18.145s, learning 0.165s)
               Value function loss: 124.2138
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 587.49
               Mean episode length: 148.55
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 18.31s
                        Total time: 34646.92s
                               ETA: 995295.7s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 859 steps/s (collection: 18.896s, learning 0.162s)
               Value function loss: 102.2001
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 593.25
               Mean episode length: 148.78
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 19.06s
                        Total time: 34665.98s
                               ETA: 995536.9s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 863 steps/s (collection: 18.824s, learning 0.161s)
               Value function loss: 128.5059
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 581.97
               Mean episode length: 147.42
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 18.98s
                        Total time: 34684.96s
                               ETA: 995775.8s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 888 steps/s (collection: 18.269s, learning 0.173s)
               Value function loss: 143.6352
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 593.74
               Mean episode length: 148.12
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 18.44s
                        Total time: 34703.41s
                               ETA: 995999.1s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 869 steps/s (collection: 18.639s, learning 0.196s)
               Value function loss: 130.2755
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 573.21
               Mean episode length: 147.47
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 18.83s
                        Total time: 34722.24s
                               ETA: 996233.5s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 886 steps/s (collection: 18.307s, learning 0.165s)
               Value function loss: 112.9738
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 610.95
               Mean episode length: 148.80
                  Mean reward/step: 3.80
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 18.47s
                        Total time: 34740.71s
                               ETA: 996457.3s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 895 steps/s (collection: 18.120s, learning 0.169s)
               Value function loss: 130.6595
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 600.71
               Mean episode length: 148.93
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 18.29s
                        Total time: 34759.00s
                               ETA: 996675.7s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.401s, learning 0.169s)
               Value function loss: 120.3393
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 592.77
               Mean episode length: 149.23
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 18.57s
                        Total time: 34777.57s
                               ETA: 996902.0s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 867 steps/s (collection: 18.725s, learning 0.163s)
               Value function loss: 114.9666
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 594.41
               Mean episode length: 149.07
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 18.89s
                        Total time: 34796.46s
                               ETA: 997137.3s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 874 steps/s (collection: 18.555s, learning 0.189s)
               Value function loss: 112.3619
                    Surrogate loss: -0.0212
             Mean action noise std: 0.73
                       Mean reward: 556.14
               Mean episode length: 148.48
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 18.74s
                        Total time: 34815.20s
                               ETA: 997368.4s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 877 steps/s (collection: 18.474s, learning 0.187s)
               Value function loss: 114.6051
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 569.40
               Mean episode length: 147.78
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 18.66s
                        Total time: 34833.87s
                               ETA: 997596.9s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.528s, learning 0.172s)
               Value function loss: 101.7131
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 557.09
               Mean episode length: 148.36
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 18.70s
                        Total time: 34852.57s
                               ETA: 997826.4s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 890 steps/s (collection: 18.243s, learning 0.160s)
               Value function loss: 118.3844
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 557.88
               Mean episode length: 146.45
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 18.40s
                        Total time: 34870.97s
                               ETA: 998047.2s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.399s, learning 0.166s)
               Value function loss: 124.6119
                    Surrogate loss: -0.0195
             Mean action noise std: 0.73
                       Mean reward: 564.84
               Mean episode length: 149.07
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 18.56s
                        Total time: 34889.53s
                               ETA: 998272.5s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 881 steps/s (collection: 18.423s, learning 0.158s)
               Value function loss: 117.7596
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 565.52
               Mean episode length: 149.09
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 18.58s
                        Total time: 34908.11s
                               ETA: 998498.1s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 862 steps/s (collection: 18.830s, learning 0.166s)
               Value function loss: 119.1114
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 575.12
               Mean episode length: 148.09
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 19.00s
                        Total time: 34927.11s
                               ETA: 998735.5s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 892 steps/s (collection: 18.190s, learning 0.158s)
               Value function loss: 127.6144
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 580.78
               Mean episode length: 148.77
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 18.35s
                        Total time: 34945.46s
                               ETA: 998954.1s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 867 steps/s (collection: 18.713s, learning 0.176s)
               Value function loss: 128.5185
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 601.59
               Mean episode length: 149.46
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 18.89s
                        Total time: 34964.35s
                               ETA: 999188.1s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 882 steps/s (collection: 18.392s, learning 0.179s)
               Value function loss: 128.5944
                    Surrogate loss: 0.0111
             Mean action noise std: 0.73
                       Mean reward: 575.99
               Mean episode length: 149.71
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 18.57s
                        Total time: 34982.92s
                               ETA: 999412.9s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 878 steps/s (collection: 18.473s, learning 0.173s)
               Value function loss: 130.2928
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 608.26
               Mean episode length: 149.10
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 18.65s
                        Total time: 35001.56s
                               ETA: 999639.7s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 893 steps/s (collection: 18.170s, learning 0.176s)
               Value function loss: 118.7008
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 568.72
               Mean episode length: 147.10
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 18.35s
                        Total time: 35019.91s
                               ETA: 999857.7s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 900 steps/s (collection: 18.031s, learning 0.171s)
               Value function loss: 124.8582
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 594.05
               Mean episode length: 148.85
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 18.20s
                        Total time: 35038.11s
                               ETA: 1000071.5s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 913 steps/s (collection: 17.774s, learning 0.164s)
               Value function loss: 126.8191
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 599.55
               Mean episode length: 148.65
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 17.94s
                        Total time: 35056.05s
                               ETA: 1000277.6s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 888 steps/s (collection: 18.277s, learning 0.163s)
               Value function loss: 115.8742
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 554.48
               Mean episode length: 149.46
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 18.44s
                        Total time: 35074.49s
                               ETA: 1000497.9s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 878 steps/s (collection: 18.471s, learning 0.184s)
               Value function loss: 113.4524
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 558.09
               Mean episode length: 146.43
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 18.65s
                        Total time: 35093.14s
                               ETA: 1000724.3s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 894 steps/s (collection: 18.143s, learning 0.168s)
               Value function loss: 116.2263
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 574.96
               Mean episode length: 147.54
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 18.31s
                        Total time: 35111.45s
                               ETA: 1000940.6s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 907 steps/s (collection: 17.898s, learning 0.164s)
               Value function loss: 135.8436
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 610.39
               Mean episode length: 149.36
                  Mean reward/step: 3.79
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 18.06s
                        Total time: 35129.51s
                               ETA: 1001149.7s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 876 steps/s (collection: 18.535s, learning 0.161s)
               Value function loss: 132.4990
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 570.18
               Mean episode length: 147.14
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 18.70s
                        Total time: 35148.21s
                               ETA: 1001376.8s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 860 steps/s (collection: 18.867s, learning 0.175s)
               Value function loss: 142.3544
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 567.30
               Mean episode length: 149.52
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 19.04s
                        Total time: 35167.25s
                               ETA: 1001613.5s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 904 steps/s (collection: 17.931s, learning 0.187s)
               Value function loss: 132.1990
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 565.59
               Mean episode length: 147.66
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 18.12s
                        Total time: 35185.37s
                               ETA: 1001823.8s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 881 steps/s (collection: 18.435s, learning 0.162s)
               Value function loss: 128.8689
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 578.62
               Mean episode length: 148.74
                  Mean reward/step: 3.86
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 18.60s
                        Total time: 35203.97s
                               ETA: 1002047.6s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 895 steps/s (collection: 18.112s, learning 0.190s)
               Value function loss: 149.7909
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 585.24
               Mean episode length: 148.76
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 18.30s
                        Total time: 35222.27s
                               ETA: 1002262.9s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 1227 steps/s (collection: 13.179s, learning 0.167s)
               Value function loss: 132.8846
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 547.02
               Mean episode length: 146.95
                  Mean reward/step: 3.83
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 13.35s
                        Total time: 35235.61s
                               ETA: 1002337.0s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.549s, learning 0.171s)
               Value function loss: 121.1134
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 564.10
               Mean episode length: 149.22
                  Mean reward/step: 3.85
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 9.72s
                        Total time: 35245.33s
                               ETA: 1002308.0s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.625s, learning 0.191s)
               Value function loss: 142.7338
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 561.70
               Mean episode length: 149.33
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 9.82s
                        Total time: 35255.15s
                               ETA: 1002281.7s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.685s, learning 0.178s)
               Value function loss: 135.6457
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 552.00
               Mean episode length: 148.63
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 9.86s
                        Total time: 35265.01s
                               ETA: 1002256.8s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.286s, learning 0.181s)
               Value function loss: 118.3313
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 553.19
               Mean episode length: 147.47
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 9.47s
                        Total time: 35274.48s
                               ETA: 1002220.6s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.301s, learning 0.175s)
               Value function loss: 126.8242
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 588.37
               Mean episode length: 149.31
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 9.48s
                        Total time: 35283.96s
                               ETA: 1002184.7s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.329s, learning 0.163s)
               Value function loss: 122.3115
                    Surrogate loss: 0.0075
             Mean action noise std: 0.73
                       Mean reward: 561.21
               Mean episode length: 148.31
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 9.49s
                        Total time: 35293.45s
                               ETA: 1002149.3s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.586s, learning 0.174s)
               Value function loss: 122.5144
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 582.94
               Mean episode length: 148.70
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 9.76s
                        Total time: 35303.21s
                               ETA: 1002121.4s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.329s, learning 0.174s)
               Value function loss: 136.9673
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 567.90
               Mean episode length: 148.72
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 9.50s
                        Total time: 35312.71s
                               ETA: 1002086.3s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.593s, learning 0.174s)
               Value function loss: 114.9915
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 591.07
               Mean episode length: 149.89
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 9.77s
                        Total time: 35322.48s
                               ETA: 1002058.8s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.475s, learning 0.164s)
               Value function loss: 126.1492
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 561.58
               Mean episode length: 146.97
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 9.64s
                        Total time: 35332.12s
                               ETA: 1002027.5s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.604s, learning 0.165s)
               Value function loss: 114.7273
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 597.30
               Mean episode length: 148.82
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 9.77s
                        Total time: 35341.88s
                               ETA: 1002000.0s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.226s, learning 0.161s)
               Value function loss: 113.3806
                    Surrogate loss: -0.0197
             Mean action noise std: 0.73
                       Mean reward: 590.88
               Mean episode length: 148.52
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 9.39s
                        Total time: 35351.27s
                               ETA: 1001961.7s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.232s, learning 0.162s)
               Value function loss: 102.5152
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 562.57
               Mean episode length: 148.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 9.39s
                        Total time: 35360.67s
                               ETA: 1001923.6s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.334s, learning 0.166s)
               Value function loss: 119.6744
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 595.15
               Mean episode length: 149.23
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 9.50s
                        Total time: 35370.17s
                               ETA: 1001888.5s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.475s, learning 0.168s)
               Value function loss: 136.3090
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 590.06
               Mean episode length: 149.50
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 9.64s
                        Total time: 35379.81s
                               ETA: 1001857.4s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.693s, learning 0.169s)
               Value function loss: 118.0449
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 580.85
               Mean episode length: 147.28
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 8.86s
                        Total time: 35388.67s
                               ETA: 1001804.3s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.323s, learning 0.164s)
               Value function loss: 128.6509
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 558.34
               Mean episode length: 147.83
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 9.49s
                        Total time: 35398.16s
                               ETA: 1001768.9s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 1616 steps/s (collection: 9.975s, learning 0.163s)
               Value function loss: 115.9164
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 589.42
               Mean episode length: 149.06
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 10.14s
                        Total time: 35408.30s
                               ETA: 1001751.9s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.609s, learning 0.163s)
               Value function loss: 115.4984
                    Surrogate loss: -0.0216
             Mean action noise std: 0.73
                       Mean reward: 582.46
               Mean episode length: 149.61
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 9.77s
                        Total time: 35418.07s
                               ETA: 1001724.6s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.325s, learning 0.169s)
               Value function loss: 123.9978
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 584.56
               Mean episode length: 149.26
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 9.49s
                        Total time: 35427.56s
                               ETA: 1001689.4s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.511s, learning 0.163s)
               Value function loss: 119.7255
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 600.39
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 9.67s
                        Total time: 35437.24s
                               ETA: 1001659.3s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.429s, learning 0.158s)
               Value function loss: 124.0800
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 567.96
               Mean episode length: 148.39
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 9.59s
                        Total time: 35446.82s
                               ETA: 1001626.8s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.331s, learning 0.158s)
               Value function loss: 136.5239
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 554.63
               Mean episode length: 147.44
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 9.49s
                        Total time: 35456.31s
                               ETA: 1001591.6s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.633s, learning 0.162s)
               Value function loss: 129.3658
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 595.76
               Mean episode length: 148.11
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 9.79s
                        Total time: 35466.11s
                               ETA: 1001564.9s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.165s, learning 0.160s)
               Value function loss: 132.3413
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 604.06
               Mean episode length: 150.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 9.32s
                        Total time: 35475.43s
                               ETA: 1001525.1s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1624 steps/s (collection: 9.923s, learning 0.164s)
               Value function loss: 138.9506
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 589.29
               Mean episode length: 147.70
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 10.09s
                        Total time: 35485.52s
                               ETA: 1001506.7s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.691s, learning 0.174s)
               Value function loss: 143.8588
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 580.71
               Mean episode length: 148.59
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 9.87s
                        Total time: 35495.38s
                               ETA: 1001482.1s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.709s, learning 0.157s)
               Value function loss: 143.5471
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 602.37
               Mean episode length: 147.73
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 9.87s
                        Total time: 35505.25s
                               ETA: 1001457.5s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.259s, learning 0.164s)
               Value function loss: 111.0546
                    Surrogate loss: -0.0182
             Mean action noise std: 0.73
                       Mean reward: 570.29
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 9.42s
                        Total time: 35514.67s
                               ETA: 1001420.5s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.164s)
               Value function loss: 141.5906
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 588.89
               Mean episode length: 147.66
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 9.69s
                        Total time: 35524.36s
                               ETA: 1001390.8s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.360s, learning 0.166s)
               Value function loss: 141.2024
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 596.99
               Mean episode length: 149.39
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 9.53s
                        Total time: 35533.89s
                               ETA: 1001356.7s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.483s, learning 0.167s)
               Value function loss: 134.6991
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 599.63
               Mean episode length: 146.70
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 9.65s
                        Total time: 35543.54s
                               ETA: 1001326.1s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.546s, learning 0.161s)
               Value function loss: 154.5706
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 572.91
               Mean episode length: 147.32
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 9.71s
                        Total time: 35553.24s
                               ETA: 1001297.1s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.296s, learning 0.167s)
               Value function loss: 150.4197
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 552.92
               Mean episode length: 146.43
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 9.46s
                        Total time: 35562.71s
                               ETA: 1001261.2s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.502s, learning 0.163s)
               Value function loss: 161.8775
                    Surrogate loss: -0.0204
             Mean action noise std: 0.73
                       Mean reward: 596.71
               Mean episode length: 148.28
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 9.67s
                        Total time: 35572.37s
                               ETA: 1001231.1s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.321s, learning 0.158s)
               Value function loss: 180.9564
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 612.64
               Mean episode length: 148.14
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 9.48s
                        Total time: 35581.85s
                               ETA: 1001195.7s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.585s, learning 0.160s)
               Value function loss: 155.0356
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 587.42
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 9.75s
                        Total time: 35591.59s
                               ETA: 1001167.8s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.853s, learning 0.174s)
               Value function loss: 143.2846
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 585.83
               Mean episode length: 148.80
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 10.03s
                        Total time: 35601.62s
                               ETA: 1001147.8s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1699 steps/s (collection: 9.478s, learning 0.165s)
               Value function loss: 155.0563
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 609.91
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 9.64s
                        Total time: 35611.26s
                               ETA: 1001117.1s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.346s, learning 0.161s)
               Value function loss: 157.5504
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 606.31
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 9.51s
                        Total time: 35620.77s
                               ETA: 1001082.6s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.577s, learning 0.162s)
               Value function loss: 131.9727
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 599.58
               Mean episode length: 147.51
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 9.74s
                        Total time: 35630.51s
                               ETA: 1001054.6s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.480s, learning 0.171s)
               Value function loss: 137.5285
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 605.04
               Mean episode length: 149.24
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 9.65s
                        Total time: 35640.16s
                               ETA: 1001024.1s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.646s, learning 0.162s)
               Value function loss: 166.2424
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 619.98
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 9.81s
                        Total time: 35649.97s
                               ETA: 1000998.0s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.304s, learning 0.155s)
               Value function loss: 132.2091
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 626.91
               Mean episode length: 149.85
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 9.46s
                        Total time: 35659.43s
                               ETA: 1000962.2s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1616 steps/s (collection: 9.970s, learning 0.164s)
               Value function loss: 155.9741
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 606.89
               Mean episode length: 148.48
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 10.13s
                        Total time: 35669.56s
                               ETA: 1000945.3s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.610s, learning 0.165s)
               Value function loss: 173.4700
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 612.10
               Mean episode length: 149.20
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 9.78s
                        Total time: 35679.34s
                               ETA: 1000918.4s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.665s, learning 0.174s)
               Value function loss: 146.2660
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 623.66
               Mean episode length: 149.97
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 9.84s
                        Total time: 35689.18s
                               ETA: 1000893.2s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.407s, learning 0.161s)
               Value function loss: 129.1627
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 591.44
               Mean episode length: 147.82
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 9.57s
                        Total time: 35698.74s
                               ETA: 1000860.5s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.614s, learning 0.161s)
               Value function loss: 152.2363
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 646.57
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 9.77s
                        Total time: 35708.52s
                               ETA: 1000833.6s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.643s, learning 0.165s)
               Value function loss: 155.0508
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 627.98
               Mean episode length: 149.33
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 9.81s
                        Total time: 35718.33s
                               ETA: 1000807.6s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1648 steps/s (collection: 9.773s, learning 0.163s)
               Value function loss: 131.3649
                    Surrogate loss: -0.0206
             Mean action noise std: 0.73
                       Mean reward: 615.69
               Mean episode length: 148.48
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 9.94s
                        Total time: 35728.26s
                               ETA: 1000785.2s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.635s, learning 0.167s)
               Value function loss: 134.9568
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 579.92
               Mean episode length: 149.01
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 9.80s
                        Total time: 35738.06s
                               ETA: 1000759.1s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.131s, learning 0.159s)
               Value function loss: 132.9500
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 595.57
               Mean episode length: 148.48
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 9.29s
                        Total time: 35747.35s
                               ETA: 1000718.6s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.529s, learning 0.166s)
               Value function loss: 126.2741
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 633.78
               Mean episode length: 149.64
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 9.70s
                        Total time: 35757.05s
                               ETA: 1000689.6s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1761 steps/s (collection: 9.143s, learning 0.158s)
               Value function loss: 134.9340
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 630.48
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 9.30s
                        Total time: 35766.35s
                               ETA: 1000649.4s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1642 steps/s (collection: 9.814s, learning 0.159s)
               Value function loss: 145.5786
                    Surrogate loss: 0.0016
             Mean action noise std: 0.73
                       Mean reward: 626.97
               Mean episode length: 148.75
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 9.97s
                        Total time: 35776.33s
                               ETA: 1000628.2s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.697s, learning 0.162s)
               Value function loss: 139.5727
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 592.89
               Mean episode length: 149.12
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 9.86s
                        Total time: 35786.18s
                               ETA: 1000603.7s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.896s, learning 0.160s)
               Value function loss: 137.9694
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 617.47
               Mean episode length: 149.01
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 10.06s
                        Total time: 35796.24s
                               ETA: 1000584.7s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.360s, learning 0.173s)
               Value function loss: 131.3712
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 630.48
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 9.53s
                        Total time: 35805.77s
                               ETA: 1000551.1s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.288s, learning 0.171s)
               Value function loss: 130.2809
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 592.09
               Mean episode length: 149.14
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 9.46s
                        Total time: 35815.23s
                               ETA: 1000515.4s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.160s)
               Value function loss: 124.4877
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 593.95
               Mean episode length: 149.64
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 9.70s
                        Total time: 35824.93s
                               ETA: 1000486.5s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.187s, learning 0.163s)
               Value function loss: 130.0213
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 608.72
               Mean episode length: 148.42
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 9.35s
                        Total time: 35834.28s
                               ETA: 1000447.9s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.734s, learning 0.188s)
               Value function loss: 130.5218
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 601.15
               Mean episode length: 148.61
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 9.92s
                        Total time: 35844.20s
                               ETA: 1000425.2s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.829s, learning 0.191s)
               Value function loss: 134.2116
                    Surrogate loss: -0.0200
             Mean action noise std: 0.73
                       Mean reward: 620.68
               Mean episode length: 149.92
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 10.02s
                        Total time: 35854.22s
                               ETA: 1000405.3s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.528s, learning 0.179s)
               Value function loss: 123.7580
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 600.26
               Mean episode length: 148.75
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 9.71s
                        Total time: 35863.93s
                               ETA: 1000376.6s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.330s, learning 0.171s)
               Value function loss: 120.9027
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 619.77
               Mean episode length: 149.22
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 9.50s
                        Total time: 35873.43s
                               ETA: 1000342.2s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.595s, learning 0.183s)
               Value function loss: 117.4020
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 599.45
               Mean episode length: 148.96
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 9.78s
                        Total time: 35883.21s
                               ETA: 1000315.6s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.378s, learning 0.161s)
               Value function loss: 100.0830
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 624.49
               Mean episode length: 149.19
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 9.54s
                        Total time: 35892.74s
                               ETA: 1000282.3s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.282s, learning 0.174s)
               Value function loss: 125.6748
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 602.89
               Mean episode length: 150.00
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 9.46s
                        Total time: 35902.20s
                               ETA: 1000246.7s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.068s, learning 0.163s)
               Value function loss: 125.1129
                    Surrogate loss: -0.0188
             Mean action noise std: 0.73
                       Mean reward: 602.88
               Mean episode length: 147.23
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 9.23s
                        Total time: 35911.43s
                               ETA: 1000204.9s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.578s, learning 0.163s)
               Value function loss: 117.1895
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 591.15
               Mean episode length: 148.94
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 9.74s
                        Total time: 35921.17s
                               ETA: 1000177.3s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.384s, learning 0.164s)
               Value function loss: 109.9995
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 604.53
               Mean episode length: 149.45
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 9.55s
                        Total time: 35930.72s
                               ETA: 1000144.3s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1639 steps/s (collection: 9.826s, learning 0.167s)
               Value function loss: 101.9407
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 589.92
               Mean episode length: 148.98
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 9.99s
                        Total time: 35940.71s
                               ETA: 1000123.7s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.453s, learning 0.170s)
               Value function loss: 114.6860
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 590.57
               Mean episode length: 148.89
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 9.62s
                        Total time: 35950.34s
                               ETA: 1000092.8s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.328s, learning 0.164s)
               Value function loss: 121.0743
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 587.67
               Mean episode length: 148.31
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 9.49s
                        Total time: 35959.83s
                               ETA: 1000058.3s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.461s, learning 0.166s)
               Value function loss: 108.3736
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 565.01
               Mean episode length: 147.98
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 9.63s
                        Total time: 35969.46s
                               ETA: 1000027.5s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.252s, learning 0.167s)
               Value function loss: 126.6583
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 588.40
               Mean episode length: 147.69
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 9.42s
                        Total time: 35978.88s
                               ETA: 999991.0s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1604 steps/s (collection: 10.044s, learning 0.167s)
               Value function loss: 104.0527
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 580.61
               Mean episode length: 149.60
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 10.21s
                        Total time: 35989.09s
                               ETA: 999976.6s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.532s, learning 0.164s)
               Value function loss: 104.6577
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 606.55
               Mean episode length: 149.44
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 9.70s
                        Total time: 35998.78s
                               ETA: 999947.8s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1746 steps/s (collection: 9.222s, learning 0.160s)
               Value function loss: 96.9956
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 589.54
               Mean episode length: 148.67
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 9.38s
                        Total time: 36008.17s
                               ETA: 999910.3s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.219s, learning 0.167s)
               Value function loss: 120.7752
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 598.31
               Mean episode length: 148.40
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 9.39s
                        Total time: 36017.55s
                               ETA: 999872.9s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1628 steps/s (collection: 9.892s, learning 0.167s)
               Value function loss: 107.6987
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 596.48
               Mean episode length: 148.94
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 10.06s
                        Total time: 36027.61s
                               ETA: 999854.2s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.488s, learning 0.168s)
               Value function loss: 125.8103
                    Surrogate loss: -0.0211
             Mean action noise std: 0.73
                       Mean reward: 573.59
               Mean episode length: 149.64
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 9.66s
                        Total time: 36037.27s
                               ETA: 999824.4s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.766s, learning 0.167s)
               Value function loss: 107.7639
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 599.14
               Mean episode length: 149.11
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 9.93s
                        Total time: 36047.20s
                               ETA: 999802.2s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1785 steps/s (collection: 9.007s, learning 0.167s)
               Value function loss: 94.4749
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 568.78
               Mean episode length: 149.04
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 9.17s
                        Total time: 36056.37s
                               ETA: 999759.0s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.642s, learning 0.167s)
               Value function loss: 100.4315
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 618.01
               Mean episode length: 148.99
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 9.81s
                        Total time: 36066.18s
                               ETA: 999733.4s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.471s, learning 0.165s)
               Value function loss: 92.3435
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 614.47
               Mean episode length: 148.40
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 9.64s
                        Total time: 36075.82s
                               ETA: 999703.1s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.400s, learning 0.163s)
               Value function loss: 104.6501
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 575.14
               Mean episode length: 148.76
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 9.56s
                        Total time: 36085.38s
                               ETA: 999670.7s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.237s, learning 0.171s)
               Value function loss: 115.6807
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 617.89
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 9.41s
                        Total time: 36094.79s
                               ETA: 999634.0s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.235s, learning 0.175s)
               Value function loss: 108.7543
                    Surrogate loss: -0.0184
             Mean action noise std: 0.73
                       Mean reward: 605.48
               Mean episode length: 149.75
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 9.41s
                        Total time: 36104.20s
                               ETA: 999597.4s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.396s, learning 0.162s)
               Value function loss: 107.8162
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 580.14
               Mean episode length: 149.57
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 9.56s
                        Total time: 36113.76s
                               ETA: 999564.9s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.614s, learning 0.175s)
               Value function loss: 104.0337
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 601.23
               Mean episode length: 149.26
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 9.79s
                        Total time: 36123.54s
                               ETA: 999538.9s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1687 steps/s (collection: 9.540s, learning 0.168s)
               Value function loss: 103.2588
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 597.73
               Mean episode length: 148.83
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 9.71s
                        Total time: 36133.25s
                               ETA: 999510.6s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.381s, learning 0.182s)
               Value function loss: 113.8280
                    Surrogate loss: -0.0206
             Mean action noise std: 0.73
                       Mean reward: 584.62
               Mean episode length: 149.62
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 9.56s
                        Total time: 36142.81s
                               ETA: 999478.3s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.538s, learning 0.159s)
               Value function loss: 122.7156
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 591.37
               Mean episode length: 147.05
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 9.70s
                        Total time: 36152.51s
                               ETA: 999449.7s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.386s, learning 0.165s)
               Value function loss: 119.4793
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 586.35
               Mean episode length: 149.68
                  Mean reward/step: 3.99
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 9.55s
                        Total time: 36162.06s
                               ETA: 999417.1s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.709s, learning 0.163s)
               Value function loss: 110.3044
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 585.07
               Mean episode length: 145.83
                  Mean reward/step: 3.94
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 9.87s
                        Total time: 36171.93s
                               ETA: 999393.3s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.224s, learning 0.166s)
               Value function loss: 110.8189
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 602.27
               Mean episode length: 149.13
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 9.39s
                        Total time: 36181.32s
                               ETA: 999356.3s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.433s, learning 0.159s)
               Value function loss: 105.8979
                    Surrogate loss: -0.0211
             Mean action noise std: 0.73
                       Mean reward: 598.35
               Mean episode length: 147.15
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 9.59s
                        Total time: 36190.92s
                               ETA: 999324.9s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.636s, learning 0.175s)
               Value function loss: 94.0557
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 611.15
               Mean episode length: 148.64
                  Mean reward/step: 3.89
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 9.81s
                        Total time: 36200.73s
                               ETA: 999299.5s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.436s, learning 0.163s)
               Value function loss: 121.1728
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 608.87
               Mean episode length: 149.05
                  Mean reward/step: 3.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 9.60s
                        Total time: 36210.33s
                               ETA: 999268.3s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.357s, learning 0.168s)
               Value function loss: 116.0529
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 616.55
               Mean episode length: 149.69
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 9.52s
                        Total time: 36219.85s
                               ETA: 999235.1s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.446s, learning 0.158s)
               Value function loss: 105.7170
                    Surrogate loss: -0.0190
             Mean action noise std: 0.73
                       Mean reward: 578.48
               Mean episode length: 149.37
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 9.60s
                        Total time: 36229.46s
                               ETA: 999204.0s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.316s, learning 0.160s)
               Value function loss: 102.9778
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 578.92
               Mean episode length: 150.00
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 9.48s
                        Total time: 36238.93s
                               ETA: 999169.4s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.628s, learning 0.159s)
               Value function loss: 103.7286
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 619.73
               Mean episode length: 150.00
                  Mean reward/step: 3.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 9.79s
                        Total time: 36248.72s
                               ETA: 999143.5s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.317s, learning 0.158s)
               Value function loss: 109.5645
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 582.76
               Mean episode length: 148.62
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 9.48s
                        Total time: 36258.19s
                               ETA: 999108.9s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.505s, learning 0.159s)
               Value function loss: 99.5118
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 589.73
               Mean episode length: 149.15
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 9.66s
                        Total time: 36267.86s
                               ETA: 999079.6s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.620s, learning 0.158s)
               Value function loss: 106.4820
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 542.73
               Mean episode length: 148.87
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 9.78s
                        Total time: 36277.64s
                               ETA: 999053.4s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.250s, learning 0.163s)
               Value function loss: 117.8212
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 594.19
               Mean episode length: 148.20
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 9.41s
                        Total time: 36287.05s
                               ETA: 999017.1s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.708s, learning 0.166s)
               Value function loss: 112.2670
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 572.38
               Mean episode length: 149.16
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 9.87s
                        Total time: 36296.92s
                               ETA: 998993.6s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.302s, learning 0.166s)
               Value function loss: 125.6765
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 597.64
               Mean episode length: 148.92
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 9.47s
                        Total time: 36306.39s
                               ETA: 998958.9s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1737 steps/s (collection: 9.263s, learning 0.167s)
               Value function loss: 117.2027
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 584.53
               Mean episode length: 149.16
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 9.43s
                        Total time: 36315.82s
                               ETA: 998923.1s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.480s, learning 0.176s)
               Value function loss: 117.7925
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 589.00
               Mean episode length: 148.60
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 9.66s
                        Total time: 36325.48s
                               ETA: 998893.6s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.510s, learning 0.172s)
               Value function loss: 122.3851
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 608.31
               Mean episode length: 148.38
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 9.68s
                        Total time: 36335.16s
                               ETA: 998864.9s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.501s, learning 0.162s)
               Value function loss: 130.4356
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 606.62
               Mean episode length: 148.53
                  Mean reward/step: 4.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 9.66s
                        Total time: 36344.82s
                               ETA: 998835.6s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.617s, learning 0.163s)
               Value function loss: 116.3511
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 598.14
               Mean episode length: 148.13
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 9.78s
                        Total time: 36354.60s
                               ETA: 998809.5s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.398s, learning 0.165s)
               Value function loss: 125.2081
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 613.40
               Mean episode length: 149.20
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 9.56s
                        Total time: 36364.16s
                               ETA: 998777.5s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.800s, learning 0.172s)
               Value function loss: 133.3720
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 598.87
               Mean episode length: 147.55
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 9.97s
                        Total time: 36374.14s
                               ETA: 998756.8s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.167s, learning 0.169s)
               Value function loss: 106.6747
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 585.34
               Mean episode length: 147.87
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 9.34s
                        Total time: 36383.47s
                               ETA: 998718.5s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.593s, learning 0.168s)
               Value function loss: 128.2539
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 594.99
               Mean episode length: 147.89
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 9.76s
                        Total time: 36393.23s
                               ETA: 998692.0s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1784 steps/s (collection: 9.001s, learning 0.178s)
               Value function loss: 135.4017
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 564.84
               Mean episode length: 149.69
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 9.18s
                        Total time: 36402.41s
                               ETA: 998649.5s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.414s, learning 0.165s)
               Value function loss: 122.0471
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 595.48
               Mean episode length: 148.95
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 9.58s
                        Total time: 36411.99s
                               ETA: 998618.0s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.438s, learning 0.165s)
               Value function loss: 113.5398
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 592.33
               Mean episode length: 147.84
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 9.60s
                        Total time: 36421.60s
                               ETA: 998587.2s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.514s, learning 0.160s)
               Value function loss: 121.4065
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 612.02
               Mean episode length: 149.79
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 9.67s
                        Total time: 36431.27s
                               ETA: 998558.3s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.868s, learning 0.185s)
               Value function loss: 117.7477
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 604.65
               Mean episode length: 149.61
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 10.05s
                        Total time: 36441.32s
                               ETA: 998539.8s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.402s, learning 0.166s)
               Value function loss: 128.6351
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 616.57
               Mean episode length: 148.82
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 9.57s
                        Total time: 36450.89s
                               ETA: 998508.1s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.649s, learning 0.159s)
               Value function loss: 129.9128
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 606.61
               Mean episode length: 147.44
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 9.81s
                        Total time: 36460.70s
                               ETA: 998482.9s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.376s, learning 0.164s)
               Value function loss: 108.5107
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 594.76
               Mean episode length: 146.77
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 9.54s
                        Total time: 36470.24s
                               ETA: 998450.4s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.694s, learning 0.167s)
               Value function loss: 101.7793
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 610.26
               Mean episode length: 148.63
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 9.86s
                        Total time: 36480.10s
                               ETA: 998426.6s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.142s, learning 0.174s)
               Value function loss: 108.5346
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 630.91
               Mean episode length: 149.51
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 9.32s
                        Total time: 36489.41s
                               ETA: 998388.0s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1724 steps/s (collection: 9.334s, learning 0.169s)
               Value function loss: 113.4373
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 602.19
               Mean episode length: 149.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 9.50s
                        Total time: 36498.92s
                               ETA: 998354.6s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1652 steps/s (collection: 9.745s, learning 0.170s)
               Value function loss: 115.0542
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 600.02
               Mean episode length: 147.32
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 9.91s
                        Total time: 36508.83s
                               ETA: 998332.3s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.405s, learning 0.175s)
               Value function loss: 119.9502
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 627.70
               Mean episode length: 149.02
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 9.58s
                        Total time: 36518.41s
                               ETA: 998301.0s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.281s, learning 0.190s)
               Value function loss: 101.0435
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 619.05
               Mean episode length: 148.21
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 9.47s
                        Total time: 36527.88s
                               ETA: 998266.7s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.505s, learning 0.189s)
               Value function loss: 106.4522
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 609.86
               Mean episode length: 148.51
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 9.69s
                        Total time: 36537.58s
                               ETA: 998238.4s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 1769 steps/s (collection: 9.093s, learning 0.166s)
               Value function loss: 107.7708
                    Surrogate loss: -0.0178
             Mean action noise std: 0.73
                       Mean reward: 633.73
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 9.26s
                        Total time: 36546.83s
                               ETA: 998198.3s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 1720 steps/s (collection: 9.328s, learning 0.193s)
               Value function loss: 114.5732
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 605.57
               Mean episode length: 147.07
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 9.52s
                        Total time: 36556.36s
                               ETA: 998165.4s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.595s, learning 0.179s)
               Value function loss: 105.2315
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 634.51
               Mean episode length: 148.66
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 9.77s
                        Total time: 36566.13s
                               ETA: 998139.4s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.685s, learning 0.180s)
               Value function loss: 117.1893
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 615.49
               Mean episode length: 149.40
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 9.86s
                        Total time: 36575.99s
                               ETA: 998115.9s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.382s, learning 0.173s)
               Value function loss: 110.7755
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 605.33
               Mean episode length: 149.55
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 9.56s
                        Total time: 36585.55s
                               ETA: 998084.0s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.282s, learning 0.170s)
               Value function loss: 118.2053
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 620.41
               Mean episode length: 149.14
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 9.45s
                        Total time: 36595.00s
                               ETA: 998049.3s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.601s, learning 0.167s)
               Value function loss: 100.9858
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 629.26
               Mean episode length: 148.47
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 9.77s
                        Total time: 36604.77s
                               ETA: 998023.2s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.399s, learning 0.170s)
               Value function loss: 101.8310
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 628.77
               Mean episode length: 149.24
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 9.57s
                        Total time: 36614.34s
                               ETA: 997991.6s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.542s, learning 0.160s)
               Value function loss: 120.2512
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 626.84
               Mean episode length: 148.08
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 9.70s
                        Total time: 36624.04s
                               ETA: 997963.7s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.464s, learning 0.163s)
               Value function loss: 124.4351
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 622.04
               Mean episode length: 150.00
                  Mean reward/step: 4.32
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 9.63s
                        Total time: 36633.67s
                               ETA: 997933.8s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.427s, learning 0.165s)
               Value function loss: 115.1409
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 625.55
               Mean episode length: 150.00
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 9.59s
                        Total time: 36643.26s
                               ETA: 997902.9s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1638 steps/s (collection: 9.830s, learning 0.171s)
               Value function loss: 97.1361
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 598.11
               Mean episode length: 148.45
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 10.00s
                        Total time: 36653.26s
                               ETA: 997883.2s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.560s, learning 0.160s)
               Value function loss: 97.8998
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 628.87
               Mean episode length: 149.62
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 9.72s
                        Total time: 36662.98s
                               ETA: 997855.8s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.489s, learning 0.164s)
               Value function loss: 125.3465
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 616.27
               Mean episode length: 147.81
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 9.65s
                        Total time: 36672.63s
                               ETA: 997826.7s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.733s, learning 0.176s)
               Value function loss: 117.1085
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 643.89
               Mean episode length: 148.84
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 9.91s
                        Total time: 36682.54s
                               ETA: 997804.5s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.969s, learning 0.173s)
               Value function loss: 107.5094
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 639.01
               Mean episode length: 149.69
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 9.14s
                        Total time: 36691.68s
                               ETA: 997761.4s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.690s, learning 0.168s)
               Value function loss: 121.8145
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 622.32
               Mean episode length: 147.83
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 9.86s
                        Total time: 36701.54s
                               ETA: 997737.9s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1622 steps/s (collection: 9.930s, learning 0.169s)
               Value function loss: 93.2392
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 622.56
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 10.10s
                        Total time: 36711.64s
                               ETA: 997720.9s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.467s, learning 0.169s)
               Value function loss: 103.2381
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 632.15
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 9.64s
                        Total time: 36721.28s
                               ETA: 997691.3s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.319s, learning 0.159s)
               Value function loss: 101.9213
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 636.80
               Mean episode length: 149.47
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 9.48s
                        Total time: 36730.76s
                               ETA: 997657.4s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.204s, learning 0.167s)
               Value function loss: 97.1118
                    Surrogate loss: -0.0205
             Mean action noise std: 0.73
                       Mean reward: 647.76
               Mean episode length: 149.85
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 9.37s
                        Total time: 36740.13s
                               ETA: 997620.7s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.661s, learning 0.166s)
               Value function loss: 120.6947
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: 625.70
               Mean episode length: 149.57
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 9.83s
                        Total time: 36749.96s
                               ETA: 997596.3s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.359s, learning 0.168s)
               Value function loss: 127.7128
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 625.67
               Mean episode length: 149.95
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 9.53s
                        Total time: 36759.48s
                               ETA: 997563.9s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.353s, learning 0.165s)
               Value function loss: 111.9621
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 612.55
               Mean episode length: 149.64
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 9.52s
                        Total time: 36769.00s
                               ETA: 997531.1s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1767 steps/s (collection: 9.113s, learning 0.159s)
               Value function loss: 102.8144
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 616.33
               Mean episode length: 147.92
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 9.27s
                        Total time: 36778.27s
                               ETA: 997491.7s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.624s, learning 0.165s)
               Value function loss: 108.1559
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 631.11
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 9.79s
                        Total time: 36788.06s
                               ETA: 997466.3s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1671 steps/s (collection: 9.641s, learning 0.163s)
               Value function loss: 104.6896
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 638.70
               Mean episode length: 149.83
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 9.80s
                        Total time: 36797.87s
                               ETA: 997441.4s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.622s, learning 0.166s)
               Value function loss: 124.4274
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 607.90
               Mean episode length: 149.16
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 9.79s
                        Total time: 36807.65s
                               ETA: 997416.1s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.291s, learning 0.167s)
               Value function loss: 127.3636
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 620.50
               Mean episode length: 149.11
                  Mean reward/step: 4.34
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 9.46s
                        Total time: 36817.11s
                               ETA: 997381.8s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1592 steps/s (collection: 10.123s, learning 0.167s)
               Value function loss: 123.5500
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 644.60
               Mean episode length: 149.84
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 10.29s
                        Total time: 36827.40s
                               ETA: 997370.0s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.398s, learning 0.166s)
               Value function loss: 108.9103
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 646.40
               Mean episode length: 150.00
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 9.56s
                        Total time: 36836.97s
                               ETA: 997338.6s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.563s, learning 0.159s)
               Value function loss: 121.4794
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 629.89
               Mean episode length: 149.35
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 9.72s
                        Total time: 36846.69s
                               ETA: 997311.5s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1637 steps/s (collection: 9.844s, learning 0.162s)
               Value function loss: 123.7999
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 629.02
               Mean episode length: 148.67
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 10.01s
                        Total time: 36856.69s
                               ETA: 997292.1s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.510s, learning 0.159s)
               Value function loss: 150.5578
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 618.86
               Mean episode length: 148.74
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 9.67s
                        Total time: 36866.36s
                               ETA: 997263.5s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.615s, learning 0.167s)
               Value function loss: 161.6217
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 650.54
               Mean episode length: 149.08
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 9.78s
                        Total time: 36876.15s
                               ETA: 997238.1s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.500s, learning 0.163s)
               Value function loss: 158.5276
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 635.08
               Mean episode length: 148.86
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 9.66s
                        Total time: 36885.81s
                               ETA: 997209.4s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.866s, learning 0.171s)
               Value function loss: 162.5235
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 634.98
               Mean episode length: 149.73
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 10.04s
                        Total time: 36895.84s
                               ETA: 997190.9s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.420s, learning 0.166s)
               Value function loss: 137.0560
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 636.14
               Mean episode length: 149.84
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 9.59s
                        Total time: 36905.43s
                               ETA: 997160.1s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.580s, learning 0.162s)
               Value function loss: 136.5980
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 632.92
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 9.74s
                        Total time: 36915.17s
                               ETA: 997133.6s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.671s, learning 0.160s)
               Value function loss: 114.7201
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 616.41
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 9.83s
                        Total time: 36925.00s
                               ETA: 997109.5s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 1668 steps/s (collection: 9.655s, learning 0.168s)
               Value function loss: 143.2882
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 628.80
               Mean episode length: 150.00
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 9.82s
                        Total time: 36934.83s
                               ETA: 997085.2s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.620s, learning 0.159s)
               Value function loss: 145.7339
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 629.97
               Mean episode length: 148.44
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 9.78s
                        Total time: 36944.61s
                               ETA: 997059.7s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.434s, learning 0.162s)
               Value function loss: 113.5959
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 634.90
               Mean episode length: 149.96
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 9.60s
                        Total time: 36954.20s
                               ETA: 997029.3s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.252s, learning 0.161s)
               Value function loss: 113.0039
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 637.29
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 9.41s
                        Total time: 36963.61s
                               ETA: 996994.0s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.057s, learning 0.163s)
               Value function loss: 133.6348
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 651.54
               Mean episode length: 149.04
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 9.22s
                        Total time: 36972.83s
                               ETA: 996953.5s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.237s, learning 0.170s)
               Value function loss: 115.8795
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 632.45
               Mean episode length: 149.28
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 9.41s
                        Total time: 36982.24s
                               ETA: 996918.0s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.784s, learning 0.161s)
               Value function loss: 109.8452
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 637.90
               Mean episode length: 149.84
                  Mean reward/step: 4.31
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 9.94s
                        Total time: 36992.19s
                               ETA: 996897.0s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1769 steps/s (collection: 9.100s, learning 0.161s)
               Value function loss: 128.9183
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 624.95
               Mean episode length: 149.11
                  Mean reward/step: 4.29
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 9.26s
                        Total time: 37001.45s
                               ETA: 996857.7s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.375s, learning 0.161s)
               Value function loss: 114.5199
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 624.24
               Mean episode length: 149.73
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 9.54s
                        Total time: 37010.98s
                               ETA: 996825.7s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.599s, learning 0.158s)
               Value function loss: 120.4093
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 621.82
               Mean episode length: 149.34
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 9.76s
                        Total time: 37020.74s
                               ETA: 996799.7s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.276s, learning 0.162s)
               Value function loss: 151.3994
                    Surrogate loss: 0.0025
             Mean action noise std: 0.73
                       Mean reward: 645.42
               Mean episode length: 149.41
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 9.44s
                        Total time: 37030.18s
                               ETA: 996765.2s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.562s, learning 0.162s)
               Value function loss: 143.6656
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 643.78
               Mean episode length: 148.51
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 9.72s
                        Total time: 37039.90s
                               ETA: 996738.3s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.506s, learning 0.174s)
               Value function loss: 129.2283
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 635.80
               Mean episode length: 150.00
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 9.68s
                        Total time: 37049.58s
                               ETA: 996710.3s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.532s, learning 0.162s)
               Value function loss: 136.3921
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 640.21
               Mean episode length: 149.73
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 9.69s
                        Total time: 37059.28s
                               ETA: 996682.6s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.287s, learning 0.167s)
               Value function loss: 144.7776
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 641.64
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 9.45s
                        Total time: 37068.73s
                               ETA: 996648.6s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.734s, learning 0.168s)
               Value function loss: 122.2543
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 640.89
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 9.90s
                        Total time: 37078.63s
                               ETA: 996626.5s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.475s, learning 0.158s)
               Value function loss: 119.6571
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 645.45
               Mean episode length: 149.55
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 9.63s
                        Total time: 37088.27s
                               ETA: 996597.3s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.718s, learning 0.157s)
               Value function loss: 130.3979
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 638.88
               Mean episode length: 149.34
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 9.88s
                        Total time: 37098.14s
                               ETA: 996574.5s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.176s, learning 0.161s)
               Value function loss: 120.5394
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 631.54
               Mean episode length: 149.20
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 9.34s
                        Total time: 37107.48s
                               ETA: 996537.3s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.672s, learning 0.159s)
               Value function loss: 139.6064
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 614.45
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 9.83s
                        Total time: 37117.31s
                               ETA: 996513.4s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.287s, learning 0.167s)
               Value function loss: 150.8405
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 614.08
               Mean episode length: 148.45
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 9.45s
                        Total time: 37126.76s
                               ETA: 996479.4s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 1717 steps/s (collection: 9.374s, learning 0.166s)
               Value function loss: 124.1432
                    Surrogate loss: -0.0216
             Mean action noise std: 0.73
                       Mean reward: 630.68
               Mean episode length: 149.15
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 9.54s
                        Total time: 37136.30s
                               ETA: 996447.7s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.170s, learning 0.161s)
               Value function loss: 119.3196
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 614.29
               Mean episode length: 149.63
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 9.33s
                        Total time: 37145.63s
                               ETA: 996410.4s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 1706 steps/s (collection: 9.440s, learning 0.163s)
               Value function loss: 119.3553
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 663.99
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 9.60s
                        Total time: 37155.24s
                               ETA: 996380.4s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.499s, learning 0.161s)
               Value function loss: 133.2982
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 619.86
               Mean episode length: 148.59
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 9.66s
                        Total time: 37164.90s
                               ETA: 996352.0s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.417s, learning 0.171s)
               Value function loss: 137.2967
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 625.71
               Mean episode length: 149.30
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 9.59s
                        Total time: 37174.48s
                               ETA: 996321.7s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.327s, learning 0.164s)
               Value function loss: 144.3910
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 631.98
               Mean episode length: 148.75
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 9.49s
                        Total time: 37183.98s
                               ETA: 996288.7s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.511s, learning 0.158s)
               Value function loss: 137.9000
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 596.51
               Mean episode length: 148.13
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 9.67s
                        Total time: 37193.64s
                               ETA: 996260.5s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.562s, learning 0.160s)
               Value function loss: 126.1974
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 600.55
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 9.72s
                        Total time: 37203.37s
                               ETA: 996233.8s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.414s, learning 0.162s)
               Value function loss: 123.2033
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 643.35
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 9.58s
                        Total time: 37212.94s
                               ETA: 996203.2s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.479s, learning 0.171s)
               Value function loss: 137.4089
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 614.64
               Mean episode length: 149.79
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 9.65s
                        Total time: 37222.59s
                               ETA: 996174.5s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.191s, learning 0.166s)
               Value function loss: 139.5787
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 613.80
               Mean episode length: 148.82
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 9.36s
                        Total time: 37231.95s
                               ETA: 996138.1s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.461s, learning 0.161s)
               Value function loss: 137.1113
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 623.76
               Mean episode length: 149.44
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 9.62s
                        Total time: 37241.57s
                               ETA: 996108.7s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.320s, learning 0.163s)
               Value function loss: 129.1038
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 633.93
               Mean episode length: 149.97
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 9.48s
                        Total time: 37251.06s
                               ETA: 996075.7s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.685s, learning 0.178s)
               Value function loss: 134.9384
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 630.87
               Mean episode length: 149.26
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 9.86s
                        Total time: 37260.92s
                               ETA: 996052.8s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1757 steps/s (collection: 9.160s, learning 0.160s)
               Value function loss: 147.0717
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 613.31
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 9.32s
                        Total time: 37270.24s
                               ETA: 996015.4s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.761s, learning 0.160s)
               Value function loss: 158.9930
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 612.41
               Mean episode length: 148.68
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 9.92s
                        Total time: 37280.16s
                               ETA: 995994.0s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.138s, learning 0.171s)
               Value function loss: 127.2948
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 626.60
               Mean episode length: 149.75
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 9.31s
                        Total time: 37289.47s
                               ETA: 995956.4s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1733 steps/s (collection: 9.295s, learning 0.157s)
               Value function loss: 146.3793
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 634.58
               Mean episode length: 149.97
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 9.45s
                        Total time: 37298.92s
                               ETA: 995922.5s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 1707 steps/s (collection: 9.434s, learning 0.161s)
               Value function loss: 132.8058
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 602.50
               Mean episode length: 149.70
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 9.59s
                        Total time: 37308.52s
                               ETA: 995892.5s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 1630 steps/s (collection: 9.884s, learning 0.162s)
               Value function loss: 131.8250
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 638.94
               Mean episode length: 149.91
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 10.05s
                        Total time: 37318.56s
                               ETA: 995874.6s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.280s, learning 0.161s)
               Value function loss: 112.3007
                    Surrogate loss: 0.0061
             Mean action noise std: 0.73
                       Mean reward: 625.56
               Mean episode length: 149.11
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 9.44s
                        Total time: 37328.00s
                               ETA: 995840.5s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.634s, learning 0.160s)
               Value function loss: 106.7571
                    Surrogate loss: -0.0202
             Mean action noise std: 0.73
                       Mean reward: 617.92
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 9.79s
                        Total time: 37337.80s
                               ETA: 995815.8s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.648s, learning 0.164s)
               Value function loss: 132.2074
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 616.93
               Mean episode length: 149.55
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 9.81s
                        Total time: 37347.61s
                               ETA: 995791.6s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.701s, learning 0.157s)
               Value function loss: 127.4919
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 632.68
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 9.86s
                        Total time: 37357.47s
                               ETA: 995768.7s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1632 steps/s (collection: 9.872s, learning 0.162s)
               Value function loss: 116.0734
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 554.32
               Mean episode length: 149.91
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 10.03s
                        Total time: 37367.50s
                               ETA: 995750.5s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.323s, learning 0.167s)
               Value function loss: 100.3564
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 595.49
               Mean episode length: 149.88
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 9.49s
                        Total time: 37376.99s
                               ETA: 995717.7s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.558s, learning 0.164s)
               Value function loss: 105.6679
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 617.72
               Mean episode length: 149.65
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 9.72s
                        Total time: 37386.71s
                               ETA: 995691.2s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.454s, learning 0.167s)
               Value function loss: 117.9573
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 602.71
               Mean episode length: 149.77
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 9.62s
                        Total time: 37396.34s
                               ETA: 995661.9s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.459s, learning 0.158s)
               Value function loss: 120.9539
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 618.38
               Mean episode length: 149.99
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 9.62s
                        Total time: 37405.95s
                               ETA: 995632.6s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.315s, learning 0.158s)
               Value function loss: 122.2849
                    Surrogate loss: -0.0193
             Mean action noise std: 0.73
                       Mean reward: 596.37
               Mean episode length: 149.24
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 9.47s
                        Total time: 37415.43s
                               ETA: 995599.5s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.677s, learning 0.159s)
               Value function loss: 118.0735
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 609.68
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 9.84s
                        Total time: 37425.26s
                               ETA: 995576.0s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.852s, learning 0.165s)
               Value function loss: 94.0622
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 615.27
               Mean episode length: 149.02
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 10.02s
                        Total time: 37435.28s
                               ETA: 995557.3s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.488s, learning 0.163s)
               Value function loss: 102.8266
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 622.86
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 9.65s
                        Total time: 37444.93s
                               ETA: 995529.0s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.556s, learning 0.163s)
               Value function loss: 115.9773
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 642.49
               Mean episode length: 149.02
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 9.72s
                        Total time: 37454.65s
                               ETA: 995502.4s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.674s, learning 0.161s)
               Value function loss: 124.5069
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 625.88
               Mean episode length: 148.62
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 9.84s
                        Total time: 37464.48s
                               ETA: 995478.9s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.270s, learning 0.170s)
               Value function loss: 120.0415
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 616.89
               Mean episode length: 149.14
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 9.44s
                        Total time: 37473.92s
                               ETA: 995445.0s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.585s, learning 0.171s)
               Value function loss: 125.5801
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 586.49
               Mean episode length: 149.74
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 9.76s
                        Total time: 37483.68s
                               ETA: 995419.5s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.313s, learning 0.174s)
               Value function loss: 107.6569
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 601.20
               Mean episode length: 149.51
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 9.49s
                        Total time: 37493.17s
                               ETA: 995386.8s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.456s, learning 0.162s)
               Value function loss: 108.3419
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 616.93
               Mean episode length: 149.65
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 9.62s
                        Total time: 37502.78s
                               ETA: 995357.6s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.463s, learning 0.163s)
               Value function loss: 110.2375
                    Surrogate loss: -0.0197
             Mean action noise std: 0.73
                       Mean reward: 621.17
               Mean episode length: 149.77
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 9.63s
                        Total time: 37512.41s
                               ETA: 995328.6s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.341s, learning 0.177s)
               Value function loss: 104.1851
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 623.24
               Mean episode length: 148.70
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 9.52s
                        Total time: 37521.93s
                               ETA: 995296.8s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.591s, learning 0.160s)
               Value function loss: 121.3226
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 622.49
               Mean episode length: 148.50
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 9.75s
                        Total time: 37531.68s
                               ETA: 995271.2s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.762s, learning 0.164s)
               Value function loss: 113.4217
                    Surrogate loss: -0.0187
             Mean action noise std: 0.73
                       Mean reward: 631.92
               Mean episode length: 150.00
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 8.93s
                        Total time: 37540.61s
                               ETA: 995223.7s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.521s, learning 0.166s)
               Value function loss: 117.1149
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 622.91
               Mean episode length: 149.71
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 9.69s
                        Total time: 37550.29s
                               ETA: 995196.3s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.696s, learning 0.166s)
               Value function loss: 90.8628
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 633.49
               Mean episode length: 149.06
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 9.86s
                        Total time: 37560.15s
                               ETA: 995173.7s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.555s, learning 0.161s)
               Value function loss: 105.1160
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 606.06
               Mean episode length: 148.98
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 9.72s
                        Total time: 37569.87s
                               ETA: 995147.2s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.289s, learning 0.173s)
               Value function loss: 99.2055
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 627.88
               Mean episode length: 149.89
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 9.46s
                        Total time: 37579.33s
                               ETA: 995113.9s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.193s, learning 0.168s)
               Value function loss: 100.1072
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 626.72
               Mean episode length: 149.84
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 9.36s
                        Total time: 37588.69s
                               ETA: 995078.0s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 1654 steps/s (collection: 9.743s, learning 0.162s)
               Value function loss: 106.2136
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 607.74
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 9.90s
                        Total time: 37598.60s
                               ETA: 995056.5s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.248s, learning 0.161s)
               Value function loss: 107.8959
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 636.82
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 9.41s
                        Total time: 37608.01s
                               ETA: 995021.9s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.627s, learning 0.159s)
               Value function loss: 96.2288
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 610.53
               Mean episode length: 148.97
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 9.79s
                        Total time: 37617.79s
                               ETA: 994997.3s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.249s, learning 0.166s)
               Value function loss: 92.8277
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 625.13
               Mean episode length: 149.90
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 9.41s
                        Total time: 37627.21s
                               ETA: 994962.9s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.404s, learning 0.162s)
               Value function loss: 97.3980
                    Surrogate loss: -0.0171
             Mean action noise std: 0.73
                       Mean reward: 630.02
               Mean episode length: 149.93
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 9.57s
                        Total time: 37636.77s
                               ETA: 994932.5s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.326s, learning 0.167s)
               Value function loss: 108.3464
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 628.62
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 9.49s
                        Total time: 37646.27s
                               ETA: 994900.1s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 1672 steps/s (collection: 9.634s, learning 0.163s)
               Value function loss: 125.1476
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 638.04
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 9.80s
                        Total time: 37656.06s
                               ETA: 994875.9s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.387s, learning 0.164s)
               Value function loss: 120.2606
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 622.53
               Mean episode length: 147.57
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 9.55s
                        Total time: 37665.61s
                               ETA: 994845.1s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.323s, learning 0.161s)
               Value function loss: 100.8646
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 617.70
               Mean episode length: 149.87
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 9.48s
                        Total time: 37675.10s
                               ETA: 994812.6s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 1739 steps/s (collection: 9.252s, learning 0.165s)
               Value function loss: 94.2667
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 600.19
               Mean episode length: 149.63
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 9.42s
                        Total time: 37684.52s
                               ETA: 994778.3s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.502s, learning 0.159s)
               Value function loss: 105.6319
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 645.86
               Mean episode length: 149.89
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 9.66s
                        Total time: 37694.18s
                               ETA: 994750.4s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.283s, learning 0.162s)
               Value function loss: 110.9734
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 639.53
               Mean episode length: 149.87
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 9.44s
                        Total time: 37703.62s
                               ETA: 994716.9s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 1748 steps/s (collection: 9.214s, learning 0.158s)
               Value function loss: 105.6756
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 631.32
               Mean episode length: 149.66
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 9.37s
                        Total time: 37712.99s
                               ETA: 994681.4s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.448s, learning 0.162s)
               Value function loss: 120.1079
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 614.10
               Mean episode length: 148.91
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 9.61s
                        Total time: 37722.60s
                               ETA: 994652.3s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.569s, learning 0.160s)
               Value function loss: 104.9530
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 618.53
               Mean episode length: 149.69
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 9.73s
                        Total time: 37732.33s
                               ETA: 994626.3s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 1631 steps/s (collection: 9.870s, learning 0.170s)
               Value function loss: 117.7155
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 603.10
               Mean episode length: 148.74
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 10.04s
                        Total time: 37742.37s
                               ETA: 994608.5s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.329s, learning 0.161s)
               Value function loss: 135.4249
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 625.47
               Mean episode length: 147.66
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 9.49s
                        Total time: 37751.86s
                               ETA: 994576.2s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.326s, learning 0.169s)
               Value function loss: 115.9301
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 619.38
               Mean episode length: 148.88
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 9.49s
                        Total time: 37761.35s
                               ETA: 994544.1s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 1718 steps/s (collection: 9.370s, learning 0.165s)
               Value function loss: 135.5371
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 650.36
               Mean episode length: 148.21
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 9.54s
                        Total time: 37770.89s
                               ETA: 994513.0s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 1641 steps/s (collection: 9.810s, learning 0.173s)
               Value function loss: 116.8162
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 626.38
               Mean episode length: 148.81
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 9.98s
                        Total time: 37780.87s
                               ETA: 994493.7s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 1760 steps/s (collection: 9.148s, learning 0.160s)
               Value function loss: 118.7434
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 635.72
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 9.31s
                        Total time: 37790.18s
                               ETA: 994456.7s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 1603 steps/s (collection: 10.058s, learning 0.160s)
               Value function loss: 106.7957
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 622.62
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 10.22s
                        Total time: 37800.40s
                               ETA: 994443.6s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.109s, learning 0.170s)
               Value function loss: 99.5260
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 643.55
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 9.28s
                        Total time: 37809.68s
                               ETA: 994405.9s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.473s, learning 0.159s)
               Value function loss: 129.7197
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 633.24
               Mean episode length: 148.83
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 9.63s
                        Total time: 37819.31s
                               ETA: 994377.4s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.231s, learning 0.158s)
               Value function loss: 89.1319
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 615.76
               Mean episode length: 149.77
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 9.39s
                        Total time: 37828.70s
                               ETA: 994342.6s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.504s, learning 0.172s)
               Value function loss: 127.5939
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 617.74
               Mean episode length: 149.95
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 9.68s
                        Total time: 37838.38s
                               ETA: 994315.3s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 1661 steps/s (collection: 9.697s, learning 0.166s)
               Value function loss: 119.7435
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 615.49
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 9.86s
                        Total time: 37848.24s
                               ETA: 994292.9s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 1768 steps/s (collection: 9.101s, learning 0.161s)
               Value function loss: 117.5253
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 615.96
               Mean episode length: 149.10
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 9.26s
                        Total time: 37857.50s
                               ETA: 994254.8s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.215s, learning 0.161s)
               Value function loss: 105.7916
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 633.55
               Mean episode length: 149.47
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 9.38s
                        Total time: 37866.88s
                               ETA: 994219.7s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.307s, learning 0.187s)
               Value function loss: 107.9824
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 625.97
               Mean episode length: 149.36
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 9.49s
                        Total time: 37876.37s
                               ETA: 994187.6s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 1688 steps/s (collection: 9.521s, learning 0.183s)
               Value function loss: 121.3025
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 634.63
               Mean episode length: 149.97
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 9.70s
                        Total time: 37886.07s
                               ETA: 994161.1s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 1722 steps/s (collection: 9.349s, learning 0.165s)
               Value function loss: 107.7880
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 630.86
               Mean episode length: 149.88
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 9.51s
                        Total time: 37895.59s
                               ETA: 994129.7s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.369s, learning 0.162s)
               Value function loss: 113.2227
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 629.35
               Mean episode length: 150.00
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 9.53s
                        Total time: 37905.12s
                               ETA: 994098.6s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.749s, learning 0.159s)
               Value function loss: 107.9678
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 635.88
               Mean episode length: 149.61
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 9.91s
                        Total time: 37915.03s
                               ETA: 994077.5s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.599s, learning 0.160s)
               Value function loss: 100.0018
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 612.85
               Mean episode length: 149.74
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 9.76s
                        Total time: 37924.79s
                               ETA: 994052.5s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 1750 steps/s (collection: 9.198s, learning 0.162s)
               Value function loss: 102.7038
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 618.54
               Mean episode length: 149.70
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 9.36s
                        Total time: 37934.15s
                               ETA: 994017.0s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1645 steps/s (collection: 9.799s, learning 0.157s)
               Value function loss: 91.6969
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: 635.16
               Mean episode length: 150.00
                  Mean reward/step: 4.29
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 9.96s
                        Total time: 37944.10s
                               ETA: 993997.2s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.661s, learning 0.162s)
               Value function loss: 109.0927
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 634.74
               Mean episode length: 150.00
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 9.82s
                        Total time: 37953.92s
                               ETA: 993973.9s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.665s, learning 0.162s)
               Value function loss: 110.9674
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 627.24
               Mean episode length: 148.88
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 9.83s
                        Total time: 37963.75s
                               ETA: 993950.7s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1640 steps/s (collection: 9.824s, learning 0.161s)
               Value function loss: 93.6116
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 610.87
               Mean episode length: 148.92
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 9.99s
                        Total time: 37973.74s
                               ETA: 993931.6s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.471s, learning 0.175s)
               Value function loss: 122.7503
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 623.92
               Mean episode length: 149.90
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 9.65s
                        Total time: 37983.38s
                               ETA: 993903.7s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.747s, learning 0.173s)
               Value function loss: 99.1739
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 629.30
               Mean episode length: 149.52
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 9.92s
                        Total time: 37993.30s
                               ETA: 993882.9s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.675s, learning 0.163s)
               Value function loss: 121.8455
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 642.93
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 9.84s
                        Total time: 38003.14s
                               ETA: 993860.0s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.714s, learning 0.159s)
               Value function loss: 107.0996
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 634.36
               Mean episode length: 148.73
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 9.87s
                        Total time: 38013.01s
                               ETA: 993838.1s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1710 steps/s (collection: 9.418s, learning 0.160s)
               Value function loss: 117.5735
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 644.67
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 9.58s
                        Total time: 38022.59s
                               ETA: 993808.4s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1745 steps/s (collection: 9.222s, learning 0.162s)
               Value function loss: 109.3110
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 645.04
               Mean episode length: 149.13
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 9.38s
                        Total time: 38031.98s
                               ETA: 993773.7s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1740 steps/s (collection: 9.241s, learning 0.171s)
               Value function loss: 108.8566
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 643.22
               Mean episode length: 149.58
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 9.41s
                        Total time: 38041.39s
                               ETA: 993739.7s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.466s, learning 0.156s)
               Value function loss: 108.4049
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 658.65
               Mean episode length: 148.99
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 9.62s
                        Total time: 38051.01s
                               ETA: 993711.2s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.149s, learning 0.164s)
               Value function loss: 101.7082
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 626.56
               Mean episode length: 149.58
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 9.31s
                        Total time: 38060.32s
                               ETA: 993674.7s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.588s, learning 0.163s)
               Value function loss: 143.7181
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 648.72
               Mean episode length: 148.89
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 9.75s
                        Total time: 38070.07s
                               ETA: 993649.6s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.493s, learning 0.164s)
               Value function loss: 113.3141
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 636.50
               Mean episode length: 149.67
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 9.66s
                        Total time: 38079.73s
                               ETA: 993622.0s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1730 steps/s (collection: 9.304s, learning 0.165s)
               Value function loss: 115.7248
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 635.51
               Mean episode length: 149.98
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 9.47s
                        Total time: 38089.20s
                               ETA: 993589.6s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.314s, learning 0.161s)
               Value function loss: 87.0238
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 614.11
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 9.48s
                        Total time: 38098.68s
                               ETA: 993557.3s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.569s, learning 0.160s)
               Value function loss: 118.8385
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 617.01
               Mean episode length: 150.00
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 9.73s
                        Total time: 38108.40s
                               ETA: 993531.7s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.605s, learning 0.174s)
               Value function loss: 107.4702
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 648.61
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 9.78s
                        Total time: 38118.18s
                               ETA: 993507.4s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1759 steps/s (collection: 9.149s, learning 0.165s)
               Value function loss: 112.4877
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 606.64
               Mean episode length: 148.62
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 9.31s
                        Total time: 38127.50s
                               ETA: 993470.9s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.785s, learning 0.157s)
               Value function loss: 106.3681
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 627.15
               Mean episode length: 150.00
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.94s
                        Total time: 38136.44s
                               ETA: 993424.8s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.273s, learning 0.162s)
               Value function loss: 108.8652
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 608.96
               Mean episode length: 150.00
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 9.44s
                        Total time: 38145.87s
                               ETA: 993391.6s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.459s, learning 0.162s)
               Value function loss: 95.6824
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: 615.81
               Mean episode length: 149.57
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 9.62s
                        Total time: 38155.50s
                               ETA: 993363.2s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1629 steps/s (collection: 9.886s, learning 0.167s)
               Value function loss: 100.3246
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 635.80
               Mean episode length: 150.00
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 10.05s
                        Total time: 38165.55s
                               ETA: 993346.1s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1771 steps/s (collection: 9.084s, learning 0.167s)
               Value function loss: 109.7476
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 615.05
               Mean episode length: 149.57
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 9.25s
                        Total time: 38174.80s
                               ETA: 993308.1s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.679s, learning 0.165s)
               Value function loss: 107.0048
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 615.12
               Mean episode length: 150.00
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 9.84s
                        Total time: 38184.64s
                               ETA: 993285.5s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1735 steps/s (collection: 9.275s, learning 0.163s)
               Value function loss: 120.6027
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 618.79
               Mean episode length: 148.96
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 9.44s
                        Total time: 38194.08s
                               ETA: 993252.4s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.476s, learning 0.161s)
               Value function loss: 136.6644
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 643.41
               Mean episode length: 150.00
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 9.64s
                        Total time: 38203.72s
                               ETA: 993224.5s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.595s, learning 0.179s)
               Value function loss: 118.5471
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 637.19
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 9.77s
                        Total time: 38213.49s
                               ETA: 993200.1s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1698 steps/s (collection: 9.473s, learning 0.174s)
               Value function loss: 120.3745
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 641.05
               Mean episode length: 149.79
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 9.65s
                        Total time: 38223.14s
                               ETA: 993172.5s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.510s, learning 0.165s)
               Value function loss: 119.7627
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 658.28
               Mean episode length: 150.00
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 9.68s
                        Total time: 38232.82s
                               ETA: 993145.6s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.523s, learning 0.171s)
               Value function loss: 119.7695
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 650.08
               Mean episode length: 150.00
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 9.69s
                        Total time: 38242.51s
                               ETA: 993119.2s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1716 steps/s (collection: 9.380s, learning 0.164s)
               Value function loss: 124.9336
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 607.89
               Mean episode length: 149.97
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 9.54s
                        Total time: 38252.05s
                               ETA: 993088.9s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.392s, learning 0.159s)
               Value function loss: 135.3824
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 650.88
               Mean episode length: 150.00
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 9.55s
                        Total time: 38261.60s
                               ETA: 993058.8s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.461s, learning 0.162s)
               Value function loss: 136.3719
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 633.73
               Mean episode length: 150.00
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 9.62s
                        Total time: 38271.23s
                               ETA: 993030.6s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.401s, learning 0.166s)
               Value function loss: 121.2069
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 638.15
               Mean episode length: 150.00
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 9.57s
                        Total time: 38280.79s
                               ETA: 993000.9s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.519s, learning 0.159s)
               Value function loss: 131.3325
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 643.22
               Mean episode length: 150.00
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 9.68s
                        Total time: 38290.47s
                               ETA: 992974.1s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1775 steps/s (collection: 9.067s, learning 0.162s)
               Value function loss: 119.8872
                    Surrogate loss: 0.0024
             Mean action noise std: 0.73
                       Mean reward: 647.13
               Mean episode length: 150.00
                  Mean reward/step: 4.28
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 9.23s
                        Total time: 38299.70s
                               ETA: 992935.7s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1732 steps/s (collection: 9.289s, learning 0.165s)
               Value function loss: 146.2629
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 657.74
               Mean episode length: 150.00
                  Mean reward/step: 4.29
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 9.45s
                        Total time: 38309.15s
                               ETA: 992903.1s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.660s, learning 0.165s)
               Value function loss: 150.9912
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 626.74
               Mean episode length: 150.00
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 9.82s
                        Total time: 38318.98s
                               ETA: 992880.2s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1650 steps/s (collection: 9.746s, learning 0.181s)
               Value function loss: 121.6543
                    Surrogate loss: 0.0046
             Mean action noise std: 0.73
                       Mean reward: 644.95
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 9.93s
                        Total time: 38328.91s
                               ETA: 992859.9s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.620s, learning 0.187s)
               Value function loss: 99.5149
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 627.05
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 9.81s
                        Total time: 38338.71s
                               ETA: 992836.5s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.183s, learning 0.165s)
               Value function loss: 121.7399
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 643.22
               Mean episode length: 149.61
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 9.35s
                        Total time: 38348.06s
                               ETA: 992801.3s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.839s, learning 0.180s)
               Value function loss: 128.2357
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 618.54
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 10.02s
                        Total time: 38358.08s
                               ETA: 992783.4s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.558s, learning 0.174s)
               Value function loss: 104.7562
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 642.25
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 9.73s
                        Total time: 38367.81s
                               ETA: 992758.1s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.518s, learning 0.173s)
               Value function loss: 140.6147
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 632.18
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 9.69s
                        Total time: 38377.50s
                               ETA: 992731.7s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.309s, learning 0.167s)
               Value function loss: 119.7136
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: 621.84
               Mean episode length: 149.06
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 9.48s
                        Total time: 38386.98s
                               ETA: 992699.8s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1676 steps/s (collection: 9.598s, learning 0.176s)
               Value function loss: 98.5923
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 614.72
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 9.77s
                        Total time: 38396.75s
                               ETA: 992675.6s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1663 steps/s (collection: 9.658s, learning 0.194s)
               Value function loss: 104.9234
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 632.85
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 9.85s
                        Total time: 38406.60s
                               ETA: 992653.4s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.544s, learning 0.178s)
               Value function loss: 104.0216
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 624.04
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 9.72s
                        Total time: 38416.32s
                               ETA: 992627.9s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.306s, learning 0.172s)
               Value function loss: 94.4122
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 643.41
               Mean episode length: 150.00
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 9.48s
                        Total time: 38425.80s
                               ETA: 992596.1s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1693 steps/s (collection: 9.507s, learning 0.167s)
               Value function loss: 110.1641
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 619.57
               Mean episode length: 148.94
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 9.67s
                        Total time: 38435.47s
                               ETA: 992569.3s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 1704 steps/s (collection: 9.448s, learning 0.165s)
               Value function loss: 123.4773
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 640.33
               Mean episode length: 150.00
                  Mean reward/step: 4.26
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 9.61s
                        Total time: 38445.09s
                               ETA: 992541.0s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.306s, learning 0.166s)
               Value function loss: 105.9570
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 630.78
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 9.47s
                        Total time: 38454.56s
                               ETA: 992509.1s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1719 steps/s (collection: 9.348s, learning 0.183s)
               Value function loss: 111.8456
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 639.89
               Mean episode length: 149.40
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 9.53s
                        Total time: 38464.09s
                               ETA: 992478.7s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1715 steps/s (collection: 9.390s, learning 0.160s)
               Value function loss: 119.6835
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 625.16
               Mean episode length: 148.93
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 9.55s
                        Total time: 38473.64s
                               ETA: 992448.8s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.405s, learning 0.169s)
               Value function loss: 113.0794
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 632.28
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 9.57s
                        Total time: 38483.22s
                               ETA: 992419.5s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.748s, learning 0.172s)
               Value function loss: 103.2656
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 621.56
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 9.92s
                        Total time: 38493.13s
                               ETA: 992399.2s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.536s, learning 0.162s)
               Value function loss: 124.8416
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 644.96
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 9.70s
                        Total time: 38502.83s
                               ETA: 992373.1s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 1690 steps/s (collection: 9.530s, learning 0.162s)
               Value function loss: 115.4336
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 631.08
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 9.69s
                        Total time: 38512.52s
                               ETA: 992346.9s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 1646 steps/s (collection: 9.794s, learning 0.159s)
               Value function loss: 117.0681
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 634.73
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 9.95s
                        Total time: 38522.48s
                               ETA: 992327.5s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1665 steps/s (collection: 9.667s, learning 0.173s)
               Value function loss: 107.8875
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 631.73
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 9.84s
                        Total time: 38532.32s
                               ETA: 992305.1s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 1743 steps/s (collection: 9.232s, learning 0.168s)
               Value function loss: 137.9256
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 606.25
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 9.40s
                        Total time: 38541.72s
                               ETA: 992271.4s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1588 steps/s (collection: 10.135s, learning 0.178s)
               Value function loss: 99.8883
                    Surrogate loss: -0.0197
             Mean action noise std: 0.73
                       Mean reward: 636.43
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 10.31s
                        Total time: 38552.03s
                               ETA: 992261.2s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.652s, learning 0.174s)
               Value function loss: 123.3252
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 637.62
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 9.83s
                        Total time: 38561.86s
                               ETA: 992238.5s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.389s, learning 0.169s)
               Value function loss: 141.8188
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 615.70
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 9.56s
                        Total time: 38571.41s
                               ETA: 992208.9s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.985s, learning 0.175s)
               Value function loss: 118.6633
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 609.72
               Mean episode length: 149.60
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 9.16s
                        Total time: 38580.57s
                               ETA: 992169.1s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1694 steps/s (collection: 9.509s, learning 0.162s)
               Value function loss: 105.0724
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 634.34
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 9.67s
                        Total time: 38590.25s
                               ETA: 992142.4s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 1713 steps/s (collection: 9.397s, learning 0.167s)
               Value function loss: 102.0611
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 637.52
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 9.56s
                        Total time: 38599.81s
                               ETA: 992113.0s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 1714 steps/s (collection: 9.395s, learning 0.159s)
               Value function loss: 114.9782
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 621.51
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 9.55s
                        Total time: 38609.36s
                               ETA: 992083.4s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 1708 steps/s (collection: 9.426s, learning 0.163s)
               Value function loss: 102.2435
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 615.90
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 9.59s
                        Total time: 38618.95s
                               ETA: 992054.6s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.425s, learning 0.160s)
               Value function loss: 113.3333
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 605.64
               Mean episode length: 149.73
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 9.59s
                        Total time: 38628.54s
                               ETA: 992025.8s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 1731 steps/s (collection: 9.301s, learning 0.159s)
               Value function loss: 120.8214
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 629.12
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 9.46s
                        Total time: 38638.00s
                               ETA: 991993.8s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.344s, learning 0.171s)
               Value function loss: 119.0993
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 595.83
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 9.52s
                        Total time: 38647.51s
                               ETA: 991963.1s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.305s, learning 0.178s)
               Value function loss: 121.0142
                    Surrogate loss: 0.0075
             Mean action noise std: 0.73
                       Mean reward: 619.72
               Mean episode length: 148.96
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 9.48s
                        Total time: 38657.00s
                               ETA: 991931.7s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.849s, learning 0.168s)
               Value function loss: 141.0675
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 627.79
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 10.02s
                        Total time: 38667.01s
                               ETA: 991914.0s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.204s, learning 0.160s)
               Value function loss: 133.8519
                    Surrogate loss: 0.0089
             Mean action noise std: 0.73
                       Mean reward: 601.08
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 9.36s
                        Total time: 38676.38s
                               ETA: 991879.5s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1630 steps/s (collection: 9.869s, learning 0.178s)
               Value function loss: 143.8566
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 637.70
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 10.05s
                        Total time: 38686.42s
                               ETA: 991862.6s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 1778 steps/s (collection: 9.050s, learning 0.163s)
               Value function loss: 107.4901
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 608.37
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 9.21s
                        Total time: 38695.64s
                               ETA: 991824.3s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1677 steps/s (collection: 9.598s, learning 0.170s)
               Value function loss: 116.2905
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 594.98
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 9.77s
                        Total time: 38705.41s
                               ETA: 991800.3s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1712 steps/s (collection: 9.388s, learning 0.177s)
               Value function loss: 125.9015
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 628.83
               Mean episode length: 149.36
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 9.56s
                        Total time: 38714.97s
                               ETA: 991771.0s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1682 steps/s (collection: 9.563s, learning 0.173s)
               Value function loss: 127.9264
                    Surrogate loss: 0.0091
             Mean action noise std: 0.73
                       Mean reward: 610.70
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 9.74s
                        Total time: 38724.71s
                               ETA: 991746.1s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1670 steps/s (collection: 9.639s, learning 0.169s)
               Value function loss: 115.2301
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 610.54
               Mean episode length: 149.86
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 9.81s
                        Total time: 38734.51s
                               ETA: 991723.1s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1683 steps/s (collection: 9.562s, learning 0.168s)
               Value function loss: 119.9903
                    Surrogate loss: 0.0021
             Mean action noise std: 0.73
                       Mean reward: 623.83
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 9.73s
                        Total time: 38744.24s
                               ETA: 991698.1s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.625s, learning 0.167s)
               Value function loss: 103.7698
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 608.77
               Mean episode length: 150.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 9.79s
                        Total time: 38754.04s
                               ETA: 991674.7s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.664s, learning 0.166s)
               Value function loss: 108.8642
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 610.86
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 9.83s
                        Total time: 38763.87s
                               ETA: 991652.2s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.622s, learning 0.158s)
               Value function loss: 96.6732
                    Surrogate loss: -0.0185
             Mean action noise std: 0.73
                       Mean reward: 614.21
               Mean episode length: 150.00
                  Mean reward/step: 4.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 9.78s
                        Total time: 38773.65s
                               ETA: 991628.5s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.487s, learning 0.165s)
               Value function loss: 86.9955
                    Surrogate loss: -0.0189
             Mean action noise std: 0.73
                       Mean reward: 622.22
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 9.65s
                        Total time: 38783.30s
                               ETA: 991601.6s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1649 steps/s (collection: 9.762s, learning 0.173s)
               Value function loss: 104.0889
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 608.95
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 9.94s
                        Total time: 38793.23s
                               ETA: 991581.8s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1700 steps/s (collection: 9.476s, learning 0.161s)
               Value function loss: 102.5602
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 612.83
               Mean episode length: 149.75
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 9.64s
                        Total time: 38802.87s
                               ETA: 991554.5s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1697 steps/s (collection: 9.494s, learning 0.160s)
               Value function loss: 99.4762
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 615.23
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 9.65s
                        Total time: 38812.52s
                               ETA: 991527.6s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1726 steps/s (collection: 9.320s, learning 0.171s)
               Value function loss: 94.1949
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 606.73
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 9.49s
                        Total time: 38822.02s
                               ETA: 991496.6s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1680 steps/s (collection: 9.579s, learning 0.168s)
               Value function loss: 102.8178
                    Surrogate loss: -0.0194
             Mean action noise std: 0.73
                       Mean reward: 612.82
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 9.75s
                        Total time: 38831.76s
                               ETA: 991472.1s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.437s, learning 0.172s)
               Value function loss: 92.8148
                    Surrogate loss: -0.0228
             Mean action noise std: 0.73
                       Mean reward: 621.28
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 9.61s
                        Total time: 38841.37s
                               ETA: 991444.1s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.311s, learning 0.161s)
               Value function loss: 111.3423
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 599.80
               Mean episode length: 150.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 9.47s
                        Total time: 38850.84s
                               ETA: 991412.6s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1678 steps/s (collection: 9.601s, learning 0.161s)
               Value function loss: 120.7902
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 604.05
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 9.76s
                        Total time: 38860.61s
                               ETA: 991388.4s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.750s, learning 0.159s)
               Value function loss: 128.1305
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 636.23
               Mean episode length: 150.00
                  Mean reward/step: 4.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 9.91s
                        Total time: 38870.51s
                               ETA: 991368.1s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.416s, learning 0.168s)
               Value function loss: 111.0547
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 614.57
               Mean episode length: 150.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 9.58s
                        Total time: 38880.10s
                               ETA: 991339.5s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1669 steps/s (collection: 9.635s, learning 0.181s)
               Value function loss: 111.9762
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 622.30
               Mean episode length: 150.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 9.82s
                        Total time: 38889.91s
                               ETA: 991316.8s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1727 steps/s (collection: 9.310s, learning 0.174s)
               Value function loss: 122.8035
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 613.89
               Mean episode length: 148.97
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 9.48s
                        Total time: 38899.40s
                               ETA: 991285.7s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1684 steps/s (collection: 9.566s, learning 0.160s)
               Value function loss: 121.9707
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 606.24
               Mean episode length: 149.34
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 9.73s
                        Total time: 38909.13s
                               ETA: 991260.7s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1666 steps/s (collection: 9.670s, learning 0.158s)
               Value function loss: 115.0840
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 616.70
               Mean episode length: 150.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 9.83s
                        Total time: 38918.95s
                               ETA: 991238.4s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1701 steps/s (collection: 9.466s, learning 0.164s)
               Value function loss: 133.0683
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 592.71
               Mean episode length: 150.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 9.63s
                        Total time: 38928.58s
                               ETA: 991211.0s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1754 steps/s (collection: 9.173s, learning 0.165s)
               Value function loss: 103.0730
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 617.79
               Mean episode length: 150.00
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 9.34s
                        Total time: 38937.92s
                               ETA: 991176.1s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1705 steps/s (collection: 9.447s, learning 0.161s)
               Value function loss: 107.4572
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 587.67
               Mean episode length: 150.00
                  Mean reward/step: 3.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 9.61s
                        Total time: 38947.53s
                               ETA: 991148.2s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1747 steps/s (collection: 9.214s, learning 0.164s)
               Value function loss: 93.5160
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 613.57
               Mean episode length: 150.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 9.38s
                        Total time: 38956.91s
                               ETA: 991114.4s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.517s, learning 0.160s)
               Value function loss: 86.4190
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 598.57
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 9.68s
                        Total time: 38966.59s
                               ETA: 991088.3s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1615 steps/s (collection: 9.977s, learning 0.163s)
               Value function loss: 99.0963
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 604.56
               Mean episode length: 149.56
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 10.14s
                        Total time: 38976.73s
                               ETA: 991073.9s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1711 steps/s (collection: 9.413s, learning 0.161s)
               Value function loss: 118.1739
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 600.09
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 9.57s
                        Total time: 38986.30s
                               ETA: 991045.1s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1667 steps/s (collection: 9.664s, learning 0.164s)
               Value function loss: 94.4235
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 607.00
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 9.83s
                        Total time: 38996.13s
                               ETA: 991022.8s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.417s, learning 0.168s)
               Value function loss: 93.7711
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 601.40
               Mean episode length: 149.38
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 9.59s
                        Total time: 39005.71s
                               ETA: 990994.3s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1723 steps/s (collection: 9.351s, learning 0.158s)
               Value function loss: 116.0289
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 615.90
               Mean episode length: 150.00
                  Mean reward/step: 4.12
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 9.51s
                        Total time: 39015.22s
                               ETA: 990963.9s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 1627 steps/s (collection: 9.911s, learning 0.158s)
               Value function loss: 102.7023
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 584.91
               Mean episode length: 148.93
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 10.07s
                        Total time: 39025.29s
                               ETA: 990947.8s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1659 steps/s (collection: 9.712s, learning 0.159s)
               Value function loss: 114.1664
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 614.22
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 9.87s
                        Total time: 39035.16s
                               ETA: 990926.6s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 1681 steps/s (collection: 9.580s, learning 0.165s)
               Value function loss: 130.4959
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 620.78
               Mean episode length: 150.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 9.74s
                        Total time: 39044.91s
                               ETA: 990902.2s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 1695 steps/s (collection: 9.500s, learning 0.165s)
               Value function loss: 127.0069
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 605.96
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 9.66s
                        Total time: 39054.57s
                               ETA: 990875.8s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 1673 steps/s (collection: 9.627s, learning 0.161s)
               Value function loss: 102.6563
                    Surrogate loss: -0.0201
             Mean action noise std: 0.73
                       Mean reward: 643.59
               Mean episode length: 150.00
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 9.79s
                        Total time: 39064.36s
                               ETA: 990852.6s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1686 steps/s (collection: 9.548s, learning 0.166s)
               Value function loss: 108.9936
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 620.40
               Mean episode length: 149.12
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 9.71s
                        Total time: 39074.07s
                               ETA: 990827.4s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1679 steps/s (collection: 9.590s, learning 0.165s)
               Value function loss: 118.9451
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 613.38
               Mean episode length: 148.96
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 9.76s
                        Total time: 39083.83s
                               ETA: 990803.4s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.527s, learning 0.158s)
               Value function loss: 98.4132
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 614.15
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 9.69s
                        Total time: 39093.51s
                               ETA: 990777.5s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1696 steps/s (collection: 9.495s, learning 0.165s)
               Value function loss: 123.0835
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 619.66
               Mean episode length: 148.89
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 9.66s
                        Total time: 39103.17s
                               ETA: 990751.0s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1689 steps/s (collection: 9.535s, learning 0.163s)
               Value function loss: 114.1161
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 628.66
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 9.70s
                        Total time: 39112.87s
                               ETA: 990725.5s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1634 steps/s (collection: 9.854s, learning 0.167s)
               Value function loss: 103.3318
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 596.12
               Mean episode length: 149.73
                  Mean reward/step: 4.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 10.02s
                        Total time: 39122.89s
                               ETA: 990708.2s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 1734 steps/s (collection: 9.283s, learning 0.162s)
               Value function loss: 98.9004
                    Surrogate loss: -0.0158
             Mean action noise std: 0.73
                       Mean reward: 604.21
               Mean episode length: 150.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 9.45s
                        Total time: 39132.34s
                               ETA: 990676.3s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 1646 steps/s (collection: 9.795s, learning 0.158s)
               Value function loss: 97.3147
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 635.84
               Mean episode length: 150.00
                  Mean reward/step: 4.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 9.95s
                        Total time: 39142.29s
                               ETA: 990657.3s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.183s, learning 0.164s)
               Value function loss: 114.2329
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 632.63
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 9.35s
                        Total time: 39151.64s
                               ETA: 990623.0s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.462s, learning 0.159s)
               Value function loss: 107.0617
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 610.40
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 9.62s
                        Total time: 39161.26s
                               ETA: 990595.5s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 1728 steps/s (collection: 9.311s, learning 0.169s)
               Value function loss: 128.4262
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 620.19
               Mean episode length: 149.06
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 9.48s
                        Total time: 39170.74s
                               ETA: 990564.6s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 1647 steps/s (collection: 9.780s, learning 0.164s)
               Value function loss: 96.5459
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 620.10
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 9.94s
                        Total time: 39180.68s
                               ETA: 990545.3s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 1721 steps/s (collection: 9.356s, learning 0.160s)
               Value function loss: 100.3805
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 628.53
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 9.52s
                        Total time: 39190.20s
                               ETA: 990515.3s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 1702 steps/s (collection: 9.462s, learning 0.164s)
               Value function loss: 111.2130
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 624.64
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 9.63s
                        Total time: 39199.83s
                               ETA: 990488.0s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 1691 steps/s (collection: 9.511s, learning 0.176s)
               Value function loss: 97.5740
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 627.57
               Mean episode length: 150.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 9.69s
                        Total time: 39209.51s
                               ETA: 990462.3s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 1709 steps/s (collection: 9.413s, learning 0.171s)
               Value function loss: 95.0250
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 622.59
               Mean episode length: 150.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 9.58s
                        Total time: 39219.09s
                               ETA: 990434.0s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 1605 steps/s (collection: 10.036s, learning 0.167s)
               Value function loss: 102.4260
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 617.79
               Mean episode length: 150.00
                  Mean reward/step: 4.21
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 10.20s
                        Total time: 39229.30s
                               ETA: 990421.4s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 1674 steps/s (collection: 9.614s, learning 0.168s)
               Value function loss: 105.1482
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 634.22
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 9.78s
                        Total time: 39239.08s
                               ETA: 990398.1s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 1601 steps/s (collection: 10.070s, learning 0.162s)
               Value function loss: 105.6452
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 636.46
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 10.23s
                        Total time: 39249.31s
                               ETA: 990386.2s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 1692 steps/s (collection: 9.521s, learning 0.160s)
               Value function loss: 93.8035
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 614.72
               Mean episode length: 150.00
                  Mean reward/step: 4.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 9.68s
                        Total time: 39258.99s
                               ETA: 990360.4s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 1675 steps/s (collection: 9.605s, learning 0.175s)
               Value function loss: 131.0212
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 626.98
               Mean episode length: 150.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 9.78s
                        Total time: 39268.77s
                               ETA: 990337.1s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 1738 steps/s (collection: 9.249s, learning 0.178s)
               Value function loss: 112.8716
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 630.62
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 9.43s
                        Total time: 39278.20s
                               ETA: 990304.8s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 1664 steps/s (collection: 9.674s, learning 0.172s)
               Value function loss: 107.1885
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 637.96
               Mean episode length: 150.00
                  Mean reward/step: 4.15
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 9.85s
                        Total time: 39288.05s
                               ETA: 990283.2s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 1660 steps/s (collection: 9.695s, learning 0.169s)
               Value function loss: 137.4128
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 638.28
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 9.86s
                        Total time: 39297.91s
                               ETA: 990262.0s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 1635 steps/s (collection: 9.833s, learning 0.182s)
               Value function loss: 99.1186
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 638.73
               Mean episode length: 150.00
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 10.02s
                        Total time: 39307.93s
                               ETA: 990244.7s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 1643 steps/s (collection: 9.803s, learning 0.168s)
               Value function loss: 91.8574
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 635.73
               Mean episode length: 150.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 9.97s
                        Total time: 39317.90s
                               ETA: 990226.2s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 1703 steps/s (collection: 9.453s, learning 0.164s)
               Value function loss: 85.3252
                    Surrogate loss: -0.0160
             Mean action noise std: 0.73
                       Mean reward: 633.00
               Mean episode length: 150.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 9.62s
                        Total time: 39327.51s
                               ETA: 990198.9s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 1749 steps/s (collection: 9.204s, learning 0.162s)
               Value function loss: 96.6715
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 616.45
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 9.37s
                        Total time: 39336.88s
                               ETA: 990165.2s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 1651 steps/s (collection: 9.762s, learning 0.161s)
               Value function loss: 91.1457
                    Surrogate loss: -0.0198
             Mean action noise std: 0.73
                       Mean reward: 626.18
               Mean episode length: 150.00
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 9.92s
                        Total time: 39346.80s
                               ETA: 990145.5s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 1685 steps/s (collection: 9.559s, learning 0.163s)
               Value function loss: 111.4520
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 613.11
               Mean episode length: 150.00
                  Mean reward/step: 4.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 9.72s
                        Total time: 39356.52s
                               ETA: 990120.8s

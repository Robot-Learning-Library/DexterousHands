/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-2qm9rmdq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandKettle_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/2qm9rmdq
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:3
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Using VHACD cache directory '/data/zihan/.isaacgym/vhacd'
Started convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-1.obj'
Started convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-2.obj'
Started convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-3.obj'
Started convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-4.obj'
Finished convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-4.obj': 64 hulls
Finished convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-3.obj': 64 hulls
Finished convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-2.obj': 64 hulls
Finished convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-1.obj': 64 hulls
Found existing convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-1.obj'
Found existing convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-2.obj'
Found existing convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-3.obj'
Found existing convex decomposition for mesh '../assets/mjcf/kettle/textured_objs/original-4.obj'
Started convex decomposition for mesh '../assets/mjcf/bucket/100454/textured_objs/original-1.obj'
Finished convex decomposition for mesh '../assets/mjcf/bucket/100454/textured_objs/original-1.obj': 12 hulls
Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=3, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=3, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:3', seed=None, sim_device='cuda:3', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandKettle', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_kettle', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 125, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.1, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 4, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandKettle', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandKettle_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 5590
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:3
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 621 steps/s (collection: 26.170s, learning 0.185s)
               Value function loss: 6.0070
                    Surrogate loss: -0.0007
             Mean action noise std: 0.80
                  Mean reward/step: -1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 26.35s
                        Total time: 26.35s
                               ETA: 2635453.2s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 638 steps/s (collection: 24.694s, learning 0.964s)
               Value function loss: 9.6306
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                  Mean reward/step: -1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 25.66s
                        Total time: 52.01s
                               ETA: 2600629.8s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 688 steps/s (collection: 23.620s, learning 0.170s)
               Value function loss: 18.4487
                    Surrogate loss: -0.0120
             Mean action noise std: 0.80
                  Mean reward/step: -1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 23.79s
                        Total time: 75.80s
                               ETA: 2526708.1s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 671 steps/s (collection: 24.152s, learning 0.247s)
               Value function loss: 12.8932
                    Surrogate loss: -0.0148
             Mean action noise std: 0.80
                  Mean reward/step: -1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 24.40s
                        Total time: 100.20s
                               ETA: 2504962.4s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 683 steps/s (collection: 23.793s, learning 0.174s)
               Value function loss: 6.9159
                    Surrogate loss: 0.0024
             Mean action noise std: 0.80
                  Mean reward/step: -1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 23.97s
                        Total time: 124.17s
                               ETA: 2483276.2s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 676 steps/s (collection: 24.034s, learning 0.169s)
               Value function loss: 5.2362
                    Surrogate loss: 0.0022
             Mean action noise std: 0.80
                  Mean reward/step: -1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 24.20s
                        Total time: 148.37s
                               ETA: 2472747.0s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 673 steps/s (collection: 24.149s, learning 0.162s)
               Value function loss: 2.5716
                    Surrogate loss: -0.0103
             Mean action noise std: 0.80
                  Mean reward/step: -1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 24.31s
                        Total time: 172.68s
                               ETA: 2466764.4s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 679 steps/s (collection: 23.939s, learning 0.190s)
               Value function loss: 2.0202
                    Surrogate loss: -0.0037
             Mean action noise std: 0.80
                  Mean reward/step: -1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 24.13s
                        Total time: 196.81s
                               ETA: 2459994.2s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 664 steps/s (collection: 24.492s, learning 0.161s)
               Value function loss: 1.1122
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                  Mean reward/step: -1.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 24.65s
                        Total time: 221.47s
                               ETA: 2460544.2s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 676 steps/s (collection: 24.034s, learning 0.167s)
               Value function loss: 0.8479
                    Surrogate loss: -0.0175
             Mean action noise std: 0.80
                  Mean reward/step: -1.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 24.20s
                        Total time: 245.67s
                               ETA: 2456457.5s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 664 steps/s (collection: 24.459s, learning 0.181s)
               Value function loss: 0.5924
                    Surrogate loss: -0.0279
             Mean action noise std: 0.80
                  Mean reward/step: -1.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 24.64s
                        Total time: 270.31s
                               ETA: 2457093.7s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 670 steps/s (collection: 24.280s, learning 0.164s)
               Value function loss: 0.7053
                    Surrogate loss: 0.0057
             Mean action noise std: 0.80
                  Mean reward/step: -1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 24.44s
                        Total time: 294.75s
                               ETA: 2455988.5s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 699 steps/s (collection: 23.256s, learning 0.172s)
               Value function loss: 1.0188
                    Surrogate loss: -0.0013
             Mean action noise std: 0.80
                  Mean reward/step: -1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 23.43s
                        Total time: 318.18s
                               ETA: 2447237.8s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 676 steps/s (collection: 24.053s, learning 0.171s)
               Value function loss: 1.1533
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                  Mean reward/step: -0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 24.22s
                        Total time: 342.40s
                               ETA: 2445418.9s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 695 steps/s (collection: 23.349s, learning 0.192s)
               Value function loss: 1.8172
                    Surrogate loss: 0.0072
             Mean action noise std: 0.80
                  Mean reward/step: -0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 23.54s
                        Total time: 365.94s
                               ETA: 2439283.7s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 626 steps/s (collection: 25.973s, learning 0.161s)
               Value function loss: 182.5909
                    Surrogate loss: 0.0416
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 26.13s
                        Total time: 392.08s
                               ETA: 2450122.0s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.914s, learning 0.169s)
               Value function loss: 1.2595
                    Surrogate loss: -0.0190
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -1.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 24.08s
                        Total time: 416.16s
                               ETA: 2447613.3s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 675 steps/s (collection: 24.071s, learning 0.174s)
               Value function loss: 3.0124
                    Surrogate loss: 0.0049
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -1.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 24.24s
                        Total time: 440.41s
                               ETA: 2446281.7s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 678 steps/s (collection: 23.931s, learning 0.199s)
               Value function loss: 1.1764
                    Surrogate loss: 0.0228
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -1.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 24.13s
                        Total time: 464.54s
                               ETA: 2444484.3s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 671 steps/s (collection: 24.193s, learning 0.193s)
               Value function loss: 0.3469
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.99
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 24.39s
                        Total time: 488.92s
                               ETA: 2444144.7s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.912s, learning 0.160s)
               Value function loss: 0.4099
                    Surrogate loss: 0.0025
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 24.07s
                        Total time: 512.99s
                               ETA: 2442342.2s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 667 steps/s (collection: 24.258s, learning 0.283s)
               Value function loss: 1.0949
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 24.54s
                        Total time: 537.54s
                               ETA: 2442830.9s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.916s, learning 0.188s)
               Value function loss: 2.4256
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 24.10s
                        Total time: 561.64s
                               ETA: 2441373.8s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 693 steps/s (collection: 23.451s, learning 0.186s)
               Value function loss: 4.2678
                    Surrogate loss: -0.0078
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 23.64s
                        Total time: 585.28s
                               ETA: 2438090.7s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.525s, learning 0.176s)
               Value function loss: 1.9503
                    Surrogate loss: 0.0041
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 23.70s
                        Total time: 608.98s
                               ETA: 2435325.0s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 669 steps/s (collection: 23.712s, learning 0.767s)
               Value function loss: 1.8718
                    Surrogate loss: -0.0001
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 24.48s
                        Total time: 633.46s
                               ETA: 2435762.7s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.731s, learning 0.169s)
               Value function loss: 2.3788
                    Surrogate loss: 0.0032
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 23.90s
                        Total time: 657.36s
                               ETA: 2434022.8s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 665 steps/s (collection: 24.467s, learning 0.171s)
               Value function loss: 2.0490
                    Surrogate loss: -0.0027
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 24.64s
                        Total time: 681.99s
                               ETA: 2435037.1s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.939s, learning 0.170s)
               Value function loss: 3.1292
                    Surrogate loss: -0.0055
             Mean action noise std: 0.80
                       Mean reward: -156.37
               Mean episode length: 124.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 24.11s
                        Total time: 706.10s
                               ETA: 2434158.5s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.778s, learning 0.192s)
               Value function loss: 2.8861
                    Surrogate loss: 0.0053
             Mean action noise std: 0.80
                       Mean reward: -155.92
               Mean episode length: 123.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 23.97s
                        Total time: 730.07s
                               ETA: 2432874.2s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 665 steps/s (collection: 24.467s, learning 0.166s)
               Value function loss: 3.0243
                    Surrogate loss: 0.0416
             Mean action noise std: 0.80
                       Mean reward: -154.56
               Mean episode length: 123.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 24.63s
                        Total time: 754.71s
                               ETA: 2433808.9s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 637 steps/s (collection: 25.514s, learning 0.176s)
               Value function loss: 12.2777
                    Surrogate loss: 0.0185
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 25.69s
                        Total time: 780.40s
                               ETA: 2437986.0s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 673 steps/s (collection: 24.142s, learning 0.188s)
               Value function loss: 3.1805
                    Surrogate loss: 0.0034
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 24.33s
                        Total time: 804.73s
                               ETA: 2437786.0s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 682 steps/s (collection: 23.813s, learning 0.176s)
               Value function loss: 2.8181
                    Surrogate loss: 0.0594
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 23.99s
                        Total time: 828.72s
                               ETA: 2436595.1s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 684 steps/s (collection: 23.783s, learning 0.169s)
               Value function loss: 0.6596
                    Surrogate loss: -0.0025
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 23.95s
                        Total time: 852.67s
                               ETA: 2435364.3s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 673 steps/s (collection: 24.141s, learning 0.171s)
               Value function loss: 0.1795
                    Surrogate loss: -0.0174
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 24.31s
                        Total time: 876.98s
                               ETA: 2435200.4s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.292s, learning 0.186s)
               Value function loss: 0.3329
                    Surrogate loss: 0.0006
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -0.28
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 23.48s
                        Total time: 900.46s
                               ETA: 2432789.9s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 882 steps/s (collection: 18.397s, learning 0.163s)
               Value function loss: 0.7801
                    Surrogate loss: 0.0393
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 18.56s
                        Total time: 919.02s
                               ETA: 2417568.0s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.437s, learning 0.156s)
               Value function loss: 1.2755
                    Surrogate loss: 0.0151
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 12.59s
                        Total time: 931.61s
                               ETA: 2387833.9s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.543s, learning 0.163s)
               Value function loss: 2.2232
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 12.71s
                        Total time: 944.32s
                               ETA: 2359867.7s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.708s, learning 0.168s)
               Value function loss: 2.0443
                    Surrogate loss: 0.0023
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 12.88s
                        Total time: 957.19s
                               ETA: 2333679.0s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.354s, learning 0.162s)
               Value function loss: 1.8998
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 12.52s
                        Total time: 969.71s
                               ETA: 2307880.2s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.241s, learning 0.157s)
               Value function loss: 1.5587
                    Surrogate loss: 0.0050
             Mean action noise std: 0.80
                       Mean reward: -66.99
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 12.40s
                        Total time: 982.11s
                               ETA: 2283006.9s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.399s, learning 0.266s)
               Value function loss: 1.3210
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: -66.82
               Mean episode length: 124.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 12.67s
                        Total time: 994.77s
                               ETA: 2259870.0s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1337 steps/s (collection: 11.994s, learning 0.259s)
               Value function loss: 1.2354
                    Surrogate loss: -0.0288
             Mean action noise std: 0.80
                       Mean reward: -66.61
               Mean episode length: 124.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 12.25s
                        Total time: 1007.02s
                               ETA: 2236845.5s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.387s, learning 0.182s)
               Value function loss: 1.3002
                    Surrogate loss: -0.0309
             Mean action noise std: 0.80
                       Mean reward: -65.85
               Mean episode length: 124.03
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 12.57s
                        Total time: 1019.59s
                               ETA: 2215508.4s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1195 steps/s (collection: 13.522s, learning 0.179s)
               Value function loss: 9.6083
                    Surrogate loss: 0.0041
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 13.70s
                        Total time: 1033.29s
                               ETA: 2197487.1s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.690s, learning 0.244s)
               Value function loss: 2.9477
                    Surrogate loss: 0.1223
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -1.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 12.93s
                        Total time: 1046.23s
                               ETA: 2178616.4s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.258s, learning 0.206s)
               Value function loss: 4.7074
                    Surrogate loss: 0.0240
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 12.46s
                        Total time: 1058.69s
                               ETA: 2159558.4s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.050s, learning 0.167s)
               Value function loss: 1.6393
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 12.22s
                        Total time: 1070.91s
                               ETA: 2140767.0s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.150s, learning 0.197s)
               Value function loss: 0.2873
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 12.35s
                        Total time: 1083.25s
                               ETA: 2122966.1s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.414s, learning 0.197s)
               Value function loss: 0.1249
                    Surrogate loss: -0.0216
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 12.61s
                        Total time: 1095.87s
                               ETA: 2106358.8s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.038s, learning 0.205s)
               Value function loss: 0.5378
                    Surrogate loss: -0.0048
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 12.24s
                        Total time: 1108.11s
                               ETA: 2089682.5s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.968s, learning 0.160s)
               Value function loss: 1.7311
                    Surrogate loss: 0.0008
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 12.13s
                        Total time: 1120.24s
                               ETA: 2073411.7s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.707s, learning 0.170s)
               Value function loss: 2.7324
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 11.88s
                        Total time: 1132.11s
                               ETA: 2057276.7s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.889s, learning 0.205s)
               Value function loss: 1.9347
                    Surrogate loss: 0.0057
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 12.09s
                        Total time: 1144.21s
                               ETA: 2042103.6s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.050s, learning 0.191s)
               Value function loss: 1.7591
                    Surrogate loss: -0.0014
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 12.24s
                        Total time: 1156.45s
                               ETA: 2027721.1s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.273s, learning 0.161s)
               Value function loss: 1.4149
                    Surrogate loss: 0.0011
             Mean action noise std: 0.80
                       Mean reward: -35.36
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 12.43s
                        Total time: 1168.88s
                               ETA: 2014166.2s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.863s, learning 0.162s)
               Value function loss: 1.3265
                    Surrogate loss: -0.0141
             Mean action noise std: 0.80
                       Mean reward: -36.09
               Mean episode length: 124.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 12.02s
                        Total time: 1180.91s
                               ETA: 2000376.9s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.164s, learning 0.164s)
               Value function loss: 1.3254
                    Surrogate loss: -0.0355
             Mean action noise std: 0.80
                       Mean reward: -36.14
               Mean episode length: 123.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 12.33s
                        Total time: 1193.24s
                               ETA: 1987552.2s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.810s, learning 0.200s)
               Value function loss: 1.7496
                    Surrogate loss: -0.0201
             Mean action noise std: 0.80
                       Mean reward: -35.57
               Mean episode length: 123.22
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 12.01s
                        Total time: 1205.25s
                               ETA: 1974628.0s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.636s, learning 0.197s)
               Value function loss: 1.6981
                    Surrogate loss: -0.0311
             Mean action noise std: 0.80
                       Mean reward: -37.23
               Mean episode length: 122.79
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 11.83s
                        Total time: 1217.08s
                               ETA: 1961833.4s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1253 steps/s (collection: 12.879s, learning 0.194s)
               Value function loss: 14.2056
                    Surrogate loss: 0.0242
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 13.07s
                        Total time: 1230.15s
                               ETA: 1951412.3s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.402s, learning 0.159s)
               Value function loss: 1.7711
                    Surrogate loss: 0.0001
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 12.56s
                        Total time: 1242.71s
                               ETA: 1940516.9s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.166s, learning 0.170s)
               Value function loss: 3.0929
                    Surrogate loss: -0.0015
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 12.34s
                        Total time: 1255.05s
                               ETA: 1929609.7s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.884s, learning 0.157s)
               Value function loss: 0.9255
                    Surrogate loss: 0.0483
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 12.04s
                        Total time: 1267.09s
                               ETA: 1918586.0s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.117s, learning 0.215s)
               Value function loss: 0.1633
                    Surrogate loss: -0.0226
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 12.33s
                        Total time: 1279.42s
                               ETA: 1908325.6s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1341 steps/s (collection: 11.983s, learning 0.228s)
               Value function loss: 0.2690
                    Surrogate loss: -0.0188
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 12.21s
                        Total time: 1291.63s
                               ETA: 1898188.8s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.217s, learning 0.160s)
               Value function loss: 0.7586
                    Surrogate loss: -0.0043
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 12.38s
                        Total time: 1304.01s
                               ETA: 1888586.0s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1271 steps/s (collection: 12.714s, learning 0.169s)
               Value function loss: 2.0083
                    Surrogate loss: -0.0001
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 12.88s
                        Total time: 1316.89s
                               ETA: 1879980.1s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.369s, learning 0.157s)
               Value function loss: 2.1985
                    Surrogate loss: 0.0113
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 12.53s
                        Total time: 1329.42s
                               ETA: 1871112.0s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.150s, learning 0.173s)
               Value function loss: 2.1253
                    Surrogate loss: -0.0079
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 12.32s
                        Total time: 1341.74s
                               ETA: 1862209.9s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.065s, learning 0.161s)
               Value function loss: 1.9957
                    Surrogate loss: -0.0019
             Mean action noise std: 0.80
                       Mean reward: -26.21
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 12.23s
                        Total time: 1353.97s
                               ETA: 1853417.6s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.504s, learning 0.163s)
               Value function loss: 1.2707
                    Surrogate loss: -0.0123
             Mean action noise std: 0.80
                       Mean reward: -25.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 12.67s
                        Total time: 1366.64s
                               ETA: 1845458.4s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.071s, learning 0.190s)
               Value function loss: 1.2336
                    Surrogate loss: -0.0223
             Mean action noise std: 0.80
                       Mean reward: -26.68
               Mean episode length: 124.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 12.26s
                        Total time: 1378.90s
                               ETA: 1837171.1s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.473s, learning 0.188s)
               Value function loss: 1.1880
                    Surrogate loss: -0.0306
             Mean action noise std: 0.80
                       Mean reward: -26.45
               Mean episode length: 124.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 12.66s
                        Total time: 1391.56s
                               ETA: 1829626.8s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.271s, learning 0.185s)
               Value function loss: 1.3619
                    Surrogate loss: -0.0197
             Mean action noise std: 0.80
                       Mean reward: -25.96
               Mean episode length: 124.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 12.46s
                        Total time: 1404.02s
                               ETA: 1822011.2s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.417s, learning 0.158s)
               Value function loss: 3.0684
                    Surrogate loss: 0.0039
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 12.58s
                        Total time: 1416.59s
                               ETA: 1814744.2s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1210 steps/s (collection: 13.368s, learning 0.166s)
               Value function loss: 12.4029
                    Surrogate loss: 0.0081
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 13.53s
                        Total time: 1430.12s
                               ETA: 1808872.7s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.594s, learning 0.156s)
               Value function loss: 2.8909
                    Surrogate loss: 0.0282
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 12.75s
                        Total time: 1442.87s
                               ETA: 1802168.7s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.700s, learning 0.180s)
               Value function loss: 2.7323
                    Surrogate loss: -0.0066
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -0.74
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 12.88s
                        Total time: 1455.75s
                               ETA: 1795789.8s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.511s, learning 0.165s)
               Value function loss: 0.3704
                    Surrogate loss: -0.0037
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 12.68s
                        Total time: 1468.43s
                               ETA: 1789318.4s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.473s, learning 0.203s)
               Value function loss: 0.1708
                    Surrogate loss: -0.0165
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 12.68s
                        Total time: 1481.11s
                               ETA: 1783002.1s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.716s, learning 0.290s)
               Value function loss: 0.6432
                    Surrogate loss: -0.0069
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 12.01s
                        Total time: 1493.11s
                               ETA: 1776039.1s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.676s, learning 0.193s)
               Value function loss: 2.1676
                    Surrogate loss: 0.0010
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 12.87s
                        Total time: 1505.98s
                               ETA: 1770254.0s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.515s, learning 0.205s)
               Value function loss: 2.4217
                    Surrogate loss: 0.0044
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 12.72s
                        Total time: 1518.70s
                               ETA: 1764430.7s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.724s, learning 0.203s)
               Value function loss: 2.5562
                    Surrogate loss: -0.0068
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 11.93s
                        Total time: 1530.63s
                               ETA: 1757830.8s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.576s, learning 0.160s)
               Value function loss: 1.6430
                    Surrogate loss: 0.0028
             Mean action noise std: 0.80
                       Mean reward: -20.20
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 12.74s
                        Total time: 1543.37s
                               ETA: 1752298.3s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.431s, learning 0.167s)
               Value function loss: 1.2858
                    Surrogate loss: -0.0159
             Mean action noise std: 0.80
                       Mean reward: -20.95
               Mean episode length: 124.18
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 12.60s
                        Total time: 1555.96s
                               ETA: 1746735.3s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.166s, learning 0.215s)
               Value function loss: 1.0015
                    Surrogate loss: -0.0098
             Mean action noise std: 0.80
                       Mean reward: -21.35
               Mean episode length: 123.51
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 12.38s
                        Total time: 1568.34s
                               ETA: 1741053.6s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.212s, learning 0.163s)
               Value function loss: 1.0326
                    Surrogate loss: -0.0222
             Mean action noise std: 0.80
                       Mean reward: -21.63
               Mean episode length: 123.51
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 12.38s
                        Total time: 1580.72s
                               ETA: 1735491.4s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.076s, learning 0.171s)
               Value function loss: 1.0970
                    Surrogate loss: -0.0427
             Mean action noise std: 0.80
                       Mean reward: -21.77
               Mean episode length: 123.19
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 12.25s
                        Total time: 1592.97s
                               ETA: 1729910.2s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.284s, learning 0.204s)
               Value function loss: 1.3167
                    Surrogate loss: -0.0306
             Mean action noise std: 0.80
                       Mean reward: -23.00
               Mean episode length: 122.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 12.49s
                        Total time: 1605.45s
                               ETA: 1724707.3s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1165 steps/s (collection: 13.828s, learning 0.231s)
               Value function loss: 14.7301
                    Surrogate loss: 0.0278
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 4.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 14.06s
                        Total time: 1619.51s
                               ETA: 1721284.5s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1264 steps/s (collection: 12.749s, learning 0.205s)
               Value function loss: 1.1724
                    Surrogate loss: -0.0088
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: -1.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 12.95s
                        Total time: 1632.47s
                               ETA: 1716771.8s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.336s, learning 0.167s)
               Value function loss: 2.3633
                    Surrogate loss: 0.0096
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 12.50s
                        Total time: 1644.97s
                               ETA: 1711882.8s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.983s, learning 0.166s)
               Value function loss: 1.7103
                    Surrogate loss: -0.0134
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 12.15s
                        Total time: 1657.12s
                               ETA: 1706730.8s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1264 steps/s (collection: 12.748s, learning 0.204s)
               Value function loss: 0.4610
                    Surrogate loss: -0.0221
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 12.95s
                        Total time: 1670.07s
                               ETA: 1702501.7s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.259s, learning 0.260s)
               Value function loss: 0.3156
                    Surrogate loss: -0.0283
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 12.52s
                        Total time: 1682.59s
                               ETA: 1697921.6s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1263 steps/s (collection: 12.690s, learning 0.273s)
               Value function loss: 1.0236
                    Surrogate loss: -0.0108
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 12.96s
                        Total time: 1695.55s
                               ETA: 1693876.0s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.202s, learning 0.244s)
               Value function loss: 2.2838
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 12.45s
                        Total time: 1708.00s
                               ETA: 1689399.1s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.481s, learning 0.176s)
               Value function loss: 2.4309
                    Surrogate loss: -0.0003
             Mean action noise std: 0.80
                       Mean reward: -13.96
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 12.66s
                        Total time: 1720.66s
                               ETA: 1685215.5s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.603s, learning 0.186s)
               Value function loss: 1.8386
                    Surrogate loss: 0.0161
             Mean action noise std: 0.80
                       Mean reward: -13.94
               Mean episode length: 124.48
                  Mean reward/step: 0.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 12.79s
                        Total time: 1733.45s
                               ETA: 1681240.8s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.426s, learning 0.165s)
               Value function loss: 1.4320
                    Surrogate loss: 0.0004
             Mean action noise std: 0.80
                       Mean reward: -14.16
               Mean episode length: 124.48
                  Mean reward/step: 0.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 12.59s
                        Total time: 1746.04s
                               ETA: 1677152.3s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.388s, learning 0.220s)
               Value function loss: 1.1512
                    Surrogate loss: 0.0015
             Mean action noise std: 0.80
                       Mean reward: -13.73
               Mean episode length: 124.48
                  Mean reward/step: 0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 12.61s
                        Total time: 1758.64s
                               ETA: 1673158.1s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.788s, learning 0.167s)
               Value function loss: 1.1606
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                       Mean reward: -12.77
               Mean episode length: 123.90
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 11.96s
                        Total time: 1770.60s
                               ETA: 1668624.1s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.209s, learning 0.269s)
               Value function loss: 1.1349
                    Surrogate loss: -0.0094
             Mean action noise std: 0.80
                       Mean reward: -11.51
               Mean episode length: 123.90
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 12.48s
                        Total time: 1783.08s
                               ETA: 1664662.2s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.087s, learning 0.162s)
               Value function loss: 1.2125
                    Surrogate loss: -0.0243
             Mean action noise std: 0.80
                       Mean reward: -11.82
               Mean episode length: 123.67
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 12.25s
                        Total time: 1795.33s
                               ETA: 1660562.3s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.803s, learning 0.202s)
               Value function loss: 1.3017
                    Surrogate loss: -0.0350
             Mean action noise std: 0.80
                       Mean reward: -13.25
               Mean episode length: 123.50
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 13.00s
                        Total time: 1808.33s
                               ETA: 1657229.6s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1179 steps/s (collection: 13.737s, learning 0.156s)
               Value function loss: 19.3930
                    Surrogate loss: 0.0426
             Mean action noise std: 0.80
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 4.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 13.89s
                        Total time: 1822.23s
                               ETA: 1654764.3s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.312s, learning 0.159s)
               Value function loss: 1.6342
                    Surrogate loss: 0.0204
             Mean action noise std: 0.80
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 12.47s
                        Total time: 1834.70s
                               ETA: 1651062.3s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.106s, learning 0.159s)
               Value function loss: 3.2586
                    Surrogate loss: 0.0221
             Mean action noise std: 0.80
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 12.26s
                        Total time: 1846.96s
                               ETA: 1647243.0s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.504s, learning 0.159s)
               Value function loss: 0.9244
                    Surrogate loss: 0.0018
             Mean action noise std: 0.80
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 12.66s
                        Total time: 1859.63s
                               ETA: 1643843.0s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.418s, learning 0.163s)
               Value function loss: 0.3730
                    Surrogate loss: -0.0331
             Mean action noise std: 0.80
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 12.58s
                        Total time: 1872.21s
                               ETA: 1640430.1s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.596s, learning 0.161s)
               Value function loss: 1.0586
                    Surrogate loss: -0.0090
             Mean action noise std: 0.80
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 12.76s
                        Total time: 1884.96s
                               ETA: 1637229.4s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.555s, learning 0.156s)
               Value function loss: 2.3714
                    Surrogate loss: -0.0008
             Mean action noise std: 0.79
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 12.71s
                        Total time: 1897.67s
                               ETA: 1634044.3s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.102s, learning 0.164s)
               Value function loss: 2.5358
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: -9.40
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 12.27s
                        Total time: 1909.94s
                               ETA: 1630534.0s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.166s, learning 0.168s)
               Value function loss: 2.0094
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: -9.94
               Mean episode length: 124.40
                  Mean reward/step: 0.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 12.33s
                        Total time: 1922.27s
                               ETA: 1627140.0s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.071s, learning 0.195s)
               Value function loss: 1.6848
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: -9.89
               Mean episode length: 124.40
                  Mean reward/step: 0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 12.27s
                        Total time: 1934.54s
                               ETA: 1623745.5s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.203s, learning 0.201s)
               Value function loss: 1.1813
                    Surrogate loss: 0.0095
             Mean action noise std: 0.80
                       Mean reward: -10.46
               Mean episode length: 123.98
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 12.40s
                        Total time: 1946.94s
                               ETA: 1620522.3s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.322s, learning 0.235s)
               Value function loss: 1.2419
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: -9.92
               Mean episode length: 122.94
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 12.56s
                        Total time: 1959.50s
                               ETA: 1617478.6s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.477s, learning 0.191s)
               Value function loss: 1.1506
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: -8.92
               Mean episode length: 122.94
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 12.67s
                        Total time: 1972.17s
                               ETA: 1614576.3s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.704s, learning 0.167s)
               Value function loss: 1.2511
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: -7.61
               Mean episode length: 122.78
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 12.87s
                        Total time: 1985.04s
                               ETA: 1611885.6s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.107s, learning 0.204s)
               Value function loss: 1.5241
                    Surrogate loss: -0.0216
             Mean action noise std: 0.79
                       Mean reward: -6.66
               Mean episode length: 122.50
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 12.31s
                        Total time: 1997.35s
                               ETA: 1608786.7s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1245 steps/s (collection: 12.889s, learning 0.263s)
               Value function loss: 25.1478
                    Surrogate loss: 0.0218
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 4.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 13.15s
                        Total time: 2010.50s
                               ETA: 1606409.1s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1264 steps/s (collection: 12.717s, learning 0.239s)
               Value function loss: 0.3524
                    Surrogate loss: -0.0190
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: -1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 12.96s
                        Total time: 2023.46s
                               ETA: 1603914.1s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.355s, learning 0.220s)
               Value function loss: 1.8558
                    Surrogate loss: 0.0219
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 12.57s
                        Total time: 2036.04s
                               ETA: 1601158.0s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.240s, learning 0.202s)
               Value function loss: 1.4770
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 12.44s
                        Total time: 2048.48s
                               ETA: 1598341.0s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.237s, learning 0.193s)
               Value function loss: 0.4410
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 12.43s
                        Total time: 2060.91s
                               ETA: 1595558.1s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.442s, learning 0.157s)
               Value function loss: 0.6735
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 12.60s
                        Total time: 2073.51s
                               ETA: 1592948.0s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.610s, learning 0.175s)
               Value function loss: 1.9833
                    Surrogate loss: 0.0136
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 12.78s
                        Total time: 2086.29s
                               ETA: 1590518.7s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.509s, learning 0.162s)
               Value function loss: 2.5447
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 1.76
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 12.67s
                        Total time: 2098.96s
                               ETA: 1588040.2s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.743s, learning 0.279s)
               Value function loss: 2.1888
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 1.77
               Mean episode length: 125.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 12.02s
                        Total time: 2110.99s
                               ETA: 1585111.9s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.199s, learning 0.160s)
               Value function loss: 1.9047
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 2.16
               Mean episode length: 125.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 12.36s
                        Total time: 2123.34s
                               ETA: 1582478.1s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.865s, learning 0.169s)
               Value function loss: 3.0272
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 2.16
               Mean episode length: 125.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 12.03s
                        Total time: 2135.38s
                               ETA: 1579642.6s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.138s, learning 0.161s)
               Value function loss: 5.2846
                    Surrogate loss: -0.0022
             Mean action noise std: 0.79
                       Mean reward: 1.74
               Mean episode length: 124.63
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 12.30s
                        Total time: 2147.68s
                               ETA: 1577043.0s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.438s, learning 0.160s)
               Value function loss: 8.2421
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 0.98
               Mean episode length: 124.29
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 12.60s
                        Total time: 2160.28s
                               ETA: 1574699.1s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1268 steps/s (collection: 12.753s, learning 0.160s)
               Value function loss: 11.4845
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: 0.28
               Mean episode length: 123.78
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 12.91s
                        Total time: 2173.19s
                               ETA: 1572617.0s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.190s, learning 0.208s)
               Value function loss: 12.3869
                    Surrogate loss: -0.0026
             Mean action noise std: 0.79
                       Mean reward: 0.33
               Mean episode length: 123.66
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 12.40s
                        Total time: 2185.59s
                               ETA: 1570195.1s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.161s, learning 0.169s)
               Value function loss: 17.1662
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 0.81
               Mean episode length: 123.66
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 12.33s
                        Total time: 2197.92s
                               ETA: 1567759.4s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1198 steps/s (collection: 13.514s, learning 0.157s)
               Value function loss: 99.6876
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 4.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 13.67s
                        Total time: 2211.59s
                               ETA: 1566307.0s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.320s, learning 0.168s)
               Value function loss: 4.4626
                    Surrogate loss: 0.0304
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 12.49s
                        Total time: 2224.08s
                               ETA: 1564042.9s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.149s, learning 0.229s)
               Value function loss: 3.4101
                    Surrogate loss: 0.0326
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 12.38s
                        Total time: 2236.46s
                               ETA: 1561733.8s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.024s, learning 0.171s)
               Value function loss: 1.3718
                    Surrogate loss: -0.0190
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 12.20s
                        Total time: 2248.65s
                               ETA: 1559329.5s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.707s, learning 0.189s)
               Value function loss: 0.8347
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 12.90s
                        Total time: 2261.55s
                               ETA: 1557441.6s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.138s, learning 0.157s)
               Value function loss: 1.3205
                    Surrogate loss: 0.0407
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 12.30s
                        Total time: 2273.84s
                               ETA: 1555168.1s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.003s, learning 0.210s)
               Value function loss: 3.1204
                    Surrogate loss: 0.0104
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 12.21s
                        Total time: 2286.06s
                               ETA: 1552869.1s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.239s, learning 0.170s)
               Value function loss: 2.6715
                    Surrogate loss: 0.0005
             Mean action noise std: 0.79
                       Mean reward: 4.52
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 12.41s
                        Total time: 2298.46s
                               ETA: 1550733.0s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.275s, learning 0.161s)
               Value function loss: 3.2407
                    Surrogate loss: 0.0040
             Mean action noise std: 0.79
                       Mean reward: 3.79
               Mean episode length: 124.47
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 12.44s
                        Total time: 2310.90s
                               ETA: 1548643.7s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.191s, learning 0.162s)
               Value function loss: 2.0623
                    Surrogate loss: 0.0157
             Mean action noise std: 0.79
                       Mean reward: 3.05
               Mean episode length: 123.96
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 12.35s
                        Total time: 2323.25s
                               ETA: 1546527.5s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.404s, learning 0.187s)
               Value function loss: 2.2830
                    Surrogate loss: 0.0015
             Mean action noise std: 0.79
                       Mean reward: 2.13
               Mean episode length: 123.49
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 12.59s
                        Total time: 2335.84s
                               ETA: 1544596.1s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.937s, learning 0.271s)
               Value function loss: 1.5347
                    Surrogate loss: 0.0263
             Mean action noise std: 0.79
                       Mean reward: 2.37
               Mean episode length: 123.49
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 12.21s
                        Total time: 2348.05s
                               ETA: 1542438.1s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.399s, learning 0.257s)
               Value function loss: 1.7449
                    Surrogate loss: 0.0186
             Mean action noise std: 0.79
                       Mean reward: 2.11
               Mean episode length: 123.49
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 12.66s
                        Total time: 2360.71s
                               ETA: 1540600.8s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.320s, learning 0.162s)
               Value function loss: 1.6636
                    Surrogate loss: 0.0054
             Mean action noise std: 0.79
                       Mean reward: 1.67
               Mean episode length: 123.31
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 12.48s
                        Total time: 2373.19s
                               ETA: 1538675.0s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.436s, learning 0.172s)
               Value function loss: 1.6965
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 3.34
               Mean episode length: 123.31
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 12.61s
                        Total time: 2385.80s
                               ETA: 1536854.0s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.535s, learning 0.176s)
               Value function loss: 1.6746
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 4.93
               Mean episode length: 123.31
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 12.71s
                        Total time: 2398.51s
                               ETA: 1535122.3s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1193 steps/s (collection: 13.566s, learning 0.161s)
               Value function loss: 29.2700
                    Surrogate loss: 0.0537
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 4.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 13.73s
                        Total time: 2412.24s
                               ETA: 1534058.8s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.354s, learning 0.189s)
               Value function loss: 3.1827
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 12.54s
                        Total time: 2424.78s
                               ETA: 1532260.5s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.246s, learning 0.181s)
               Value function loss: 2.7938
                    Surrogate loss: 0.0744
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 12.43s
                        Total time: 2437.21s
                               ETA: 1530411.9s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.345s, learning 0.166s)
               Value function loss: 0.8159
                    Surrogate loss: 0.0094
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 12.51s
                        Total time: 2449.72s
                               ETA: 1528638.4s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.169s, learning 0.172s)
               Value function loss: 0.9336
                    Surrogate loss: 0.0365
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 12.34s
                        Total time: 2462.06s
                               ETA: 1526781.5s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.382s, learning 0.164s)
               Value function loss: 1.9889
                    Surrogate loss: 0.0446
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 12.55s
                        Total time: 2474.60s
                               ETA: 1525073.7s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.157s, learning 0.163s)
               Value function loss: 1.6477
                    Surrogate loss: 0.0265
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 12.32s
                        Total time: 2486.92s
                               ETA: 1523248.5s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.080s, learning 0.229s)
               Value function loss: 1.5291
                    Surrogate loss: 0.0720
             Mean action noise std: 0.79
                       Mean reward: 12.44
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 12.31s
                        Total time: 2499.23s
                               ETA: 1521438.1s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.696s, learning 0.211s)
               Value function loss: 1.5113
                    Surrogate loss: 0.0031
             Mean action noise std: 0.79
                       Mean reward: 12.46
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 12.91s
                        Total time: 2512.14s
                               ETA: 1520012.0s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.494s, learning 0.226s)
               Value function loss: 1.2807
                    Surrogate loss: 0.0544
             Mean action noise std: 0.79
                       Mean reward: 11.71
               Mean episode length: 125.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 12.72s
                        Total time: 2524.86s
                               ETA: 1518489.9s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.426s, learning 0.170s)
               Value function loss: 1.8190
                    Surrogate loss: 0.0237
             Mean action noise std: 0.79
                       Mean reward: 10.53
               Mean episode length: 124.18
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 12.60s
                        Total time: 2537.45s
                               ETA: 1516911.7s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.107s, learning 0.208s)
               Value function loss: 2.7834
                    Surrogate loss: 0.0055
             Mean action noise std: 0.79
                       Mean reward: 9.53
               Mean episode length: 123.88
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 12.31s
                        Total time: 2549.77s
                               ETA: 1515185.1s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.315s, learning 0.161s)
               Value function loss: 4.1498
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: 8.42
               Mean episode length: 123.66
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 12.48s
                        Total time: 2562.25s
                               ETA: 1513574.2s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.076s, learning 0.164s)
               Value function loss: 5.6526
                    Surrogate loss: 0.0067
             Mean action noise std: 0.79
                       Mean reward: 7.78
               Mean episode length: 123.50
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 12.24s
                        Total time: 2574.48s
                               ETA: 1511843.4s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.209s, learning 0.167s)
               Value function loss: 2.4752
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: -2.27
               Mean episode length: 123.07
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 12.38s
                        Total time: 2586.86s
                               ETA: 1510212.4s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1189 steps/s (collection: 13.608s, learning 0.161s)
               Value function loss: 51.2518
                    Surrogate loss: 0.0348
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 4.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 13.77s
                        Total time: 2600.63s
                               ETA: 1509408.9s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.469s, learning 0.159s)
               Value function loss: 0.5032
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: -1.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 12.63s
                        Total time: 2613.26s
                               ETA: 1507956.1s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.440s, learning 0.172s)
               Value function loss: 1.8788
                    Surrogate loss: 0.0119
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 12.61s
                        Total time: 2625.87s
                               ETA: 1506510.5s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.579s, learning 0.165s)
               Value function loss: 0.6756
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 12.74s
                        Total time: 2638.61s
                               ETA: 1505156.4s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.304s, learning 0.197s)
               Value function loss: 0.4111
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 12.50s
                        Total time: 2651.12s
                               ETA: 1503679.6s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.454s, learning 0.171s)
               Value function loss: 1.6000
                    Surrogate loss: 0.0427
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 12.62s
                        Total time: 2663.74s
                               ETA: 1502289.5s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.467s, learning 0.160s)
               Value function loss: 1.4745
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 5.09
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 12.63s
                        Total time: 2676.37s
                               ETA: 1500916.0s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.464s, learning 0.250s)
               Value function loss: 2.1134
                    Surrogate loss: 0.0183
             Mean action noise std: 0.79
                       Mean reward: 3.00
               Mean episode length: 123.96
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 12.71s
                        Total time: 2689.08s
                               ETA: 1499606.1s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.871s, learning 0.169s)
               Value function loss: 2.6662
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 0.97
               Mean episode length: 123.62
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 12.04s
                        Total time: 2701.12s
                               ETA: 1497937.0s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.449s, learning 0.167s)
               Value function loss: 1.3556
                    Surrogate loss: 0.0176
             Mean action noise std: 0.79
                       Mean reward: -1.19
               Mean episode length: 122.50
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 12.62s
                        Total time: 2713.74s
                               ETA: 1496604.2s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.339s, learning 0.169s)
               Value function loss: 1.6135
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: -2.79
               Mean episode length: 121.12
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 12.51s
                        Total time: 2726.25s
                               ETA: 1495226.7s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.659s, learning 0.181s)
               Value function loss: 1.6832
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: -3.82
               Mean episode length: 120.36
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 12.84s
                        Total time: 2739.09s
                               ETA: 1494044.6s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.474s, learning 0.179s)
               Value function loss: 1.5848
                    Surrogate loss: 0.0119
             Mean action noise std: 0.79
                       Mean reward: -5.26
               Mean episode length: 120.06
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 12.65s
                        Total time: 2751.74s
                               ETA: 1492774.3s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.291s, learning 0.170s)
               Value function loss: 1.6603
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: -9.01
               Mean episode length: 119.59
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 12.46s
                        Total time: 2764.20s
                               ETA: 1491413.8s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.177s, learning 0.178s)
               Value function loss: 1.9002
                    Surrogate loss: 0.0015
             Mean action noise std: 0.79
                       Mean reward: -14.17
               Mean episode length: 118.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 12.36s
                        Total time: 2776.56s
                               ETA: 1490010.9s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.190s, learning 0.164s)
               Value function loss: 2.5378
                    Surrogate loss: 0.0153
             Mean action noise std: 0.79
                       Mean reward: -19.48
               Mean episode length: 118.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 12.35s
                        Total time: 2788.91s
                               ETA: 1488622.4s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1181 steps/s (collection: 13.704s, learning 0.168s)
               Value function loss: 34.4053
                    Surrogate loss: 0.0259
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 4.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 13.87s
                        Total time: 2802.78s
                               ETA: 1488054.1s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.497s, learning 0.273s)
               Value function loss: 0.9374
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 12.77s
                        Total time: 2815.55s
                               ETA: 1486909.8s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.361s, learning 0.160s)
               Value function loss: 0.9763
                    Surrogate loss: 0.0091
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 12.52s
                        Total time: 2828.07s
                               ETA: 1485646.4s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.020s, learning 0.167s)
               Value function loss: 0.3117
                    Surrogate loss: -0.0210
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 12.19s
                        Total time: 2840.26s
                               ETA: 1484221.6s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.478s, learning 0.227s)
               Value function loss: 1.2858
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 12.70s
                        Total time: 2852.96s
                               ETA: 1483080.9s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.555s, learning 0.178s)
               Value function loss: 2.4075
                    Surrogate loss: 0.0183
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 12.73s
                        Total time: 2865.70s
                               ETA: 1481966.7s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.483s, learning 0.184s)
               Value function loss: 1.2630
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: -4.23
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 12.67s
                        Total time: 2878.37s
                               ETA: 1480829.8s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.081s, learning 0.166s)
               Value function loss: 1.4719
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: -4.53
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 12.25s
                        Total time: 2890.61s
                               ETA: 1479489.6s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.113s, learning 0.163s)
               Value function loss: 2.0838
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: -3.72
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 12.28s
                        Total time: 2902.89s
                               ETA: 1478177.6s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.672s, learning 0.173s)
               Value function loss: 1.4607
                    Surrogate loss: 0.0014
             Mean action noise std: 0.79
                       Mean reward: -3.53
               Mean episode length: 124.48
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 12.85s
                        Total time: 2915.73s
                               ETA: 1477167.1s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.393s, learning 0.208s)
               Value function loss: 1.3055
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: -3.80
               Mean episode length: 124.02
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 12.60s
                        Total time: 2928.34s
                               ETA: 1476043.7s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.551s, learning 0.205s)
               Value function loss: 1.4123
                    Surrogate loss: -0.0042
             Mean action noise std: 0.79
                       Mean reward: -3.43
               Mean episode length: 123.35
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 12.76s
                        Total time: 2941.09s
                               ETA: 1475008.8s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.399s, learning 0.221s)
               Value function loss: 1.4089
                    Surrogate loss: 0.0111
             Mean action noise std: 0.79
                       Mean reward: -3.45
               Mean episode length: 123.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 12.62s
                        Total time: 2953.71s
                               ETA: 1473916.7s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.400s, learning 0.254s)
               Value function loss: 1.4921
                    Surrogate loss: 0.0328
             Mean action noise std: 0.79
                       Mean reward: -4.49
               Mean episode length: 123.15
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 12.65s
                        Total time: 2966.36s
                               ETA: 1472851.8s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.054s, learning 0.210s)
               Value function loss: 2.0962
                    Surrogate loss: 0.0149
             Mean action noise std: 0.79
                       Mean reward: -4.73
               Mean episode length: 122.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 12.26s
                        Total time: 2978.63s
                               ETA: 1471605.1s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1291 steps/s (collection: 12.456s, learning 0.225s)
               Value function loss: 8.8761
                    Surrogate loss: 0.0298
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 12.68s
                        Total time: 2991.31s
                               ETA: 1470575.4s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1181 steps/s (collection: 13.694s, learning 0.174s)
               Value function loss: 19.2910
                    Surrogate loss: 0.0095
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: -1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 13.87s
                        Total time: 3005.18s
                               ETA: 1470136.4s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.557s, learning 0.246s)
               Value function loss: 0.8118
                    Surrogate loss: 0.0025
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 12.80s
                        Total time: 3017.98s
                               ETA: 1469183.1s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.102s, learning 0.242s)
               Value function loss: 0.5628
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 12.34s
                        Total time: 3030.33s
                               ETA: 1468016.5s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.535s, learning 0.180s)
               Value function loss: 0.6349
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 12.71s
                        Total time: 3043.04s
                               ETA: 1467039.8s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.278s, learning 0.160s)
               Value function loss: 1.7212
                    Surrogate loss: 0.0388
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 12.44s
                        Total time: 3055.48s
                               ETA: 1465939.6s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.424s, learning 0.199s)
               Value function loss: 1.9760
                    Surrogate loss: 0.0118
             Mean action noise std: 0.79
                       Mean reward: -1.00
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 12.62s
                        Total time: 3068.10s
                               ETA: 1464937.8s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.440s, learning 0.182s)
               Value function loss: 1.8792
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: -1.19
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 12.62s
                        Total time: 3080.72s
                               ETA: 1463945.1s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.962s, learning 0.158s)
               Value function loss: 1.3433
                    Surrogate loss: 0.0714
             Mean action noise std: 0.79
                       Mean reward: -1.12
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 12.12s
                        Total time: 3092.84s
                               ETA: 1462724.1s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.243s, learning 0.159s)
               Value function loss: 1.9601
                    Surrogate loss: 0.0136
             Mean action noise std: 0.79
                       Mean reward: -1.55
               Mean episode length: 123.90
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 12.40s
                        Total time: 3105.24s
                               ETA: 1461647.3s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.625s, learning 0.163s)
               Value function loss: 1.2079
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: -2.51
               Mean episode length: 123.41
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 12.79s
                        Total time: 3118.03s
                               ETA: 1460761.7s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.286s, learning 0.158s)
               Value function loss: 1.0905
                    Surrogate loss: 0.0095
             Mean action noise std: 0.79
                       Mean reward: -2.47
               Mean episode length: 123.02
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 12.44s
                        Total time: 3130.48s
                               ETA: 1459723.5s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.319s, learning 0.157s)
               Value function loss: 1.2145
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: -2.48
               Mean episode length: 123.02
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 12.48s
                        Total time: 3142.95s
                               ETA: 1458710.2s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.210s, learning 0.180s)
               Value function loss: 1.1889
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: -3.27
               Mean episode length: 122.81
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 12.39s
                        Total time: 3155.34s
                               ETA: 1457665.9s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.919s, learning 0.216s)
               Value function loss: 1.6258
                    Surrogate loss: 0.0136
             Mean action noise std: 0.79
                       Mean reward: -3.39
               Mean episode length: 122.49
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 12.14s
                        Total time: 3167.48s
                               ETA: 1456514.1s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.756s, learning 0.205s)
               Value function loss: 1.7479
                    Surrogate loss: 0.0176
             Mean action noise std: 0.79
                       Mean reward: -3.92
               Mean episode length: 122.29
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 11.96s
                        Total time: 3179.44s
                               ETA: 1455293.0s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1233 steps/s (collection: 13.123s, learning 0.159s)
               Value function loss: 33.0973
                    Surrogate loss: 0.0333
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 4.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 13.28s
                        Total time: 3192.72s
                               ETA: 1454685.4s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.136s, learning 0.157s)
               Value function loss: 0.3257
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: -1.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 12.29s
                        Total time: 3205.01s
                               ETA: 1453634.1s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.772s, learning 0.202s)
               Value function loss: 0.5073
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 11.97s
                        Total time: 3216.99s
                               ETA: 1452448.2s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.279s, learning 0.190s)
               Value function loss: 0.4600
                    Surrogate loss: 0.0024
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 12.47s
                        Total time: 3229.46s
                               ETA: 1451495.0s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.085s, learning 0.251s)
               Value function loss: 0.9404
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 12.34s
                        Total time: 3241.79s
                               ETA: 1450491.1s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.394s, learning 0.184s)
               Value function loss: 1.6610
                    Surrogate loss: 0.0066
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 12.58s
                        Total time: 3254.37s
                               ETA: 1449603.6s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.511s, learning 0.155s)
               Value function loss: 1.2289
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: -1.79
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 12.67s
                        Total time: 3267.04s
                               ETA: 1448763.3s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1240 steps/s (collection: 13.048s, learning 0.165s)
               Value function loss: 1.2470
                    Surrogate loss: 0.0354
             Mean action noise std: 0.79
                       Mean reward: -1.81
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 13.21s
                        Total time: 3280.25s
                               ETA: 1448171.5s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.908s, learning 0.168s)
               Value function loss: 1.7692
                    Surrogate loss: 0.0016
             Mean action noise std: 0.79
                       Mean reward: -2.93
               Mean episode length: 124.37
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 12.08s
                        Total time: 3292.32s
                               ETA: 1447085.6s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.588s, learning 0.221s)
               Value function loss: 1.2422
                    Surrogate loss: 0.0112
             Mean action noise std: 0.79
                       Mean reward: -6.10
               Mean episode length: 123.35
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 12.81s
                        Total time: 3305.13s
                               ETA: 1446329.6s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1245 steps/s (collection: 12.946s, learning 0.206s)
               Value function loss: 0.9039
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: -6.98
               Mean episode length: 122.86
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 13.15s
                        Total time: 3318.29s
                               ETA: 1445729.3s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.413s, learning 0.160s)
               Value function loss: 1.1222
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: -6.59
               Mean episode length: 122.86
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 12.57s
                        Total time: 3330.86s
                               ETA: 1444882.9s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.571s, learning 0.171s)
               Value function loss: 1.3602
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: -5.51
               Mean episode length: 122.86
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 12.74s
                        Total time: 3343.60s
                               ETA: 1444117.0s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.225s, learning 0.176s)
               Value function loss: 1.6747
                    Surrogate loss: 0.0275
             Mean action noise std: 0.79
                       Mean reward: -6.05
               Mean episode length: 122.48
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 12.40s
                        Total time: 3356.00s
                               ETA: 1443210.8s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.979s, learning 0.159s)
               Value function loss: 1.9958
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: -4.33
               Mean episode length: 122.34
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 12.14s
                        Total time: 3368.14s
                               ETA: 1442199.5s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.139s, learning 0.158s)
               Value function loss: 1.8225
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: -0.53
               Mean episode length: 124.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 12.30s
                        Total time: 3380.44s
                               ETA: 1441264.9s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1174 steps/s (collection: 13.779s, learning 0.169s)
               Value function loss: 19.1666
                    Surrogate loss: 0.0299
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 4.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 13.95s
                        Total time: 3394.38s
                               ETA: 1441039.0s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.633s, learning 0.158s)
               Value function loss: 0.8270
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 12.79s
                        Total time: 3407.18s
                               ETA: 1440325.6s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.611s, learning 0.240s)
               Value function loss: 0.6384
                    Surrogate loss: 0.0005
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 12.85s
                        Total time: 3420.03s
                               ETA: 1439643.4s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.666s, learning 0.180s)
               Value function loss: 0.6409
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 12.85s
                        Total time: 3432.87s
                               ETA: 1438965.0s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.736s, learning 0.245s)
               Value function loss: 1.2103
                    Surrogate loss: 0.0278
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 12.98s
                        Total time: 3445.85s
                               ETA: 1438348.1s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.865s, learning 0.158s)
               Value function loss: 1.5471
                    Surrogate loss: 0.0099
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 13.02s
                        Total time: 3458.88s
                               ETA: 1437753.8s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1245 steps/s (collection: 12.865s, learning 0.287s)
               Value function loss: 1.5087
                    Surrogate loss: 0.0083
             Mean action noise std: 0.79
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 13.15s
                        Total time: 3472.03s
                               ETA: 1437217.8s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.339s, learning 0.215s)
               Value function loss: 0.9367
                    Surrogate loss: 0.0223
             Mean action noise std: 0.79
                       Mean reward: 2.70
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 12.55s
                        Total time: 3484.58s
                               ETA: 1436439.6s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.401s, learning 0.194s)
               Value function loss: 0.9731
                    Surrogate loss: 0.0180
             Mean action noise std: 0.79
                       Mean reward: 2.81
               Mean episode length: 124.41
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 12.59s
                        Total time: 3497.18s
                               ETA: 1435684.6s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.763s, learning 0.171s)
               Value function loss: 1.1035
                    Surrogate loss: 0.0074
             Mean action noise std: 0.79
                       Mean reward: 1.82
               Mean episode length: 123.94
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 12.93s
                        Total time: 3510.11s
                               ETA: 1435074.4s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.283s, learning 0.161s)
               Value function loss: 1.0082
                    Surrogate loss: 0.0125
             Mean action noise std: 0.79
                       Mean reward: 0.08
               Mean episode length: 123.13
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 12.44s
                        Total time: 3522.56s
                               ETA: 1434269.5s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.933s, learning 0.156s)
               Value function loss: 1.0359
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: -0.09
               Mean episode length: 122.77
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 12.09s
                        Total time: 3534.64s
                               ETA: 1433327.0s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.515s, learning 0.166s)
               Value function loss: 1.0873
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: -1.52
               Mean episode length: 122.01
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 12.68s
                        Total time: 3547.33s
                               ETA: 1432631.0s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.504s, learning 0.270s)
               Value function loss: 1.4436
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: -2.44
               Mean episode length: 121.64
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 12.77s
                        Total time: 3560.10s
                               ETA: 1431978.1s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1249 steps/s (collection: 12.933s, learning 0.180s)
               Value function loss: 2.4200
                    Surrogate loss: 0.0080
             Mean action noise std: 0.79
                       Mean reward: -3.53
               Mean episode length: 121.93
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 13.11s
                        Total time: 3573.21s
                               ETA: 1431465.8s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1195 steps/s (collection: 13.523s, learning 0.185s)
               Value function loss: 29.3572
                    Surrogate loss: 0.0197
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 4.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 13.71s
                        Total time: 3586.92s
                               ETA: 1431195.1s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1249 steps/s (collection: 12.935s, learning 0.175s)
               Value function loss: 0.2686
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: -1.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 13.11s
                        Total time: 3600.03s
                               ETA: 1430688.9s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.851s, learning 0.176s)
               Value function loss: 0.4198
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 12.03s
                        Total time: 3612.06s
                               ETA: 1429758.0s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.317s, learning 0.172s)
               Value function loss: 0.4161
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 12.49s
                        Total time: 3624.55s
                               ETA: 1429016.4s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.212s, learning 0.178s)
               Value function loss: 0.8829
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 12.39s
                        Total time: 3636.94s
                               ETA: 1428241.7s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.329s, learning 0.193s)
               Value function loss: 1.1696
                    Surrogate loss: 0.0059
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 12.52s
                        Total time: 3649.46s
                               ETA: 1427524.8s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.662s, learning 0.249s)
               Value function loss: 3.5984
                    Surrogate loss: 0.0460
             Mean action noise std: 0.79
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 12.91s
                        Total time: 3662.37s
                               ETA: 1426964.8s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.524s, learning 0.178s)
               Value function loss: 4.5882
                    Surrogate loss: 0.0138
             Mean action noise std: 0.79
                       Mean reward: 3.35
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 12.70s
                        Total time: 3675.07s
                               ETA: 1426328.1s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.371s, learning 0.238s)
               Value function loss: 1.7748
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 3.78
               Mean episode length: 125.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 12.61s
                        Total time: 3687.68s
                               ETA: 1425660.0s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.131s, learning 0.173s)
               Value function loss: 1.8028
                    Surrogate loss: 0.0934
             Mean action noise std: 0.79
                       Mean reward: -0.05
               Mean episode length: 122.84
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 12.30s
                        Total time: 3699.98s
                               ETA: 1424879.5s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.760s, learning 0.212s)
               Value function loss: 1.6150
                    Surrogate loss: 0.0055
             Mean action noise std: 0.79
                       Mean reward: -2.80
               Mean episode length: 121.46
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 11.97s
                        Total time: 3711.96s
                               ETA: 1423977.6s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.699s, learning 0.284s)
               Value function loss: 1.3373
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: -5.16
               Mean episode length: 120.74
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 12.98s
                        Total time: 3724.94s
                               ETA: 1423468.8s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.392s, learning 0.178s)
               Value function loss: 1.3673
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: -6.74
               Mean episode length: 120.15
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 12.57s
                        Total time: 3737.51s
                               ETA: 1422806.5s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.517s, learning 0.209s)
               Value function loss: 1.7067
                    Surrogate loss: 0.0092
             Mean action noise std: 0.79
                       Mean reward: -6.50
               Mean episode length: 119.41
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 12.73s
                        Total time: 3750.23s
                               ETA: 1422208.4s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.637s, learning 0.199s)
               Value function loss: 2.2506
                    Surrogate loss: 0.0203
             Mean action noise std: 0.79
                       Mean reward: -7.13
               Mean episode length: 119.41
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 12.84s
                        Total time: 3763.07s
                               ETA: 1421656.2s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.579s, learning 0.252s)
               Value function loss: 2.7018
                    Surrogate loss: 0.0031
             Mean action noise std: 0.79
                       Mean reward: -1.38
               Mean episode length: 123.25
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 12.83s
                        Total time: 3775.90s
                               ETA: 1421106.3s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1174 steps/s (collection: 13.783s, learning 0.162s)
               Value function loss: 26.2434
                    Surrogate loss: 0.0357
             Mean action noise std: 0.79
                       Mean reward: 3.25
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 4.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 13.94s
                        Total time: 3789.84s
                               ETA: 1420978.1s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.499s, learning 0.168s)
               Value function loss: 0.4607
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 3.25
               Mean episode length: 125.00
                  Mean reward/step: -0.98
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 12.67s
                        Total time: 3802.51s
                               ETA: 1420373.4s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.358s, learning 0.159s)
               Value function loss: 0.5236
                    Surrogate loss: 0.0041
             Mean action noise std: 0.79
                       Mean reward: 3.25
               Mean episode length: 125.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 12.52s
                        Total time: 3815.03s
                               ETA: 1419717.1s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.679s, learning 0.167s)
               Value function loss: 0.9982
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 3.25
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 12.85s
                        Total time: 3827.87s
                               ETA: 1419187.7s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.212s, learning 0.161s)
               Value function loss: 3.0907
                    Surrogate loss: 0.0134
             Mean action noise std: 0.79
                       Mean reward: 3.25
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 12.37s
                        Total time: 3840.25s
                               ETA: 1418487.6s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.487s, learning 0.333s)
               Value function loss: 3.1739
                    Surrogate loss: 0.0041
             Mean action noise std: 0.79
                       Mean reward: 3.25
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 12.82s
                        Total time: 3853.07s
                               ETA: 1417956.7s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.733s, learning 0.208s)
               Value function loss: 2.3078
                    Surrogate loss: -0.0034
             Mean action noise std: 0.79
                       Mean reward: 2.63
               Mean episode length: 124.78
                  Mean reward/step: 0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 12.94s
                        Total time: 3866.01s
                               ETA: 1417474.4s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1261 steps/s (collection: 12.780s, learning 0.206s)
               Value function loss: 1.1529
                    Surrogate loss: 0.1148
             Mean action noise std: 0.79
                       Mean reward: 2.24
               Mean episode length: 124.78
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 12.99s
                        Total time: 3878.99s
                               ETA: 1417011.7s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1248 steps/s (collection: 12.969s, learning 0.156s)
               Value function loss: 0.9705
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 0.80
               Mean episode length: 124.25
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 13.13s
                        Total time: 3892.12s
                               ETA: 1416603.3s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1248 steps/s (collection: 12.834s, learning 0.290s)
               Value function loss: 1.2360
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: -0.43
               Mean episode length: 123.75
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 13.12s
                        Total time: 3905.24s
                               ETA: 1416197.2s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.232s, learning 0.166s)
               Value function loss: 1.1584
                    Surrogate loss: 0.0671
             Mean action noise std: 0.79
                       Mean reward: 2.26
               Mean episode length: 122.81
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 12.40s
                        Total time: 3917.64s
                               ETA: 1415531.6s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.860s, learning 0.157s)
               Value function loss: 1.2340
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: 1.45
               Mean episode length: 122.43
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 12.02s
                        Total time: 3929.66s
                               ETA: 1414733.5s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.452s, learning 0.159s)
               Value function loss: 1.2456
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 0.12
               Mean episode length: 121.09
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 12.61s
                        Total time: 3942.27s
                               ETA: 1414154.0s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.823s, learning 0.158s)
               Value function loss: 1.5582
                    Surrogate loss: 0.0126
             Mean action noise std: 0.79
                       Mean reward: 0.16
               Mean episode length: 121.09
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 12.98s
                        Total time: 3955.25s
                               ETA: 1413711.1s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.487s, learning 0.165s)
               Value function loss: 1.8223
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 1.37
               Mean episode length: 122.63
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 12.65s
                        Total time: 3967.90s
                               ETA: 1413154.0s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.108s, learning 0.165s)
               Value function loss: 1.9017
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 2.41
               Mean episode length: 124.14
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 12.27s
                        Total time: 3980.17s
                               ETA: 1412466.1s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1233 steps/s (collection: 13.114s, learning 0.164s)
               Value function loss: 22.5849
                    Surrogate loss: 0.0183
             Mean action noise std: 0.79
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 4.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 13.28s
                        Total time: 3993.45s
                               ETA: 1412138.5s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.088s, learning 0.160s)
               Value function loss: 0.6750
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 12.25s
                        Total time: 4005.70s
                               ETA: 1411450.0s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.450s, learning 0.186s)
               Value function loss: 0.9075
                    Surrogate loss: 0.0152
             Mean action noise std: 0.79
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 12.64s
                        Total time: 4018.34s
                               ETA: 1410902.9s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.597s, learning 0.164s)
               Value function loss: 1.9116
                    Surrogate loss: 0.0131
             Mean action noise std: 0.79
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 12.76s
                        Total time: 4031.10s
                               ETA: 1410403.3s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.504s, learning 0.170s)
               Value function loss: 1.6786
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 12.67s
                        Total time: 4043.77s
                               ETA: 1409876.6s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.342s, learning 0.202s)
               Value function loss: 1.5420
                    Surrogate loss: 0.0256
             Mean action noise std: 0.79
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 12.54s
                        Total time: 4056.32s
                               ETA: 1409308.1s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.939s, learning 0.247s)
               Value function loss: 0.9598
                    Surrogate loss: 0.0067
             Mean action noise std: 0.79
                       Mean reward: 9.12
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 12.19s
                        Total time: 4068.50s
                               ETA: 1408619.8s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.709s, learning 0.196s)
               Value function loss: 0.9595
                    Surrogate loss: 0.0262
             Mean action noise std: 0.79
                       Mean reward: 9.28
               Mean episode length: 125.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 12.90s
                        Total time: 4081.41s
                               ETA: 1408183.9s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.157s, learning 0.172s)
               Value function loss: 2.9354
                    Surrogate loss: 0.0059
             Mean action noise std: 0.79
                       Mean reward: 8.40
               Mean episode length: 123.87
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 12.33s
                        Total time: 4093.74s
                               ETA: 1407553.2s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.419s, learning 0.340s)
               Value function loss: 4.7977
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 11.25
               Mean episode length: 123.38
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 12.76s
                        Total time: 4106.49s
                               ETA: 1407074.0s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.522s, learning 0.243s)
               Value function loss: 5.9731
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 11.10
               Mean episode length: 122.51
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 12.76s
                        Total time: 4119.26s
                               ETA: 1406599.9s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.541s, learning 0.160s)
               Value function loss: 12.5244
                    Surrogate loss: -0.0009
             Mean action noise std: 0.79
                       Mean reward: 10.27
               Mean episode length: 121.86
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 12.70s
                        Total time: 4131.96s
                               ETA: 1406107.7s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.091s, learning 0.161s)
               Value function loss: 8.6442
                    Surrogate loss: -0.0019
             Mean action noise std: 0.79
                       Mean reward: 10.01
               Mean episode length: 121.39
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 12.25s
                        Total time: 4144.21s
                               ETA: 1405466.3s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.399s, learning 0.186s)
               Value function loss: 13.9306
                    Surrogate loss: 0.0022
             Mean action noise std: 0.79
                       Mean reward: 7.45
               Mean episode length: 121.75
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 12.58s
                        Total time: 4156.80s
                               ETA: 1404941.4s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.101s, learning 0.159s)
               Value function loss: 16.2266
                    Surrogate loss: 0.0036
             Mean action noise std: 0.79
                       Mean reward: 7.08
               Mean episode length: 123.30
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 12.26s
                        Total time: 4169.06s
                               ETA: 1404310.7s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1220 steps/s (collection: 13.260s, learning 0.159s)
               Value function loss: 164.8438
                    Surrogate loss: -0.0015
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 4.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 13.42s
                        Total time: 4182.48s
                               ETA: 1404073.2s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.415s, learning 0.187s)
               Value function loss: 7.5449
                    Surrogate loss: 0.0689
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: -1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 12.60s
                        Total time: 4195.08s
                               ETA: 1403564.0s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.581s, learning 0.181s)
               Value function loss: 1.5006
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 12.76s
                        Total time: 4207.84s
                               ETA: 1403111.3s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.474s, learning 0.206s)
               Value function loss: 0.7014
                    Surrogate loss: 0.0065
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 12.68s
                        Total time: 4220.52s
                               ETA: 1402634.1s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.898s, learning 0.160s)
               Value function loss: 0.9436
                    Surrogate loss: 0.0162
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 12.06s
                        Total time: 4232.58s
                               ETA: 1401954.1s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.128s, learning 0.244s)
               Value function loss: 0.9661
                    Surrogate loss: 0.0161
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 12.37s
                        Total time: 4244.95s
                               ETA: 1401382.0s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1249 steps/s (collection: 12.946s, learning 0.164s)
               Value function loss: 1.7203
                    Surrogate loss: 0.0201
             Mean action noise std: 0.79
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 13.11s
                        Total time: 4258.06s
                               ETA: 1401056.8s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.344s, learning 0.203s)
               Value function loss: 1.8701
                    Surrogate loss: 0.0152
             Mean action noise std: 0.79
                       Mean reward: 13.97
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 12.55s
                        Total time: 4270.61s
                               ETA: 1400548.7s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.288s, learning 0.248s)
               Value function loss: 2.0619
                    Surrogate loss: 0.0029
             Mean action noise std: 0.79
                       Mean reward: 13.54
               Mean episode length: 125.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 12.54s
                        Total time: 4283.14s
                               ETA: 1400040.3s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.721s, learning 0.289s)
               Value function loss: 2.3666
                    Surrogate loss: 0.0186
             Mean action noise std: 0.79
                       Mean reward: 11.37
               Mean episode length: 123.30
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 13.01s
                        Total time: 4296.15s
                               ETA: 1399689.6s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.302s, learning 0.161s)
               Value function loss: 1.7999
                    Surrogate loss: 0.0106
             Mean action noise std: 0.79
                       Mean reward: 9.33
               Mean episode length: 122.38
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 12.46s
                        Total time: 4308.62s
                               ETA: 1399163.3s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.119s, learning 0.165s)
               Value function loss: 1.8359
                    Surrogate loss: 0.0035
             Mean action noise std: 0.79
                       Mean reward: 6.97
               Mean episode length: 122.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 12.28s
                        Total time: 4320.90s
                               ETA: 1398582.8s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.416s, learning 0.181s)
               Value function loss: 1.4671
                    Surrogate loss: 0.0014
             Mean action noise std: 0.79
                       Mean reward: 3.71
               Mean episode length: 121.08
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 12.60s
                        Total time: 4333.50s
                               ETA: 1398106.8s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.420s, learning 0.167s)
               Value function loss: 1.4655
                    Surrogate loss: -0.0023
             Mean action noise std: 0.79
                       Mean reward: 2.73
               Mean episode length: 122.37
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 12.59s
                        Total time: 4346.08s
                               ETA: 1397630.5s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.083s, learning 0.251s)
               Value function loss: 1.6434
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 4.10
               Mean episode length: 123.05
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 12.33s
                        Total time: 4358.42s
                               ETA: 1397076.2s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.967s, learning 0.163s)
               Value function loss: 1.9599
                    Surrogate loss: 0.0050
             Mean action noise std: 0.79
                       Mean reward: 4.06
               Mean episode length: 123.69
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 12.13s
                        Total time: 4370.55s
                               ETA: 1396460.3s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1186 steps/s (collection: 13.465s, learning 0.348s)
               Value function loss: 26.8782
                    Surrogate loss: 0.0324
             Mean action noise std: 0.79
                       Mean reward: 8.65
               Mean episode length: 125.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 4.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 13.81s
                        Total time: 4384.36s
                               ETA: 1396383.9s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.209s, learning 0.159s)
               Value function loss: 0.4920
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 8.65
               Mean episode length: 125.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 12.37s
                        Total time: 4396.73s
                               ETA: 1395849.4s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.598s, learning 0.174s)
               Value function loss: 0.5371
                    Surrogate loss: 0.0136
             Mean action noise std: 0.79
                       Mean reward: 8.65
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 11.77s
                        Total time: 4408.50s
                               ETA: 1395129.9s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.888s, learning 0.158s)
               Value function loss: 1.4040
                    Surrogate loss: 0.0174
             Mean action noise std: 0.79
                       Mean reward: 8.65
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 12.05s
                        Total time: 4420.55s
                               ETA: 1394500.8s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.256s, learning 0.245s)
               Value function loss: 2.1199
                    Surrogate loss: 0.0148
             Mean action noise std: 0.79
                       Mean reward: 8.65
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 12.50s
                        Total time: 4433.05s
                               ETA: 1394019.0s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.357s, learning 0.174s)
               Value function loss: 2.8690
                    Surrogate loss: 0.0411
             Mean action noise std: 0.79
                       Mean reward: 8.65
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 12.53s
                        Total time: 4445.58s
                               ETA: 1393549.4s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.452s, learning 0.156s)
               Value function loss: 1.3662
                    Surrogate loss: 0.0136
             Mean action noise std: 0.79
                       Mean reward: 8.78
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 12.61s
                        Total time: 4458.19s
                               ETA: 1393106.6s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.289s, learning 0.179s)
               Value function loss: 1.4607
                    Surrogate loss: 0.0066
             Mean action noise std: 0.79
                       Mean reward: 8.56
               Mean episode length: 125.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 12.47s
                        Total time: 4470.66s
                               ETA: 1392623.2s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.934s, learning 0.160s)
               Value function loss: 2.1098
                    Surrogate loss: 0.0098
             Mean action noise std: 0.78
                       Mean reward: 6.77
               Mean episode length: 123.81
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 12.09s
                        Total time: 4482.75s
                               ETA: 1392026.5s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.984s, learning 0.163s)
               Value function loss: 1.3054
                    Surrogate loss: 0.0067
             Mean action noise std: 0.78
                       Mean reward: -0.38
               Mean episode length: 121.30
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 12.15s
                        Total time: 4494.90s
                               ETA: 1391449.9s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.171s, learning 0.165s)
               Value function loss: 1.2313
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 1.23
               Mean episode length: 121.30
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 12.34s
                        Total time: 4507.23s
                               ETA: 1390935.0s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.782s, learning 0.160s)
               Value function loss: 1.4275
                    Surrogate loss: 0.0069
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 120.54
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 11.94s
                        Total time: 4519.18s
                               ETA: 1390302.2s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.109s, learning 0.160s)
               Value function loss: 1.5583
                    Surrogate loss: 0.0139
             Mean action noise std: 0.78
                       Mean reward: 3.03
               Mean episode length: 121.17
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 12.27s
                        Total time: 4531.45s
                               ETA: 1389773.4s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.449s, learning 0.259s)
               Value function loss: 1.7261
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 7.32
               Mean episode length: 123.91
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 12.71s
                        Total time: 4544.15s
                               ETA: 1389382.1s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.831s, learning 0.161s)
               Value function loss: 2.2091
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 7.32
               Mean episode length: 124.11
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 11.99s
                        Total time: 4556.15s
                               ETA: 1388774.5s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.951s, learning 0.166s)
               Value function loss: 8.0020
                    Surrogate loss: 0.0074
             Mean action noise std: 0.78
                       Mean reward: 11.12
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 12.12s
                        Total time: 4568.26s
                               ETA: 1388208.6s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1171 steps/s (collection: 13.819s, learning 0.165s)
               Value function loss: 10.0751
                    Surrogate loss: 0.0275
             Mean action noise std: 0.78
                       Mean reward: 10.63
               Mean episode length: 124.27
                  Mean reward/step: -1.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 13.98s
                        Total time: 4582.25s
                               ETA: 1388211.6s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.945s, learning 0.207s)
               Value function loss: 2.4898
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 10.63
               Mean episode length: 124.27
                  Mean reward/step: -0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 12.15s
                        Total time: 4594.40s
                               ETA: 1387661.5s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.242s, learning 0.184s)
               Value function loss: 2.2544
                    Surrogate loss: 0.0247
             Mean action noise std: 0.78
                       Mean reward: 10.63
               Mean episode length: 124.27
                  Mean reward/step: -0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 12.43s
                        Total time: 4606.82s
                               ETA: 1387196.9s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1271 steps/s (collection: 12.727s, learning 0.158s)
               Value function loss: 1.1075
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 10.63
               Mean episode length: 124.27
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 12.89s
                        Total time: 4619.71s
                               ETA: 1386873.0s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.420s, learning 0.180s)
               Value function loss: 0.8590
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: 10.63
               Mean episode length: 124.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 12.60s
                        Total time: 4632.31s
                               ETA: 1386465.7s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.427s, learning 0.159s)
               Value function loss: 0.8427
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 10.63
               Mean episode length: 124.27
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 12.59s
                        Total time: 4644.90s
                               ETA: 1386056.5s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.100s, learning 0.160s)
               Value function loss: 1.3915
                    Surrogate loss: 0.0230
             Mean action noise std: 0.78
                       Mean reward: 10.70
               Mean episode length: 124.27
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 12.26s
                        Total time: 4657.16s
                               ETA: 1385552.7s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.389s, learning 0.205s)
               Value function loss: 1.7953
                    Surrogate loss: 0.0283
             Mean action noise std: 0.78
                       Mean reward: 10.91
               Mean episode length: 124.27
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 12.59s
                        Total time: 4669.75s
                               ETA: 1385150.7s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.577s, learning 0.187s)
               Value function loss: 1.4341
                    Surrogate loss: 0.0078
             Mean action noise std: 0.78
                       Mean reward: 8.28
               Mean episode length: 122.56
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 12.76s
                        Total time: 4682.51s
                               ETA: 1384801.5s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.683s, learning 0.156s)
               Value function loss: 1.4913
                    Surrogate loss: 0.0053
             Mean action noise std: 0.78
                       Mean reward: 5.07
               Mean episode length: 119.49
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 12.84s
                        Total time: 4695.35s
                               ETA: 1384476.2s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.667s, learning 0.163s)
               Value function loss: 1.2144
                    Surrogate loss: 0.0128
             Mean action noise std: 0.78
                       Mean reward: 2.16
               Mean episode length: 118.66
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 12.83s
                        Total time: 4708.18s
                               ETA: 1384150.3s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.191s, learning 0.163s)
               Value function loss: 1.3460
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 2.21
               Mean episode length: 119.39
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 12.35s
                        Total time: 4720.54s
                               ETA: 1383686.7s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.882s, learning 0.170s)
               Value function loss: 1.3786
                    Surrogate loss: 0.0073
             Mean action noise std: 0.78
                       Mean reward: -0.17
               Mean episode length: 119.94
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 12.05s
                        Total time: 4732.59s
                               ETA: 1383137.2s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.146s, learning 0.187s)
               Value function loss: 1.5431
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 2.78
               Mean episode length: 123.69
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 12.33s
                        Total time: 4744.92s
                               ETA: 1382672.9s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.158s, learning 0.166s)
               Value function loss: 1.8131
                    Surrogate loss: 0.0048
             Mean action noise std: 0.78
                       Mean reward: 5.47
               Mean episode length: 124.64
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 12.32s
                        Total time: 4757.25s
                               ETA: 1382208.8s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1227 steps/s (collection: 13.182s, learning 0.162s)
               Value function loss: 28.5016
                    Surrogate loss: 0.0476
             Mean action noise std: 0.78
                       Mean reward: 8.08
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 4.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 13.34s
                        Total time: 4770.59s
                               ETA: 1382042.9s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.532s, learning 0.181s)
               Value function loss: 0.3570
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 8.41
               Mean episode length: 125.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 12.71s
                        Total time: 4783.30s
                               ETA: 1381695.5s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.013s, learning 0.161s)
               Value function loss: 0.6936
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: 8.41
               Mean episode length: 125.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 12.17s
                        Total time: 4795.48s
                               ETA: 1381194.7s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.138s, learning 0.205s)
               Value function loss: 0.8784
                    Surrogate loss: 0.0231
             Mean action noise std: 0.78
                       Mean reward: 8.41
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 12.34s
                        Total time: 4807.82s
                               ETA: 1380745.2s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.988s, learning 0.161s)
               Value function loss: 1.3517
                    Surrogate loss: 0.0083
             Mean action noise std: 0.78
                       Mean reward: 8.41
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 12.15s
                        Total time: 4819.97s
                               ETA: 1380242.4s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1267 steps/s (collection: 12.771s, learning 0.156s)
               Value function loss: 1.5291
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 7.73
               Mean episode length: 124.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 12.93s
                        Total time: 4832.90s
                               ETA: 1379965.0s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.275s, learning 0.156s)
               Value function loss: 2.1073
                    Surrogate loss: 0.0061
             Mean action noise std: 0.78
                       Mean reward: 7.73
               Mean episode length: 124.42
                  Mean reward/step: 0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 12.43s
                        Total time: 4845.33s
                               ETA: 1379547.7s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.253s, learning 0.162s)
               Value function loss: 3.2284
                    Surrogate loss: 0.0011
             Mean action noise std: 0.78
                       Mean reward: 7.49
               Mean episode length: 124.42
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 12.41s
                        Total time: 4857.74s
                               ETA: 1379128.2s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.400s, learning 0.193s)
               Value function loss: 4.7785
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 7.65
               Mean episode length: 122.69
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 12.59s
                        Total time: 4870.33s
                               ETA: 1378761.4s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.188s, learning 0.163s)
               Value function loss: 6.7011
                    Surrogate loss: 0.0042
             Mean action noise std: 0.78
                       Mean reward: 6.25
               Mean episode length: 120.57
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 12.35s
                        Total time: 4882.69s
                               ETA: 1378328.4s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.290s, learning 0.229s)
               Value function loss: 1.5834
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: -3.18
               Mean episode length: 118.86
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 12.52s
                        Total time: 4895.20s
                               ETA: 1377944.8s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.229s, learning 0.173s)
               Value function loss: 1.0531
                    Surrogate loss: 0.0070
             Mean action noise std: 0.78
                       Mean reward: -4.04
               Mean episode length: 118.86
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 12.40s
                        Total time: 4907.61s
                               ETA: 1377530.7s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.434s, learning 0.161s)
               Value function loss: 1.3079
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: -1.49
               Mean episode length: 121.36
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 12.60s
                        Total time: 4920.20s
                               ETA: 1377172.9s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.016s, learning 0.169s)
               Value function loss: 2.2580
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 5.86
               Mean episode length: 122.86
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 12.18s
                        Total time: 4932.39s
                               ETA: 1376702.5s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.104s, learning 0.161s)
               Value function loss: 3.1844
                    Surrogate loss: 0.0024
             Mean action noise std: 0.78
                       Mean reward: 10.17
               Mean episode length: 124.04
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 12.27s
                        Total time: 4944.65s
                               ETA: 1376257.1s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.326s, learning 0.191s)
               Value function loss: 2.9433
                    Surrogate loss: -0.0014
             Mean action noise std: 0.78
                       Mean reward: 11.25
               Mean episode length: 124.70
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 12.52s
                        Total time: 4957.17s
                               ETA: 1375884.1s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1186 steps/s (collection: 13.497s, learning 0.311s)
               Value function loss: 20.7025
                    Surrogate loss: 0.0283
             Mean action noise std: 0.78
                       Mean reward: 10.27
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 4.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 13.81s
                        Total time: 4970.98s
                               ETA: 1375870.1s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.705s, learning 0.200s)
               Value function loss: 1.0199
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 10.27
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 12.90s
                        Total time: 4983.88s
                               ETA: 1375606.9s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.333s, learning 0.166s)
               Value function loss: 0.8793
                    Surrogate loss: -0.0039
             Mean action noise std: 0.78
                       Mean reward: 10.27
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 12.50s
                        Total time: 4996.38s
                               ETA: 1375233.5s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1257 steps/s (collection: 12.865s, learning 0.161s)
               Value function loss: 0.7960
                    Surrogate loss: 0.0081
             Mean action noise std: 0.78
                       Mean reward: 10.27
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 13.03s
                        Total time: 5009.41s
                               ETA: 1375006.4s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.395s, learning 0.162s)
               Value function loss: 0.8086
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: 10.27
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 12.56s
                        Total time: 5021.96s
                               ETA: 1374652.4s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.572s, learning 0.163s)
               Value function loss: 0.9472
                    Surrogate loss: 0.0072
             Mean action noise std: 0.78
                       Mean reward: 10.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 12.73s
                        Total time: 5034.70s
                               ETA: 1374348.7s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.331s, learning 0.216s)
               Value function loss: 1.1652
                    Surrogate loss: 0.0086
             Mean action noise std: 0.78
                       Mean reward: 10.45
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 12.55s
                        Total time: 5047.25s
                               ETA: 1373995.6s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.619s, learning 0.169s)
               Value function loss: 1.8216
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 10.08
               Mean episode length: 124.36
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 12.79s
                        Total time: 5060.03s
                               ETA: 1373709.8s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.977s, learning 0.166s)
               Value function loss: 2.7425
                    Surrogate loss: 0.0021
             Mean action noise std: 0.78
                       Mean reward: 8.63
               Mean episode length: 122.68
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 12.14s
                        Total time: 5072.18s
                               ETA: 1373250.8s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.910s, learning 0.179s)
               Value function loss: 2.8297
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 9.49
               Mean episode length: 121.23
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 13.09s
                        Total time: 5085.27s
                               ETA: 1373049.6s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.280s, learning 0.170s)
               Value function loss: 3.9621
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 11.48
               Mean episode length: 120.81
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 12.45s
                        Total time: 5097.72s
                               ETA: 1372677.3s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.643s, learning 0.210s)
               Value function loss: 3.0995
                    Surrogate loss: 0.0033
             Mean action noise std: 0.78
                       Mean reward: 11.95
               Mean episode length: 121.14
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 12.85s
                        Total time: 5110.57s
                               ETA: 1372415.2s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1256 steps/s (collection: 12.874s, learning 0.164s)
               Value function loss: 3.2469
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 11.62
               Mean episode length: 122.30
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 13.04s
                        Total time: 5123.61s
                               ETA: 1372204.0s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1291 steps/s (collection: 12.522s, learning 0.163s)
               Value function loss: 5.4211
                    Surrogate loss: -0.0025
             Mean action noise std: 0.78
                       Mean reward: 12.23
               Mean episode length: 124.01
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 12.68s
                        Total time: 5136.29s
                               ETA: 1371899.6s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1347 steps/s (collection: 12.000s, learning 0.163s)
               Value function loss: 19.7790
                    Surrogate loss: 0.0034
             Mean action noise std: 0.78
                       Mean reward: 0.47
               Mean episode length: 124.39
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 12.16s
                        Total time: 5148.46s
                               ETA: 1371457.7s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1224 steps/s (collection: 13.218s, learning 0.158s)
               Value function loss: 26.6080
                    Surrogate loss: 0.0198
             Mean action noise std: 0.78
                       Mean reward: 14.54
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 13.38s
                        Total time: 5161.83s
                               ETA: 1371340.4s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.569s, learning 0.173s)
               Value function loss: 0.2295
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 14.46
               Mean episode length: 125.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 12.74s
                        Total time: 5174.57s
                               ETA: 1371055.4s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.516s, learning 0.159s)
               Value function loss: 0.4264
                    Surrogate loss: 0.0035
             Mean action noise std: 0.78
                       Mean reward: 14.46
               Mean episode length: 125.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 12.68s
                        Total time: 5187.25s
                               ETA: 1370754.5s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.066s, learning 0.179s)
               Value function loss: 0.5740
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 14.46
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 12.25s
                        Total time: 5199.49s
                               ETA: 1370341.8s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.260s, learning 0.187s)
               Value function loss: 0.9104
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: 13.84
               Mean episode length: 124.63
                  Mean reward/step: -0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 12.45s
                        Total time: 5211.94s
                               ETA: 1369984.3s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.301s, learning 0.163s)
               Value function loss: 0.6684
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 13.10
               Mean episode length: 124.20
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 12.46s
                        Total time: 5224.41s
                               ETA: 1369632.8s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.991s, learning 0.170s)
               Value function loss: 0.9729
                    Surrogate loss: 0.0119
             Mean action noise std: 0.78
                       Mean reward: 13.15
               Mean episode length: 124.20
                  Mean reward/step: 0.27
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 12.16s
                        Total time: 5236.57s
                               ETA: 1369204.1s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.410s, learning 0.174s)
               Value function loss: 0.8775
                    Surrogate loss: 0.0199
             Mean action noise std: 0.78
                       Mean reward: 13.15
               Mean episode length: 124.20
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 12.58s
                        Total time: 5249.15s
                               ETA: 1368887.7s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1254 steps/s (collection: 12.884s, learning 0.178s)
               Value function loss: 1.1104
                    Surrogate loss: 0.0182
             Mean action noise std: 0.78
                       Mean reward: 10.48
               Mean episode length: 121.55
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 13.06s
                        Total time: 5262.21s
                               ETA: 1368697.4s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.370s, learning 0.177s)
               Value function loss: 1.3179
                    Surrogate loss: 0.0128
             Mean action noise std: 0.78
                       Mean reward: 8.33
               Mean episode length: 119.93
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 12.55s
                        Total time: 5274.76s
                               ETA: 1368374.2s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.176s, learning 0.174s)
               Value function loss: 1.5227
                    Surrogate loss: 0.0001
             Mean action noise std: 0.78
                       Mean reward: 6.00
               Mean episode length: 118.97
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 12.35s
                        Total time: 5287.11s
                               ETA: 1368001.9s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.609s, learning 0.168s)
               Value function loss: 1.3387
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 7.64
               Mean episode length: 120.39
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 12.78s
                        Total time: 5299.89s
                               ETA: 1367741.4s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.122s, learning 0.157s)
               Value function loss: 1.3522
                    Surrogate loss: 0.1284
             Mean action noise std: 0.78
                       Mean reward: 7.98
               Mean episode length: 121.33
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 12.28s
                        Total time: 5312.17s
                               ETA: 1367354.1s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.848s, learning 0.163s)
               Value function loss: 1.4576
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 9.64
               Mean episode length: 122.99
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 13.01s
                        Total time: 5325.18s
                               ETA: 1367156.7s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.854s, learning 0.163s)
               Value function loss: 1.5737
                    Surrogate loss: -0.0023
             Mean action noise std: 0.78
                       Mean reward: 11.07
               Mean episode length: 123.76
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 13.02s
                        Total time: 5338.19s
                               ETA: 1366961.7s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.053s, learning 0.176s)
               Value function loss: 1.7524
                    Surrogate loss: 0.0141
             Mean action noise std: 0.78
                       Mean reward: 10.93
               Mean episode length: 124.47
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 12.23s
                        Total time: 5350.42s
                               ETA: 1366566.3s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1232 steps/s (collection: 13.131s, learning 0.157s)
               Value function loss: 24.6998
                    Surrogate loss: 0.0344
             Mean action noise std: 0.78
                       Mean reward: 13.39
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 4.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 13.29s
                        Total time: 5363.71s
                               ETA: 1366442.8s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.232s, learning 0.162s)
               Value function loss: 0.3781
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 13.45
               Mean episode length: 125.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 12.39s
                        Total time: 5376.10s
                               ETA: 1366092.9s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1255 steps/s (collection: 12.797s, learning 0.257s)
               Value function loss: 0.5781
                    Surrogate loss: 0.0108
             Mean action noise std: 0.78
                       Mean reward: 13.45
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 13.05s
                        Total time: 5389.16s
                               ETA: 1365911.8s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.588s, learning 0.210s)
               Value function loss: 0.6591
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 13.45
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 12.80s
                        Total time: 5401.96s
                               ETA: 1365666.7s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.336s, learning 0.188s)
               Value function loss: 2.0854
                    Surrogate loss: 0.0041
             Mean action noise std: 0.78
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 12.52s
                        Total time: 5414.48s
                               ETA: 1365353.7s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.141s, learning 0.175s)
               Value function loss: 2.1365
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 13.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 12.32s
                        Total time: 5426.80s
                               ETA: 1364990.0s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1240 steps/s (collection: 12.976s, learning 0.226s)
               Value function loss: 1.2599
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 13.42
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 13.20s
                        Total time: 5440.00s
                               ETA: 1364850.4s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.954s, learning 0.163s)
               Value function loss: 0.7374
                    Surrogate loss: 0.0145
             Mean action noise std: 0.78
                       Mean reward: 13.41
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 12.12s
                        Total time: 5452.12s
                               ETA: 1364439.9s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.469s, learning 0.203s)
               Value function loss: 0.8475
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 10.26
               Mean episode length: 124.38
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 12.67s
                        Total time: 5464.79s
                               ETA: 1364169.9s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.597s, learning 0.160s)
               Value function loss: 1.1561
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 9.50
               Mean episode length: 122.92
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 12.76s
                        Total time: 5477.54s
                               ETA: 1363922.2s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.190s, learning 0.171s)
               Value function loss: 1.2023
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 8.39
               Mean episode length: 121.13
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 12.36s
                        Total time: 5489.90s
                               ETA: 1363577.4s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.634s, learning 0.165s)
               Value function loss: 1.1248
                    Surrogate loss: 0.0359
             Mean action noise std: 0.78
                       Mean reward: 9.96
               Mean episode length: 121.75
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 12.80s
                        Total time: 5502.70s
                               ETA: 1363342.8s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1265 steps/s (collection: 12.786s, learning 0.162s)
               Value function loss: 1.3508
                    Surrogate loss: 0.0013
             Mean action noise std: 0.78
                       Mean reward: 9.71
               Mean episode length: 122.50
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 12.95s
                        Total time: 5515.65s
                               ETA: 1363146.2s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.195s, learning 0.165s)
               Value function loss: 1.4007
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 9.45
               Mean episode length: 123.89
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 12.36s
                        Total time: 5528.01s
                               ETA: 1362805.3s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.326s, learning 0.162s)
               Value function loss: 1.4914
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 9.20
               Mean episode length: 124.17
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 12.49s
                        Total time: 5540.50s
                               ETA: 1362497.8s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.197s, learning 0.166s)
               Value function loss: 1.4376
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 8.85
               Mean episode length: 124.56
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 12.36s
                        Total time: 5552.86s
                               ETA: 1362160.9s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1206 steps/s (collection: 13.381s, learning 0.200s)
               Value function loss: 17.2525
                    Surrogate loss: 0.0156
             Mean action noise std: 0.78
                       Mean reward: 10.40
               Mean episode length: 125.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 4.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 13.58s
                        Total time: 5566.44s
                               ETA: 1362123.6s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.287s, learning 0.165s)
               Value function loss: 0.5403
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 10.40
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 12.45s
                        Total time: 5578.90s
                               ETA: 1361811.0s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.405s, learning 0.201s)
               Value function loss: 0.8611
                    Surrogate loss: 0.0176
             Mean action noise std: 0.78
                       Mean reward: 9.77
               Mean episode length: 124.37
                  Mean reward/step: -0.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 12.61s
                        Total time: 5591.50s
                               ETA: 1361537.3s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1263 steps/s (collection: 12.789s, learning 0.173s)
               Value function loss: 1.2257
                    Surrogate loss: 0.0013
             Mean action noise std: 0.78
                       Mean reward: 9.87
               Mean episode length: 124.37
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 12.96s
                        Total time: 5604.46s
                               ETA: 1361351.4s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.462s, learning 0.158s)
               Value function loss: 1.1868
                    Surrogate loss: 0.0053
             Mean action noise std: 0.78
                       Mean reward: 9.81
               Mean episode length: 124.37
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 12.62s
                        Total time: 5617.08s
                               ETA: 1361083.3s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.043s, learning 0.163s)
               Value function loss: 0.8888
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 8.95
               Mean episode length: 123.59
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 12.21s
                        Total time: 5629.29s
                               ETA: 1360716.3s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.174s, learning 0.168s)
               Value function loss: 0.7293
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 9.13
               Mean episode length: 123.59
                  Mean reward/step: 0.34
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 12.34s
                        Total time: 5641.63s
                               ETA: 1360384.0s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.273s, learning 0.174s)
               Value function loss: 1.0157
                    Surrogate loss: 0.0039
             Mean action noise std: 0.78
                       Mean reward: 8.49
               Mean episode length: 123.59
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 12.45s
                        Total time: 5654.08s
                               ETA: 1360078.5s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.347s, learning 0.221s)
               Value function loss: 1.0275
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 7.90
               Mean episode length: 121.24
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 12.57s
                        Total time: 5666.64s
                               ETA: 1359803.4s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.366s, learning 0.186s)
               Value function loss: 1.3115
                    Surrogate loss: 0.0089
             Mean action noise std: 0.78
                       Mean reward: 6.76
               Mean episode length: 119.71
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 12.55s
                        Total time: 5679.20s
                               ETA: 1359525.8s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.148s, learning 0.165s)
               Value function loss: 0.9444
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 9.22
               Mean episode length: 121.48
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 12.31s
                        Total time: 5691.51s
                               ETA: 1359192.2s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.561s, learning 0.162s)
               Value function loss: 1.2241
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 10.03
               Mean episode length: 122.26
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 12.72s
                        Total time: 5704.23s
                               ETA: 1358958.1s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1299 steps/s (collection: 12.453s, learning 0.156s)
               Value function loss: 1.4362
                    Surrogate loss: 0.0022
             Mean action noise std: 0.78
                       Mean reward: 11.70
               Mean episode length: 123.26
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 12.61s
                        Total time: 5716.84s
                               ETA: 1358697.9s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.168s, learning 0.228s)
               Value function loss: 1.4659
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 11.45
               Mean episode length: 123.96
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 12.40s
                        Total time: 5729.24s
                               ETA: 1358388.4s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.898s, learning 0.178s)
               Value function loss: 4.1463
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 12.39
               Mean episode length: 124.32
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 12.08s
                        Total time: 5741.31s
                               ETA: 1358004.6s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1222 steps/s (collection: 13.245s, learning 0.157s)
               Value function loss: 22.5785
                    Surrogate loss: 0.0048
             Mean action noise std: 0.78
                       Mean reward: 16.52
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 4.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 13.40s
                        Total time: 5754.72s
                               ETA: 1357935.5s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.260s, learning 0.157s)
               Value function loss: 0.6165
                    Surrogate loss: 0.0820
             Mean action noise std: 0.78
                       Mean reward: 16.68
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 12.42s
                        Total time: 5767.13s
                               ETA: 1357634.7s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.443s, learning 0.157s)
               Value function loss: 0.9191
                    Surrogate loss: 0.0049
             Mean action noise std: 0.78
                       Mean reward: 16.75
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 12.60s
                        Total time: 5779.73s
                               ETA: 1357378.4s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.613s, learning 0.171s)
               Value function loss: 0.9086
                    Surrogate loss: 0.0108
             Mean action noise std: 0.78
                       Mean reward: 16.75
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 12.78s
                        Total time: 5792.52s
                               ETA: 1357166.2s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.741s, learning 0.164s)
               Value function loss: 0.8420
                    Surrogate loss: 0.0339
             Mean action noise std: 0.78
                       Mean reward: 16.76
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 12.91s
                        Total time: 5805.42s
                               ETA: 1356983.3s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.619s, learning 0.206s)
               Value function loss: 0.6361
                    Surrogate loss: 0.0062
             Mean action noise std: 0.78
                       Mean reward: 16.77
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 12.83s
                        Total time: 5818.25s
                               ETA: 1356782.5s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.878s, learning 0.210s)
               Value function loss: 0.8599
                    Surrogate loss: 0.0003
             Mean action noise std: 0.78
                       Mean reward: 16.60
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 12.09s
                        Total time: 5830.33s
                               ETA: 1356411.0s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.658s, learning 0.222s)
               Value function loss: 2.2178
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 16.81
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 12.88s
                        Total time: 5843.21s
                               ETA: 1356224.9s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.877s, learning 0.217s)
               Value function loss: 2.3600
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 16.51
               Mean episode length: 125.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 13.09s
                        Total time: 5856.31s
                               ETA: 1356089.4s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1265 steps/s (collection: 12.793s, learning 0.157s)
               Value function loss: 3.4547
                    Surrogate loss: 0.0081
             Mean action noise std: 0.78
                       Mean reward: 13.32
               Mean episode length: 123.23
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 12.95s
                        Total time: 5869.26s
                               ETA: 1355921.1s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.889s, learning 0.164s)
               Value function loss: 3.1272
                    Surrogate loss: -0.0012
             Mean action noise std: 0.78
                       Mean reward: 10.13
               Mean episode length: 122.76
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 12.05s
                        Total time: 5881.31s
                               ETA: 1355547.0s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.252s, learning 0.263s)
               Value function loss: 2.7795
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 8.52
               Mean episode length: 123.20
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 12.52s
                        Total time: 5893.83s
                               ETA: 1355280.7s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.366s, learning 0.167s)
               Value function loss: 1.6054
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 7.42
               Mean episode length: 123.50
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 12.53s
                        Total time: 5906.36s
                               ETA: 1355019.6s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.199s, learning 0.181s)
               Value function loss: 1.6638
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 8.46
               Mean episode length: 123.86
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 12.38s
                        Total time: 5918.74s
                               ETA: 1354724.7s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.626s, learning 0.251s)
               Value function loss: 1.7859
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 10.35
               Mean episode length: 124.76
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 12.88s
                        Total time: 5931.62s
                               ETA: 1354544.5s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.214s, learning 0.166s)
               Value function loss: 1.8885
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 8.83
               Mean episode length: 124.76
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 12.38s
                        Total time: 5944.00s
                               ETA: 1354251.9s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1180 steps/s (collection: 13.716s, learning 0.162s)
               Value function loss: 166.2357
                    Surrogate loss: 0.0147
             Mean action noise std: 0.78
                       Mean reward: 11.30
               Mean episode length: 125.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 4.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 13.88s
                        Total time: 5957.87s
                               ETA: 1354301.1s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.704s, learning 0.200s)
               Value function loss: 0.3397
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 11.30
               Mean episode length: 125.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 12.90s
                        Total time: 5970.78s
                               ETA: 1354129.0s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.361s, learning 0.308s)
               Value function loss: 0.8843
                    Surrogate loss: 0.0058
             Mean action noise std: 0.78
                       Mean reward: 11.34
               Mean episode length: 125.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 12.67s
                        Total time: 5983.45s
                               ETA: 1353904.6s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.948s, learning 0.205s)
               Value function loss: 0.8915
                    Surrogate loss: -0.0043
             Mean action noise std: 0.78
                       Mean reward: 10.18
               Mean episode length: 124.40
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 12.15s
                        Total time: 5995.60s
                               ETA: 1353564.6s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1263 steps/s (collection: 12.809s, learning 0.159s)
               Value function loss: 0.8314
                    Surrogate loss: 0.0026
             Mean action noise std: 0.78
                       Mean reward: 10.02
               Mean episode length: 124.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 12.97s
                        Total time: 6008.57s
                               ETA: 1353409.6s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.316s, learning 0.186s)
               Value function loss: 0.9912
                    Surrogate loss: 0.0081
             Mean action noise std: 0.78
                       Mean reward: 9.99
               Mean episode length: 124.40
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 12.50s
                        Total time: 6021.07s
                               ETA: 1353150.5s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.948s, learning 0.161s)
               Value function loss: 0.6841
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 9.07
               Mean episode length: 123.90
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 12.11s
                        Total time: 6033.18s
                               ETA: 1352804.4s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.667s, learning 0.224s)
               Value function loss: 0.7807
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 9.21
               Mean episode length: 123.90
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 12.89s
                        Total time: 6046.07s
                               ETA: 1352634.8s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.603s, learning 0.193s)
               Value function loss: 1.5374
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 8.12
               Mean episode length: 122.74
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 12.80s
                        Total time: 6058.87s
                               ETA: 1352444.7s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.383s, learning 0.348s)
               Value function loss: 1.8763
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 7.78
               Mean episode length: 121.69
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 12.73s
                        Total time: 6071.60s
                               ETA: 1352241.0s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.169s, learning 0.155s)
               Value function loss: 2.3839
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 8.55
               Mean episode length: 122.48
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 12.32s
                        Total time: 6083.92s
                               ETA: 1351947.7s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.668s, learning 0.172s)
               Value function loss: 2.3211
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 4.50
               Mean episode length: 122.52
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 12.84s
                        Total time: 6096.76s
                               ETA: 1351770.1s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.460s, learning 0.165s)
               Value function loss: 1.1289
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 2.96
               Mean episode length: 123.03
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 12.62s
                        Total time: 6109.39s
                               ETA: 1351545.4s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.512s, learning 0.191s)
               Value function loss: 1.0801
                    Surrogate loss: -0.0201
             Mean action noise std: 0.78
                       Mean reward: 5.77
               Mean episode length: 124.14
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 12.70s
                        Total time: 6122.09s
                               ETA: 1351339.1s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.492s, learning 0.161s)
               Value function loss: 1.3734
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 9.36
               Mean episode length: 124.83
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 12.65s
                        Total time: 6134.74s
                               ETA: 1351122.5s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.376s, learning 0.156s)
               Value function loss: 2.9013
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 9.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 12.53s
                        Total time: 6147.27s
                               ETA: 1350880.4s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1210 steps/s (collection: 13.332s, learning 0.207s)
               Value function loss: 11.2743
                    Surrogate loss: 0.0191
             Mean action noise std: 0.78
                       Mean reward: 9.24
               Mean episode length: 125.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 13.54s
                        Total time: 6160.81s
                               ETA: 1350859.9s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.710s, learning 0.164s)
               Value function loss: 0.8804
                    Surrogate loss: 0.0058
             Mean action noise std: 0.78
                       Mean reward: 9.24
               Mean episode length: 125.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 12.87s
                        Total time: 6173.69s
                               ETA: 1350693.9s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.101s, learning 0.160s)
               Value function loss: 0.8790
                    Surrogate loss: 0.0041
             Mean action noise std: 0.78
                       Mean reward: 8.66
               Mean episode length: 124.45
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 12.26s
                        Total time: 6185.95s
                               ETA: 1350394.9s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.022s, learning 0.178s)
               Value function loss: 0.5262
                    Surrogate loss: 0.0112
             Mean action noise std: 0.78
                       Mean reward: 8.66
               Mean episode length: 124.45
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 12.20s
                        Total time: 6198.15s
                               ETA: 1350083.7s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.378s, learning 0.180s)
               Value function loss: 0.4596
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 9.12
               Mean episode length: 124.45
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 12.56s
                        Total time: 6210.70s
                               ETA: 1349851.9s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1257 steps/s (collection: 12.872s, learning 0.157s)
               Value function loss: 0.4695
                    Surrogate loss: 0.0149
             Mean action noise std: 0.78
                       Mean reward: 9.08
               Mean episode length: 124.45
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 13.03s
                        Total time: 6223.73s
                               ETA: 1349723.0s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.315s, learning 0.162s)
               Value function loss: 0.4896
                    Surrogate loss: 0.0059
             Mean action noise std: 0.78
                       Mean reward: 8.75
               Mean episode length: 124.45
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 12.48s
                        Total time: 6236.21s
                               ETA: 1349475.2s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.207s, learning 0.165s)
               Value function loss: 0.6230
                    Surrogate loss: 0.0165
             Mean action noise std: 0.78
                       Mean reward: 8.48
               Mean episode length: 124.09
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 12.37s
                        Total time: 6248.58s
                               ETA: 1349205.8s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.359s, learning 0.162s)
               Value function loss: 0.9051
                    Surrogate loss: 0.0032
             Mean action noise std: 0.78
                       Mean reward: 5.00
               Mean episode length: 123.03
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 12.52s
                        Total time: 6261.10s
                               ETA: 1348969.7s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.942s, learning 0.161s)
               Value function loss: 1.2785
                    Surrogate loss: -0.0035
             Mean action noise std: 0.78
                       Mean reward: 3.44
               Mean episode length: 123.11
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 12.10s
                        Total time: 6273.21s
                               ETA: 1348644.5s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.959s, learning 0.163s)
               Value function loss: 0.9638
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 3.33
               Mean episode length: 123.23
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 12.12s
                        Total time: 6285.33s
                               ETA: 1348324.6s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.581s, learning 0.168s)
               Value function loss: 1.0483
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 4.90
               Mean episode length: 123.77
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 12.75s
                        Total time: 6298.08s
                               ETA: 1348140.4s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.139s, learning 0.161s)
               Value function loss: 1.2106
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 4.11
               Mean episode length: 123.98
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 12.30s
                        Total time: 6310.38s
                               ETA: 1347861.2s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.458s, learning 0.157s)
               Value function loss: 1.9048
                    Surrogate loss: 0.0112
             Mean action noise std: 0.78
                       Mean reward: 6.11
               Mean episode length: 124.55
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 12.61s
                        Total time: 6322.99s
                               ETA: 1347650.0s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.326s, learning 0.183s)
               Value function loss: 2.0965
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 8.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 12.51s
                        Total time: 6335.50s
                               ETA: 1347417.4s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 823 steps/s (collection: 19.738s, learning 0.161s)
               Value function loss: 20.9811
                    Surrogate loss: 0.0424
             Mean action noise std: 0.78
                       Mean reward: 9.43
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 4.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 19.90s
                        Total time: 6355.40s
                               ETA: 1348753.9s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 688 steps/s (collection: 23.644s, learning 0.161s)
               Value function loss: 0.9944
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 9.70
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 23.80s
                        Total time: 6379.20s
                               ETA: 1350911.9s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 704 steps/s (collection: 23.076s, learning 0.171s)
               Value function loss: 0.3692
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 9.72
               Mean episode length: 125.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 23.25s
                        Total time: 6402.45s
                               ETA: 1352942.5s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 684 steps/s (collection: 23.788s, learning 0.158s)
               Value function loss: 0.4493
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 9.85
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 23.95s
                        Total time: 6426.40s
                               ETA: 1355112.1s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 670 steps/s (collection: 24.211s, learning 0.234s)
               Value function loss: 0.5530
                    Surrogate loss: 0.0038
             Mean action noise std: 0.78
                       Mean reward: 9.63
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 24.45s
                        Total time: 6450.84s
                               ETA: 1357377.3s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 676 steps/s (collection: 24.060s, learning 0.166s)
               Value function loss: 0.3069
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: 9.95
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 24.23s
                        Total time: 6475.07s
                               ETA: 1359586.8s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 695 steps/s (collection: 23.355s, learning 0.198s)
               Value function loss: 0.2972
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 9.89
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 23.55s
                        Total time: 6498.62s
                               ETA: 1361645.9s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.724s, learning 0.168s)
               Value function loss: 0.3570
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 10.38
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 23.89s
                        Total time: 6522.51s
                               ETA: 1363767.0s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 696 steps/s (collection: 23.351s, learning 0.166s)
               Value function loss: 0.7580
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 8.71
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 23.52s
                        Total time: 6546.03s
                               ETA: 1365801.2s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 696 steps/s (collection: 23.360s, learning 0.164s)
               Value function loss: 1.3237
                    Surrogate loss: -0.0025
             Mean action noise std: 0.78
                       Mean reward: 3.50
               Mean episode length: 123.90
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 23.52s
                        Total time: 6569.55s
                               ETA: 1367828.0s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 674 steps/s (collection: 24.039s, learning 0.261s)
               Value function loss: 1.0108
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 0.48
               Mean episode length: 123.90
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 24.30s
                        Total time: 6593.86s
                               ETA: 1370007.7s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.860s, learning 0.176s)
               Value function loss: 0.9530
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 0.15
               Mean episode length: 124.44
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 24.04s
                        Total time: 6617.89s
                               ETA: 1372123.2s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 689 steps/s (collection: 23.553s, learning 0.213s)
               Value function loss: 1.0406
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 2.54
               Mean episode length: 124.99
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 23.77s
                        Total time: 6641.66s
                               ETA: 1374173.9s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.900s, learning 0.157s)
               Value function loss: 1.3790
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 3.16
               Mean episode length: 124.99
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 24.06s
                        Total time: 6665.71s
                               ETA: 1376276.3s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 674 steps/s (collection: 24.126s, learning 0.164s)
               Value function loss: 1.9793
                    Surrogate loss: 0.0027
             Mean action noise std: 0.78
                       Mean reward: 2.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 24.29s
                        Total time: 6690.00s
                               ETA: 1378417.7s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 684 steps/s (collection: 23.782s, learning 0.167s)
               Value function loss: 2.2766
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 2.37
               Mean episode length: 124.97
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 23.95s
                        Total time: 6713.95s
                               ETA: 1380480.1s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 647 steps/s (collection: 25.130s, learning 0.166s)
               Value function loss: 23.9394
                    Surrogate loss: 0.0243
             Mean action noise std: 0.78
                       Mean reward: 3.33
               Mean episode length: 125.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 4.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 25.30s
                        Total time: 6739.25s
                               ETA: 1382810.4s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 688 steps/s (collection: 23.571s, learning 0.233s)
               Value function loss: 0.3460
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 2.68
               Mean episode length: 124.46
                  Mean reward/step: -0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 23.80s
                        Total time: 6763.05s
                               ETA: 1384825.3s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.902s, learning 0.185s)
               Value function loss: 0.3984
                    Surrogate loss: 0.0095
             Mean action noise std: 0.78
                       Mean reward: 2.60
               Mean episode length: 124.46
                  Mean reward/step: -0.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 24.09s
                        Total time: 6787.14s
                               ETA: 1386889.8s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 663 steps/s (collection: 24.499s, learning 0.205s)
               Value function loss: 0.4296
                    Surrogate loss: 0.0078
             Mean action noise std: 0.78
                       Mean reward: 2.57
               Mean episode length: 124.46
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 24.70s
                        Total time: 6811.84s
                               ETA: 1389071.5s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 653 steps/s (collection: 24.869s, learning 0.205s)
               Value function loss: 0.4277
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 2.48
               Mean episode length: 124.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 25.07s
                        Total time: 6836.92s
                               ETA: 1391319.6s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 669 steps/s (collection: 24.290s, learning 0.173s)
               Value function loss: 0.2836
                    Surrogate loss: 0.0161
             Mean action noise std: 0.77
                       Mean reward: 2.84
               Mean episode length: 124.46
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 24.46s
                        Total time: 6861.38s
                               ETA: 1393434.2s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.413s, learning 0.163s)
               Value function loss: 0.2728
                    Surrogate loss: -0.0005
             Mean action noise std: 0.77
                       Mean reward: 2.66
               Mean episode length: 124.46
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 23.58s
                        Total time: 6884.96s
                               ETA: 1395360.3s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.279s, learning 0.244s)
               Value function loss: 0.3453
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 3.52
               Mean episode length: 124.46
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 24.52s
                        Total time: 6909.48s
                               ETA: 1397470.0s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.396s, learning 0.193s)
               Value function loss: 0.9251
                    Surrogate loss: 0.0025
             Mean action noise std: 0.77
                       Mean reward: 2.98
               Mean episode length: 123.87
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 23.59s
                        Total time: 6933.07s
                               ETA: 1399382.6s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 688 steps/s (collection: 23.624s, learning 0.165s)
               Value function loss: 1.0923
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 3.62
               Mean episode length: 124.41
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 23.79s
                        Total time: 6956.86s
                               ETA: 1401327.8s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 667 steps/s (collection: 24.346s, learning 0.183s)
               Value function loss: 0.8755
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 3.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 24.53s
                        Total time: 6981.38s
                               ETA: 1403413.5s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 666 steps/s (collection: 24.346s, learning 0.224s)
               Value function loss: 0.9602
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 2.64
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 24.57s
                        Total time: 7005.95s
                               ETA: 1405498.9s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.936s, learning 0.166s)
               Value function loss: 1.2345
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 1.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 24.10s
                        Total time: 7030.06s
                               ETA: 1407482.3s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 693 steps/s (collection: 23.467s, learning 0.161s)
               Value function loss: 1.3286
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: -0.25
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 23.63s
                        Total time: 7053.68s
                               ETA: 1409362.8s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 682 steps/s (collection: 23.859s, learning 0.162s)
               Value function loss: 1.7016
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: -1.11
               Mean episode length: 124.95
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 24.02s
                        Total time: 7077.70s
                               ETA: 1411314.1s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 674 steps/s (collection: 24.102s, learning 0.186s)
               Value function loss: 22.5699
                    Surrogate loss: 0.0528
             Mean action noise std: 0.77
                       Mean reward: -1.15
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 4.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 24.29s
                        Total time: 7101.99s
                               ETA: 1413310.6s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 666 steps/s (collection: 24.411s, learning 0.169s)
               Value function loss: 0.2435
                    Surrogate loss: -0.0002
             Mean action noise std: 0.77
                       Mean reward: -1.25
               Mean episode length: 125.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 24.58s
                        Total time: 7126.57s
                               ETA: 1415357.2s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 676 steps/s (collection: 23.937s, learning 0.266s)
               Value function loss: 0.2370
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: -0.99
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 24.20s
                        Total time: 7150.78s
                               ETA: 1417320.8s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 674 steps/s (collection: 24.120s, learning 0.167s)
               Value function loss: 0.3769
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: -0.78
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 24.29s
                        Total time: 7175.06s
                               ETA: 1419293.1s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 674 steps/s (collection: 24.101s, learning 0.176s)
               Value function loss: 0.4490
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: -0.68
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 24.28s
                        Total time: 7199.34s
                               ETA: 1421255.4s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 665 steps/s (collection: 24.391s, learning 0.223s)
               Value function loss: 0.6524
                    Surrogate loss: 0.0121
             Mean action noise std: 0.77
                       Mean reward: -0.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 24.61s
                        Total time: 7223.95s
                               ETA: 1423276.1s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.328s, learning 0.171s)
               Value function loss: 0.3793
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: -0.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 24.50s
                        Total time: 7248.45s
                               ETA: 1425266.4s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1162 steps/s (collection: 13.883s, learning 0.206s)
               Value function loss: 0.5704
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: -0.89
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 14.09s
                        Total time: 7262.54s
                               ETA: 1425205.8s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1271 steps/s (collection: 12.725s, learning 0.161s)
               Value function loss: 0.9591
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: -0.97
               Mean episode length: 124.91
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 12.89s
                        Total time: 7275.43s
                               ETA: 1424909.6s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.373s, learning 0.163s)
               Value function loss: 1.1276
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 0.02
               Mean episode length: 124.91
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 12.54s
                        Total time: 7287.96s
                               ETA: 1424546.2s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.291s, learning 0.159s)
               Value function loss: 7.1690
                    Surrogate loss: 0.0113
             Mean action noise std: 0.77
                       Mean reward: -2.62
               Mean episode length: 124.46
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 12.45s
                        Total time: 7300.41s
                               ETA: 1424167.4s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.466s, learning 0.157s)
               Value function loss: 1.0901
                    Surrogate loss: 0.0036
             Mean action noise std: 0.77
                       Mean reward: -2.07
               Mean episode length: 124.25
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 12.62s
                        Total time: 7313.04s
                               ETA: 1423823.8s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.175s, learning 0.162s)
               Value function loss: 0.9703
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: -1.94
               Mean episode length: 124.25
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 12.34s
                        Total time: 7325.37s
                               ETA: 1423425.9s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.035s, learning 0.158s)
               Value function loss: 1.1603
                    Surrogate loss: 0.0096
             Mean action noise std: 0.77
                       Mean reward: -0.02
               Mean episode length: 124.70
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 12.19s
                        Total time: 7337.57s
                               ETA: 1423001.5s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.194s, learning 0.156s)
               Value function loss: 1.4623
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 1.23
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 12.35s
                        Total time: 7349.92s
                               ETA: 1422609.1s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.093s, learning 0.200s)
               Value function loss: 1.5841
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 0.03
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 12.29s
                        Total time: 7362.21s
                               ETA: 1422207.1s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1232 steps/s (collection: 13.115s, learning 0.182s)
               Value function loss: 21.3013
                    Surrogate loss: 0.0162
             Mean action noise std: 0.77
                       Mean reward: 0.09
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 4.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 13.30s
                        Total time: 7375.50s
                               ETA: 1422000.2s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1240 steps/s (collection: 13.049s, learning 0.156s)
               Value function loss: 0.1871
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 0.16
               Mean episode length: 125.00
                  Mean reward/step: -0.91
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 13.20s
                        Total time: 7388.71s
                               ETA: 1421776.3s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.968s, learning 0.166s)
               Value function loss: 0.3022
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 0.16
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 12.13s
                        Total time: 7400.84s
                               ETA: 1421347.7s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.376s, learning 0.162s)
               Value function loss: 0.3518
                    Surrogate loss: 0.0118
             Mean action noise std: 0.77
                       Mean reward: -0.54
               Mean episode length: 124.54
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 12.54s
                        Total time: 7413.38s
                               ETA: 1420998.1s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.511s, learning 0.157s)
               Value function loss: 0.3604
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: -0.49
               Mean episode length: 124.54
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 12.67s
                        Total time: 7426.05s
                               ETA: 1420674.6s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.693s, learning 0.159s)
               Value function loss: 0.3184
                    Surrogate loss: 0.0101
             Mean action noise std: 0.77
                       Mean reward: -0.52
               Mean episode length: 124.54
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 12.85s
                        Total time: 7438.90s
                               ETA: 1420387.5s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.538s, learning 0.184s)
               Value function loss: 0.2535
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: -0.16
               Mean episode length: 124.54
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 12.72s
                        Total time: 7451.62s
                               ETA: 1420076.5s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.299s, learning 0.164s)
               Value function loss: 0.4377
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: -0.13
               Mean episode length: 124.54
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 12.46s
                        Total time: 7464.09s
                               ETA: 1419717.6s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.966s, learning 0.169s)
               Value function loss: 0.8018
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: -1.84
               Mean episode length: 123.93
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 12.14s
                        Total time: 7476.22s
                               ETA: 1419297.8s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.309s, learning 0.270s)
               Value function loss: 1.0216
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: -2.96
               Mean episode length: 123.39
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 12.58s
                        Total time: 7488.80s
                               ETA: 1418963.6s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.374s, learning 0.160s)
               Value function loss: 0.8753
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: -2.84
               Mean episode length: 124.46
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 12.53s
                        Total time: 7501.33s
                               ETA: 1418622.1s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1271 steps/s (collection: 12.656s, learning 0.228s)
               Value function loss: 0.9210
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: -2.11
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 12.88s
                        Total time: 7514.22s
                               ETA: 1418348.0s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.740s, learning 0.195s)
               Value function loss: 1.1067
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: -3.45
               Mean episode length: 124.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 12.94s
                        Total time: 7527.15s
                               ETA: 1418084.4s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.133s, learning 0.172s)
               Value function loss: 1.2004
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: -3.93
               Mean episode length: 124.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 12.31s
                        Total time: 7539.46s
                               ETA: 1417703.3s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.312s, learning 0.160s)
               Value function loss: 1.4385
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: -3.55
               Mean episode length: 124.78
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 12.47s
                        Total time: 7551.93s
                               ETA: 1417355.1s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.980s, learning 0.204s)
               Value function loss: 1.5161
                    Surrogate loss: 0.0093
             Mean action noise std: 0.77
                       Mean reward: -3.07
               Mean episode length: 124.75
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 12.18s
                        Total time: 7564.12s
                               ETA: 1416953.9s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1186 steps/s (collection: 13.604s, learning 0.211s)
               Value function loss: 16.7601
                    Surrogate loss: 0.0181
             Mean action noise std: 0.77
                       Mean reward: -1.34
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 13.81s
                        Total time: 7577.93s
                               ETA: 1416859.2s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.741s, learning 0.164s)
               Value function loss: 0.3438
                    Surrogate loss: -0.0001
             Mean action noise std: 0.77
                       Mean reward: -1.34
               Mean episode length: 125.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 12.90s
                        Total time: 7590.83s
                               ETA: 1416594.9s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.200s, learning 0.165s)
               Value function loss: 0.3971
                    Surrogate loss: 0.0058
             Mean action noise std: 0.77
                       Mean reward: -1.58
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 12.36s
                        Total time: 7603.20s
                               ETA: 1416231.1s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.243s, learning 0.158s)
               Value function loss: 0.5015
                    Surrogate loss: 0.0222
             Mean action noise std: 0.77
                       Mean reward: -1.63
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 12.40s
                        Total time: 7615.60s
                               ETA: 1415875.2s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.689s, learning 0.163s)
               Value function loss: 0.3394
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: -1.78
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 12.85s
                        Total time: 7628.45s
                               ETA: 1415604.4s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.440s, learning 0.214s)
               Value function loss: 0.2523
                    Surrogate loss: 0.0003
             Mean action noise std: 0.77
                       Mean reward: -1.74
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 12.65s
                        Total time: 7641.11s
                               ETA: 1415297.8s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.173s, learning 0.163s)
               Value function loss: 0.2397
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: -1.76
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 12.34s
                        Total time: 7653.44s
                               ETA: 1414933.5s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.995s, learning 0.182s)
               Value function loss: 0.3649
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: -2.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 12.18s
                        Total time: 7665.62s
                               ETA: 1414541.2s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.458s, learning 0.162s)
               Value function loss: 0.7130
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: -4.03
               Mean episode length: 124.98
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 12.62s
                        Total time: 7678.24s
                               ETA: 1414231.9s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1261 steps/s (collection: 12.818s, learning 0.165s)
               Value function loss: 0.8231
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: -7.02
               Mean episode length: 124.98
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 12.98s
                        Total time: 7691.22s
                               ETA: 1413990.5s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.518s, learning 0.156s)
               Value function loss: 0.7555
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: -6.10
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 12.67s
                        Total time: 7703.89s
                               ETA: 1413693.2s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1267 steps/s (collection: 12.766s, learning 0.160s)
               Value function loss: 0.8400
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: -7.20
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 12.93s
                        Total time: 7716.82s
                               ETA: 1413443.1s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.419s, learning 0.211s)
               Value function loss: 1.0366
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: -6.57
               Mean episode length: 124.77
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 12.63s
                        Total time: 7729.45s
                               ETA: 1413139.8s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.166s, learning 0.168s)
               Value function loss: 1.3116
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: -7.11
               Mean episode length: 124.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 12.33s
                        Total time: 7741.79s
                               ETA: 1412783.6s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.899s, learning 0.224s)
               Value function loss: 1.7380
                    Surrogate loss: 0.0131
             Mean action noise std: 0.77
                       Mean reward: -7.41
               Mean episode length: 124.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 12.12s
                        Total time: 7753.91s
                               ETA: 1412390.1s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1218 steps/s (collection: 13.289s, learning 0.158s)
               Value function loss: 21.7163
                    Surrogate loss: 0.0146
             Mean action noise std: 0.77
                       Mean reward: -6.82
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 4.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 13.45s
                        Total time: 7767.36s
                               ETA: 1412238.7s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.953s, learning 0.200s)
               Value function loss: 0.2691
                    Surrogate loss: 0.0042
             Mean action noise std: 0.77
                       Mean reward: -6.92
               Mean episode length: 125.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 12.15s
                        Total time: 7779.51s
                               ETA: 1411853.1s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.918s, learning 0.215s)
               Value function loss: 0.2329
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: -6.95
               Mean episode length: 125.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 12.13s
                        Total time: 7791.64s
                               ETA: 1411465.1s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.475s, learning 0.167s)
               Value function loss: 0.6134
                    Surrogate loss: 0.0034
             Mean action noise std: 0.77
                       Mean reward: -6.94
               Mean episode length: 125.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 12.64s
                        Total time: 7804.28s
                               ETA: 1411170.5s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.149s, learning 0.173s)
               Value function loss: 0.3815
                    Surrogate loss: 0.0069
             Mean action noise std: 0.77
                       Mean reward: -7.25
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 12.32s
                        Total time: 7816.60s
                               ETA: 1410819.1s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.797s, learning 0.209s)
               Value function loss: 0.2351
                    Surrogate loss: 0.0109
             Mean action noise std: 0.77
                       Mean reward: -7.38
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 13.01s
                        Total time: 7829.61s
                               ETA: 1410592.4s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1268 steps/s (collection: 12.719s, learning 0.194s)
               Value function loss: 0.2293
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: -7.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 12.91s
                        Total time: 7842.52s
                               ETA: 1410349.6s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.638s, learning 0.167s)
               Value function loss: 0.2487
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: -7.06
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 12.81s
                        Total time: 7855.33s
                               ETA: 1410088.4s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.431s, learning 0.240s)
               Value function loss: 0.5308
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: -5.80
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 12.67s
                        Total time: 7868.00s
                               ETA: 1409804.1s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.525s, learning 0.274s)
               Value function loss: 0.9071
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: -1.96
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 12.80s
                        Total time: 7880.80s
                               ETA: 1409543.4s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.675s, learning 0.162s)
               Value function loss: 0.9612
                    Surrogate loss: 0.0000
             Mean action noise std: 0.77
                       Mean reward: -1.84
               Mean episode length: 124.52
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 12.84s
                        Total time: 7893.64s
                               ETA: 1409290.5s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.166s, learning 0.193s)
               Value function loss: 0.8346
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: -4.39
               Mean episode length: 123.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 12.36s
                        Total time: 7906.00s
                               ETA: 1408953.3s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.346s, learning 0.248s)
               Value function loss: 0.8444
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: -6.28
               Mean episode length: 123.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 12.59s
                        Total time: 7918.59s
                               ETA: 1408659.1s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.095s, learning 0.182s)
               Value function loss: 1.0645
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: -7.71
               Mean episode length: 124.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 12.28s
                        Total time: 7930.87s
                               ETA: 1408309.5s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.126s, learning 0.202s)
               Value function loss: 1.3046
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: -5.13
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 12.33s
                        Total time: 7943.19s
                               ETA: 1407970.2s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.094s, learning 0.273s)
               Value function loss: 1.4874
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: -4.70
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 12.37s
                        Total time: 7955.56s
                               ETA: 1407639.1s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1200 steps/s (collection: 13.485s, learning 0.159s)
               Value function loss: 18.9879
                    Surrogate loss: 0.0378
             Mean action noise std: 0.77
                       Mean reward: -3.81
               Mean episode length: 125.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 4.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 13.64s
                        Total time: 7969.21s
                               ETA: 1407534.5s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.288s, learning 0.156s)
               Value function loss: 0.1528
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: -3.64
               Mean episode length: 125.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 12.44s
                        Total time: 7981.65s
                               ETA: 1407218.6s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.497s, learning 0.164s)
               Value function loss: 0.2170
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: -3.76
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 12.66s
                        Total time: 7994.31s
                               ETA: 1406942.1s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.205s, learning 0.168s)
               Value function loss: 0.3720
                    Surrogate loss: 0.0178
             Mean action noise std: 0.77
                       Mean reward: -3.72
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 12.37s
                        Total time: 8006.68s
                               ETA: 1406615.7s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1229 steps/s (collection: 13.071s, learning 0.256s)
               Value function loss: 0.4199
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: -3.73
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 13.33s
                        Total time: 8020.01s
                               ETA: 1406458.0s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.576s, learning 0.161s)
               Value function loss: 0.2368
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: -4.32
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 12.74s
                        Total time: 8032.75s
                               ETA: 1406197.4s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1255 steps/s (collection: 12.841s, learning 0.205s)
               Value function loss: 0.2131
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: -4.16
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 13.05s
                        Total time: 8045.79s
                               ETA: 1405991.7s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.212s, learning 0.168s)
               Value function loss: 0.2535
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: -3.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 12.38s
                        Total time: 8058.17s
                               ETA: 1405670.5s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.367s, learning 0.174s)
               Value function loss: 0.5723
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: -5.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 12.54s
                        Total time: 8070.71s
                               ETA: 1405378.4s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.395s, learning 0.189s)
               Value function loss: 0.8237
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: -6.65
               Mean episode length: 124.45
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 12.58s
                        Total time: 8083.30s
                               ETA: 1405094.7s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.375s, learning 0.164s)
               Value function loss: 0.7184
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: -5.45
               Mean episode length: 124.45
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 12.54s
                        Total time: 8095.84s
                               ETA: 1404804.1s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.537s, learning 0.165s)
               Value function loss: 0.7486
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: -4.63
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 12.70s
                        Total time: 8108.54s
                               ETA: 1404542.8s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.340s, learning 0.178s)
               Value function loss: 0.8762
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: -5.14
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 12.52s
                        Total time: 8121.05s
                               ETA: 1404250.5s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.428s, learning 0.244s)
               Value function loss: 0.9852
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: -7.70
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 12.67s
                        Total time: 8133.73s
                               ETA: 1403985.8s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.191s, learning 0.158s)
               Value function loss: 1.3342
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: -8.45
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 12.35s
                        Total time: 8146.08s
                               ETA: 1403666.3s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.122s, learning 0.177s)
               Value function loss: 2.7338
                    Surrogate loss: 0.0076
             Mean action noise std: 0.77
                       Mean reward: -7.57
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 12.30s
                        Total time: 8158.37s
                               ETA: 1403339.2s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1199 steps/s (collection: 13.479s, learning 0.183s)
               Value function loss: 17.9368
                    Surrogate loss: 0.0370
             Mean action noise std: 0.77
                       Mean reward: -7.63
               Mean episode length: 125.00
                  Mean reward/step: -0.95
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 13.66s
                        Total time: 8172.04s
                               ETA: 1403247.4s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.355s, learning 0.161s)
               Value function loss: 0.2217
                    Surrogate loss: -0.0012
             Mean action noise std: 0.77
                       Mean reward: -7.63
               Mean episode length: 125.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 12.52s
                        Total time: 8184.55s
                               ETA: 1402959.3s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.591s, learning 0.202s)
               Value function loss: 0.4192
                    Surrogate loss: 0.0016
             Mean action noise std: 0.77
                       Mean reward: -7.39
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 12.79s
                        Total time: 8197.35s
                               ETA: 1402719.6s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.766s, learning 0.209s)
               Value function loss: 0.7395
                    Surrogate loss: 0.0429
             Mean action noise std: 0.77
                       Mean reward: -7.19
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 12.98s
                        Total time: 8210.32s
                               ETA: 1402511.8s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.606s, learning 0.156s)
               Value function loss: 0.2605
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: -7.18
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 12.76s
                        Total time: 8223.08s
                               ETA: 1402268.4s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.327s, learning 0.160s)
               Value function loss: 0.2859
                    Surrogate loss: 0.0087
             Mean action noise std: 0.77
                       Mean reward: -7.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 12.49s
                        Total time: 8235.57s
                               ETA: 1401979.0s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.436s, learning 0.182s)
               Value function loss: 0.2996
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: -7.25
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 12.62s
                        Total time: 8248.19s
                               ETA: 1401712.6s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1257 steps/s (collection: 12.742s, learning 0.289s)
               Value function loss: 0.4407
                    Surrogate loss: 0.0051
             Mean action noise std: 0.77
                       Mean reward: -6.88
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 13.03s
                        Total time: 8261.22s
                               ETA: 1401517.2s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.518s, learning 0.247s)
               Value function loss: 0.9369
                    Surrogate loss: 0.0059
             Mean action noise std: 0.77
                       Mean reward: -3.03
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 12.76s
                        Total time: 8273.98s
                               ETA: 1401277.4s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.365s, learning 0.193s)
               Value function loss: 0.9641
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: -2.75
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 12.56s
                        Total time: 8286.54s
                               ETA: 1401003.3s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1247 steps/s (collection: 12.787s, learning 0.342s)
               Value function loss: 0.8660
                    Surrogate loss: 0.0074
             Mean action noise std: 0.77
                       Mean reward: -2.66
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 13.13s
                        Total time: 8299.67s
                               ETA: 1400826.6s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.395s, learning 0.171s)
               Value function loss: 0.9709
                    Surrogate loss: 0.0066
             Mean action noise std: 0.77
                       Mean reward: -2.71
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 12.57s
                        Total time: 8312.24s
                               ETA: 1400555.5s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.220s, learning 0.171s)
               Value function loss: 1.0086
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: -2.38
               Mean episode length: 124.78
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 12.39s
                        Total time: 8324.63s
                               ETA: 1400255.9s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.634s, learning 0.174s)
               Value function loss: 1.3498
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: -2.62
               Mean episode length: 124.78
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 12.81s
                        Total time: 8337.44s
                               ETA: 1400027.3s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.208s, learning 0.204s)
               Value function loss: 1.3533
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: -2.28
               Mean episode length: 124.94
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 12.41s
                        Total time: 8349.85s
                               ETA: 1399733.2s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1209 steps/s (collection: 13.347s, learning 0.204s)
               Value function loss: 18.8846
                    Surrogate loss: 0.0331
             Mean action noise std: 0.77
                       Mean reward: -0.08
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 4.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 13.55s
                        Total time: 8363.40s
                               ETA: 1399630.5s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.462s, learning 0.155s)
               Value function loss: 0.4308
                    Surrogate loss: 0.0100
             Mean action noise std: 0.77
                       Mean reward: -0.16
               Mean episode length: 125.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 12.62s
                        Total time: 8376.02s
                               ETA: 1399372.1s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.903s, learning 0.170s)
               Value function loss: 0.2214
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: -0.08
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 12.07s
                        Total time: 8388.09s
                               ETA: 1399023.8s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.375s, learning 0.159s)
               Value function loss: 0.6287
                    Surrogate loss: 0.0197
             Mean action noise std: 0.77
                       Mean reward: 0.35
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 12.53s
                        Total time: 8400.62s
                               ETA: 1398753.3s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.189s, learning 0.158s)
               Value function loss: 0.5152
                    Surrogate loss: 0.0158
             Mean action noise std: 0.77
                       Mean reward: 0.43
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 12.35s
                        Total time: 8412.97s
                               ETA: 1398452.6s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1256 steps/s (collection: 12.835s, learning 0.200s)
               Value function loss: 0.3001
                    Surrogate loss: 0.0142
             Mean action noise std: 0.77
                       Mean reward: 0.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 13.04s
                        Total time: 8426.01s
                               ETA: 1398267.0s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.648s, learning 0.155s)
               Value function loss: 0.3026
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 0.35
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 12.80s
                        Total time: 8438.81s
                               ETA: 1398043.7s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1347 steps/s (collection: 12.002s, learning 0.155s)
               Value function loss: 0.3839
                    Surrogate loss: 0.0026
             Mean action noise std: 0.77
                       Mean reward: 0.91
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 12.16s
                        Total time: 8450.97s
                               ETA: 1397714.1s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.587s, learning 0.158s)
               Value function loss: 0.6910
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 1.27
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 12.74s
                        Total time: 8463.71s
                               ETA: 1397482.7s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.572s, learning 0.162s)
               Value function loss: 1.1282
                    Surrogate loss: 0.0012
             Mean action noise std: 0.77
                       Mean reward: 2.95
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 11.73s
                        Total time: 8475.45s
                               ETA: 1397085.3s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.471s, learning 0.185s)
               Value function loss: 0.9604
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 4.34
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 12.66s
                        Total time: 8488.10s
                               ETA: 1396841.0s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.589s, learning 0.244s)
               Value function loss: 1.1575
                    Surrogate loss: 0.0009
             Mean action noise std: 0.77
                       Mean reward: 3.69
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 12.83s
                        Total time: 8500.94s
                               ETA: 1396626.5s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.378s, learning 0.163s)
               Value function loss: 1.0949
                    Surrogate loss: 0.0128
             Mean action noise std: 0.77
                       Mean reward: 4.75
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 12.54s
                        Total time: 8513.48s
                               ETA: 1396364.8s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.724s, learning 0.210s)
               Value function loss: 1.3402
                    Surrogate loss: 0.0030
             Mean action noise std: 0.77
                       Mean reward: 3.35
               Mean episode length: 124.75
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 12.93s
                        Total time: 8526.41s
                               ETA: 1396168.4s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.738s, learning 0.173s)
               Value function loss: 1.7171
                    Surrogate loss: 0.0222
             Mean action noise std: 0.77
                       Mean reward: 2.32
               Mean episode length: 124.59
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 12.91s
                        Total time: 8539.32s
                               ETA: 1395968.6s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.241s, learning 0.180s)
               Value function loss: 1.7741
                    Surrogate loss: -0.0015
             Mean action noise std: 0.77
                       Mean reward: 5.25
               Mean episode length: 124.90
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 12.42s
                        Total time: 8551.74s
                               ETA: 1395689.4s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1185 steps/s (collection: 13.655s, learning 0.160s)
               Value function loss: 16.6457
                    Surrogate loss: 0.0482
             Mean action noise std: 0.77
                       Mean reward: 7.09
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 13.82s
                        Total time: 8565.56s
                               ETA: 1395638.4s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.530s, learning 0.192s)
               Value function loss: 0.2600
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 7.09
               Mean episode length: 125.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 12.72s
                        Total time: 8578.28s
                               ETA: 1395409.6s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.372s, learning 0.162s)
               Value function loss: 0.4696
                    Surrogate loss: 0.0101
             Mean action noise std: 0.77
                       Mean reward: 7.15
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 12.53s
                        Total time: 8590.81s
                               ETA: 1395151.1s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.352s, learning 0.157s)
               Value function loss: 0.8371
                    Surrogate loss: -0.0012
             Mean action noise std: 0.77
                       Mean reward: 6.95
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 12.51s
                        Total time: 8603.32s
                               ETA: 1394889.3s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.465s, learning 0.179s)
               Value function loss: 0.6996
                    Surrogate loss: 0.0227
             Mean action noise std: 0.77
                       Mean reward: 7.08
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 12.64s
                        Total time: 8615.97s
                               ETA: 1394650.1s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.228s, learning 0.303s)
               Value function loss: 0.4064
                    Surrogate loss: 0.0203
             Mean action noise std: 0.77
                       Mean reward: 6.97
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 12.53s
                        Total time: 8628.50s
                               ETA: 1394393.3s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.512s, learning 0.290s)
               Value function loss: 0.3362
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: 6.88
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 12.80s
                        Total time: 8641.30s
                               ETA: 1394181.2s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.714s, learning 0.226s)
               Value function loss: 0.4050
                    Surrogate loss: 0.0047
             Mean action noise std: 0.77
                       Mean reward: 6.92
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 12.94s
                        Total time: 8654.24s
                               ETA: 1393991.8s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.366s, learning 0.156s)
               Value function loss: 0.8267
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 5.37
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 12.52s
                        Total time: 8666.76s
                               ETA: 1393735.8s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.637s, learning 0.226s)
               Value function loss: 1.0870
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 3.21
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 12.86s
                        Total time: 8679.62s
                               ETA: 1393535.3s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1271 steps/s (collection: 12.683s, learning 0.206s)
               Value function loss: 0.8752
                    Surrogate loss: 0.0082
             Mean action noise std: 0.77
                       Mean reward: 3.27
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 12.89s
                        Total time: 8692.51s
                               ETA: 1393339.6s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.185s, learning 0.160s)
               Value function loss: 1.0108
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 1.00
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 12.34s
                        Total time: 8704.86s
                               ETA: 1393057.5s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.376s, learning 0.160s)
               Value function loss: 1.2838
                    Surrogate loss: -0.0002
             Mean action noise std: 0.77
                       Mean reward: 1.34
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 12.54s
                        Total time: 8717.39s
                               ETA: 1392806.8s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.512s, learning 0.158s)
               Value function loss: 1.5414
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 3.63
               Mean episode length: 124.84
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 11.67s
                        Total time: 8729.06s
                               ETA: 1392418.6s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.166s, learning 0.162s)
               Value function loss: 2.0093
                    Surrogate loss: 0.0054
             Mean action noise std: 0.77
                       Mean reward: 3.61
               Mean episode length: 124.84
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 12.33s
                        Total time: 8741.39s
                               ETA: 1392136.6s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1195 steps/s (collection: 13.429s, learning 0.277s)
               Value function loss: 25.3372
                    Surrogate loss: 0.0224
             Mean action noise std: 0.77
                       Mean reward: 6.90
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 4.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 13.71s
                        Total time: 8755.10s
                               ETA: 1392074.4s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1263 steps/s (collection: 12.744s, learning 0.222s)
               Value function loss: 0.2644
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 6.71
               Mean episode length: 125.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 12.97s
                        Total time: 8768.06s
                               ETA: 1391894.9s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.161s, learning 0.171s)
               Value function loss: 0.2711
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 6.78
               Mean episode length: 125.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 12.33s
                        Total time: 8780.40s
                               ETA: 1391615.7s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.216s, learning 0.231s)
               Value function loss: 0.6343
                    Surrogate loss: 0.0066
             Mean action noise std: 0.77
                       Mean reward: 6.92
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 12.45s
                        Total time: 8792.84s
                               ETA: 1391355.4s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.488s, learning 0.268s)
               Value function loss: 5.3497
                    Surrogate loss: 0.0312
             Mean action noise std: 0.77
                       Mean reward: 6.80
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 12.76s
                        Total time: 8805.60s
                               ETA: 1391144.7s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.945s, learning 0.226s)
               Value function loss: 1.1984
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 6.70
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 12.17s
                        Total time: 8817.77s
                               ETA: 1390842.2s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.146s, learning 0.159s)
               Value function loss: 0.9787
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 7.27
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 12.31s
                        Total time: 8830.08s
                               ETA: 1390562.0s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.171s, learning 0.189s)
               Value function loss: 1.2438
                    Surrogate loss: 0.0046
             Mean action noise std: 0.77
                       Mean reward: 6.62
               Mean episode length: 124.26
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 12.36s
                        Total time: 8842.44s
                               ETA: 1390291.0s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.415s, learning 0.162s)
               Value function loss: 1.9924
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 6.44
               Mean episode length: 124.26
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 12.58s
                        Total time: 8855.01s
                               ETA: 1390055.2s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.119s, learning 0.212s)
               Value function loss: 2.4698
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 0.12
               Mean episode length: 123.63
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 12.33s
                        Total time: 8867.34s
                               ETA: 1389781.2s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.236s, learning 0.169s)
               Value function loss: 2.3370
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: -0.14
               Mean episode length: 123.53
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 12.41s
                        Total time: 8879.75s
                               ETA: 1389519.8s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.209s, learning 0.188s)
               Value function loss: 2.0832
                    Surrogate loss: 0.1008
             Mean action noise std: 0.77
                       Mean reward: 5.71
               Mean episode length: 122.93
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 12.40s
                        Total time: 8892.15s
                               ETA: 1389257.9s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.603s, learning 0.161s)
               Value function loss: 1.5820
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 10.03
               Mean episode length: 123.50
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 11.76s
                        Total time: 8903.91s
                               ETA: 1388898.0s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.421s, learning 0.229s)
               Value function loss: 2.7468
                    Surrogate loss: -0.0002
             Mean action noise std: 0.77
                       Mean reward: 11.39
               Mean episode length: 123.91
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 12.65s
                        Total time: 8916.56s
                               ETA: 1388677.2s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.304s, learning 0.169s)
               Value function loss: 2.5882
                    Surrogate loss: 0.0032
             Mean action noise std: 0.77
                       Mean reward: 8.85
               Mean episode length: 124.34
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 12.47s
                        Total time: 8929.03s
                               ETA: 1388429.6s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.631s, learning 0.164s)
               Value function loss: 2.2642
                    Surrogate loss: 0.0016
             Mean action noise std: 0.77
                       Mean reward: 7.93
               Mean episode length: 124.80
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 12.80s
                        Total time: 8941.83s
                               ETA: 1388232.7s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1201 steps/s (collection: 13.476s, learning 0.161s)
               Value function loss: 25.7188
                    Surrogate loss: 0.0482
             Mean action noise std: 0.77
                       Mean reward: 9.60
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 13.64s
                        Total time: 8955.47s
                               ETA: 1388167.0s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1265 steps/s (collection: 12.784s, learning 0.166s)
               Value function loss: 0.3655
                    Surrogate loss: 0.0019
             Mean action noise std: 0.77
                       Mean reward: 9.91
               Mean episode length: 125.00
                  Mean reward/step: -0.89
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 12.95s
                        Total time: 8968.42s
                               ETA: 1387995.0s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.924s, learning 0.161s)
               Value function loss: 0.4340
                    Surrogate loss: 0.0064
             Mean action noise std: 0.77
                       Mean reward: 10.07
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 12.09s
                        Total time: 8980.50s
                               ETA: 1387689.9s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.182s, learning 0.160s)
               Value function loss: 1.1144
                    Surrogate loss: 0.0634
             Mean action noise std: 0.77
                       Mean reward: 9.96
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 12.34s
                        Total time: 8992.84s
                               ETA: 1387425.3s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.234s, learning 0.187s)
               Value function loss: 1.5476
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 9.70
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 12.42s
                        Total time: 9005.26s
                               ETA: 1387173.6s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.283s, learning 0.157s)
               Value function loss: 1.3735
                    Surrogate loss: 0.0107
             Mean action noise std: 0.77
                       Mean reward: 9.80
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 12.44s
                        Total time: 9017.70s
                               ETA: 1386925.7s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.497s, learning 0.162s)
               Value function loss: 1.1608
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 9.44
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 12.66s
                        Total time: 9030.36s
                               ETA: 1386712.1s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.691s, learning 0.163s)
               Value function loss: 1.1358
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 8.60
               Mean episode length: 124.34
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 12.85s
                        Total time: 9043.22s
                               ETA: 1386528.9s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.291s, learning 0.162s)
               Value function loss: 1.2527
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 6.84
               Mean episode length: 124.34
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 12.45s
                        Total time: 9055.67s
                               ETA: 1386285.0s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.027s, learning 0.161s)
               Value function loss: 1.5470
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 4.65
               Mean episode length: 123.84
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 12.19s
                        Total time: 9067.86s
                               ETA: 1386001.3s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.111s, learning 0.184s)
               Value function loss: 1.2835
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 5.06
               Mean episode length: 123.66
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 12.30s
                        Total time: 9080.15s
                               ETA: 1385734.8s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.381s, learning 0.271s)
               Value function loss: 1.5546
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 4.85
               Mean episode length: 123.66
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 12.65s
                        Total time: 9092.81s
                               ETA: 1385523.4s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.657s, learning 0.182s)
               Value function loss: 3.3515
                    Surrogate loss: 0.0026
             Mean action noise std: 0.76
                       Mean reward: 3.38
               Mean episode length: 122.94
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 12.84s
                        Total time: 9105.65s
                               ETA: 1385341.1s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.540s, learning 0.253s)
               Value function loss: 2.6120
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 2.95
               Mean episode length: 123.56
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 12.79s
                        Total time: 9118.44s
                               ETA: 1385152.3s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.204s, learning 0.159s)
               Value function loss: 5.0584
                    Surrogate loss: 0.0018
             Mean action noise std: 0.76
                       Mean reward: 5.12
               Mean episode length: 124.88
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 12.36s
                        Total time: 9130.80s
                               ETA: 1384898.8s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.358s, learning 0.228s)
               Value function loss: 6.2498
                    Surrogate loss: 0.0064
             Mean action noise std: 0.76
                       Mean reward: 3.93
               Mean episode length: 124.87
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 12.59s
                        Total time: 9143.39s
                               ETA: 1384679.9s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1175 steps/s (collection: 13.760s, learning 0.180s)
               Value function loss: 18.9723
                    Surrogate loss: 0.0804
             Mean action noise std: 0.76
                       Mean reward: 4.12
               Mean episode length: 125.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 4.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 13.94s
                        Total time: 9157.33s
                               ETA: 1384666.2s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.137s, learning 0.230s)
               Value function loss: 0.4465
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 4.12
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 12.37s
                        Total time: 9169.70s
                               ETA: 1384415.1s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.258s, learning 0.232s)
               Value function loss: 0.5755
                    Surrogate loss: 0.0062
             Mean action noise std: 0.76
                       Mean reward: 4.78
               Mean episode length: 125.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 12.49s
                        Total time: 9182.19s
                               ETA: 1384183.2s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.072s, learning 0.231s)
               Value function loss: 0.9452
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 4.64
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 12.30s
                        Total time: 9194.49s
                               ETA: 1383923.8s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.397s, learning 0.164s)
               Value function loss: 1.8837
                    Surrogate loss: 0.0387
             Mean action noise std: 0.76
                       Mean reward: 4.32
               Mean episode length: 124.54
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 12.56s
                        Total time: 9207.05s
                               ETA: 1383704.0s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.532s, learning 0.262s)
               Value function loss: 0.4827
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 3.69
               Mean episode length: 124.54
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 12.79s
                        Total time: 9219.84s
                               ETA: 1383519.8s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.258s, learning 0.167s)
               Value function loss: 0.3809
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 3.92
               Mean episode length: 124.54
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 12.42s
                        Total time: 9232.27s
                               ETA: 1383280.8s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.807s, learning 0.174s)
               Value function loss: 0.6302
                    Surrogate loss: 0.0056
             Mean action noise std: 0.76
                       Mean reward: 3.42
               Mean episode length: 124.54
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 11.98s
                        Total time: 9244.25s
                               ETA: 1382976.1s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.143s, learning 0.163s)
               Value function loss: 1.2808
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 1.68
               Mean episode length: 123.98
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 12.31s
                        Total time: 9256.56s
                               ETA: 1382720.7s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.558s, learning 0.205s)
               Value function loss: 1.3134
                    Surrogate loss: 0.0015
             Mean action noise std: 0.76
                       Mean reward: 2.97
               Mean episode length: 124.44
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 12.76s
                        Total time: 9269.32s
                               ETA: 1382534.2s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.675s, learning 0.185s)
               Value function loss: 1.1687
                    Surrogate loss: 0.0005
             Mean action noise std: 0.76
                       Mean reward: 4.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 12.86s
                        Total time: 9282.18s
                               ETA: 1382362.8s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.623s, learning 0.183s)
               Value function loss: 1.3923
                    Surrogate loss: 0.0024
             Mean action noise std: 0.76
                       Mean reward: 5.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 12.81s
                        Total time: 9294.99s
                               ETA: 1382183.8s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.354s, learning 0.183s)
               Value function loss: 1.6749
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 5.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 12.54s
                        Total time: 9307.52s
                               ETA: 1381965.3s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.467s, learning 0.169s)
               Value function loss: 2.3005
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 5.77
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 12.64s
                        Total time: 9320.16s
                               ETA: 1381762.2s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.657s, learning 0.211s)
               Value function loss: 2.8137
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 6.10
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 12.87s
                        Total time: 9333.03s
                               ETA: 1381594.0s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1230 steps/s (collection: 13.156s, learning 0.164s)
               Value function loss: 37.4985
                    Surrogate loss: 0.0256
             Mean action noise std: 0.76
                       Mean reward: 3.66
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 4.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 13.32s
                        Total time: 9346.35s
                               ETA: 1381493.0s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.687s, learning 0.170s)
               Value function loss: 0.3321
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 3.45
               Mean episode length: 125.00
                  Mean reward/step: -1.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 12.86s
                        Total time: 9359.20s
                               ETA: 1381323.9s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.377s, learning 0.162s)
               Value function loss: 0.3422
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 3.60
               Mean episode length: 125.00
                  Mean reward/step: -0.77
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 12.54s
                        Total time: 9371.74s
                               ETA: 1381108.4s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.544s, learning 0.182s)
               Value function loss: 0.5227
                    Surrogate loss: 0.0044
             Mean action noise std: 0.76
                       Mean reward: 3.97
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 12.73s
                        Total time: 9384.47s
                               ETA: 1380921.0s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.312s, learning 0.210s)
               Value function loss: 1.2824
                    Surrogate loss: 0.0241
             Mean action noise std: 0.76
                       Mean reward: 4.36
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 12.52s
                        Total time: 9396.99s
                               ETA: 1380704.2s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.384s, learning 0.180s)
               Value function loss: 0.3697
                    Surrogate loss: -0.0002
             Mean action noise std: 0.76
                       Mean reward: 4.41
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 12.56s
                        Total time: 9409.55s
                               ETA: 1380494.2s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.538s, learning 0.200s)
               Value function loss: 0.3622
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: 4.21
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 12.74s
                        Total time: 9422.29s
                               ETA: 1380310.2s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.391s, learning 0.166s)
               Value function loss: 0.3644
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 4.05
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 12.56s
                        Total time: 9434.85s
                               ETA: 1380100.2s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.387s, learning 0.169s)
               Value function loss: 1.0779
                    Surrogate loss: -0.0018
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 12.56s
                        Total time: 9447.40s
                               ETA: 1379890.7s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.552s, learning 0.160s)
               Value function loss: 1.9695
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 2.57
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 12.71s
                        Total time: 9460.12s
                               ETA: 1379704.6s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.139s, learning 0.174s)
               Value function loss: 1.7575
                    Surrogate loss: 0.0110
             Mean action noise std: 0.76
                       Mean reward: 0.74
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 12.31s
                        Total time: 9472.43s
                               ETA: 1379460.8s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1268 steps/s (collection: 12.750s, learning 0.163s)
               Value function loss: 1.5769
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 1.93
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 12.91s
                        Total time: 9485.34s
                               ETA: 1379305.0s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.151s, learning 0.305s)
               Value function loss: 1.7303
                    Surrogate loss: 0.0017
             Mean action noise std: 0.76
                       Mean reward: 3.46
               Mean episode length: 124.54
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 12.46s
                        Total time: 9497.80s
                               ETA: 1379083.2s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.062s, learning 0.202s)
               Value function loss: 2.2947
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 3.10
               Mean episode length: 124.54
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 12.26s
                        Total time: 9510.06s
                               ETA: 1378834.1s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.675s, learning 0.161s)
               Value function loss: 2.7506
                    Surrogate loss: 0.0029
             Mean action noise std: 0.76
                       Mean reward: 4.77
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 12.84s
                        Total time: 9522.90s
                               ETA: 1378668.6s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.143s, learning 0.180s)
               Value function loss: 2.8469
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 3.15
               Mean episode length: 124.93
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 12.32s
                        Total time: 9535.22s
                               ETA: 1378429.4s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1203 steps/s (collection: 13.442s, learning 0.167s)
               Value function loss: 30.6241
                    Surrogate loss: 0.0258
             Mean action noise std: 0.76
                       Mean reward: 4.04
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 4.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 13.61s
                        Total time: 9548.83s
                               ETA: 1378376.5s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.479s, learning 0.156s)
               Value function loss: 0.3435
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: 3.94
               Mean episode length: 125.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 12.64s
                        Total time: 9561.47s
                               ETA: 1378183.4s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.922s, learning 0.203s)
               Value function loss: 0.3596
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 3.93
               Mean episode length: 125.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 12.12s
                        Total time: 9573.59s
                               ETA: 1377917.3s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.457s, learning 0.249s)
               Value function loss: 0.5976
                    Surrogate loss: 0.0058
             Mean action noise std: 0.76
                       Mean reward: 3.87
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 12.71s
                        Total time: 9586.30s
                               ETA: 1377735.5s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.693s, learning 0.162s)
               Value function loss: 0.5211
                    Surrogate loss: 0.0119
             Mean action noise std: 0.76
                       Mean reward: 3.81
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 12.85s
                        Total time: 9599.15s
                               ETA: 1377575.4s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.738s, learning 0.159s)
               Value function loss: 0.3599
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 3.80
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 12.90s
                        Total time: 9612.05s
                               ETA: 1377421.9s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1264 steps/s (collection: 12.783s, learning 0.171s)
               Value function loss: 0.3853
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 4.12
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 12.95s
                        Total time: 9625.00s
                               ETA: 1377277.0s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1276 steps/s (collection: 12.678s, learning 0.158s)
               Value function loss: 0.5455
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 3.95
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 12.84s
                        Total time: 9637.84s
                               ETA: 1377115.6s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.291s, learning 0.190s)
               Value function loss: 1.2497
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 6.53
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 12.48s
                        Total time: 9650.32s
                               ETA: 1376904.0s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.295s, learning 0.171s)
               Value function loss: 1.8167
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 8.22
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 12.47s
                        Total time: 9662.79s
                               ETA: 1376690.6s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.711s, learning 0.163s)
               Value function loss: 1.5030
                    Surrogate loss: 0.0027
             Mean action noise std: 0.76
                       Mean reward: 6.08
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 12.87s
                        Total time: 9675.66s
                               ETA: 1376536.0s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.324s, learning 0.184s)
               Value function loss: 1.4819
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 4.70
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 12.51s
                        Total time: 9688.17s
                               ETA: 1376329.8s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.949s, learning 0.171s)
               Value function loss: 2.2541
                    Surrogate loss: -0.0003
             Mean action noise std: 0.76
                       Mean reward: 2.04
               Mean episode length: 124.73
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 12.12s
                        Total time: 9700.29s
                               ETA: 1376069.2s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.319s, learning 0.184s)
               Value function loss: 2.3241
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 4.09
               Mean episode length: 124.73
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 12.50s
                        Total time: 9712.79s
                               ETA: 1375863.5s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1254 steps/s (collection: 12.815s, learning 0.245s)
               Value function loss: 2.9123
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 4.79
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 13.06s
                        Total time: 9725.85s
                               ETA: 1375737.1s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.642s, learning 0.183s)
               Value function loss: 6.8053
                    Surrogate loss: 0.0219
             Mean action noise std: 0.76
                       Mean reward: 7.48
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 12.82s
                        Total time: 9738.68s
                               ETA: 1375577.8s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1185 steps/s (collection: 13.668s, learning 0.157s)
               Value function loss: 27.5426
                    Surrogate loss: 0.0154
             Mean action noise std: 0.76
                       Mean reward: 7.67
               Mean episode length: 125.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 13.82s
                        Total time: 9752.50s
                               ETA: 1375559.9s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.263s, learning 0.161s)
               Value function loss: 0.3491
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 7.67
               Mean episode length: 125.00
                  Mean reward/step: -0.82
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 12.42s
                        Total time: 9764.93s
                               ETA: 1375344.9s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.846s, learning 0.163s)
               Value function loss: 0.3926
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 8.42
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 12.01s
                        Total time: 9776.94s
                               ETA: 1375072.0s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.234s, learning 0.157s)
               Value function loss: 0.7066
                    Surrogate loss: 0.0060
             Mean action noise std: 0.76
                       Mean reward: 8.34
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 12.39s
                        Total time: 9789.33s
                               ETA: 1374853.5s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.201s, learning 0.164s)
               Value function loss: 0.7405
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 8.05
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 12.37s
                        Total time: 9801.69s
                               ETA: 1374632.0s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.306s, learning 0.166s)
               Value function loss: 0.5591
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 7.95
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 12.47s
                        Total time: 9814.16s
                               ETA: 1374426.0s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.954s, learning 0.206s)
               Value function loss: 0.7465
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 7.39
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 12.16s
                        Total time: 9826.32s
                               ETA: 1374176.9s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.387s, learning 0.157s)
               Value function loss: 1.5433
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 6.76
               Mean episode length: 124.66
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 12.54s
                        Total time: 9838.87s
                               ETA: 1373982.0s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.693s, learning 0.158s)
               Value function loss: 2.2983
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 5.37
               Mean episode length: 124.09
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 12.85s
                        Total time: 9851.72s
                               ETA: 1373830.4s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.509s, learning 0.171s)
               Value function loss: 2.2210
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 5.46
               Mean episode length: 124.43
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 12.68s
                        Total time: 9864.40s
                               ETA: 1373655.4s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.516s, learning 0.228s)
               Value function loss: 2.2701
                    Surrogate loss: -0.0014
             Mean action noise std: 0.76
                       Mean reward: 3.45
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 12.74s
                        Total time: 9877.14s
                               ETA: 1373489.9s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.641s, learning 0.180s)
               Value function loss: 2.0490
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 1.39
               Mean episode length: 124.71
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 12.82s
                        Total time: 9889.96s
                               ETA: 1373335.5s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.087s, learning 0.162s)
               Value function loss: 2.2351
                    Surrogate loss: 0.0051
             Mean action noise std: 0.76
                       Mean reward: 0.54
               Mean episode length: 124.71
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 12.25s
                        Total time: 9902.21s
                               ETA: 1373102.1s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.448s, learning 0.207s)
               Value function loss: 2.7716
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 3.23
               Mean episode length: 124.70
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 12.66s
                        Total time: 9914.87s
                               ETA: 1372925.6s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.476s, learning 0.164s)
               Value function loss: 2.9651
                    Surrogate loss: 0.0146
             Mean action noise std: 0.76
                       Mean reward: 3.59
               Mean episode length: 124.52
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 12.64s
                        Total time: 9927.51s
                               ETA: 1372747.4s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1222 steps/s (collection: 13.185s, learning 0.214s)
               Value function loss: 38.2171
                    Surrogate loss: 0.0341
             Mean action noise std: 0.76
                       Mean reward: 8.14
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 4.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 13.40s
                        Total time: 9940.91s
                               ETA: 1372674.5s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.252s, learning 0.157s)
               Value function loss: 0.3994
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: 8.07
               Mean episode length: 125.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 12.41s
                        Total time: 9953.31s
                               ETA: 1372465.3s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.379s, learning 0.164s)
               Value function loss: 0.4302
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 7.91
               Mean episode length: 125.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 12.54s
                        Total time: 9965.86s
                               ETA: 1372275.0s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.257s, learning 0.169s)
               Value function loss: 0.7215
                    Surrogate loss: 0.0102
             Mean action noise std: 0.76
                       Mean reward: 7.93
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 12.43s
                        Total time: 9978.28s
                               ETA: 1372069.2s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.190s, learning 0.156s)
               Value function loss: 1.6105
                    Surrogate loss: 0.0129
             Mean action noise std: 0.76
                       Mean reward: 8.23
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 12.35s
                        Total time: 9990.63s
                               ETA: 1371852.9s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.344s, learning 0.171s)
               Value function loss: 1.0663
                    Surrogate loss: 0.0036
             Mean action noise std: 0.76
                       Mean reward: 8.19
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 12.52s
                        Total time: 10003.14s
                               ETA: 1371660.4s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.111s, learning 0.163s)
               Value function loss: 0.8398
                    Surrogate loss: 0.0084
             Mean action noise std: 0.76
                       Mean reward: 8.54
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 12.27s
                        Total time: 10015.42s
                               ETA: 1371435.4s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.583s, learning 0.189s)
               Value function loss: 0.8085
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 8.00
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 12.77s
                        Total time: 10028.19s
                               ETA: 1371279.1s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.075s, learning 0.157s)
               Value function loss: 1.4008
                    Surrogate loss: -0.0004
             Mean action noise std: 0.76
                       Mean reward: 6.78
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 12.23s
                        Total time: 10040.42s
                               ETA: 1371049.5s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.510s, learning 0.157s)
               Value function loss: 2.2386
                    Surrogate loss: 0.0080
             Mean action noise std: 0.76
                       Mean reward: 5.95
               Mean episode length: 124.44
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 12.67s
                        Total time: 10053.09s
                               ETA: 1370879.7s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.371s, learning 0.300s)
               Value function loss: 1.7640
                    Surrogate loss: 0.0002
             Mean action noise std: 0.76
                       Mean reward: 6.64
               Mean episode length: 124.41
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 12.67s
                        Total time: 10065.76s
                               ETA: 1370710.9s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.523s, learning 0.294s)
               Value function loss: 1.7312
                    Surrogate loss: 0.0166
             Mean action noise std: 0.76
                       Mean reward: 3.70
               Mean episode length: 124.97
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 12.82s
                        Total time: 10078.58s
                               ETA: 1370562.5s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.333s, learning 0.216s)
               Value function loss: 1.9894
                    Surrogate loss: 0.0021
             Mean action noise std: 0.76
                       Mean reward: 4.16
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 12.55s
                        Total time: 10091.13s
                               ETA: 1370378.0s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.440s, learning 0.184s)
               Value function loss: 2.3390
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 6.46
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 12.62s
                        Total time: 10103.75s
                               ETA: 1370204.1s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.055s, learning 0.174s)
               Value function loss: 3.1112
                    Surrogate loss: 0.0047
             Mean action noise std: 0.76
                       Mean reward: 5.10
               Mean episode length: 124.87
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 12.23s
                        Total time: 10115.98s
                               ETA: 1369977.2s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.272s, learning 0.185s)
               Value function loss: 2.9712
                    Surrogate loss: 0.0037
             Mean action noise std: 0.76
                       Mean reward: 3.60
               Mean episode length: 124.87
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 12.46s
                        Total time: 10128.44s
                               ETA: 1369781.7s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1255 steps/s (collection: 12.887s, learning 0.165s)
               Value function loss: 32.8280
                    Surrogate loss: 0.0225
             Mean action noise std: 0.76
                       Mean reward: 5.08
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 4.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 13.05s
                        Total time: 10141.49s
                               ETA: 1369666.9s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.573s, learning 0.164s)
               Value function loss: 1.3671
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 5.03
               Mean episode length: 125.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 12.74s
                        Total time: 10154.23s
                               ETA: 1369510.1s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.932s, learning 0.155s)
               Value function loss: 0.6476
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 5.33
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 13.09s
                        Total time: 10167.32s
                               ETA: 1369400.8s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.435s, learning 0.159s)
               Value function loss: 0.6267
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 5.65
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 12.59s
                        Total time: 10179.91s
                               ETA: 1369225.4s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.425s, learning 0.157s)
               Value function loss: 0.8895
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 5.50
               Mean episode length: 125.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 12.58s
                        Total time: 10192.49s
                               ETA: 1369048.8s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1256 steps/s (collection: 12.880s, learning 0.156s)
               Value function loss: 1.1413
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 5.42
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 13.04s
                        Total time: 10205.53s
                               ETA: 1368933.5s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.148s, learning 0.279s)
               Value function loss: 0.8773
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 4.98
               Mean episode length: 124.67
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 12.43s
                        Total time: 10217.95s
                               ETA: 1368736.9s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.379s, learning 0.206s)
               Value function loss: 0.8041
                    Surrogate loss: 0.0268
             Mean action noise std: 0.76
                       Mean reward: 3.13
               Mean episode length: 123.99
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 12.58s
                        Total time: 10230.54s
                               ETA: 1368562.0s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.471s, learning 0.207s)
               Value function loss: 1.5181
                    Surrogate loss: 0.0055
             Mean action noise std: 0.76
                       Mean reward: 1.73
               Mean episode length: 123.40
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 12.68s
                        Total time: 10243.22s
                               ETA: 1368399.9s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.447s, learning 0.193s)
               Value function loss: 1.8904
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 1.78
               Mean episode length: 124.41
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 12.64s
                        Total time: 10255.86s
                               ETA: 1368233.1s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1261 steps/s (collection: 12.751s, learning 0.239s)
               Value function loss: 1.4683
                    Surrogate loss: 0.0025
             Mean action noise std: 0.76
                       Mean reward: 0.87
               Mean episode length: 124.84
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 12.99s
                        Total time: 10268.85s
                               ETA: 1368113.5s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.260s, learning 0.166s)
               Value function loss: 1.5471
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: -0.80
               Mean episode length: 124.53
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 12.43s
                        Total time: 10281.27s
                               ETA: 1367919.0s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.593s, learning 0.177s)
               Value function loss: 1.9201
                    Surrogate loss: 0.0239
             Mean action noise std: 0.76
                       Mean reward: -0.76
               Mean episode length: 124.53
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 12.77s
                        Total time: 10294.04s
                               ETA: 1367770.9s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.198s, learning 0.176s)
               Value function loss: 2.2020
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 2.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 12.37s
                        Total time: 10306.42s
                               ETA: 1367570.6s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.462s, learning 0.174s)
               Value function loss: 3.0129
                    Surrogate loss: 0.0005
             Mean action noise std: 0.76
                       Mean reward: 2.37
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 12.64s
                        Total time: 10319.05s
                               ETA: 1367405.4s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1208 steps/s (collection: 13.385s, learning 0.171s)
               Value function loss: 37.8608
                    Surrogate loss: 0.0319
             Mean action noise std: 0.76
                       Mean reward: -1.07
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 13.56s
                        Total time: 10332.61s
                               ETA: 1367362.3s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1254 steps/s (collection: 12.889s, learning 0.172s)
               Value function loss: 0.2881
                    Surrogate loss: -0.0190
             Mean action noise std: 0.76
                       Mean reward: -1.34
               Mean episode length: 125.00
                  Mean reward/step: -0.97
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 13.06s
                        Total time: 10345.67s
                               ETA: 1367253.9s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.556s, learning 0.163s)
               Value function loss: 0.2379
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: -1.13
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 12.72s
                        Total time: 10358.39s
                               ETA: 1367100.6s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.110s, learning 0.174s)
               Value function loss: 0.3384
                    Surrogate loss: 0.0088
             Mean action noise std: 0.76
                       Mean reward: -1.76
               Mean episode length: 125.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 12.28s
                        Total time: 10370.67s
                               ETA: 1366890.5s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.377s, learning 0.175s)
               Value function loss: 0.4796
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: -1.71
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 12.55s
                        Total time: 10383.22s
                               ETA: 1366716.1s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.109s, learning 0.162s)
               Value function loss: 0.9693
                    Surrogate loss: 0.0194
             Mean action noise std: 0.76
                       Mean reward: -1.78
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 12.27s
                        Total time: 10395.50s
                               ETA: 1366505.2s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.612s, learning 0.158s)
               Value function loss: 0.9232
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: -1.98
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 12.77s
                        Total time: 10408.27s
                               ETA: 1366360.2s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.679s, learning 0.191s)
               Value function loss: 0.7022
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: -2.00
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 12.87s
                        Total time: 10421.14s
                               ETA: 1366228.7s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.617s, learning 0.169s)
               Value function loss: 0.9034
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: -1.61
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 12.79s
                        Total time: 10433.92s
                               ETA: 1366086.5s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.535s, learning 0.261s)
               Value function loss: 1.5527
                    Surrogate loss: 0.0002
             Mean action noise std: 0.76
                       Mean reward: -4.58
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 12.80s
                        Total time: 10446.72s
                               ETA: 1365946.0s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.425s, learning 0.190s)
               Value function loss: 1.5917
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: -5.13
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 12.62s
                        Total time: 10459.33s
                               ETA: 1365782.3s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.226s, learning 0.171s)
               Value function loss: 1.2703
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: -4.46
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 12.40s
                        Total time: 10471.73s
                               ETA: 1365590.5s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.133s, learning 0.204s)
               Value function loss: 1.4935
                    Surrogate loss: 0.0037
             Mean action noise std: 0.76
                       Mean reward: -4.71
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 12.34s
                        Total time: 10484.07s
                               ETA: 1365391.3s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.157s, learning 0.160s)
               Value function loss: 1.8296
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: -5.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 12.32s
                        Total time: 10496.38s
                               ETA: 1365190.1s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.340s, learning 0.184s)
               Value function loss: 2.3046
                    Surrogate loss: 0.0094
             Mean action noise std: 0.76
                       Mean reward: -5.31
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 12.52s
                        Total time: 10508.91s
                               ETA: 1365016.2s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.622s, learning 0.199s)
               Value function loss: 2.5478
                    Surrogate loss: 0.0011
             Mean action noise std: 0.76
                       Mean reward: -4.21
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 12.82s
                        Total time: 10521.73s
                               ETA: 1364881.2s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1165 steps/s (collection: 13.889s, learning 0.163s)
               Value function loss: 30.6287
                    Surrogate loss: 0.0223
             Mean action noise std: 0.76
                       Mean reward: -6.76
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 4.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 14.05s
                        Total time: 10535.78s
                               ETA: 1364906.0s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.402s, learning 0.165s)
               Value function loss: 0.6617
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: -6.57
               Mean episode length: 125.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 12.57s
                        Total time: 10548.35s
                               ETA: 1364738.6s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.132s, learning 0.193s)
               Value function loss: 0.1955
                    Surrogate loss: -0.0197
             Mean action noise std: 0.76
                       Mean reward: -6.75
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 12.33s
                        Total time: 10560.67s
                               ETA: 1364540.4s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.400s, learning 0.185s)
               Value function loss: 0.3044
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: -6.74
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 12.58s
                        Total time: 10573.26s
                               ETA: 1364376.2s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1235 steps/s (collection: 13.094s, learning 0.164s)
               Value function loss: 0.4394
                    Surrogate loss: 0.0044
             Mean action noise std: 0.76
                       Mean reward: -6.38
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 13.26s
                        Total time: 10586.51s
                               ETA: 1364299.2s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.303s, learning 0.220s)
               Value function loss: 0.8101
                    Surrogate loss: 0.0012
             Mean action noise std: 0.76
                       Mean reward: -6.81
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 12.52s
                        Total time: 10599.04s
                               ETA: 1364127.6s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1263 steps/s (collection: 12.782s, learning 0.181s)
               Value function loss: 0.4429
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: -7.36
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 12.96s
                        Total time: 10612.00s
                               ETA: 1364013.0s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.448s, learning 0.180s)
               Value function loss: 0.5428
                    Surrogate loss: -0.0000
             Mean action noise std: 0.76
                       Mean reward: -8.13
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 12.63s
                        Total time: 10624.63s
                               ETA: 1363855.7s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.771s, learning 0.165s)
               Value function loss: 0.8727
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: -10.26
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 12.94s
                        Total time: 10637.56s
                               ETA: 1363738.3s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.530s, learning 0.204s)
               Value function loss: 1.3748
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: -14.67
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 12.73s
                        Total time: 10650.30s
                               ETA: 1363595.4s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.533s, learning 0.166s)
               Value function loss: 1.1966
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: -16.85
               Mean episode length: 124.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 12.70s
                        Total time: 10663.00s
                               ETA: 1363448.2s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1256 steps/s (collection: 12.866s, learning 0.169s)
               Value function loss: 1.1667
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: -17.05
               Mean episode length: 124.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 13.04s
                        Total time: 10676.03s
                               ETA: 1363344.4s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.376s, learning 0.187s)
               Value function loss: 1.3022
                    Surrogate loss: 0.0029
             Mean action noise std: 0.76
                       Mean reward: -17.25
               Mean episode length: 124.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 12.56s
                        Total time: 10688.59s
                               ETA: 1363180.5s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.435s, learning 0.159s)
               Value function loss: 1.5128
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: -15.14
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 12.59s
                        Total time: 10701.19s
                               ETA: 1363021.0s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.608s, learning 0.204s)
               Value function loss: 1.9499
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: -16.12
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 12.81s
                        Total time: 10714.00s
                               ETA: 1362889.6s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.749s, learning 0.159s)
               Value function loss: 2.0907
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: -14.55
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 12.91s
                        Total time: 10726.91s
                               ETA: 1362770.7s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1198 steps/s (collection: 13.463s, learning 0.205s)
               Value function loss: 28.0719
                    Surrogate loss: 0.0314
             Mean action noise std: 0.76
                       Mean reward: -14.61
               Mean episode length: 125.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 4.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 13.67s
                        Total time: 10740.58s
                               ETA: 1362748.4s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.212s, learning 0.159s)
               Value function loss: 0.2112
                    Surrogate loss: -0.0166
             Mean action noise std: 0.76
                       Mean reward: -14.61
               Mean episode length: 125.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 12.37s
                        Total time: 10752.95s
                               ETA: 1362561.8s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1243 steps/s (collection: 13.019s, learning 0.160s)
               Value function loss: 0.2021
                    Surrogate loss: -0.0018
             Mean action noise std: 0.76
                       Mean reward: -14.42
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 13.18s
                        Total time: 10766.13s
                               ETA: 1362478.0s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.670s, learning 0.184s)
               Value function loss: 0.3944
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: -14.36
               Mean episode length: 125.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 12.85s
                        Total time: 10778.98s
                               ETA: 1362353.2s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.844s, learning 0.160s)
               Value function loss: 0.4211
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: -14.32
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 13.00s
                        Total time: 10791.98s
                               ETA: 1362247.7s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.062s, learning 0.183s)
               Value function loss: 0.5194
                    Surrogate loss: -0.0108
             Mean action noise std: 0.76
                       Mean reward: -14.73
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 12.25s
                        Total time: 10804.23s
                               ETA: 1362046.8s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.619s, learning 0.159s)
               Value function loss: 0.5521
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: -14.69
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 12.78s
                        Total time: 10817.01s
                               ETA: 1361913.4s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1254 steps/s (collection: 12.898s, learning 0.158s)
               Value function loss: 0.6570
                    Surrogate loss: 0.0131
             Mean action noise std: 0.76
                       Mean reward: -16.47
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 13.06s
                        Total time: 10830.06s
                               ETA: 1361815.3s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.604s, learning 0.177s)
               Value function loss: 1.0958
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: -17.46
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 12.78s
                        Total time: 10842.84s
                               ETA: 1361682.8s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.606s, learning 0.224s)
               Value function loss: 1.1999
                    Surrogate loss: 0.0051
             Mean action noise std: 0.76
                       Mean reward: -17.35
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 12.83s
                        Total time: 10855.67s
                               ETA: 1361556.8s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1249 steps/s (collection: 12.944s, learning 0.169s)
               Value function loss: 0.8730
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: -18.54
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 13.11s
                        Total time: 10868.79s
                               ETA: 1361466.6s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.326s, learning 0.161s)
               Value function loss: 1.0007
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: -18.47
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 12.49s
                        Total time: 10881.28s
                               ETA: 1361298.3s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.612s, learning 0.200s)
               Value function loss: 1.2133
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: -17.31
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 12.81s
                        Total time: 10894.09s
                               ETA: 1361171.0s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.499s, learning 0.176s)
               Value function loss: 1.5582
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: -18.41
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 12.68s
                        Total time: 10906.76s
                               ETA: 1361026.9s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.393s, learning 0.167s)
               Value function loss: 1.9476
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: -17.13
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 12.56s
                        Total time: 10919.32s
                               ETA: 1360868.7s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1180 steps/s (collection: 13.721s, learning 0.161s)
               Value function loss: 27.5703
                    Surrogate loss: 0.0262
             Mean action noise std: 0.76
                       Mean reward: -18.12
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 4.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 13.88s
                        Total time: 10933.21s
                               ETA: 1360875.4s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1260 steps/s (collection: 12.792s, learning 0.206s)
               Value function loss: 0.1258
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: -18.09
               Mean episode length: 125.00
                  Mean reward/step: -0.96
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 13.00s
                        Total time: 10946.20s
                               ETA: 1360772.1s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.332s, learning 0.164s)
               Value function loss: 0.1239
                    Surrogate loss: -0.0214
             Mean action noise std: 0.76
                       Mean reward: -18.18
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 12.50s
                        Total time: 10958.70s
                               ETA: 1360606.9s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.975s, learning 0.163s)
               Value function loss: 0.3356
                    Surrogate loss: 0.0154
             Mean action noise std: 0.76
                       Mean reward: -17.97
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 12.14s
                        Total time: 10970.84s
                               ETA: 1360397.7s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.125s, learning 0.169s)
               Value function loss: 0.3274
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: -17.93
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 12.29s
                        Total time: 10983.13s
                               ETA: 1360208.2s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.554s, learning 0.217s)
               Value function loss: 0.3462
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: -17.80
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 12.77s
                        Total time: 10995.90s
                               ETA: 1360078.1s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.798s, learning 0.224s)
               Value function loss: 0.4324
                    Surrogate loss: 0.0121
             Mean action noise std: 0.76
                       Mean reward: -17.61
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 13.02s
                        Total time: 11008.93s
                               ETA: 1359979.3s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.510s, learning 0.162s)
               Value function loss: 0.4341
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: -17.55
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 12.67s
                        Total time: 11021.60s
                               ETA: 1359837.5s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1250 steps/s (collection: 12.893s, learning 0.206s)
               Value function loss: 0.8147
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: -16.23
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 13.10s
                        Total time: 11034.70s
                               ETA: 1359748.7s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.609s, learning 0.190s)
               Value function loss: 1.3417
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: -14.93
               Mean episode length: 124.29
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 12.80s
                        Total time: 11047.50s
                               ETA: 1359623.2s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.406s, learning 0.189s)
               Value function loss: 1.1695
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: -15.63
               Mean episode length: 124.29
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 12.59s
                        Total time: 11060.09s
                               ETA: 1359472.8s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.111s, learning 0.189s)
               Value function loss: 1.0595
                    Surrogate loss: 0.0033
             Mean action noise std: 0.76
                       Mean reward: -15.64
               Mean episode length: 124.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 12.30s
                        Total time: 11072.39s
                               ETA: 1359286.5s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.511s, learning 0.163s)
               Value function loss: 1.2556
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: -14.42
               Mean episode length: 124.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 12.67s
                        Total time: 11085.06s
                               ETA: 1359146.5s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.525s, learning 0.169s)
               Value function loss: 1.6170
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: -13.63
               Mean episode length: 124.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 12.69s
                        Total time: 11097.76s
                               ETA: 1359009.3s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.890s, learning 0.202s)
               Value function loss: 2.0834
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: -13.49
               Mean episode length: 124.52
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 13.09s
                        Total time: 11110.85s
                               ETA: 1358921.2s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.468s, learning 0.171s)
               Value function loss: 2.0901
                    Surrogate loss: 0.0042
             Mean action noise std: 0.76
                       Mean reward: -13.55
               Mean episode length: 124.72
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 12.64s
                        Total time: 11123.49s
                               ETA: 1358777.8s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1201 steps/s (collection: 13.470s, learning 0.166s)
               Value function loss: 29.3971
                    Surrogate loss: 0.0174
             Mean action noise std: 0.76
                       Mean reward: -11.85
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 4.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 13.64s
                        Total time: 11137.12s
                               ETA: 1358756.4s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.727s, learning 0.165s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: -11.68
               Mean episode length: 125.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 12.89s
                        Total time: 11150.02s
                               ETA: 1358644.4s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.337s, learning 0.170s)
               Value function loss: 0.1413
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: -12.05
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 12.51s
                        Total time: 11162.52s
                               ETA: 1358485.9s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.764s, learning 0.254s)
               Value function loss: 0.3347
                    Surrogate loss: 0.0105
             Mean action noise std: 0.76
                       Mean reward: -12.33
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 13.02s
                        Total time: 11175.54s
                               ETA: 1358389.8s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.752s, learning 0.157s)
               Value function loss: 0.3775
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: -12.59
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 12.91s
                        Total time: 11188.45s
                               ETA: 1358280.6s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1246 steps/s (collection: 12.983s, learning 0.157s)
               Value function loss: 0.4676
                    Surrogate loss: 0.0079
             Mean action noise std: 0.76
                       Mean reward: -12.86
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 13.14s
                        Total time: 11201.59s
                               ETA: 1358199.6s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1242 steps/s (collection: 13.028s, learning 0.158s)
               Value function loss: 0.4559
                    Surrogate loss: 0.0119
             Mean action noise std: 0.76
                       Mean reward: -12.89
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 13.19s
                        Total time: 11214.78s
                               ETA: 1358124.3s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.593s, learning 0.159s)
               Value function loss: 0.5430
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: -12.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 12.75s
                        Total time: 11227.53s
                               ETA: 1357996.8s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.177s, learning 0.166s)
               Value function loss: 1.1138
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: -10.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 12.34s
                        Total time: 11239.87s
                               ETA: 1357820.1s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.166s, learning 0.158s)
               Value function loss: 1.6515
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: -10.55
               Mean episode length: 124.52
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 12.32s
                        Total time: 11252.19s
                               ETA: 1357641.5s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.258s, learning 0.158s)
               Value function loss: 1.4368
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: -9.52
               Mean episode length: 124.52
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 12.42s
                        Total time: 11264.61s
                               ETA: 1357474.4s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.181s, learning 0.159s)
               Value function loss: 1.5430
                    Surrogate loss: 0.0013
             Mean action noise std: 0.76
                       Mean reward: -8.68
               Mean episode length: 123.85
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 12.34s
                        Total time: 11276.95s
                               ETA: 1357298.6s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.510s, learning 0.162s)
               Value function loss: 2.2334
                    Surrogate loss: 0.0067
             Mean action noise std: 0.76
                       Mean reward: -7.84
               Mean episode length: 124.33
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 12.67s
                        Total time: 11289.62s
                               ETA: 1357163.0s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.562s, learning 0.292s)
               Value function loss: 1.9503
                    Surrogate loss: 0.0047
             Mean action noise std: 0.76
                       Mean reward: -6.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 12.85s
                        Total time: 11302.47s
                               ETA: 1357049.6s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.263s, learning 0.206s)
               Value function loss: 2.7797
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: -4.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 12.47s
                        Total time: 11314.94s
                               ETA: 1356890.2s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.112s, learning 0.195s)
               Value function loss: 4.4779
                    Surrogate loss: 0.0032
             Mean action noise std: 0.76
                       Mean reward: -3.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 12.31s
                        Total time: 11327.25s
                               ETA: 1356711.8s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1170 steps/s (collection: 13.834s, learning 0.169s)
               Value function loss: 22.1406
                    Surrogate loss: 0.0215
             Mean action noise std: 0.76
                       Mean reward: -3.57
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 14.00s
                        Total time: 11341.25s
                               ETA: 1356736.6s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1297 steps/s (collection: 12.464s, learning 0.165s)
               Value function loss: 0.5892
                    Surrogate loss: -0.0202
             Mean action noise std: 0.76
                       Mean reward: -3.57
               Mean episode length: 125.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 12.63s
                        Total time: 11353.88s
                               ETA: 1356597.2s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.195s, learning 0.156s)
               Value function loss: 0.3853
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: -3.37
               Mean episode length: 125.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 12.35s
                        Total time: 11366.23s
                               ETA: 1356425.1s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.539s, learning 0.163s)
               Value function loss: 0.3171
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: -3.35
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 12.70s
                        Total time: 11378.93s
                               ETA: 1356295.0s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1252 steps/s (collection: 12.918s, learning 0.166s)
               Value function loss: 0.3889
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: -3.17
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 13.08s
                        Total time: 11392.02s
                               ETA: 1356210.8s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1291 steps/s (collection: 12.498s, learning 0.187s)
               Value function loss: 0.5630
                    Surrogate loss: 0.0139
             Mean action noise std: 0.76
                       Mean reward: -3.26
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 12.69s
                        Total time: 11404.70s
                               ETA: 1356079.4s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1231 steps/s (collection: 13.114s, learning 0.186s)
               Value function loss: 0.8123
                    Surrogate loss: 0.0090
             Mean action noise std: 0.76
                       Mean reward: -3.62
               Mean episode length: 124.40
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 13.30s
                        Total time: 11418.00s
                               ETA: 1356021.3s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.460s, learning 0.173s)
               Value function loss: 0.9016
                    Surrogate loss: 0.0120
             Mean action noise std: 0.76
                       Mean reward: -4.46
               Mean episode length: 124.40
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 12.63s
                        Total time: 11430.64s
                               ETA: 1355884.0s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.345s, learning 0.159s)
               Value function loss: 1.5966
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: -3.63
               Mean episode length: 123.84
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 12.50s
                        Total time: 11443.14s
                               ETA: 1355731.8s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.591s, learning 0.188s)
               Value function loss: 1.5049
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: -1.02
               Mean episode length: 124.44
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 12.78s
                        Total time: 11455.92s
                               ETA: 1355612.5s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.794s, learning 0.187s)
               Value function loss: 1.3661
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 1.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 12.98s
                        Total time: 11468.90s
                               ETA: 1355517.4s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1249 steps/s (collection: 12.942s, learning 0.166s)
               Value function loss: 1.6545
                    Surrogate loss: -0.0076
             Mean action noise std: 0.76
                       Mean reward: 1.97
               Mean episode length: 124.64
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 13.11s
                        Total time: 11482.01s
                               ETA: 1355437.5s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.495s, learning 0.263s)
               Value function loss: 2.0351
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 0.35
               Mean episode length: 124.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 12.76s
                        Total time: 11494.77s
                               ETA: 1355316.4s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1273 steps/s (collection: 12.668s, learning 0.199s)
               Value function loss: 2.5261
                    Surrogate loss: 0.0361
             Mean action noise std: 0.76
                       Mean reward: -0.44
               Mean episode length: 124.74
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 12.87s
                        Total time: 11507.63s
                               ETA: 1355208.4s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.771s, learning 0.163s)
               Value function loss: 3.1504
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 0.33
               Mean episode length: 124.89
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 12.93s
                        Total time: 11520.57s
                               ETA: 1355108.5s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1208 steps/s (collection: 13.387s, learning 0.176s)
               Value function loss: 34.7733
                    Surrogate loss: 0.0167
             Mean action noise std: 0.76
                       Mean reward: 3.37
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 4.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 13.56s
                        Total time: 11534.13s
                               ETA: 1355082.7s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1250 steps/s (collection: 12.808s, learning 0.291s)
               Value function loss: 0.1476
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 3.63
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 13.10s
                        Total time: 11547.23s
                               ETA: 1355002.4s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1248 steps/s (collection: 12.967s, learning 0.161s)
               Value function loss: 0.1671
                    Surrogate loss: -0.0121
             Mean action noise std: 0.76
                       Mean reward: 3.55
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 13.13s
                        Total time: 11560.36s
                               ETA: 1354925.7s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.300s, learning 0.269s)
               Value function loss: 0.2842
                    Surrogate loss: 0.0140
             Mean action noise std: 0.76
                       Mean reward: 3.32
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 12.57s
                        Total time: 11572.93s
                               ETA: 1354783.8s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1274 steps/s (collection: 12.690s, learning 0.168s)
               Value function loss: 0.3445
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 3.52
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 12.86s
                        Total time: 11585.78s
                               ETA: 1354676.0s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1265 steps/s (collection: 12.680s, learning 0.265s)
               Value function loss: 0.3644
                    Surrogate loss: 0.0022
             Mean action noise std: 0.76
                       Mean reward: 3.37
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 12.95s
                        Total time: 11598.73s
                               ETA: 1354578.6s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.347s, learning 0.165s)
               Value function loss: 0.3637
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 2.01
               Mean episode length: 124.76
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 12.51s
                        Total time: 11611.24s
                               ETA: 1354430.8s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.432s, learning 0.170s)
               Value function loss: 0.5348
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 1.91
               Mean episode length: 124.76
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 12.60s
                        Total time: 11623.84s
                               ETA: 1354293.8s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1262 steps/s (collection: 12.811s, learning 0.164s)
               Value function loss: 0.8920
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 0.95
               Mean episode length: 124.76
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 12.97s
                        Total time: 11636.82s
                               ETA: 1354200.5s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.372s, learning 0.170s)
               Value function loss: 1.7290
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: -2.18
               Mean episode length: 124.50
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 12.54s
                        Total time: 11649.36s
                               ETA: 1354057.0s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.232s, learning 0.159s)
               Value function loss: 1.3704
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: -1.65
               Mean episode length: 124.50
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 12.39s
                        Total time: 11661.75s
                               ETA: 1353896.4s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.345s, learning 0.179s)
               Value function loss: 1.5465
                    Surrogate loss: -0.0018
             Mean action noise std: 0.76
                       Mean reward: 0.39
               Mean episode length: 124.50
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 12.52s
                        Total time: 11674.27s
                               ETA: 1353751.5s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.434s, learning 0.205s)
               Value function loss: 1.6557
                    Surrogate loss: 0.0022
             Mean action noise std: 0.76
                       Mean reward: 2.53
               Mean episode length: 124.74
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 12.64s
                        Total time: 11686.91s
                               ETA: 1353620.3s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.742s, learning 0.162s)
               Value function loss: 1.8194
                    Surrogate loss: 0.0064
             Mean action noise std: 0.76
                       Mean reward: -0.52
               Mean episode length: 124.74
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 12.90s
                        Total time: 11699.82s
                               ETA: 1353520.1s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1261 steps/s (collection: 12.779s, learning 0.206s)
               Value function loss: 2.4552
                    Surrogate loss: 0.0143
             Mean action noise std: 0.76
                       Mean reward: -1.10
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 12.99s
                        Total time: 11712.80s
                               ETA: 1353429.4s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.513s, learning 0.254s)
               Value function loss: 2.3719
                    Surrogate loss: 0.0067
             Mean action noise std: 0.76
                       Mean reward: -1.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 12.77s
                        Total time: 11725.57s
                               ETA: 1353313.6s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1243 steps/s (collection: 13.016s, learning 0.165s)
               Value function loss: 25.5708
                    Surrogate loss: 0.0167
             Mean action noise std: 0.76
                       Mean reward: 0.80
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 4.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 13.18s
                        Total time: 11738.75s
                               ETA: 1353245.9s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1254 steps/s (collection: 12.896s, learning 0.160s)
               Value function loss: 0.1922
                    Surrogate loss: -0.0141
             Mean action noise std: 0.76
                       Mean reward: 0.79
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 13.06s
                        Total time: 11751.81s
                               ETA: 1353163.8s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.358s, learning 0.206s)
               Value function loss: 0.2356
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 1.08
               Mean episode length: 125.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 12.56s
                        Total time: 11764.37s
                               ETA: 1353025.4s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.343s, learning 0.228s)
               Value function loss: 0.5275
                    Surrogate loss: -0.0002
             Mean action noise std: 0.76
                       Mean reward: 1.81
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 12.57s
                        Total time: 11776.94s
                               ETA: 1352888.1s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1248 steps/s (collection: 12.929s, learning 0.192s)
               Value function loss: 0.5898
                    Surrogate loss: 0.0093
             Mean action noise std: 0.76
                       Mean reward: 1.56
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 13.12s
                        Total time: 11790.06s
                               ETA: 1352814.2s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1286 steps/s (collection: 12.545s, learning 0.187s)
               Value function loss: 0.4020
                    Surrogate loss: -0.0129
             Mean action noise std: 0.76
                       Mean reward: 1.19
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 12.73s
                        Total time: 11802.80s
                               ETA: 1352695.9s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.150s, learning 0.169s)
               Value function loss: 0.4691
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 1.60
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 12.32s
                        Total time: 11815.11s
                               ETA: 1352530.4s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.154s, learning 0.162s)
               Value function loss: 0.6856
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 3.07
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 12.32s
                        Total time: 11827.43s
                               ETA: 1352365.0s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1275 steps/s (collection: 12.681s, learning 0.164s)
               Value function loss: 1.4291
                    Surrogate loss: 0.0014
             Mean action noise std: 0.76
                       Mean reward: 5.20
               Mean episode length: 124.43
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 12.84s
                        Total time: 11840.27s
                               ETA: 1352260.3s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.534s, learning 0.159s)
               Value function loss: 1.6035
                    Surrogate loss: 0.0055
             Mean action noise std: 0.76
                       Mean reward: 5.57
               Mean episode length: 124.43
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 12.69s
                        Total time: 11852.97s
                               ETA: 1352138.5s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.418s, learning 0.157s)
               Value function loss: 1.3052
                    Surrogate loss: 0.0044
             Mean action noise std: 0.76
                       Mean reward: 7.25
               Mean episode length: 124.60
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 12.57s
                        Total time: 11865.54s
                               ETA: 1352003.6s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.163s, learning 0.161s)
               Value function loss: 1.6385
                    Surrogate loss: 0.0173
             Mean action noise std: 0.76
                       Mean reward: 5.09
               Mean episode length: 124.27
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 12.32s
                        Total time: 11877.87s
                               ETA: 1351840.3s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.049s, learning 0.167s)
               Value function loss: 1.8585
                    Surrogate loss: -0.0009
             Mean action noise std: 0.76
                       Mean reward: 1.57
               Mean episode length: 124.03
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 12.22s
                        Total time: 11890.08s
                               ETA: 1351665.1s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.476s, learning 0.186s)
               Value function loss: 2.3924
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 3.41
               Mean episode length: 124.76
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 12.66s
                        Total time: 11902.74s
                               ETA: 1351540.9s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.533s, learning 0.166s)
               Value function loss: 3.3648
                    Surrogate loss: 0.0422
             Mean action noise std: 0.76
                       Mean reward: 3.11
               Mean episode length: 124.91
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 12.70s
                        Total time: 11915.44s
                               ETA: 1351421.2s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1174 steps/s (collection: 13.789s, learning 0.165s)
               Value function loss: 31.0420
                    Surrogate loss: 0.0229
             Mean action noise std: 0.76
                       Mean reward: 1.65
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 13.95s
                        Total time: 11929.40s
                               ETA: 1351444.0s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.851s, learning 0.165s)
               Value function loss: 0.1990
                    Surrogate loss: -0.0178
             Mean action noise std: 0.76
                       Mean reward: 1.58
               Mean episode length: 125.00
                  Mean reward/step: -0.94
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 13.02s
                        Total time: 11942.41s
                               ETA: 1351360.4s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.322s, learning 0.165s)
               Value function loss: 0.1625
                    Surrogate loss: -0.0155
             Mean action noise std: 0.76
                       Mean reward: 2.03
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 12.49s
                        Total time: 11954.90s
                               ETA: 1351217.2s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.591s, learning 0.214s)
               Value function loss: 0.2801
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 2.02
               Mean episode length: 125.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 12.80s
                        Total time: 11967.70s
                               ETA: 1351110.2s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.662s, learning 0.160s)
               Value function loss: 0.5608
                    Surrogate loss: 0.0043
             Mean action noise std: 0.76
                       Mean reward: 2.26
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 12.82s
                        Total time: 11980.53s
                               ETA: 1351005.4s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1257 steps/s (collection: 12.861s, learning 0.165s)
               Value function loss: 0.5861
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 2.47
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 13.03s
                        Total time: 11993.55s
                               ETA: 1350923.8s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.803s, learning 0.206s)
               Value function loss: 0.5121
                    Surrogate loss: 0.0028
             Mean action noise std: 0.76
                       Mean reward: 2.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 13.01s
                        Total time: 12006.56s
                               ETA: 1350840.4s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1266 steps/s (collection: 12.777s, learning 0.161s)
               Value function loss: 0.6820
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 2.70
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 12.94s
                        Total time: 12019.50s
                               ETA: 1350749.2s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.162s, learning 0.160s)
               Value function loss: 1.1248
                    Surrogate loss: 0.0067
             Mean action noise std: 0.76
                       Mean reward: 3.22
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 12.32s
                        Total time: 12031.82s
                               ETA: 1350589.0s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.359s, learning 0.163s)
               Value function loss: 1.9954
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 2.02
               Mean episode length: 123.71
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 12.52s
                        Total time: 12044.34s
                               ETA: 1350451.7s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.177s, learning 0.162s)
               Value function loss: 1.9654
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 1.23
               Mean episode length: 123.07
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 12.34s
                        Total time: 12056.68s
                               ETA: 1350294.0s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.249s, learning 0.162s)
               Value function loss: 1.8322
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 2.59
               Mean episode length: 123.77
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 12.41s
                        Total time: 12069.09s
                               ETA: 1350144.7s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.442s, learning 0.156s)
               Value function loss: 5.5344
                    Surrogate loss: -0.0000
             Mean action noise std: 0.76
                       Mean reward: 2.71
               Mean episode length: 124.55
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 12.60s
                        Total time: 12081.69s
                               ETA: 1350016.7s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1257 steps/s (collection: 12.765s, learning 0.267s)
               Value function loss: 2.3236
                    Surrogate loss: 0.0096
             Mean action noise std: 0.76
                       Mean reward: 3.49
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 13.03s
                        Total time: 12094.72s
                               ETA: 1349937.3s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1251 steps/s (collection: 12.906s, learning 0.185s)
               Value function loss: 2.8817
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: -0.46
               Mean episode length: 124.82
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 13.09s
                        Total time: 12107.81s
                               ETA: 1349864.6s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.623s, learning 0.170s)
               Value function loss: 2.7327
                    Surrogate loss: 0.0040
             Mean action noise std: 0.76
                       Mean reward: 0.98
               Mean episode length: 124.94
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 12.79s
                        Total time: 12120.61s
                               ETA: 1349759.0s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1188 steps/s (collection: 13.616s, learning 0.164s)
               Value function loss: 33.6624
                    Surrogate loss: 0.0259
             Mean action noise std: 0.76
                       Mean reward: 2.43
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 4.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 13.78s
                        Total time: 12134.39s
                               ETA: 1349763.3s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.482s, learning 0.163s)
               Value function loss: 0.5844
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 125.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 12.65s
                        Total time: 12147.03s
                               ETA: 1349641.5s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.290s, learning 0.205s)
               Value function loss: 0.2773
                    Surrogate loss: -0.0007
             Mean action noise std: 0.76
                       Mean reward: 2.27
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 12.49s
                        Total time: 12159.53s
                               ETA: 1349503.2s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.591s, learning 0.180s)
               Value function loss: 0.3317
                    Surrogate loss: 0.0036
             Mean action noise std: 0.75
                       Mean reward: 2.51
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 12.77s
                        Total time: 12172.30s
                               ETA: 1349395.8s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.100s, learning 0.160s)
               Value function loss: 0.6913
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: 2.47
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 12.26s
                        Total time: 12184.56s
                               ETA: 1349232.1s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1288 steps/s (collection: 12.553s, learning 0.160s)
               Value function loss: 0.8496
                    Surrogate loss: 0.0065
             Mean action noise std: 0.75
                       Mean reward: 2.11
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 12.71s
                        Total time: 12197.27s
                               ETA: 1349118.7s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1252 steps/s (collection: 12.918s, learning 0.158s)
               Value function loss: 0.6221
                    Surrogate loss: 0.0030
             Mean action noise std: 0.75
                       Mean reward: 1.91
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 13.08s
                        Total time: 12210.35s
                               ETA: 1349045.9s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.621s, learning 0.159s)
               Value function loss: 0.7336
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 12.78s
                        Total time: 12223.13s
                               ETA: 1348940.5s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.640s, learning 0.164s)
               Value function loss: 1.4216
                    Surrogate loss: 0.0056
             Mean action noise std: 0.75
                       Mean reward: 2.24
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 12.80s
                        Total time: 12235.93s
                               ETA: 1348837.8s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1261 steps/s (collection: 12.751s, learning 0.237s)
               Value function loss: 2.0814
                    Surrogate loss: 0.0134
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 124.47
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 12.99s
                        Total time: 12248.92s
                               ETA: 1348755.6s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1267 steps/s (collection: 12.760s, learning 0.167s)
               Value function loss: 1.5036
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 124.47
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 12.93s
                        Total time: 12261.85s
                               ETA: 1348666.9s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1309 steps/s (collection: 12.352s, learning 0.158s)
               Value function loss: 1.8658
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: 2.93
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 12.51s
                        Total time: 12274.35s
                               ETA: 1348532.5s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.858s, learning 0.160s)
               Value function loss: 2.0819
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 3.47
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 13.02s
                        Total time: 12287.37s
                               ETA: 1348454.1s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1259 steps/s (collection: 12.849s, learning 0.162s)
               Value function loss: 2.6302
                    Surrogate loss: 0.0094
             Mean action noise std: 0.75
                       Mean reward: 4.47
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 13.01s
                        Total time: 12300.38s
                               ETA: 1348375.1s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.439s, learning 0.174s)
               Value function loss: 3.4639
                    Surrogate loss: 0.0025
             Mean action noise std: 0.75
                       Mean reward: 2.62
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 12.61s
                        Total time: 12313.00s
                               ETA: 1348252.8s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.534s, learning 0.225s)
               Value function loss: 4.0081
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 4.12
               Mean episode length: 124.93
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 12.76s
                        Total time: 12325.76s
                               ETA: 1348146.5s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1141 steps/s (collection: 14.188s, learning 0.164s)
               Value function loss: 28.4741
                    Surrogate loss: 0.0374
             Mean action noise std: 0.75
                       Mean reward: 5.63
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 4.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 14.35s
                        Total time: 12340.11s
                               ETA: 1348214.6s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.400s, learning 0.165s)
               Value function loss: 0.6594
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 5.63
               Mean episode length: 125.00
                  Mean reward/step: -0.79
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 12.56s
                        Total time: 12352.67s
                               ETA: 1348087.4s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1312 steps/s (collection: 12.324s, learning 0.157s)
               Value function loss: 0.3578
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 5.55
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 12.48s
                        Total time: 12365.15s
                               ETA: 1347951.3s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.632s, learning 0.161s)
               Value function loss: 0.5950
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 5.49
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 12.79s
                        Total time: 12377.95s
                               ETA: 1347849.5s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1267 steps/s (collection: 12.766s, learning 0.159s)
               Value function loss: 0.8221
                    Surrogate loss: 0.0092
             Mean action noise std: 0.75
                       Mean reward: 5.51
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 12.93s
                        Total time: 12390.87s
                               ETA: 1347762.3s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.407s, learning 0.184s)
               Value function loss: 0.8091
                    Surrogate loss: 0.0202
             Mean action noise std: 0.75
                       Mean reward: 5.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 12.59s
                        Total time: 12403.46s
                               ETA: 1347638.9s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.512s, learning 0.163s)
               Value function loss: 0.7582
                    Surrogate loss: 0.0074
             Mean action noise std: 0.75
                       Mean reward: 5.49
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 12.67s
                        Total time: 12416.14s
                               ETA: 1347524.8s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.885s, learning 0.169s)
               Value function loss: 1.0695
                    Surrogate loss: 0.0098
             Mean action noise std: 0.75
                       Mean reward: 5.57
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 12.05s
                        Total time: 12428.19s
                               ETA: 1347343.6s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.162s, learning 0.170s)
               Value function loss: 1.9189
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 124.02
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 12.33s
                        Total time: 12440.52s
                               ETA: 1347193.0s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1287 steps/s (collection: 12.523s, learning 0.204s)
               Value function loss: 1.9618
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 0.42
               Mean episode length: 124.29
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 12.73s
                        Total time: 12453.25s
                               ETA: 1347085.4s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1300 steps/s (collection: 12.437s, learning 0.157s)
               Value function loss: 1.7234
                    Surrogate loss: 0.0007
             Mean action noise std: 0.75
                       Mean reward: 0.50
               Mean episode length: 124.58
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 12.59s
                        Total time: 12465.84s
                               ETA: 1346963.6s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.147s, learning 0.204s)
               Value function loss: 2.5840
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 1.59
               Mean episode length: 123.90
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 12.35s
                        Total time: 12478.19s
                               ETA: 1346815.8s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1254 steps/s (collection: 12.860s, learning 0.204s)
               Value function loss: 3.1061
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 4.18
               Mean episode length: 124.32
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 13.06s
                        Total time: 12491.26s
                               ETA: 1346745.1s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1255 steps/s (collection: 12.863s, learning 0.187s)
               Value function loss: 4.4856
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 3.87
               Mean episode length: 124.72
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 13.05s
                        Total time: 12504.31s
                               ETA: 1346673.1s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.239s, learning 0.190s)
               Value function loss: 3.4625
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 0.98
               Mean episode length: 124.87
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 12.43s
                        Total time: 12516.74s
                               ETA: 1346534.5s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1213 steps/s (collection: 13.304s, learning 0.203s)
               Value function loss: 39.4933
                    Surrogate loss: 0.0749
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 4.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 13.51s
                        Total time: 12530.24s
                               ETA: 1346511.9s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.640s, learning 0.163s)
               Value function loss: 0.3515
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 125.00
                  Mean reward/step: -0.93
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 12.80s
                        Total time: 12543.05s
                               ETA: 1346413.8s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.292s, learning 0.266s)
               Value function loss: 0.3241
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 3.41
               Mean episode length: 125.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 12.56s
                        Total time: 12555.60s
                               ETA: 1346289.7s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.275s, learning 0.168s)
               Value function loss: 0.4437
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 125.00
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 12.44s
                        Total time: 12568.05s
                               ETA: 1346153.5s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.374s, learning 0.157s)
               Value function loss: 0.6419
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 12.53s
                        Total time: 12580.58s
                               ETA: 1346026.9s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1244 steps/s (collection: 13.003s, learning 0.162s)
               Value function loss: 1.5018
                    Surrogate loss: 0.0169
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 13.16s
                        Total time: 12593.74s
                               ETA: 1345968.3s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.371s, learning 0.165s)
               Value function loss: 0.9483
                    Surrogate loss: 0.0030
             Mean action noise std: 0.75
                       Mean reward: 2.89
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 12.54s
                        Total time: 12606.28s
                               ETA: 1345842.7s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1258 steps/s (collection: 12.793s, learning 0.230s)
               Value function loss: 0.8812
                    Surrogate loss: 0.0202
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 13.02s
                        Total time: 12619.30s
                               ETA: 1345769.3s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1269 steps/s (collection: 12.700s, learning 0.208s)
               Value function loss: 1.5247
                    Surrogate loss: 0.0041
             Mean action noise std: 0.75
                       Mean reward: 4.24
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 12.91s
                        Total time: 12632.21s
                               ETA: 1345683.7s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.671s, learning 0.200s)
               Value function loss: 2.4241
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 4.60
               Mean episode length: 123.89
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 12.87s
                        Total time: 12645.08s
                               ETA: 1345594.3s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.655s, learning 0.166s)
               Value function loss: 1.7601
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.08
               Mean episode length: 123.53
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 12.82s
                        Total time: 12657.90s
                               ETA: 1345499.8s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.111s, learning 0.163s)
               Value function loss: 1.6246
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 1.84
               Mean episode length: 124.06
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 12.27s
                        Total time: 12670.18s
                               ETA: 1345347.5s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.595s, learning 0.170s)
               Value function loss: 2.0457
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 2.43
               Mean episode length: 124.66
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 12.76s
                        Total time: 12682.94s
                               ETA: 1345247.3s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.547s, learning 0.279s)
               Value function loss: 2.3867
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 5.04
               Mean episode length: 124.66
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 12.83s
                        Total time: 12695.77s
                               ETA: 1345153.9s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1260 steps/s (collection: 12.812s, learning 0.191s)
               Value function loss: 3.2928
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 5.09
               Mean episode length: 124.71
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 13.00s
                        Total time: 12708.77s
                               ETA: 1345079.5s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1244 steps/s (collection: 12.882s, learning 0.284s)
               Value function loss: 4.3112
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 3.85
               Mean episode length: 124.87
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 13.17s
                        Total time: 12721.94s
                               ETA: 1345022.4s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 724 steps/s (collection: 22.412s, learning 0.202s)
               Value function loss: 31.9171
                    Surrogate loss: 0.0704
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 22.61s
                        Total time: 12744.55s
                               ETA: 1345963.1s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.520s, learning 0.186s)
               Value function loss: 0.3965
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 125.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 23.71s
                        Total time: 12768.26s
                               ETA: 1347017.1s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 686 steps/s (collection: 23.702s, learning 0.156s)
               Value function loss: 0.2760
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 3.63
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 23.86s
                        Total time: 12792.12s
                               ETA: 1348084.9s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.514s, learning 0.229s)
               Value function loss: 0.4896
                    Surrogate loss: 0.0045
             Mean action noise std: 0.75
                       Mean reward: 3.91
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 23.74s
                        Total time: 12815.86s
                               ETA: 1349138.1s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.905s, learning 0.181s)
               Value function loss: 0.5064
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 3.89
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 24.09s
                        Total time: 12839.94s
                               ETA: 1350225.1s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.345s, learning 0.167s)
               Value function loss: 0.5587
                    Surrogate loss: 0.0012
             Mean action noise std: 0.75
                       Mean reward: 3.44
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 24.51s
                        Total time: 12864.46s
                               ETA: 1351354.6s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 660 steps/s (collection: 24.581s, learning 0.215s)
               Value function loss: 0.4616
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 2.81
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 24.80s
                        Total time: 12889.25s
                               ETA: 1352511.4s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 699 steps/s (collection: 23.201s, learning 0.207s)
               Value function loss: 0.5567
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 1.50
               Mean episode length: 124.31
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 23.41s
                        Total time: 12912.66s
                               ETA: 1353520.1s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.521s, learning 0.192s)
               Value function loss: 1.1834
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: -2.46
               Mean episode length: 124.31
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 23.71s
                        Total time: 12936.37s
                               ETA: 1354558.7s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 672 steps/s (collection: 24.133s, learning 0.213s)
               Value function loss: 1.6963
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: -6.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 24.35s
                        Total time: 12960.72s
                               ETA: 1355661.1s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.841s, learning 0.213s)
               Value function loss: 1.4958
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: -6.61
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 24.05s
                        Total time: 12984.77s
                               ETA: 1356730.8s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 678 steps/s (collection: 23.914s, learning 0.226s)
               Value function loss: 1.3195
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: -5.91
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 24.14s
                        Total time: 13008.91s
                               ETA: 1357807.1s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.498s, learning 0.232s)
               Value function loss: 2.1082
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: -7.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 23.73s
                        Total time: 13032.64s
                               ETA: 1358838.3s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 702 steps/s (collection: 23.130s, learning 0.184s)
               Value function loss: 1.9637
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: -5.50
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 23.31s
                        Total time: 13055.96s
                               ETA: 1359824.0s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 669 steps/s (collection: 24.218s, learning 0.245s)
               Value function loss: 2.5929
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: -4.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 24.46s
                        Total time: 13080.42s
                               ETA: 1360927.0s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 672 steps/s (collection: 24.189s, learning 0.173s)
               Value function loss: 5.0491
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: -4.75
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 24.36s
                        Total time: 13104.78s
                               ETA: 1362017.3s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 663 steps/s (collection: 24.493s, learning 0.198s)
               Value function loss: 21.7531
                    Surrogate loss: 0.0183
             Mean action noise std: 0.75
                       Mean reward: -4.83
               Mean episode length: 125.00
                  Mean reward/step: -0.92
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 24.69s
                        Total time: 13129.47s
                               ETA: 1363139.4s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 684 steps/s (collection: 23.733s, learning 0.209s)
               Value function loss: 0.1454
                    Surrogate loss: -0.0258
             Mean action noise std: 0.75
                       Mean reward: -4.83
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 23.94s
                        Total time: 13153.42s
                               ETA: 1364181.4s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.892s, learning 0.227s)
               Value function loss: 0.2208
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: -4.54
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 24.12s
                        Total time: 13177.54s
                               ETA: 1365239.5s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 670 steps/s (collection: 24.195s, learning 0.223s)
               Value function loss: 0.4739
                    Surrogate loss: 0.0025
             Mean action noise std: 0.75
                       Mean reward: -4.50
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 24.42s
                        Total time: 13201.95s
                               ETA: 1366326.3s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 687 steps/s (collection: 23.672s, learning 0.162s)
               Value function loss: 0.6238
                    Surrogate loss: 0.0110
             Mean action noise std: 0.75
                       Mean reward: -4.31
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 23.83s
                        Total time: 13225.79s
                               ETA: 1367350.4s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 665 steps/s (collection: 24.416s, learning 0.192s)
               Value function loss: 0.3974
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: -4.28
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 24.61s
                        Total time: 13250.40s
                               ETA: 1368452.3s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 662 steps/s (collection: 24.561s, learning 0.166s)
               Value function loss: 0.5248
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: -3.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 24.73s
                        Total time: 13275.12s
                               ETA: 1369564.0s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.532s, learning 0.171s)
               Value function loss: 0.8778
                    Surrogate loss: 0.0143
             Mean action noise std: 0.75
                       Mean reward: -1.85
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 23.70s
                        Total time: 13298.83s
                               ETA: 1370567.9s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 689 steps/s (collection: 23.599s, learning 0.159s)
               Value function loss: 1.7819
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: 2.27
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 23.76s
                        Total time: 13322.58s
                               ETA: 1371575.2s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.218s, learning 0.262s)
               Value function loss: 1.8065
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 1.10
               Mean episode length: 124.71
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 23.48s
                        Total time: 13346.06s
                               ETA: 1372551.8s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 677 steps/s (collection: 24.028s, learning 0.166s)
               Value function loss: 2.1160
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 1.56
               Mean episode length: 124.71
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 24.19s
                        Total time: 13370.26s
                               ETA: 1373599.8s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 665 steps/s (collection: 24.227s, learning 0.396s)
               Value function loss: 2.6626
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 0.00
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 24.62s
                        Total time: 13394.88s
                               ETA: 1374689.6s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 669 steps/s (collection: 24.321s, learning 0.167s)
               Value function loss: 2.4498
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 0.85
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 24.49s
                        Total time: 13419.37s
                               ETA: 1375763.2s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 661 steps/s (collection: 24.550s, learning 0.203s)
               Value function loss: 2.6294
                    Surrogate loss: 0.0011
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 124.85
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 24.75s
                        Total time: 13444.12s
                               ETA: 1376861.6s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.536s, learning 0.158s)
               Value function loss: 3.2609
                    Surrogate loss: 0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 124.85
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 23.69s
                        Total time: 13467.82s
                               ETA: 1377849.5s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 650 steps/s (collection: 25.018s, learning 0.159s)
               Value function loss: 40.9375
                    Surrogate loss: 0.0375
             Mean action noise std: 0.75
                       Mean reward: 2.99
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 4.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 25.18s
                        Total time: 13492.99s
                               ETA: 1378986.7s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.524s, learning 0.210s)
               Value function loss: 0.6693
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 2.89
               Mean episode length: 125.00
                  Mean reward/step: -0.88
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 23.73s
                        Total time: 13516.73s
                               ETA: 1379974.3s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.696s, learning 0.208s)
               Value function loss: 0.4456
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 3.08
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 23.90s
                        Total time: 13540.63s
                               ETA: 1380977.0s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.344s, learning 0.167s)
               Value function loss: 0.5757
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 2.80
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 24.51s
                        Total time: 13565.14s
                               ETA: 1382039.5s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.490s, learning 0.205s)
               Value function loss: 0.6266
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 23.69s
                        Total time: 13588.84s
                               ETA: 1383016.7s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 664 steps/s (collection: 24.491s, learning 0.166s)
               Value function loss: 0.9325
                    Surrogate loss: 0.0041
             Mean action noise std: 0.75
                       Mean reward: 2.46
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 24.66s
                        Total time: 13613.49s
                               ETA: 1384089.7s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 715 steps/s (collection: 22.723s, learning 0.178s)
               Value function loss: 0.7124
                    Surrogate loss: 0.0177
             Mean action noise std: 0.75
                       Mean reward: 2.16
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 22.90s
                        Total time: 13636.39s
                               ETA: 1384982.2s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1292 steps/s (collection: 12.477s, learning 0.199s)
               Value function loss: 0.8594
                    Surrogate loss: 0.0118
             Mean action noise std: 0.75
                       Mean reward: 2.11
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 12.68s
                        Total time: 13649.07s
                               ETA: 1384835.2s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1242 steps/s (collection: 13.030s, learning 0.160s)
               Value function loss: 1.1998
                    Surrogate loss: 0.0185
             Mean action noise std: 0.75
                       Mean reward: 3.44
               Mean episode length: 124.35
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 13.19s
                        Total time: 13662.26s
                               ETA: 1384740.7s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.302s, learning 0.157s)
               Value function loss: 1.8522
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 4.98
               Mean episode length: 124.50
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 12.46s
                        Total time: 13674.72s
                               ETA: 1384572.3s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.243s, learning 0.164s)
               Value function loss: 1.7102
                    Surrogate loss: 0.0030
             Mean action noise std: 0.75
                       Mean reward: 5.37
               Mean episode length: 124.01
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 12.41s
                        Total time: 13687.13s
                               ETA: 1384399.0s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.378s, learning 0.170s)
               Value function loss: 1.7068
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 6.59
               Mean episode length: 124.01
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 12.55s
                        Total time: 13699.67s
                               ETA: 1384240.3s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1293 steps/s (collection: 12.508s, learning 0.160s)
               Value function loss: 2.3754
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 6.86
               Mean episode length: 124.79
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 12.67s
                        Total time: 13712.34s
                               ETA: 1384094.0s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1260 steps/s (collection: 12.739s, learning 0.264s)
               Value function loss: 2.4346
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 5.53
               Mean episode length: 124.79
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 13.00s
                        Total time: 13725.35s
                               ETA: 1383981.6s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1270 steps/s (collection: 12.633s, learning 0.265s)
               Value function loss: 4.2114
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 6.64
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 12.90s
                        Total time: 13738.24s
                               ETA: 1383859.0s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1280 steps/s (collection: 12.608s, learning 0.187s)
               Value function loss: 3.7368
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 5.12
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 12.80s
                        Total time: 13751.04s
                               ETA: 1383726.2s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1189 steps/s (collection: 13.609s, learning 0.169s)
               Value function loss: 34.1421
                    Surrogate loss: 0.0216
             Mean action noise std: 0.75
                       Mean reward: 7.02
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 4.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 13.78s
                        Total time: 13764.82s
                               ETA: 1383692.4s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.838s, learning 0.204s)
               Value function loss: 0.1945
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 6.90
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 12.04s
                        Total time: 13776.86s
                               ETA: 1383484.4s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.150s, learning 0.169s)
               Value function loss: 0.2444
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 6.94
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 12.32s
                        Total time: 13789.18s
                               ETA: 1383304.6s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1272 steps/s (collection: 12.718s, learning 0.161s)
               Value function loss: 0.6641
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: 7.20
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 12.88s
                        Total time: 13802.06s
                               ETA: 1383181.3s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1252 steps/s (collection: 12.923s, learning 0.162s)
               Value function loss: 0.7427
                    Surrogate loss: 0.0028
             Mean action noise std: 0.75
                       Mean reward: 7.04
               Mean episode length: 125.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 13.08s
                        Total time: 13815.14s
                               ETA: 1383078.7s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.270s, learning 0.167s)
               Value function loss: 0.5605
                    Surrogate loss: 0.0059
             Mean action noise std: 0.75
                       Mean reward: 6.76
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 12.44s
                        Total time: 13827.58s
                               ETA: 1382911.5s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1263 steps/s (collection: 12.806s, learning 0.162s)
               Value function loss: 0.5791
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 6.65
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 12.97s
                        Total time: 13840.55s
                               ETA: 1382797.7s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.361s, learning 0.207s)
               Value function loss: 1.0579
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 7.02
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 12.57s
                        Total time: 13853.12s
                               ETA: 1382644.3s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1279 steps/s (collection: 12.640s, learning 0.164s)
               Value function loss: 2.0019
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 8.29
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 12.80s
                        Total time: 13865.92s
                               ETA: 1382514.5s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.363s, learning 0.158s)
               Value function loss: 2.1073
                    Surrogate loss: -0.0001
             Mean action noise std: 0.75
                       Mean reward: 9.62
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 12.52s
                        Total time: 13878.44s
                               ETA: 1382356.8s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.252s, learning 0.162s)
               Value function loss: 1.6541
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 9.36
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 12.41s
                        Total time: 13890.85s
                               ETA: 1382188.7s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1244 steps/s (collection: 12.977s, learning 0.188s)
               Value function loss: 2.5087
                    Surrogate loss: 0.0102
             Mean action noise std: 0.75
                       Mean reward: 9.09
               Mean episode length: 124.63
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 13.17s
                        Total time: 13904.02s
                               ETA: 1382095.7s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.196s, learning 0.161s)
               Value function loss: 2.0820
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 8.86
               Mean episode length: 124.63
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 12.36s
                        Total time: 13916.38s
                               ETA: 1381922.6s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.333s, learning 0.187s)
               Value function loss: 3.4028
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: 7.60
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 12.52s
                        Total time: 13928.89s
                               ETA: 1381765.8s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.931s, learning 0.169s)
               Value function loss: 4.4079
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 8.08
               Mean episode length: 124.88
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 12.10s
                        Total time: 13940.99s
                               ETA: 1381567.9s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1210 steps/s (collection: 13.323s, learning 0.216s)
               Value function loss: 157.2345
                    Surrogate loss: 0.0196
             Mean action noise std: 0.75
                       Mean reward: 7.08
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 13.54s
                        Total time: 13954.53s
                               ETA: 1381512.7s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.735s, learning 0.158s)
               Value function loss: 0.5302
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 7.08
               Mean episode length: 125.00
                  Mean reward/step: -0.90
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 12.89s
                        Total time: 13967.43s
                               ETA: 1381393.7s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.091s, learning 0.164s)
               Value function loss: 0.3402
                    Surrogate loss: 0.0045
             Mean action noise std: 0.75
                       Mean reward: 7.20
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 12.26s
                        Total time: 13979.68s
                               ETA: 1381212.0s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.262s, learning 0.165s)
               Value function loss: 0.3443
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 7.57
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 12.43s
                        Total time: 13992.11s
                               ETA: 1381047.6s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.285s, learning 0.163s)
               Value function loss: 0.5525
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 7.56
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 12.45s
                        Total time: 14004.56s
                               ETA: 1380885.5s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.702s, learning 0.160s)
               Value function loss: 0.8699
                    Surrogate loss: 0.0325
             Mean action noise std: 0.75
                       Mean reward: 6.78
               Mean episode length: 124.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 12.86s
                        Total time: 14017.42s
                               ETA: 1380764.5s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.098s, learning 0.214s)
               Value function loss: 0.9220
                    Surrogate loss: 0.0025
             Mean action noise std: 0.75
                       Mean reward: 6.77
               Mean episode length: 124.59
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 12.31s
                        Total time: 14029.73s
                               ETA: 1380589.6s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.923s, learning 0.168s)
               Value function loss: 1.0733
                    Surrogate loss: 0.0052
             Mean action noise std: 0.75
                       Mean reward: 7.10
               Mean episode length: 124.59
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 13.09s
                        Total time: 14042.82s
                               ETA: 1380491.7s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.667s, learning 0.203s)
               Value function loss: 1.4734
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 8.72
               Mean episode length: 124.59
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 12.87s
                        Total time: 14055.69s
                               ETA: 1380372.2s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.604s, learning 0.173s)
               Value function loss: 3.1318
                    Surrogate loss: 0.0077
             Mean action noise std: 0.75
                       Mean reward: 10.54
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 12.78s
                        Total time: 14068.47s
                               ETA: 1380243.7s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.791s, learning 0.170s)
               Value function loss: 3.3538
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 9.35
               Mean episode length: 124.55
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 12.96s
                        Total time: 14081.43s
                               ETA: 1380133.6s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.626s, learning 0.183s)
               Value function loss: 3.0714
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 6.30
               Mean episode length: 123.42
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 12.81s
                        Total time: 14094.24s
                               ETA: 1380008.6s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.532s, learning 0.228s)
               Value function loss: 2.6948
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 3.20
               Mean episode length: 122.79
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 12.76s
                        Total time: 14107.00s
                               ETA: 1379879.2s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.323s, learning 0.159s)
               Value function loss: 2.8447
                    Surrogate loss: 0.0009
             Mean action noise std: 0.75
                       Mean reward: 5.12
               Mean episode length: 124.15
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 12.48s
                        Total time: 14119.48s
                               ETA: 1379722.7s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.112s, learning 0.159s)
               Value function loss: 3.0999
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 6.89
               Mean episode length: 124.65
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 12.27s
                        Total time: 14131.75s
                               ETA: 1379546.1s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.426s, learning 0.159s)
               Value function loss: 3.1537
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 7.73
               Mean episode length: 124.82
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 12.59s
                        Total time: 14144.34s
                               ETA: 1379400.4s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1199 steps/s (collection: 13.464s, learning 0.195s)
               Value function loss: 34.6450
                    Surrogate loss: 0.0162
             Mean action noise std: 0.75
                       Mean reward: 8.77
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 4.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 13.66s
                        Total time: 14158.00s
                               ETA: 1379359.5s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.855s, learning 0.174s)
               Value function loss: 0.4027
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 9.20
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 12.03s
                        Total time: 14170.03s
                               ETA: 1379160.0s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.934s, learning 0.168s)
               Value function loss: 0.2978
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 9.58
               Mean episode length: 125.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 12.10s
                        Total time: 14182.13s
                               ETA: 1378968.1s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.186s, learning 0.170s)
               Value function loss: 0.7060
                    Surrogate loss: 0.0060
             Mean action noise std: 0.75
                       Mean reward: 9.45
               Mean episode length: 125.00
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 12.36s
                        Total time: 14194.48s
                               ETA: 1378801.1s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.080s, learning 0.160s)
               Value function loss: 1.5077
                    Surrogate loss: 0.0018
             Mean action noise std: 0.75
                       Mean reward: 9.91
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 12.24s
                        Total time: 14206.72s
                               ETA: 1378623.2s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.553s, learning 0.245s)
               Value function loss: 1.3221
                    Surrogate loss: 0.0043
             Mean action noise std: 0.75
                       Mean reward: 10.38
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 12.80s
                        Total time: 14219.52s
                               ETA: 1378499.7s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.480s, learning 0.157s)
               Value function loss: 1.1484
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 11.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 12.64s
                        Total time: 14232.16s
                               ETA: 1378360.8s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.645s, learning 0.162s)
               Value function loss: 2.1131
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 13.02
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 12.81s
                        Total time: 14244.96s
                               ETA: 1378238.6s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.520s, learning 0.167s)
               Value function loss: 2.7533
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 9.11
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 12.69s
                        Total time: 14257.65s
                               ETA: 1378105.0s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.272s, learning 0.201s)
               Value function loss: 3.3089
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 1.51
               Mean episode length: 123.99
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 12.47s
                        Total time: 14270.12s
                               ETA: 1377951.1s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.520s, learning 0.161s)
               Value function loss: 2.2231
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: 0.53
               Mean episode length: 123.76
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 12.68s
                        Total time: 14282.81s
                               ETA: 1377817.4s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.081s, learning 0.158s)
               Value function loss: 2.0994
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 5.31
               Mean episode length: 124.77
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 12.24s
                        Total time: 14295.04s
                               ETA: 1377641.4s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.125s, learning 0.183s)
               Value function loss: 2.3258
                    Surrogate loss: -0.0024
             Mean action noise std: 0.75
                       Mean reward: 6.59
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 12.31s
                        Total time: 14307.35s
                               ETA: 1377472.4s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.733s, learning 0.184s)
               Value function loss: 2.4463
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 124.79
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 11.92s
                        Total time: 14319.27s
                               ETA: 1377266.0s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.907s, learning 0.187s)
               Value function loss: 3.4564
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 4.25
               Mean episode length: 124.88
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 12.09s
                        Total time: 14331.36s
                               ETA: 1377077.0s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.949s, learning 0.236s)
               Value function loss: 3.7877
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: 4.31
               Mean episode length: 124.93
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 12.19s
                        Total time: 14343.55s
                               ETA: 1376897.2s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1215 steps/s (collection: 13.309s, learning 0.166s)
               Value function loss: 33.7763
                    Surrogate loss: 0.0513
             Mean action noise std: 0.75
                       Mean reward: 4.25
               Mean episode length: 125.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 4.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 13.47s
                        Total time: 14357.02s
                               ETA: 1376841.3s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.424s, learning 0.249s)
               Value function loss: 0.3560
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: 4.25
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 12.67s
                        Total time: 14369.70s
                               ETA: 1376708.8s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.312s, learning 0.160s)
               Value function loss: 0.4538
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 3.42
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 12.47s
                        Total time: 14382.17s
                               ETA: 1376557.2s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.348s, learning 0.172s)
               Value function loss: 2.1293
                    Surrogate loss: 0.0307
             Mean action noise std: 0.75
                       Mean reward: 2.73
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 12.52s
                        Total time: 14394.69s
                               ETA: 1376410.5s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.572s, learning 0.163s)
               Value function loss: 1.7359
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 2.48
               Mean episode length: 125.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 12.73s
                        Total time: 14407.42s
                               ETA: 1376284.5s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.202s, learning 0.189s)
               Value function loss: 1.6689
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 2.61
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 12.39s
                        Total time: 14419.81s
                               ETA: 1376125.9s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.193s, learning 0.183s)
               Value function loss: 3.7943
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 4.06
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 12.38s
                        Total time: 14432.19s
                               ETA: 1375966.2s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.762s, learning 0.174s)
               Value function loss: 3.6141
                    Surrogate loss: 0.0231
             Mean action noise std: 0.75
                       Mean reward: 4.17
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 12.94s
                        Total time: 14445.13s
                               ETA: 1375860.1s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.266s, learning 0.173s)
               Value function loss: 3.4589
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 0.88
               Mean episode length: 123.97
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 12.44s
                        Total time: 14457.57s
                               ETA: 1375707.0s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.671s, learning 0.171s)
               Value function loss: 3.3189
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 0.24
               Mean episode length: 123.52
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 12.84s
                        Total time: 14470.41s
                               ETA: 1375592.4s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.908s, learning 0.173s)
               Value function loss: 2.1355
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 0.02
               Mean episode length: 124.18
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 12.08s
                        Total time: 14482.49s
                               ETA: 1375405.7s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.396s, learning 0.158s)
               Value function loss: 2.3567
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 1.80
               Mean episode length: 123.71
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 12.55s
                        Total time: 14495.04s
                               ETA: 1375264.2s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.057s, learning 0.184s)
               Value function loss: 2.5679
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 2.82
               Mean episode length: 124.12
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 12.24s
                        Total time: 14507.29s
                               ETA: 1375093.3s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.599s, learning 0.174s)
               Value function loss: 2.9435
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 4.98
               Mean episode length: 124.82
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 12.77s
                        Total time: 14520.06s
                               ETA: 1374973.1s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.438s, learning 0.163s)
               Value function loss: 2.9866
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 4.17
               Mean episode length: 124.81
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 12.60s
                        Total time: 14532.66s
                               ETA: 1374836.7s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1216 steps/s (collection: 13.307s, learning 0.163s)
               Value function loss: 39.2836
                    Surrogate loss: 0.0359
             Mean action noise std: 0.75
                       Mean reward: 6.88
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 13.47s
                        Total time: 14546.13s
                               ETA: 1374782.9s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.247s, learning 0.229s)
               Value function loss: 0.5498
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 6.96
               Mean episode length: 125.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 12.48s
                        Total time: 14558.61s
                               ETA: 1374635.2s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.205s, learning 0.189s)
               Value function loss: 0.4811
                    Surrogate loss: 0.0071
             Mean action noise std: 0.75
                       Mean reward: 6.83
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 12.39s
                        Total time: 14571.00s
                               ETA: 1374480.1s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.661s, learning 0.209s)
               Value function loss: 0.6073
                    Surrogate loss: 0.0007
             Mean action noise std: 0.75
                       Mean reward: 6.83
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 12.87s
                        Total time: 14583.87s
                               ETA: 1374370.0s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.301s, learning 0.274s)
               Value function loss: 1.5116
                    Surrogate loss: 0.0307
             Mean action noise std: 0.75
                       Mean reward: 6.44
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 12.57s
                        Total time: 14596.44s
                               ETA: 1374232.3s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.022s, learning 0.161s)
               Value function loss: 1.6902
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: 5.75
               Mean episode length: 124.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 12.18s
                        Total time: 14608.63s
                               ETA: 1374058.1s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.529s, learning 0.165s)
               Value function loss: 2.1370
                    Surrogate loss: 0.0026
             Mean action noise std: 0.75
                       Mean reward: 5.76
               Mean episode length: 124.63
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 12.69s
                        Total time: 14621.32s
                               ETA: 1373932.2s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.489s, learning 0.168s)
               Value function loss: 2.7451
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 5.96
               Mean episode length: 123.96
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 12.66s
                        Total time: 14633.98s
                               ETA: 1373802.9s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.467s, learning 0.163s)
               Value function loss: 6.3145
                    Surrogate loss: 0.0012
             Mean action noise std: 0.75
                       Mean reward: 3.33
               Mean episode length: 122.73
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 12.63s
                        Total time: 14646.61s
                               ETA: 1373671.4s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.719s, learning 0.169s)
               Value function loss: 3.2750
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 0.96
               Mean episode length: 121.76
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 12.89s
                        Total time: 14659.50s
                               ETA: 1373564.4s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.115s, learning 0.156s)
               Value function loss: 2.1380
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: -0.23
               Mean episode length: 123.08
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 12.27s
                        Total time: 14671.77s
                               ETA: 1373399.7s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.036s, learning 0.162s)
               Value function loss: 2.3477
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: -1.27
               Mean episode length: 122.96
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 12.20s
                        Total time: 14683.97s
                               ETA: 1373228.4s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.121s, learning 0.164s)
               Value function loss: 2.8761
                    Surrogate loss: 0.0055
             Mean action noise std: 0.75
                       Mean reward: 0.33
               Mean episode length: 123.54
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 12.29s
                        Total time: 14696.25s
                               ETA: 1373065.6s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.381s, learning 0.161s)
               Value function loss: 2.5575
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 4.53
               Mean episode length: 124.01
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 12.54s
                        Total time: 14708.79s
                               ETA: 1372927.1s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.014s, learning 0.167s)
               Value function loss: 4.2132
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 2.70
               Mean episode length: 124.68
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 12.18s
                        Total time: 14720.97s
                               ETA: 1372755.1s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.333s, learning 0.162s)
               Value function loss: 5.4994
                    Surrogate loss: -0.0001
             Mean action noise std: 0.75
                       Mean reward: 3.93
               Mean episode length: 124.87
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 12.50s
                        Total time: 14733.47s
                               ETA: 1372612.7s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.890s, learning 0.171s)
               Value function loss: 26.3952
                    Surrogate loss: 0.0115
             Mean action noise std: 0.75
                       Mean reward: 6.52
               Mean episode length: 125.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 4.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 13.06s
                        Total time: 14746.53s
                               ETA: 1372523.2s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.658s, learning 0.256s)
               Value function loss: 0.2661
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 6.43
               Mean episode length: 125.00
                  Mean reward/step: -0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 12.91s
                        Total time: 14759.44s
                               ETA: 1372420.2s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.350s, learning 0.375s)
               Value function loss: 0.2626
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 5.09
               Mean episode length: 124.58
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 12.73s
                        Total time: 14772.17s
                               ETA: 1372299.8s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.276s, learning 0.169s)
               Value function loss: 0.5468
                    Surrogate loss: 0.0225
             Mean action noise std: 0.75
                       Mean reward: 5.79
               Mean episode length: 124.58
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 12.44s
                        Total time: 14784.61s
                               ETA: 1372153.6s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.389s, learning 0.163s)
               Value function loss: 0.6039
                    Surrogate loss: 0.0017
             Mean action noise std: 0.75
                       Mean reward: 5.45
               Mean episode length: 124.22
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 12.55s
                        Total time: 14797.16s
                               ETA: 1372017.5s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.522s, learning 0.157s)
               Value function loss: 0.6243
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 4.97
               Mean episode length: 123.67
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 12.68s
                        Total time: 14809.84s
                               ETA: 1371893.5s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.572s, learning 0.187s)
               Value function loss: 0.5747
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 4.90
               Mean episode length: 123.67
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 12.76s
                        Total time: 14822.60s
                               ETA: 1371777.1s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.603s, learning 0.213s)
               Value function loss: 0.8006
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 4.41
               Mean episode length: 123.02
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 12.82s
                        Total time: 14835.42s
                               ETA: 1371666.2s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.105s, learning 0.209s)
               Value function loss: 1.5536
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: -0.25
               Mean episode length: 122.44
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 12.31s
                        Total time: 14847.73s
                               ETA: 1371509.0s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.379s, learning 0.165s)
               Value function loss: 2.0217
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 2.10
               Mean episode length: 124.42
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 12.54s
                        Total time: 14860.28s
                               ETA: 1371373.4s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.244s, learning 0.171s)
               Value function loss: 1.6069
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: -0.82
               Mean episode length: 124.19
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 12.42s
                        Total time: 14872.69s
                               ETA: 1371226.2s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.329s, learning 0.191s)
               Value function loss: 1.8706
                    Surrogate loss: 0.0109
             Mean action noise std: 0.75
                       Mean reward: -5.57
               Mean episode length: 122.73
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 12.52s
                        Total time: 14885.21s
                               ETA: 1371088.7s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.475s, learning 0.170s)
               Value function loss: 1.8993
                    Surrogate loss: 0.0050
             Mean action noise std: 0.75
                       Mean reward: -2.63
               Mean episode length: 123.69
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 12.64s
                        Total time: 14897.86s
                               ETA: 1370963.1s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.286s, learning 0.187s)
               Value function loss: 2.3458
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 0.49
               Mean episode length: 124.76
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 12.47s
                        Total time: 14910.33s
                               ETA: 1370821.8s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.713s, learning 0.164s)
               Value function loss: 3.1057
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 1.87
               Mean episode length: 124.89
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 12.88s
                        Total time: 14923.21s
                               ETA: 1370717.9s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.420s, learning 0.188s)
               Value function loss: 11.7669
                    Surrogate loss: 0.0327
             Mean action noise std: 0.75
                       Mean reward: 1.50
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 12.61s
                        Total time: 14935.81s
                               ETA: 1370589.5s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1203 steps/s (collection: 13.450s, learning 0.165s)
               Value function loss: 25.3083
                    Surrogate loss: 0.0056
             Mean action noise std: 0.75
                       Mean reward: 1.39
               Mean episode length: 125.00
                  Mean reward/step: -0.87
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 13.62s
                        Total time: 14949.43s
                               ETA: 1370553.7s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.215s, learning 0.162s)
               Value function loss: 0.2105
                    Surrogate loss: -0.0206
             Mean action noise std: 0.75
                       Mean reward: 1.22
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 12.38s
                        Total time: 14961.81s
                               ETA: 1370404.4s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.706s, learning 0.161s)
               Value function loss: 0.3273
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 12.87s
                        Total time: 14974.67s
                               ETA: 1370300.2s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.214s, learning 0.197s)
               Value function loss: 0.8058
                    Surrogate loss: 0.0061
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 12.41s
                        Total time: 14987.08s
                               ETA: 1370154.6s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.914s, learning 0.158s)
               Value function loss: 1.1955
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 1.05
               Mean episode length: 124.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 12.07s
                        Total time: 14999.15s
                               ETA: 1369978.2s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.583s, learning 0.179s)
               Value function loss: 1.1290
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 1.14
               Mean episode length: 124.79
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 12.76s
                        Total time: 15011.92s
                               ETA: 1369865.1s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.865s, learning 0.173s)
               Value function loss: 1.3502
                    Surrogate loss: 0.0001
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 124.79
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 12.04s
                        Total time: 15023.95s
                               ETA: 1369686.1s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.545s, learning 0.162s)
               Value function loss: 2.0708
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 124.79
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 12.71s
                        Total time: 15036.66s
                               ETA: 1369568.5s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.895s, learning 0.164s)
               Value function loss: 3.8429
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: -1.66
               Mean episode length: 123.28
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 13.06s
                        Total time: 15049.72s
                               ETA: 1369483.0s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.630s, learning 0.188s)
               Value function loss: 3.9594
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: -0.79
               Mean episode length: 124.06
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 12.82s
                        Total time: 15062.54s
                               ETA: 1369375.8s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.986s, learning 0.255s)
               Value function loss: 4.6374
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: -0.69
               Mean episode length: 124.06
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 12.24s
                        Total time: 15074.78s
                               ETA: 1369216.4s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.671s, learning 0.269s)
               Value function loss: 2.3889
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 1.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 12.94s
                        Total time: 15087.72s
                               ETA: 1369120.6s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.224s, learning 0.164s)
               Value function loss: 2.4907
                    Surrogate loss: 0.0128
             Mean action noise std: 0.75
                       Mean reward: 0.88
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 12.39s
                        Total time: 15100.11s
                               ETA: 1368974.9s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.694s, learning 0.156s)
               Value function loss: 2.9402
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: -1.54
               Mean episode length: 124.36
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 12.85s
                        Total time: 15112.96s
                               ETA: 1368871.4s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.142s, learning 0.164s)
               Value function loss: 34.8957
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: -7.12
               Mean episode length: 124.74
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 12.31s
                        Total time: 15125.26s
                               ETA: 1368718.8s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1241 steps/s (collection: 13.039s, learning 0.156s)
               Value function loss: 37.1700
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 1.83
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 4.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 13.20s
                        Total time: 15138.46s
                               ETA: 1368646.8s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.898s, learning 0.162s)
               Value function loss: 0.2644
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 1.54
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 12.06s
                        Total time: 15150.52s
                               ETA: 1368472.4s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.595s, learning 0.199s)
               Value function loss: 0.2019
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 1.48
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 12.79s
                        Total time: 15163.31s
                               ETA: 1368364.5s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.069s, learning 0.192s)
               Value function loss: 0.2965
                    Surrogate loss: 0.0045
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 12.26s
                        Total time: 15175.57s
                               ETA: 1368208.8s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.241s, learning 0.194s)
               Value function loss: 0.4260
                    Surrogate loss: 0.0052
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 12.44s
                        Total time: 15188.01s
                               ETA: 1368069.0s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.741s, learning 0.242s)
               Value function loss: 0.5029
                    Surrogate loss: 0.0086
             Mean action noise std: 0.75
                       Mean reward: 0.58
               Mean episode length: 124.73
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 12.98s
                        Total time: 15200.99s
                               ETA: 1367978.7s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.567s, learning 0.239s)
               Value function loss: 0.4866
                    Surrogate loss: 0.0183
             Mean action noise std: 0.75
                       Mean reward: -0.13
               Mean episode length: 124.39
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 12.81s
                        Total time: 15213.80s
                               ETA: 1367872.6s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.408s, learning 0.172s)
               Value function loss: 0.8388
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: 1.34
               Mean episode length: 124.39
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 12.58s
                        Total time: 15226.38s
                               ETA: 1367746.5s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.438s, learning 0.224s)
               Value function loss: 1.3505
                    Surrogate loss: 0.0084
             Mean action noise std: 0.75
                       Mean reward: 0.70
               Mean episode length: 123.18
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 12.66s
                        Total time: 15239.04s
                               ETA: 1367627.8s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.667s, learning 0.173s)
               Value function loss: 2.1919
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: -0.69
               Mean episode length: 123.82
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 12.84s
                        Total time: 15251.88s
                               ETA: 1367525.3s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.218s, learning 0.205s)
               Value function loss: 1.5879
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: -1.20
               Mean episode length: 124.50
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 12.42s
                        Total time: 15264.30s
                               ETA: 1367385.6s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.303s, learning 0.172s)
               Value function loss: 1.7350
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 0.52
               Mean episode length: 124.21
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 12.47s
                        Total time: 15276.78s
                               ETA: 1367250.8s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.143s, learning 0.242s)
               Value function loss: 1.8152
                    Surrogate loss: 0.0056
             Mean action noise std: 0.75
                       Mean reward: -3.23
               Mean episode length: 124.02
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 12.39s
                        Total time: 15289.16s
                               ETA: 1367108.3s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.719s, learning 0.185s)
               Value function loss: 1.9697
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: -1.43
               Mean episode length: 124.83
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 12.90s
                        Total time: 15302.07s
                               ETA: 1367012.3s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.396s, learning 0.183s)
               Value function loss: 2.8495
                    Surrogate loss: 0.0119
             Mean action noise std: 0.75
                       Mean reward: 1.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 12.58s
                        Total time: 15314.65s
                               ETA: 1366887.4s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.467s, learning 0.162s)
               Value function loss: 2.7391
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: -1.13
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 12.63s
                        Total time: 15327.27s
                               ETA: 1366767.2s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1214 steps/s (collection: 13.280s, learning 0.206s)
               Value function loss: 29.7550
                    Surrogate loss: 0.0276
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 4.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 13.49s
                        Total time: 15340.76s
                               ETA: 1366723.6s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.287s, learning 0.160s)
               Value function loss: 0.2257
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 2.84
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 12.45s
                        Total time: 15353.21s
                               ETA: 1366587.5s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.287s, learning 0.192s)
               Value function loss: 0.2975
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 2.78
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 12.48s
                        Total time: 15365.69s
                               ETA: 1366454.5s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.521s, learning 0.178s)
               Value function loss: 0.7551
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 12.70s
                        Total time: 15378.39s
                               ETA: 1366341.3s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.061s, learning 0.157s)
               Value function loss: 1.0804
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 2.41
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 12.22s
                        Total time: 15390.60s
                               ETA: 1366185.5s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.335s, learning 0.184s)
               Value function loss: 0.9818
                    Surrogate loss: 0.0044
             Mean action noise std: 0.75
                       Mean reward: 2.51
               Mean episode length: 124.44
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 12.52s
                        Total time: 15403.12s
                               ETA: 1366056.7s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.469s, learning 0.160s)
               Value function loss: 1.1464
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 1.90
               Mean episode length: 124.10
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 12.63s
                        Total time: 15415.75s
                               ETA: 1365937.8s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.005s, learning 0.219s)
               Value function loss: 2.2546
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 3.25
               Mean episode length: 123.44
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 12.22s
                        Total time: 15427.98s
                               ETA: 1365783.2s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.597s, learning 0.199s)
               Value function loss: 2.7523
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: 1.08
               Mean episode length: 122.84
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 12.80s
                        Total time: 15440.77s
                               ETA: 1365679.6s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.336s, learning 0.198s)
               Value function loss: 2.7811
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 2.65
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 12.53s
                        Total time: 15453.31s
                               ETA: 1365553.0s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.494s, learning 0.169s)
               Value function loss: 4.2127
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 12.66s
                        Total time: 15465.97s
                               ETA: 1365437.8s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.387s, learning 0.165s)
               Value function loss: 2.0450
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 2.12
               Mean episode length: 124.63
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 12.55s
                        Total time: 15478.52s
                               ETA: 1365313.2s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.386s, learning 0.169s)
               Value function loss: 1.8554
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: -0.47
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 12.56s
                        Total time: 15491.08s
                               ETA: 1365189.0s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.484s, learning 0.195s)
               Value function loss: 2.3540
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 2.92
               Mean episode length: 124.67
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 12.68s
                        Total time: 15503.76s
                               ETA: 1365076.0s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.020s, learning 0.289s)
               Value function loss: 2.5463
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 5.22
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 12.31s
                        Total time: 15516.06s
                               ETA: 1364930.5s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1246 steps/s (collection: 12.970s, learning 0.173s)
               Value function loss: 35.2135
                    Surrogate loss: 0.0303
             Mean action noise std: 0.75
                       Mean reward: 3.74
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 4.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 13.14s
                        Total time: 15529.21s
                               ETA: 1364858.6s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.823s, learning 0.190s)
               Value function loss: 0.4928
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 3.84
               Mean episode length: 125.00
                  Mean reward/step: -0.86
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 13.01s
                        Total time: 15542.22s
                               ETA: 1364775.3s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.446s, learning 0.172s)
               Value function loss: 0.3605
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 4.29
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 12.62s
                        Total time: 15554.84s
                               ETA: 1364657.5s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.019s, learning 0.159s)
               Value function loss: 0.4605
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 3.81
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 12.18s
                        Total time: 15567.02s
                               ETA: 1364501.3s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.551s, learning 0.205s)
               Value function loss: 0.7409
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: 2.91
               Mean episode length: 124.38
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 12.76s
                        Total time: 15579.77s
                               ETA: 1364396.1s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.359s, learning 0.159s)
               Value function loss: 0.8584
                    Surrogate loss: 0.0440
             Mean action noise std: 0.75
                       Mean reward: 2.35
               Mean episode length: 124.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 12.52s
                        Total time: 15592.29s
                               ETA: 1364270.1s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.170s, learning 0.157s)
               Value function loss: 0.6064
                    Surrogate loss: 0.0191
             Mean action noise std: 0.75
                       Mean reward: 1.64
               Mean episode length: 124.38
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 12.33s
                        Total time: 15604.62s
                               ETA: 1364127.7s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.217s, learning 0.156s)
               Value function loss: 0.7053
                    Surrogate loss: 0.0047
             Mean action noise std: 0.75
                       Mean reward: 1.60
               Mean episode length: 124.38
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 12.37s
                        Total time: 15616.99s
                               ETA: 1363989.5s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.397s, learning 0.225s)
               Value function loss: 1.0717
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 0.40
               Mean episode length: 123.73
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 12.62s
                        Total time: 15629.61s
                               ETA: 1363873.3s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.283s, learning 0.171s)
               Value function loss: 1.9289
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 0.14
               Mean episode length: 124.09
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 12.45s
                        Total time: 15642.07s
                               ETA: 1363742.6s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.371s, learning 0.162s)
               Value function loss: 1.7805
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: -1.46
               Mean episode length: 124.45
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 12.53s
                        Total time: 15654.60s
                               ETA: 1363619.0s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.571s, learning 0.286s)
               Value function loss: 2.0182
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: -1.80
               Mean episode length: 124.64
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 12.86s
                        Total time: 15667.45s
                               ETA: 1363523.7s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.977s, learning 0.157s)
               Value function loss: 2.3980
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: -2.59
               Mean episode length: 124.36
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 12.13s
                        Total time: 15679.59s
                               ETA: 1363365.8s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.339s, learning 0.163s)
               Value function loss: 2.2523
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: -3.20
               Mean episode length: 124.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 12.50s
                        Total time: 15692.09s
                               ETA: 1363240.1s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.834s, learning 0.181s)
               Value function loss: 2.3104
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: -0.47
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 13.02s
                        Total time: 15705.11s
                               ETA: 1363159.1s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.548s, learning 0.164s)
               Value function loss: 2.9753
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 0.81
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 12.71s
                        Total time: 15717.82s
                               ETA: 1363051.9s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1183 steps/s (collection: 13.677s, learning 0.164s)
               Value function loss: 26.2344
                    Surrogate loss: 0.0261
             Mean action noise std: 0.75
                       Mean reward: -1.43
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 4.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 13.84s
                        Total time: 15731.66s
                               ETA: 1363042.8s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.837s, learning 0.204s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: -1.74
               Mean episode length: 125.00
                  Mean reward/step: -0.80
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 13.04s
                        Total time: 15744.70s
                               ETA: 1362964.4s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.259s, learning 0.265s)
               Value function loss: 0.1334
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: -1.26
               Mean episode length: 125.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 12.52s
                        Total time: 15757.22s
                               ETA: 1362841.4s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.745s, learning 0.233s)
               Value function loss: 0.2382
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: -1.33
               Mean episode length: 125.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 12.98s
                        Total time: 15770.20s
                               ETA: 1362757.7s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.350s, learning 0.162s)
               Value function loss: 0.4376
                    Surrogate loss: 0.0097
             Mean action noise std: 0.75
                       Mean reward: -1.56
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 12.51s
                        Total time: 15782.71s
                               ETA: 1362634.1s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.424s, learning 0.164s)
               Value function loss: 0.5538
                    Surrogate loss: 0.0237
             Mean action noise std: 0.75
                       Mean reward: -1.80
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 12.59s
                        Total time: 15795.30s
                               ETA: 1362517.2s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.609s, learning 0.230s)
               Value function loss: 0.4318
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: -1.43
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 12.84s
                        Total time: 15808.14s
                               ETA: 1362422.1s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.683s, learning 0.210s)
               Value function loss: 0.6502
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: -1.40
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 12.89s
                        Total time: 15821.04s
                               ETA: 1362331.8s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.154s, learning 0.186s)
               Value function loss: 1.1548
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: -2.87
               Mean episode length: 124.43
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 12.34s
                        Total time: 15833.38s
                               ETA: 1362194.0s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.199s, learning 0.225s)
               Value function loss: 1.9243
                    Surrogate loss: 0.0064
             Mean action noise std: 0.75
                       Mean reward: -2.84
               Mean episode length: 124.43
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 12.42s
                        Total time: 15845.80s
                               ETA: 1362063.7s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.289s, learning 0.160s)
               Value function loss: 1.3665
                    Surrogate loss: 0.0012
             Mean action noise std: 0.75
                       Mean reward: -4.84
               Mean episode length: 124.86
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 12.45s
                        Total time: 15858.25s
                               ETA: 1361935.7s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.707s, learning 0.162s)
               Value function loss: 1.4290
                    Surrogate loss: 0.0094
             Mean action noise std: 0.75
                       Mean reward: -7.13
               Mean episode length: 124.17
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 12.87s
                        Total time: 15871.12s
                               ETA: 1361843.9s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.279s, learning 0.202s)
               Value function loss: 1.4995
                    Surrogate loss: 0.0049
             Mean action noise std: 0.75
                       Mean reward: -3.78
               Mean episode length: 124.05
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 12.48s
                        Total time: 15883.60s
                               ETA: 1361719.1s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.338s, learning 0.184s)
               Value function loss: 1.6954
                    Surrogate loss: 0.0135
             Mean action noise std: 0.75
                       Mean reward: -2.00
               Mean episode length: 124.74
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 12.52s
                        Total time: 15896.12s
                               ETA: 1361597.9s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.430s, learning 0.241s)
               Value function loss: 2.3176
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: -1.32
               Mean episode length: 124.84
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 12.67s
                        Total time: 15908.79s
                               ETA: 1361489.7s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.512s, learning 0.211s)
               Value function loss: 2.3362
                    Surrogate loss: 0.0048
             Mean action noise std: 0.75
                       Mean reward: 0.02
               Mean episode length: 124.97
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 12.72s
                        Total time: 15921.52s
                               ETA: 1361386.0s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.883s, learning 0.187s)
               Value function loss: 24.0896
                    Surrogate loss: 0.0187
             Mean action noise std: 0.75
                       Mean reward: -0.72
               Mean episode length: 125.00
                  Mean reward/step: -0.73
       Mean episode length/episode: 4.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 13.07s
                        Total time: 15934.59s
                               ETA: 1361312.1s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.759s, learning 0.168s)
               Value function loss: 0.1545
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: -0.72
               Mean episode length: 125.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 12.93s
                        Total time: 15947.51s
                               ETA: 1361226.2s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.449s, learning 0.159s)
               Value function loss: 0.2418
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 0.44
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 12.61s
                        Total time: 15960.12s
                               ETA: 1361113.2s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.723s, learning 0.177s)
               Value function loss: 0.5965
                    Surrogate loss: 0.0060
             Mean action noise std: 0.75
                       Mean reward: 0.27
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 12.90s
                        Total time: 15973.02s
                               ETA: 1361025.2s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.265s, learning 0.178s)
               Value function loss: 0.8562
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 0.17
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 12.44s
                        Total time: 15985.46s
                               ETA: 1360898.5s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.739s, learning 0.157s)
               Value function loss: 1.0683
                    Surrogate loss: 0.0019
             Mean action noise std: 0.75
                       Mean reward: -0.29
               Mean episode length: 124.33
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 12.90s
                        Total time: 15998.36s
                               ETA: 1360810.4s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.316s, learning 0.166s)
               Value function loss: 1.2365
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: -0.90
               Mean episode length: 123.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 12.48s
                        Total time: 16010.84s
                               ETA: 1360687.3s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.539s, learning 0.300s)
               Value function loss: 1.6913
                    Surrogate loss: 0.0013
             Mean action noise std: 0.75
                       Mean reward: -1.67
               Mean episode length: 123.63
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 12.84s
                        Total time: 16023.68s
                               ETA: 1360594.8s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.197s, learning 0.162s)
               Value function loss: 2.8413
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: -0.95
               Mean episode length: 123.47
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 12.36s
                        Total time: 16036.04s
                               ETA: 1360461.7s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.352s, learning 0.204s)
               Value function loss: 3.0899
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: -0.50
               Mean episode length: 123.73
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 12.56s
                        Total time: 16048.59s
                               ETA: 1360345.5s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.401s, learning 0.207s)
               Value function loss: 2.9586
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: -1.82
               Mean episode length: 124.04
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 12.61s
                        Total time: 16061.20s
                               ETA: 1360233.8s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.824s, learning 0.206s)
               Value function loss: 3.2475
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 0.30
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 12.03s
                        Total time: 16073.23s
                               ETA: 1360073.4s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.694s, learning 0.207s)
               Value function loss: 3.4387
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: -0.54
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 12.90s
                        Total time: 16086.13s
                               ETA: 1359986.9s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.405s, learning 0.179s)
               Value function loss: 8.1275
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: -1.54
               Mean episode length: 124.57
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 12.58s
                        Total time: 16098.72s
                               ETA: 1359873.7s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.735s, learning 0.265s)
               Value function loss: 5.5068
                    Surrogate loss: 0.0126
             Mean action noise std: 0.75
                       Mean reward: 0.42
               Mean episode length: 124.59
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 13.00s
                        Total time: 16111.72s
                               ETA: 1359795.9s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1237 steps/s (collection: 13.082s, learning 0.159s)
               Value function loss: 64.2018
                    Surrogate loss: 0.0150
             Mean action noise std: 0.75
                       Mean reward: 0.27
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 4.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 13.24s
                        Total time: 16124.96s
                               ETA: 1359738.4s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.620s, learning 0.172s)
               Value function loss: 0.3224
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 0.48
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 12.79s
                        Total time: 16137.75s
                               ETA: 1359643.3s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.682s, learning 0.167s)
               Value function loss: 0.3263
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 0.02
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 12.85s
                        Total time: 16150.60s
                               ETA: 1359553.0s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.198s, learning 0.164s)
               Value function loss: 0.3333
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: -0.21
               Mean episode length: 124.57
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 12.36s
                        Total time: 16162.96s
                               ETA: 1359421.9s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.764s, learning 0.157s)
               Value function loss: 0.5972
                    Surrogate loss: 0.0110
             Mean action noise std: 0.75
                       Mean reward: 0.18
               Mean episode length: 124.57
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 11.92s
                        Total time: 16174.88s
                               ETA: 1359253.9s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.169s, learning 0.169s)
               Value function loss: 0.7272
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: -0.59
               Mean episode length: 124.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 12.34s
                        Total time: 16187.22s
                               ETA: 1359121.2s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.282s, learning 0.209s)
               Value function loss: 0.7372
                    Surrogate loss: 0.0073
             Mean action noise std: 0.75
                       Mean reward: -0.48
               Mean episode length: 124.57
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 12.49s
                        Total time: 16199.71s
                               ETA: 1359001.7s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.072s, learning 0.176s)
               Value function loss: 0.8812
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: -1.28
               Mean episode length: 124.57
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 12.25s
                        Total time: 16211.96s
                               ETA: 1358861.9s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.680s, learning 0.266s)
               Value function loss: 1.5702
                    Surrogate loss: 0.0065
             Mean action noise std: 0.75
                       Mean reward: -0.11
               Mean episode length: 123.96
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 12.95s
                        Total time: 16224.90s
                               ETA: 1358780.8s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.188s, learning 0.199s)
               Value function loss: 3.1632
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: 0.89
               Mean episode length: 123.90
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 12.39s
                        Total time: 16237.29s
                               ETA: 1358653.0s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.259s, learning 0.227s)
               Value function loss: 1.8463
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 2.60
               Mean episode length: 124.41
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 12.49s
                        Total time: 16249.78s
                               ETA: 1358533.7s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.567s, learning 0.188s)
               Value function loss: 1.8099
                    Surrogate loss: 0.0046
             Mean action noise std: 0.75
                       Mean reward: 3.97
               Mean episode length: 124.93
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 12.76s
                        Total time: 16262.53s
                               ETA: 1358437.1s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1243 steps/s (collection: 13.024s, learning 0.155s)
               Value function loss: 1.5811
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 2.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 13.18s
                        Total time: 16275.71s
                               ETA: 1358376.0s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.750s, learning 0.162s)
               Value function loss: 1.9346
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 1.48
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 12.91s
                        Total time: 16288.63s
                               ETA: 1358292.7s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.625s, learning 0.159s)
               Value function loss: 2.9336
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: -2.16
               Mean episode length: 124.85
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 12.78s
                        Total time: 16301.41s
                               ETA: 1358198.8s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.057s, learning 0.205s)
               Value function loss: 2.6630
                    Surrogate loss: 0.0131
             Mean action noise std: 0.75
                       Mean reward: -2.40
               Mean episode length: 124.87
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 12.26s
                        Total time: 16313.67s
                               ETA: 1358061.6s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1242 steps/s (collection: 13.020s, learning 0.171s)
               Value function loss: 27.2964
                    Surrogate loss: 0.0230
             Mean action noise std: 0.75
                       Mean reward: 0.00
               Mean episode length: 125.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 4.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 13.19s
                        Total time: 16326.86s
                               ETA: 1358001.9s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.214s, learning 0.181s)
               Value function loss: 0.1988
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 0.00
               Mean episode length: 125.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 12.39s
                        Total time: 16339.26s
                               ETA: 1357876.1s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.994s, learning 0.162s)
               Value function loss: 0.2077
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: -0.12
               Mean episode length: 125.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 12.16s
                        Total time: 16351.41s
                               ETA: 1357730.7s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.502s, learning 0.224s)
               Value function loss: 0.4999
                    Surrogate loss: 0.0127
             Mean action noise std: 0.74
                       Mean reward: -0.62
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 12.73s
                        Total time: 16364.14s
                               ETA: 1357632.7s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.110s, learning 0.245s)
               Value function loss: 0.7110
                    Surrogate loss: 0.0043
             Mean action noise std: 0.74
                       Mean reward: -0.84
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 12.36s
                        Total time: 16376.49s
                               ETA: 1357504.2s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.643s, learning 0.182s)
               Value function loss: 1.1504
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: -1.20
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 12.82s
                        Total time: 16389.32s
                               ETA: 1357414.8s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.411s, learning 0.163s)
               Value function loss: 1.7466
                    Surrogate loss: 0.0037
             Mean action noise std: 0.74
                       Mean reward: -1.64
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 12.57s
                        Total time: 16401.89s
                               ETA: 1357304.7s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.440s, learning 0.160s)
               Value function loss: 1.7995
                    Surrogate loss: 0.0007
             Mean action noise std: 0.74
                       Mean reward: -0.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 12.60s
                        Total time: 16414.49s
                               ETA: 1357197.0s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.695s, learning 0.160s)
               Value function loss: 2.6054
                    Surrogate loss: 0.0003
             Mean action noise std: 0.74
                       Mean reward: -1.42
               Mean episode length: 123.81
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 12.86s
                        Total time: 16427.35s
                               ETA: 1357110.5s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.441s, learning 0.170s)
               Value function loss: 3.5136
                    Surrogate loss: 0.0070
             Mean action noise std: 0.74
                       Mean reward: 1.86
               Mean episode length: 124.45
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 12.61s
                        Total time: 16439.96s
                               ETA: 1357004.0s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.982s, learning 0.206s)
               Value function loss: 1.9196
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 0.92
               Mean episode length: 124.58
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 12.19s
                        Total time: 16452.15s
                               ETA: 1356862.7s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.343s, learning 0.175s)
               Value function loss: 1.9075
                    Surrogate loss: 0.0284
             Mean action noise std: 0.74
                       Mean reward: -0.06
               Mean episode length: 124.58
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 12.52s
                        Total time: 16464.67s
                               ETA: 1356748.9s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.572s, learning 0.286s)
               Value function loss: 1.8289
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 1.23
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 12.86s
                        Total time: 16477.52s
                               ETA: 1356663.2s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.257s, learning 0.202s)
               Value function loss: 2.1429
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: -0.73
               Mean episode length: 124.79
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 12.46s
                        Total time: 16489.98s
                               ETA: 1356544.8s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.352s, learning 0.167s)
               Value function loss: 3.2076
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: -0.92
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 12.52s
                        Total time: 16502.50s
                               ETA: 1356431.5s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1240 steps/s (collection: 13.044s, learning 0.161s)
               Value function loss: 6.7800
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 2.67
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 13.21s
                        Total time: 16515.71s
                               ETA: 1356374.7s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1201 steps/s (collection: 13.410s, learning 0.222s)
               Value function loss: 16.9095
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 2.84
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 13.63s
                        Total time: 16529.34s
                               ETA: 1356353.1s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.215s, learning 0.200s)
               Value function loss: 0.9218
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 2.98
               Mean episode length: 125.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 12.41s
                        Total time: 16541.75s
                               ETA: 1356231.7s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.171s, learning 0.166s)
               Value function loss: 0.7320
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 3.02
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 12.34s
                        Total time: 16554.09s
                               ETA: 1356104.1s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.291s, learning 0.162s)
               Value function loss: 0.6195
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 2.55
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 12.45s
                        Total time: 16566.54s
                               ETA: 1355986.1s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.236s, learning 0.230s)
               Value function loss: 0.5315
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 1.33
               Mean episode length: 124.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 12.47s
                        Total time: 16579.01s
                               ETA: 1355869.3s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.330s, learning 0.157s)
               Value function loss: 0.6147
                    Surrogate loss: 0.0073
             Mean action noise std: 0.74
                       Mean reward: 1.15
               Mean episode length: 124.47
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 12.49s
                        Total time: 16591.50s
                               ETA: 1355754.4s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.492s, learning 0.160s)
               Value function loss: 0.7460
                    Surrogate loss: 0.0075
             Mean action noise std: 0.74
                       Mean reward: 1.20
               Mean episode length: 124.47
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 12.65s
                        Total time: 16604.15s
                               ETA: 1355653.2s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.603s, learning 0.279s)
               Value function loss: 1.0871
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: 2.27
               Mean episode length: 123.83
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 12.88s
                        Total time: 16617.03s
                               ETA: 1355571.0s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.377s, learning 0.171s)
               Value function loss: 2.4227
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 7.84
               Mean episode length: 123.83
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 12.55s
                        Total time: 16629.58s
                               ETA: 1355461.6s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.712s, learning 0.159s)
               Value function loss: 2.1530
                    Surrogate loss: -0.0023
             Mean action noise std: 0.74
                       Mean reward: 5.45
               Mean episode length: 124.52
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 12.87s
                        Total time: 16642.45s
                               ETA: 1355378.7s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.603s, learning 0.163s)
               Value function loss: 1.6891
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 5.11
               Mean episode length: 124.09
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 12.77s
                        Total time: 16655.22s
                               ETA: 1355287.3s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.421s, learning 0.161s)
               Value function loss: 2.5839
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: 6.88
               Mean episode length: 124.57
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 12.58s
                        Total time: 16667.80s
                               ETA: 1355181.1s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.862s, learning 0.169s)
               Value function loss: 2.4740
                    Surrogate loss: 0.0055
             Mean action noise std: 0.74
                       Mean reward: 8.26
               Mean episode length: 124.72
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 12.03s
                        Total time: 16679.83s
                               ETA: 1355030.3s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.433s, learning 0.182s)
               Value function loss: 2.5696
                    Surrogate loss: 0.0116
             Mean action noise std: 0.74
                       Mean reward: 8.24
               Mean episode length: 124.82
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 12.61s
                        Total time: 16692.44s
                               ETA: 1354927.1s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.395s, learning 0.164s)
               Value function loss: 2.7098
                    Surrogate loss: 0.0093
             Mean action noise std: 0.74
                       Mean reward: 7.00
               Mean episode length: 124.93
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 12.56s
                        Total time: 16705.00s
                               ETA: 1354819.6s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.832s, learning 0.166s)
               Value function loss: 29.4661
                    Surrogate loss: 0.0271
             Mean action noise std: 0.74
                       Mean reward: 6.78
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 4.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 13.00s
                        Total time: 16718.00s
                               ETA: 1354747.8s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.348s, learning 0.164s)
               Value function loss: 0.2276
                    Surrogate loss: -0.0203
             Mean action noise std: 0.74
                       Mean reward: 7.00
               Mean episode length: 125.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 12.51s
                        Total time: 16730.51s
                               ETA: 1354636.7s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.133s, learning 0.162s)
               Value function loss: 0.2869
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 6.55
               Mean episode length: 124.41
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 12.30s
                        Total time: 16742.81s
                               ETA: 1354508.2s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.372s, learning 0.213s)
               Value function loss: 0.3637
                    Surrogate loss: 0.0034
             Mean action noise std: 0.74
                       Mean reward: 6.17
               Mean episode length: 124.41
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 12.58s
                        Total time: 16755.39s
                               ETA: 1354403.4s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.165s, learning 0.199s)
               Value function loss: 0.5431
                    Surrogate loss: 0.0101
             Mean action noise std: 0.74
                       Mean reward: 6.20
               Mean episode length: 124.41
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 12.36s
                        Total time: 16767.76s
                               ETA: 1354280.9s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.825s, learning 0.159s)
               Value function loss: 0.7472
                    Surrogate loss: 0.0132
             Mean action noise std: 0.74
                       Mean reward: 6.06
               Mean episode length: 124.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 12.98s
                        Total time: 16780.74s
                               ETA: 1354208.5s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.753s, learning 0.162s)
               Value function loss: 0.5338
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 5.64
               Mean episode length: 123.82
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 12.91s
                        Total time: 16793.66s
                               ETA: 1354130.7s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.369s, learning 0.175s)
               Value function loss: 0.9868
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: 4.52
               Mean episode length: 122.56
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 12.54s
                        Total time: 16806.20s
                               ETA: 1354023.2s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.651s, learning 0.165s)
               Value function loss: 1.5918
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 2.80
               Mean episode length: 122.56
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 12.82s
                        Total time: 16819.02s
                               ETA: 1353937.6s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.638s, learning 0.162s)
               Value function loss: 2.6788
                    Surrogate loss: 0.0032
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 12.80s
                        Total time: 16831.82s
                               ETA: 1353850.9s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.391s, learning 0.159s)
               Value function loss: 1.7766
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: -2.26
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 12.55s
                        Total time: 16844.37s
                               ETA: 1353744.2s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.313s, learning 0.179s)
               Value function loss: 1.7869
                    Surrogate loss: 0.0106
             Mean action noise std: 0.74
                       Mean reward: -4.90
               Mean episode length: 124.59
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 12.49s
                        Total time: 16856.86s
                               ETA: 1353633.0s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.161s)
               Value function loss: 1.6891
                    Surrogate loss: 0.0078
             Mean action noise std: 0.74
                       Mean reward: -4.33
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 12.07s
                        Total time: 16868.92s
                               ETA: 1353488.0s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.390s, learning 0.163s)
               Value function loss: 2.1135
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: -7.71
               Mean episode length: 124.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 12.55s
                        Total time: 16881.48s
                               ETA: 1353382.0s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.885s, learning 0.185s)
               Value function loss: 2.9231
                    Surrogate loss: 0.0020
             Mean action noise std: 0.74
                       Mean reward: -3.85
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 12.07s
                        Total time: 16893.55s
                               ETA: 1353237.6s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.871s, learning 0.182s)
               Value function loss: 2.5285
                    Surrogate loss: 0.0014
             Mean action noise std: 0.74
                       Mean reward: -4.85
               Mean episode length: 124.95
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 12.05s
                        Total time: 16905.60s
                               ETA: 1353092.0s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1220 steps/s (collection: 13.133s, learning 0.287s)
               Value function loss: 26.2226
                    Surrogate loss: 0.0221
             Mean action noise std: 0.74
                       Mean reward: -5.07
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 4.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 13.42s
                        Total time: 16919.02s
                               ETA: 1353056.0s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.565s, learning 0.241s)
               Value function loss: 0.5787
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: -4.83
               Mean episode length: 125.00
                  Mean reward/step: -0.71
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 12.81s
                        Total time: 16931.83s
                               ETA: 1352970.9s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.124s, learning 0.169s)
               Value function loss: 1.6467
                    Surrogate loss: -0.0008
             Mean action noise std: 0.74
                       Mean reward: -4.44
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 12.29s
                        Total time: 16944.12s
                               ETA: 1352845.0s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.396s, learning 0.156s)
               Value function loss: 1.7707
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: -4.84
               Mean episode length: 124.50
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 12.55s
                        Total time: 16956.67s
                               ETA: 1352739.9s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.578s, learning 0.229s)
               Value function loss: 1.7101
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: -3.64
               Mean episode length: 124.50
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 12.81s
                        Total time: 16969.48s
                               ETA: 1352655.2s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.546s, learning 0.189s)
               Value function loss: 0.9620
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: -3.29
               Mean episode length: 124.50
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 12.74s
                        Total time: 16982.22s
                               ETA: 1352565.0s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.513s, learning 0.160s)
               Value function loss: 1.1354
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: -2.69
               Mean episode length: 124.50
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 12.67s
                        Total time: 16994.89s
                               ETA: 1352470.0s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.231s, learning 0.223s)
               Value function loss: 56.4329
                    Surrogate loss: 0.0009
             Mean action noise std: 0.74
                       Mean reward: -10.18
               Mean episode length: 123.44
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 12.45s
                        Total time: 17007.34s
                               ETA: 1352357.7s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.421s, learning 0.161s)
               Value function loss: 2.0120
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: -9.61
               Mean episode length: 123.94
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 12.58s
                        Total time: 17019.93s
                               ETA: 1352255.7s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.495s, learning 0.236s)
               Value function loss: 2.2155
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: -3.80
               Mean episode length: 124.79
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 12.73s
                        Total time: 17032.66s
                               ETA: 1352165.7s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.987s, learning 0.168s)
               Value function loss: 1.4628
                    Surrogate loss: 0.0061
             Mean action noise std: 0.74
                       Mean reward: -4.99
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 12.15s
                        Total time: 17044.81s
                               ETA: 1352030.1s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.840s, learning 0.180s)
               Value function loss: 1.5200
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: -5.19
               Mean episode length: 124.69
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 13.02s
                        Total time: 17057.83s
                               ETA: 1351963.3s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.290s, learning 0.158s)
               Value function loss: 1.8474
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: -5.82
               Mean episode length: 124.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 12.45s
                        Total time: 17070.28s
                               ETA: 1351851.3s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.159s, learning 0.157s)
               Value function loss: 2.3681
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: -4.45
               Mean episode length: 124.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 12.32s
                        Total time: 17082.60s
                               ETA: 1351728.9s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.229s, learning 0.207s)
               Value function loss: 2.7036
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: -2.88
               Mean episode length: 124.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 12.44s
                        Total time: 17095.03s
                               ETA: 1351616.2s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1228 steps/s (collection: 13.171s, learning 0.163s)
               Value function loss: 61.3374
                    Surrogate loss: 0.0385
             Mean action noise std: 0.74
                       Mean reward: -1.40
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 4.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 13.33s
                        Total time: 17108.37s
                               ETA: 1351574.6s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.579s, learning 0.163s)
               Value function loss: 1.5798
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: -1.29
               Mean episode length: 125.00
                  Mean reward/step: -0.83
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 12.74s
                        Total time: 17121.11s
                               ETA: 1351486.4s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.184s, learning 0.160s)
               Value function loss: 0.6873
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: -1.74
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 12.34s
                        Total time: 17133.45s
                               ETA: 1351366.8s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.400s, learning 0.163s)
               Value function loss: 1.1881
                    Surrogate loss: 0.0028
             Mean action noise std: 0.74
                       Mean reward: -2.38
               Mean episode length: 123.98
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 12.56s
                        Total time: 17146.02s
                               ETA: 1351264.7s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.365s, learning 0.202s)
               Value function loss: 0.4732
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: -2.91
               Mean episode length: 123.42
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 12.57s
                        Total time: 17158.58s
                               ETA: 1351163.1s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.750s, learning 0.175s)
               Value function loss: 0.7397
                    Surrogate loss: 0.0297
             Mean action noise std: 0.74
                       Mean reward: -2.59
               Mean episode length: 123.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 12.92s
                        Total time: 17171.51s
                               ETA: 1351089.7s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.329s, learning 0.209s)
               Value function loss: 0.7607
                    Surrogate loss: -0.0003
             Mean action noise std: 0.74
                       Mean reward: -2.69
               Mean episode length: 123.14
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 12.54s
                        Total time: 17184.04s
                               ETA: 1350986.0s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.839s, learning 0.161s)
               Value function loss: 0.9418
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: -1.37
               Mean episode length: 123.14
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 13.00s
                        Total time: 17197.04s
                               ETA: 1350918.8s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.448s, learning 0.187s)
               Value function loss: 1.1230
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 1.91
               Mean episode length: 123.14
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 12.63s
                        Total time: 17209.68s
                               ETA: 1350823.0s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.550s, learning 0.203s)
               Value function loss: 2.3259
                    Surrogate loss: 0.0069
             Mean action noise std: 0.74
                       Mean reward: 0.59
               Mean episode length: 123.87
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 12.75s
                        Total time: 17222.43s
                               ETA: 1350736.5s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.417s, learning 0.166s)
               Value function loss: 2.0717
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 0.71
               Mean episode length: 124.05
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 12.58s
                        Total time: 17235.01s
                               ETA: 1350637.0s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.823s, learning 0.160s)
               Value function loss: 1.5880
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: -0.02
               Mean episode length: 124.16
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 11.98s
                        Total time: 17247.00s
                               ETA: 1350490.6s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.380s, learning 0.165s)
               Value function loss: 2.4777
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: -2.70
               Mean episode length: 123.37
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 12.55s
                        Total time: 17259.54s
                               ETA: 1350388.3s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.384s, learning 0.161s)
               Value function loss: 3.3965
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: -2.96
               Mean episode length: 123.65
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 12.55s
                        Total time: 17272.09s
                               ETA: 1350286.2s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.765s, learning 0.161s)
               Value function loss: 3.1794
                    Surrogate loss: 0.0302
             Mean action noise std: 0.74
                       Mean reward: -1.77
               Mean episode length: 124.79
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 12.93s
                        Total time: 17285.01s
                               ETA: 1350214.0s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.850s, learning 0.174s)
               Value function loss: 2.6252
                    Surrogate loss: -0.0021
             Mean action noise std: 0.74
                       Mean reward: -1.86
               Mean episode length: 124.89
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 13.02s
                        Total time: 17298.04s
                               ETA: 1350149.5s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1240 steps/s (collection: 13.044s, learning 0.169s)
               Value function loss: 25.7101
                    Surrogate loss: 0.0224
             Mean action noise std: 0.74
                       Mean reward: -0.64
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 4.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 13.21s
                        Total time: 17311.25s
                               ETA: 1350099.8s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.407s, learning 0.158s)
               Value function loss: 0.3550
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: -0.75
               Mean episode length: 125.00
                  Mean reward/step: -0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 12.56s
                        Total time: 17323.82s
                               ETA: 1349999.7s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.223s, learning 0.171s)
               Value function loss: 0.5990
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: -1.35
               Mean episode length: 124.33
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 12.39s
                        Total time: 17336.21s
                               ETA: 1349886.4s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.272s, learning 0.161s)
               Value function loss: 1.1145
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: -1.30
               Mean episode length: 124.33
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 12.43s
                        Total time: 17348.64s
                               ETA: 1349776.3s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.578s, learning 0.171s)
               Value function loss: 0.8858
                    Surrogate loss: 0.0042
             Mean action noise std: 0.74
                       Mean reward: -2.91
               Mean episode length: 123.27
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 12.75s
                        Total time: 17361.39s
                               ETA: 1349691.0s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.414s, learning 0.160s)
               Value function loss: 0.8783
                    Surrogate loss: 0.0041
             Mean action noise std: 0.74
                       Mean reward: -2.39
               Mean episode length: 123.27
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 12.57s
                        Total time: 17373.97s
                               ETA: 1349592.2s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.356s, learning 0.185s)
               Value function loss: 0.7334
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: -2.00
               Mean episode length: 123.27
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 12.54s
                        Total time: 17386.51s
                               ETA: 1349490.9s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.427s, learning 0.160s)
               Value function loss: 1.2182
                    Surrogate loss: 0.0040
             Mean action noise std: 0.74
                       Mean reward: -0.76
               Mean episode length: 122.33
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 12.59s
                        Total time: 17399.09s
                               ETA: 1349393.4s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.389s, learning 0.204s)
               Value function loss: 1.8246
                    Surrogate loss: 0.0024
             Mean action noise std: 0.74
                       Mean reward: 4.25
               Mean episode length: 123.07
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 12.59s
                        Total time: 17411.69s
                               ETA: 1349296.5s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.424s, learning 0.213s)
               Value function loss: 2.7515
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 7.98
               Mean episode length: 124.47
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 12.64s
                        Total time: 17424.32s
                               ETA: 1349203.0s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.392s, learning 0.183s)
               Value function loss: 2.0026
                    Surrogate loss: 0.0006
             Mean action noise std: 0.74
                       Mean reward: 8.38
               Mean episode length: 124.54
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 12.57s
                        Total time: 17436.90s
                               ETA: 1349104.9s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.611s, learning 0.162s)
               Value function loss: 2.2102
                    Surrogate loss: 0.0014
             Mean action noise std: 0.74
                       Mean reward: 5.30
               Mean episode length: 124.14
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 12.77s
                        Total time: 17449.67s
                               ETA: 1349022.3s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1238 steps/s (collection: 12.958s, learning 0.268s)
               Value function loss: 2.5002
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 5.87
               Mean episode length: 124.41
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 13.23s
                        Total time: 17462.90s
                               ETA: 1348974.8s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.485s, learning 0.157s)
               Value function loss: 3.3423
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 6.50
               Mean episode length: 124.12
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 12.64s
                        Total time: 17475.54s
                               ETA: 1348882.2s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.535s, learning 0.162s)
               Value function loss: 6.5955
                    Surrogate loss: 0.0037
             Mean action noise std: 0.74
                       Mean reward: 4.91
               Mean episode length: 124.87
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 12.70s
                        Total time: 17488.24s
                               ETA: 1348794.1s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.458s, learning 0.172s)
               Value function loss: 3.4224
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 4.20
               Mean episode length: 124.95
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 12.63s
                        Total time: 17500.87s
                               ETA: 1348700.8s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1212 steps/s (collection: 13.307s, learning 0.205s)
               Value function loss: 27.1862
                    Surrogate loss: 0.1448
             Mean action noise std: 0.74
                       Mean reward: 5.89
               Mean episode length: 125.00
                  Mean reward/step: -0.69
       Mean episode length/episode: 4.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 13.51s
                        Total time: 17514.38s
                               ETA: 1348675.6s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.653s, learning 0.266s)
               Value function loss: 0.4121
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 5.85
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 12.92s
                        Total time: 17527.30s
                               ETA: 1348604.8s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.452s, learning 0.251s)
               Value function loss: 0.4198
                    Surrogate loss: 0.0272
             Mean action noise std: 0.74
                       Mean reward: 6.46
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 12.70s
                        Total time: 17540.00s
                               ETA: 1348517.4s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.765s, learning 0.160s)
               Value function loss: 0.6030
                    Surrogate loss: 0.0158
             Mean action noise std: 0.74
                       Mean reward: 6.58
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 12.92s
                        Total time: 17552.93s
                               ETA: 1348447.3s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.375s, learning 0.189s)
               Value function loss: 0.7722
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 6.60
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 12.56s
                        Total time: 17565.49s
                               ETA: 1348349.5s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.611s, learning 0.204s)
               Value function loss: 0.9272
                    Surrogate loss: 0.0040
             Mean action noise std: 0.74
                       Mean reward: 6.21
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 12.82s
                        Total time: 17578.31s
                               ETA: 1348271.1s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.611s, learning 0.199s)
               Value function loss: 0.8827
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 5.67
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 12.81s
                        Total time: 17591.12s
                               ETA: 1348192.5s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.410s, learning 0.211s)
               Value function loss: 1.3382
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 4.63
               Mean episode length: 123.66
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 12.62s
                        Total time: 17603.74s
                               ETA: 1348099.5s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.497s, learning 0.208s)
               Value function loss: 3.0025
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 6.82
               Mean episode length: 124.40
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 12.70s
                        Total time: 17616.44s
                               ETA: 1348013.0s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.346s, learning 0.211s)
               Value function loss: 2.9340
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 4.39
               Mean episode length: 124.48
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 12.56s
                        Total time: 17629.00s
                               ETA: 1347915.3s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.508s, learning 0.159s)
               Value function loss: 2.4835
                    Surrogate loss: 0.0043
             Mean action noise std: 0.74
                       Mean reward: 5.70
               Mean episode length: 124.13
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 12.67s
                        Total time: 17641.67s
                               ETA: 1347826.1s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.736s, learning 0.188s)
               Value function loss: 2.2656
                    Surrogate loss: 0.0038
             Mean action noise std: 0.74
                       Mean reward: 6.62
               Mean episode length: 124.26
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 12.92s
                        Total time: 17654.59s
                               ETA: 1347756.7s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.300s, learning 0.174s)
               Value function loss: 2.0247
                    Surrogate loss: -0.0002
             Mean action noise std: 0.74
                       Mean reward: 4.21
               Mean episode length: 124.68
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 12.47s
                        Total time: 17667.07s
                               ETA: 1347653.0s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.192s, learning 0.212s)
               Value function loss: 2.7350
                    Surrogate loss: -0.0004
             Mean action noise std: 0.74
                       Mean reward: 0.16
               Mean episode length: 124.34
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 12.40s
                        Total time: 17679.47s
                               ETA: 1347544.2s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.666s, learning 0.180s)
               Value function loss: 2.7317
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 2.15
               Mean episode length: 124.86
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 12.85s
                        Total time: 17692.32s
                               ETA: 1347469.1s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1191 steps/s (collection: 13.500s, learning 0.249s)
               Value function loss: 48.3271
                    Surrogate loss: 0.0261
             Mean action noise std: 0.74
                       Mean reward: 1.19
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 4.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 13.75s
                        Total time: 17706.06s
                               ETA: 1347462.9s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.079s, learning 0.206s)
               Value function loss: 0.7994
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 1.08
               Mean episode length: 125.00
                  Mean reward/step: -0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 12.29s
                        Total time: 17718.35s
                               ETA: 1347345.3s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.473s, learning 0.214s)
               Value function loss: 0.4993
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 0.11
               Mean episode length: 124.43
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 12.69s
                        Total time: 17731.04s
                               ETA: 1347258.4s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.601s, learning 0.165s)
               Value function loss: 0.5593
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 0.33
               Mean episode length: 124.43
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 12.77s
                        Total time: 17743.80s
                               ETA: 1347177.7s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.923s, learning 0.169s)
               Value function loss: 0.6537
                    Surrogate loss: 0.0162
             Mean action noise std: 0.74
                       Mean reward: -0.16
               Mean episode length: 124.43
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 13.09s
                        Total time: 17756.89s
                               ETA: 1347121.8s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.821s, learning 0.166s)
               Value function loss: 0.9217
                    Surrogate loss: 0.0207
             Mean action noise std: 0.74
                       Mean reward: -0.54
               Mean episode length: 124.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 12.99s
                        Total time: 17769.88s
                               ETA: 1347058.0s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.877s, learning 0.183s)
               Value function loss: 0.8661
                    Surrogate loss: 0.0078
             Mean action noise std: 0.74
                       Mean reward: -0.64
               Mean episode length: 124.43
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 13.06s
                        Total time: 17782.94s
                               ETA: 1346999.9s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.473s, learning 0.176s)
               Value function loss: 1.2210
                    Surrogate loss: 0.0077
             Mean action noise std: 0.74
                       Mean reward: -2.59
               Mean episode length: 123.76
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 12.65s
                        Total time: 17795.59s
                               ETA: 1346910.6s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.310s, learning 0.161s)
               Value function loss: 2.1619
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: -4.00
               Mean episode length: 123.76
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 12.47s
                        Total time: 17808.06s
                               ETA: 1346808.0s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.261s, learning 0.272s)
               Value function loss: 2.6629
                    Surrogate loss: 0.0094
             Mean action noise std: 0.74
                       Mean reward: -6.93
               Mean episode length: 123.47
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 12.53s
                        Total time: 17820.59s
                               ETA: 1346710.2s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.306s, learning 0.162s)
               Value function loss: 1.9888
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: -4.58
               Mean episode length: 124.10
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 12.47s
                        Total time: 17833.06s
                               ETA: 1346607.7s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.572s, learning 0.159s)
               Value function loss: 1.8507
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: -4.32
               Mean episode length: 123.33
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 12.73s
                        Total time: 17845.79s
                               ETA: 1346525.2s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.436s, learning 0.214s)
               Value function loss: 2.6844
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: -4.19
               Mean episode length: 124.11
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 12.65s
                        Total time: 17858.45s
                               ETA: 1346436.7s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.121s, learning 0.173s)
               Value function loss: 3.8612
                    Surrogate loss: 0.0093
             Mean action noise std: 0.74
                       Mean reward: -4.39
               Mean episode length: 124.02
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 12.29s
                        Total time: 17870.74s
                               ETA: 1346321.5s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.456s, learning 0.183s)
               Value function loss: 2.7102
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: -6.16
               Mean episode length: 124.61
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 12.64s
                        Total time: 17883.38s
                               ETA: 1346232.3s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.452s, learning 0.168s)
               Value function loss: 2.9112
                    Surrogate loss: 0.0136
             Mean action noise std: 0.74
                       Mean reward: -5.25
               Mean episode length: 124.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 12.62s
                        Total time: 17896.00s
                               ETA: 1346141.9s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.750s, learning 0.169s)
               Value function loss: 28.6403
                    Surrogate loss: 0.0177
             Mean action noise std: 0.74
                       Mean reward: -4.61
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 4.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 12.92s
                        Total time: 17908.92s
                               ETA: 1346074.0s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.499s, learning 0.166s)
               Value function loss: 0.2584
                    Surrogate loss: -0.0216
             Mean action noise std: 0.74
                       Mean reward: -4.85
               Mean episode length: 124.35
                  Mean reward/step: -0.75
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 11.66s
                        Total time: 17920.58s
                               ETA: 1345912.1s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.483s, learning 0.179s)
               Value function loss: 0.3352
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: -4.20
               Mean episode length: 124.35
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 12.66s
                        Total time: 17933.24s
                               ETA: 1345825.2s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.625s, learning 0.163s)
               Value function loss: 0.8578
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: -3.03
               Mean episode length: 124.35
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 12.79s
                        Total time: 17946.03s
                               ETA: 1345747.9s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.410s, learning 0.210s)
               Value function loss: 1.2721
                    Surrogate loss: 0.0048
             Mean action noise std: 0.74
                       Mean reward: -2.59
               Mean episode length: 124.35
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 12.62s
                        Total time: 17958.65s
                               ETA: 1345658.0s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.895s, learning 0.180s)
               Value function loss: 1.2989
                    Surrogate loss: 0.0187
             Mean action noise std: 0.74
                       Mean reward: -2.73
               Mean episode length: 124.35
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 12.08s
                        Total time: 17970.73s
                               ETA: 1345527.5s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.232s, learning 0.190s)
               Value function loss: 0.9387
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: -2.17
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 12.42s
                        Total time: 17983.15s
                               ETA: 1345423.2s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.216s, learning 0.161s)
               Value function loss: 1.3239
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: -0.66
               Mean episode length: 124.35
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 12.38s
                        Total time: 17995.53s
                               ETA: 1345315.7s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.404s, learning 0.197s)
               Value function loss: 2.3767
                    Surrogate loss: 0.0040
             Mean action noise std: 0.74
                       Mean reward: 2.64
               Mean episode length: 124.44
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 12.60s
                        Total time: 18008.13s
                               ETA: 1345224.9s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.496s, learning 0.255s)
               Value function loss: 3.3951
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 2.91
               Mean episode length: 123.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 12.75s
                        Total time: 18020.88s
                               ETA: 1345145.6s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.460s, learning 0.204s)
               Value function loss: 2.4617
                    Surrogate loss: 0.0047
             Mean action noise std: 0.74
                       Mean reward: 1.83
               Mean episode length: 124.26
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 12.66s
                        Total time: 18033.54s
                               ETA: 1345059.8s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.519s, learning 0.157s)
               Value function loss: 2.3645
                    Surrogate loss: -0.0005
             Mean action noise std: 0.74
                       Mean reward: -0.98
               Mean episode length: 123.35
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 12.68s
                        Total time: 18046.22s
                               ETA: 1344974.9s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.232s, learning 0.188s)
               Value function loss: 2.4525
                    Surrogate loss: -0.0001
             Mean action noise std: 0.74
                       Mean reward: -0.77
               Mean episode length: 123.53
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 12.42s
                        Total time: 18058.64s
                               ETA: 1344871.2s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.733s, learning 0.159s)
               Value function loss: 2.8988
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 1.64
               Mean episode length: 124.34
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 12.89s
                        Total time: 18071.53s
                               ETA: 1344802.7s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.446s, learning 0.184s)
               Value function loss: 3.5662
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 4.70
               Mean episode length: 124.84
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 12.63s
                        Total time: 18084.16s
                               ETA: 1344714.8s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.448s, learning 0.190s)
               Value function loss: 9.2570
                    Surrogate loss: 0.0183
             Mean action noise std: 0.74
                       Mean reward: 4.62
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 12.64s
                        Total time: 18096.80s
                               ETA: 1344627.7s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1228 steps/s (collection: 13.182s, learning 0.157s)
               Value function loss: 23.0844
                    Surrogate loss: 0.0056
             Mean action noise std: 0.74
                       Mean reward: 4.86
               Mean episode length: 125.00
                  Mean reward/step: -0.84
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 13.34s
                        Total time: 18110.14s
                               ETA: 1344592.7s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.505s, learning 0.209s)
               Value function loss: 0.5127
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 4.30
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 12.71s
                        Total time: 18122.85s
                               ETA: 1344511.3s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.072s, learning 0.208s)
               Value function loss: 0.4775
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 3.89
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 12.28s
                        Total time: 18135.13s
                               ETA: 1344397.9s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.450s, learning 0.164s)
               Value function loss: 0.5788
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 2.61
               Mean episode length: 124.32
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 12.61s
                        Total time: 18147.75s
                               ETA: 1344309.4s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.910s, learning 0.157s)
               Value function loss: 1.0750
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 1.52
               Mean episode length: 123.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 12.07s
                        Total time: 18159.81s
                               ETA: 1344180.5s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.904s, learning 0.165s)
               Value function loss: 1.6488
                    Surrogate loss: 0.0020
             Mean action noise std: 0.74
                       Mean reward: -0.27
               Mean episode length: 123.36
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 13.07s
                        Total time: 18172.88s
                               ETA: 1344125.9s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.381s, learning 0.160s)
               Value function loss: 3.1904
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: -4.01
               Mean episode length: 122.67
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 12.54s
                        Total time: 18185.42s
                               ETA: 1344032.2s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.236s, learning 0.203s)
               Value function loss: 1.7292
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: -7.52
               Mean episode length: 122.48
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 12.44s
                        Total time: 18197.86s
                               ETA: 1343931.3s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.656s, learning 0.184s)
               Value function loss: 2.9027
                    Surrogate loss: 0.0040
             Mean action noise std: 0.74
                       Mean reward: -2.12
               Mean episode length: 124.96
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 12.84s
                        Total time: 18210.70s
                               ETA: 1343860.1s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.690s, learning 0.158s)
               Value function loss: 2.2699
                    Surrogate loss: 0.0005
             Mean action noise std: 0.74
                       Mean reward: -2.80
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 12.85s
                        Total time: 18223.55s
                               ETA: 1343789.5s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.157s, learning 0.195s)
               Value function loss: 1.8447
                    Surrogate loss: 0.0071
             Mean action noise std: 0.74
                       Mean reward: -5.09
               Mean episode length: 124.91
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 12.35s
                        Total time: 18235.90s
                               ETA: 1343682.4s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.042s, learning 0.189s)
               Value function loss: 2.5938
                    Surrogate loss: 0.0038
             Mean action noise std: 0.74
                       Mean reward: -6.85
               Mean episode length: 124.07
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 12.23s
                        Total time: 18248.13s
                               ETA: 1343566.6s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.476s, learning 0.162s)
               Value function loss: 3.0854
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: -4.29
               Mean episode length: 123.98
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 12.64s
                        Total time: 18260.77s
                               ETA: 1343480.8s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.861s, learning 0.243s)
               Value function loss: 4.6732
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: -2.74
               Mean episode length: 124.30
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 13.10s
                        Total time: 18273.88s
                               ETA: 1343429.4s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.439s, learning 0.190s)
               Value function loss: 5.3363
                    Surrogate loss: 0.0094
             Mean action noise std: 0.74
                       Mean reward: -5.24
               Mean episode length: 124.78
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 12.63s
                        Total time: 18286.51s
                               ETA: 1343343.3s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1240 steps/s (collection: 12.896s, learning 0.312s)
               Value function loss: 31.0366
                    Surrogate loss: 0.0167
             Mean action noise std: 0.74
                       Mean reward: -3.91
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 4.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 13.21s
                        Total time: 18299.71s
                               ETA: 1343299.7s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.637s, learning 0.187s)
               Value function loss: 1.8224
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: -3.55
               Mean episode length: 125.00
                  Mean reward/step: -0.81
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 12.82s
                        Total time: 18312.54s
                               ETA: 1343228.1s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.385s, learning 0.169s)
               Value function loss: 1.2719
                    Surrogate loss: -0.0003
             Mean action noise std: 0.74
                       Mean reward: -3.33
               Mean episode length: 125.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 12.55s
                        Total time: 18325.09s
                               ETA: 1343136.7s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.611s, learning 0.286s)
               Value function loss: 1.2050
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: -3.39
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 12.90s
                        Total time: 18337.99s
                               ETA: 1343070.5s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.976s, learning 0.209s)
               Value function loss: 1.5258
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: -3.07
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 12.18s
                        Total time: 18350.17s
                               ETA: 1342952.3s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.389s, learning 0.296s)
               Value function loss: 1.8195
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: -3.37
               Mean episode length: 124.62
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 12.68s
                        Total time: 18362.86s
                               ETA: 1342870.8s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.460s, learning 0.166s)
               Value function loss: 47.2835
                    Surrogate loss: 0.0045
             Mean action noise std: 0.74
                       Mean reward: -20.56
               Mean episode length: 124.62
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 12.63s
                        Total time: 18375.48s
                               ETA: 1342785.1s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.201s, learning 0.159s)
               Value function loss: 1.4762
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: -20.53
               Mean episode length: 124.62
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 12.36s
                        Total time: 18387.84s
                               ETA: 1342680.1s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.437s, learning 0.219s)
               Value function loss: 1.9959
                    Surrogate loss: 0.0044
             Mean action noise std: 0.74
                       Mean reward: -18.81
               Mean episode length: 124.62
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 12.66s
                        Total time: 18400.50s
                               ETA: 1342596.9s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.732s, learning 0.243s)
               Value function loss: 2.8515
                    Surrogate loss: 0.0084
             Mean action noise std: 0.74
                       Mean reward: -1.59
               Mean episode length: 124.48
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 12.98s
                        Total time: 18413.48s
                               ETA: 1342537.0s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.158s, learning 0.183s)
               Value function loss: 2.0751
                    Surrogate loss: 0.0061
             Mean action noise std: 0.74
                       Mean reward: 0.72
               Mean episode length: 124.48
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 12.34s
                        Total time: 18425.82s
                               ETA: 1342431.0s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.159s, learning 0.162s)
               Value function loss: 2.2583
                    Surrogate loss: 0.0012
             Mean action noise std: 0.74
                       Mean reward: -0.38
               Mean episode length: 124.60
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 12.32s
                        Total time: 18438.14s
                               ETA: 1342323.6s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.493s, learning 0.195s)
               Value function loss: 2.3742
                    Surrogate loss: 0.0087
             Mean action noise std: 0.74
                       Mean reward: -1.02
               Mean episode length: 124.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 12.69s
                        Total time: 18450.83s
                               ETA: 1342243.1s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.609s, learning 0.181s)
               Value function loss: 2.7731
                    Surrogate loss: 0.0006
             Mean action noise std: 0.74
                       Mean reward: -0.71
               Mean episode length: 124.59
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 12.79s
                        Total time: 18463.61s
                               ETA: 1342170.1s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.347s, learning 0.174s)
               Value function loss: 6.1611
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 1.96
               Mean episode length: 124.90
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 12.52s
                        Total time: 18476.14s
                               ETA: 1342077.7s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.741s, learning 0.209s)
               Value function loss: 4.7704
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 1.88
               Mean episode length: 124.84
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 12.95s
                        Total time: 18489.09s
                               ETA: 1342016.5s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1218 steps/s (collection: 13.271s, learning 0.171s)
               Value function loss: 31.5416
                    Surrogate loss: 0.0834
             Mean action noise std: 0.74
                       Mean reward: 3.21
               Mean episode length: 124.65
                  Mean reward/step: -0.59
       Mean episode length/episode: 4.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 13.44s
                        Total time: 18502.53s
                               ETA: 1341991.1s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.341s, learning 0.167s)
               Value function loss: 1.4321
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 2.52
               Mean episode length: 123.93
                  Mean reward/step: -0.71
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 12.51s
                        Total time: 18515.04s
                               ETA: 1341897.9s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.631s, learning 0.164s)
               Value function loss: 0.8954
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 2.19
               Mean episode length: 123.93
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 12.79s
                        Total time: 18527.83s
                               ETA: 1341825.7s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.786s, learning 0.166s)
               Value function loss: 0.9184
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 1.79
               Mean episode length: 123.93
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 12.95s
                        Total time: 18540.78s
                               ETA: 1341765.0s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.296s, learning 0.349s)
               Value function loss: 113.0617
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: -10.73
               Mean episode length: 123.40
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 12.64s
                        Total time: 18553.43s
                               ETA: 1341682.1s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.428s, learning 0.207s)
               Value function loss: 3.6560
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: -9.43
               Mean episode length: 123.40
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 12.64s
                        Total time: 18566.06s
                               ETA: 1341598.6s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.968s, learning 0.166s)
               Value function loss: 4.9830
                    Surrogate loss: 0.0019
             Mean action noise std: 0.74
                       Mean reward: -11.32
               Mean episode length: 123.08
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 12.13s
                        Total time: 18578.20s
                               ETA: 1341479.0s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.617s, learning 0.164s)
               Value function loss: 2.9954
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: -11.05
               Mean episode length: 123.08
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 12.78s
                        Total time: 18590.98s
                               ETA: 1341406.3s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.361s, learning 0.161s)
               Value function loss: 4.3356
                    Surrogate loss: 0.0084
             Mean action noise std: 0.74
                       Mean reward: 0.96
               Mean episode length: 124.19
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 12.52s
                        Total time: 18603.50s
                               ETA: 1341314.9s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.858s, learning 0.208s)
               Value function loss: 4.1761
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: -0.87
               Mean episode length: 122.58
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 12.07s
                        Total time: 18615.56s
                               ETA: 1341190.9s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.708s, learning 0.164s)
               Value function loss: 3.9796
                    Surrogate loss: 0.0042
             Mean action noise std: 0.74
                       Mean reward: -3.10
               Mean episode length: 122.38
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 12.87s
                        Total time: 18628.44s
                               ETA: 1341125.1s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.577s, learning 0.175s)
               Value function loss: 3.1717
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: -4.21
               Mean episode length: 123.04
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 12.75s
                        Total time: 18641.19s
                               ETA: 1341050.7s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.370s, learning 0.161s)
               Value function loss: 3.4848
                    Surrogate loss: -0.0020
             Mean action noise std: 0.74
                       Mean reward: 2.83
               Mean episode length: 124.17
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 12.53s
                        Total time: 18653.72s
                               ETA: 1340960.5s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.536s, learning 0.168s)
               Value function loss: 99.8804
                    Surrogate loss: 0.0007
             Mean action noise std: 0.74
                       Mean reward: -14.93
               Mean episode length: 124.44
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 12.70s
                        Total time: 18666.42s
                               ETA: 1340882.8s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.313s, learning 0.164s)
               Value function loss: 3.5078
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 2.67
               Mean episode length: 124.93
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 12.48s
                        Total time: 18678.90s
                               ETA: 1340789.0s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1235 steps/s (collection: 13.078s, learning 0.179s)
               Value function loss: 41.1849
                    Surrogate loss: 0.0117
             Mean action noise std: 0.74
                       Mean reward: 3.89
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 4.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 13.26s
                        Total time: 18692.16s
                               ETA: 1340751.2s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.925s, learning 0.281s)
               Value function loss: 0.5930
                    Surrogate loss: -0.0179
             Mean action noise std: 0.74
                       Mean reward: 3.46
               Mean episode length: 124.67
                  Mean reward/step: -0.79
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 12.21s
                        Total time: 18704.37s
                               ETA: 1340638.1s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.151s, learning 0.165s)
               Value function loss: 0.4037
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 2.72
               Mean episode length: 124.67
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 12.32s
                        Total time: 18716.68s
                               ETA: 1340533.0s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.231s, learning 0.158s)
               Value function loss: 0.4647
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 124.31
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 12.39s
                        Total time: 18729.07s
                               ETA: 1340433.3s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.885s, learning 0.162s)
               Value function loss: 0.6910
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 1.31
               Mean episode length: 123.69
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 12.05s
                        Total time: 18741.12s
                               ETA: 1340309.3s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.071s, learning 0.159s)
               Value function loss: 1.1213
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 1.83
               Mean episode length: 123.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 12.23s
                        Total time: 18753.35s
                               ETA: 1340198.5s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.167s, learning 0.261s)
               Value function loss: 1.0709
                    Surrogate loss: 0.0077
             Mean action noise std: 0.74
                       Mean reward: 1.92
               Mean episode length: 123.69
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 12.43s
                        Total time: 18765.78s
                               ETA: 1340102.0s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.382s, learning 0.160s)
               Value function loss: 1.1296
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 2.61
               Mean episode length: 123.39
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 12.54s
                        Total time: 18778.32s
                               ETA: 1340013.6s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.637s, learning 0.164s)
               Value function loss: 1.8956
                    Surrogate loss: 0.0051
             Mean action noise std: 0.74
                       Mean reward: 3.20
               Mean episode length: 123.72
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 12.80s
                        Total time: 18791.12s
                               ETA: 1339944.0s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.581s, learning 0.171s)
               Value function loss: 3.2116
                    Surrogate loss: -0.0020
             Mean action noise std: 0.74
                       Mean reward: 4.95
               Mean episode length: 123.88
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 12.75s
                        Total time: 18803.87s
                               ETA: 1339870.8s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.671s, learning 0.157s)
               Value function loss: 2.9145
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 6.82
               Mean episode length: 124.47
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 12.83s
                        Total time: 18816.70s
                               ETA: 1339803.2s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.991s, learning 0.156s)
               Value function loss: 2.6208
                    Surrogate loss: 0.0004
             Mean action noise std: 0.74
                       Mean reward: 6.35
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 12.15s
                        Total time: 18828.85s
                               ETA: 1339687.3s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.493s, learning 0.158s)
               Value function loss: 2.8241
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 5.70
               Mean episode length: 123.93
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 12.65s
                        Total time: 18841.50s
                               ETA: 1339607.3s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.180s, learning 0.156s)
               Value function loss: 3.2414
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 9.03
               Mean episode length: 124.77
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 12.34s
                        Total time: 18853.83s
                               ETA: 1339505.1s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.198s, learning 0.211s)
               Value function loss: 3.6085
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 5.95
               Mean episode length: 124.72
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 12.41s
                        Total time: 18866.24s
                               ETA: 1339408.1s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.494s, learning 0.201s)
               Value function loss: 4.3011
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 7.75
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 12.69s
                        Total time: 18878.94s
                               ETA: 1339331.6s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1206 steps/s (collection: 13.418s, learning 0.156s)
               Value function loss: 64.8606
                    Surrogate loss: 0.0098
             Mean action noise std: 0.74
                       Mean reward: 8.78
               Mean episode length: 125.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 4.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 13.57s
                        Total time: 18892.51s
                               ETA: 1339317.5s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.484s, learning 0.366s)
               Value function loss: 0.4763
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 8.14
               Mean episode length: 124.36
                  Mean reward/step: -0.74
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 12.85s
                        Total time: 18905.36s
                               ETA: 1339252.0s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.660s, learning 0.257s)
               Value function loss: 0.4553
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 8.38
               Mean episode length: 124.36
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 12.92s
                        Total time: 18918.28s
                               ETA: 1339191.4s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.478s, learning 0.264s)
               Value function loss: 0.4628
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 7.44
               Mean episode length: 124.36
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 12.74s
                        Total time: 18931.02s
                               ETA: 1339118.5s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.358s, learning 0.223s)
               Value function loss: 0.7201
                    Surrogate loss: -0.0019
             Mean action noise std: 0.74
                       Mean reward: 5.64
               Mean episode length: 124.36
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 12.58s
                        Total time: 18943.60s
                               ETA: 1339034.3s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.351s, learning 0.215s)
               Value function loss: 0.8922
                    Surrogate loss: 0.0248
             Mean action noise std: 0.74
                       Mean reward: 4.64
               Mean episode length: 124.36
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 12.57s
                        Total time: 18956.17s
                               ETA: 1338949.1s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.226s, learning 0.170s)
               Value function loss: 1.0937
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 4.39
               Mean episode length: 124.36
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 12.40s
                        Total time: 18968.56s
                               ETA: 1338852.1s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.558s, learning 0.157s)
               Value function loss: 1.8132
                    Surrogate loss: 0.0020
             Mean action noise std: 0.74
                       Mean reward: 3.50
               Mean episode length: 123.76
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 12.72s
                        Total time: 18981.28s
                               ETA: 1338777.7s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.187s, learning 0.180s)
               Value function loss: 2.6622
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 4.17
               Mean episode length: 123.16
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 12.37s
                        Total time: 18993.65s
                               ETA: 1338678.7s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.851s, learning 0.253s)
               Value function loss: 3.1316
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 2.94
               Mean episode length: 123.09
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 13.10s
                        Total time: 19006.75s
                               ETA: 1338631.8s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.547s, learning 0.172s)
               Value function loss: 5.6583
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 1.75
               Mean episode length: 123.19
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 12.72s
                        Total time: 19019.47s
                               ETA: 1338557.9s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.470s, learning 0.228s)
               Value function loss: 3.6287
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 6.18
               Mean episode length: 124.31
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 12.70s
                        Total time: 19032.17s
                               ETA: 1338482.6s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.857s, learning 0.162s)
               Value function loss: 3.1666
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 4.23
               Mean episode length: 123.44
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 13.02s
                        Total time: 19045.19s
                               ETA: 1338430.0s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.960s, learning 0.200s)
               Value function loss: 3.5807
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 1.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 12.16s
                        Total time: 19057.35s
                               ETA: 1338317.1s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.260s, learning 0.170s)
               Value function loss: 3.5345
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 12.43s
                        Total time: 19069.78s
                               ETA: 1338223.3s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.133s, learning 0.211s)
               Value function loss: 2.9572
                    Surrogate loss: 0.0085
             Mean action noise std: 0.74
                       Mean reward: 3.32
               Mean episode length: 124.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 12.34s
                        Total time: 19082.12s
                               ETA: 1338123.6s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 659 steps/s (collection: 24.674s, learning 0.170s)
               Value function loss: 31.5535
                    Surrogate loss: 0.0124
             Mean action noise std: 0.74
                       Mean reward: 3.31
               Mean episode length: 124.92
                  Mean reward/step: -0.66
       Mean episode length/episode: 4.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 24.84s
                        Total time: 19106.97s
                               ETA: 1338899.9s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.136s, learning 0.226s)
               Value function loss: 0.3661
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 3.27
               Mean episode length: 124.92
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 23.36s
                        Total time: 19130.33s
                               ETA: 1339571.3s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.568s, learning 0.189s)
               Value function loss: 0.4475
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 2.90
               Mean episode length: 124.92
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 23.76s
                        Total time: 19154.08s
                               ETA: 1340269.4s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.310s, learning 0.168s)
               Value function loss: 0.7987
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 2.94
               Mean episode length: 124.92
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 23.48s
                        Total time: 19177.56s
                               ETA: 1340946.8s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 707 steps/s (collection: 23.003s, learning 0.167s)
               Value function loss: 1.2744
                    Surrogate loss: 0.0035
             Mean action noise std: 0.74
                       Mean reward: 2.06
               Mean episode length: 124.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 23.17s
                        Total time: 19200.73s
                               ETA: 1341601.8s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 702 steps/s (collection: 23.153s, learning 0.164s)
               Value function loss: 1.2230
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 2.94
               Mean episode length: 124.92
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 23.32s
                        Total time: 19224.05s
                               ETA: 1342266.2s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.366s, learning 0.294s)
               Value function loss: 1.3675
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 4.64
               Mean episode length: 124.92
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 23.66s
                        Total time: 19247.71s
                               ETA: 1342953.4s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.806s, learning 0.173s)
               Value function loss: 2.4885
                    Surrogate loss: 0.0142
             Mean action noise std: 0.74
                       Mean reward: 5.92
               Mean episode length: 124.13
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 23.98s
                        Total time: 19271.69s
                               ETA: 1343662.0s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.465s, learning 0.192s)
               Value function loss: 3.6924
                    Surrogate loss: -0.0041
             Mean action noise std: 0.74
                       Mean reward: 12.86
               Mean episode length: 124.46
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 23.66s
                        Total time: 19295.35s
                               ETA: 1344347.0s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 677 steps/s (collection: 24.026s, learning 0.162s)
               Value function loss: 3.3964
                    Surrogate loss: 0.0082
             Mean action noise std: 0.74
                       Mean reward: 11.10
               Mean episode length: 124.27
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 24.19s
                        Total time: 19319.53s
                               ETA: 1345068.0s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.142s, learning 0.233s)
               Value function loss: 2.4297
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 9.74
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 23.37s
                        Total time: 19342.91s
                               ETA: 1345731.4s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 665 steps/s (collection: 24.399s, learning 0.209s)
               Value function loss: 3.6121
                    Surrogate loss: 0.0033
             Mean action noise std: 0.74
                       Mean reward: 8.63
               Mean episode length: 124.70
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 24.61s
                        Total time: 19367.52s
                               ETA: 1346479.5s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 670 steps/s (collection: 24.258s, learning 0.168s)
               Value function loss: 3.6480
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 7.05
               Mean episode length: 124.17
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 24.43s
                        Total time: 19391.94s
                               ETA: 1347214.0s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 677 steps/s (collection: 24.014s, learning 0.172s)
               Value function loss: 3.9096
                    Surrogate loss: 0.0005
             Mean action noise std: 0.74
                       Mean reward: 9.50
               Mean episode length: 124.93
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 24.19s
                        Total time: 19416.13s
                               ETA: 1347930.6s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.126s, learning 0.168s)
               Value function loss: 3.9684
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 8.94
               Mean episode length: 124.92
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 23.29s
                        Total time: 19439.42s
                               ETA: 1348584.3s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.561s, learning 0.172s)
               Value function loss: 56.3307
                    Surrogate loss: 0.0393
             Mean action noise std: 0.74
                       Mean reward: 10.41
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 4.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 23.73s
                        Total time: 19463.16s
                               ETA: 1349267.6s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.517s, learning 0.173s)
               Value function loss: 1.6087
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 9.60
               Mean episode length: 125.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 23.69s
                        Total time: 19486.85s
                               ETA: 1349946.8s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.800s, learning 0.265s)
               Value function loss: 1.5387
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 9.43
               Mean episode length: 125.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 24.07s
                        Total time: 19510.91s
                               ETA: 1350651.1s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.127s, learning 0.166s)
               Value function loss: 1.0185
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 9.89
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 23.29s
                        Total time: 19534.20s
                               ETA: 1351300.9s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.116s, learning 0.163s)
               Value function loss: 3.7059
                    Surrogate loss: 0.0002
             Mean action noise std: 0.74
                       Mean reward: 9.95
               Mean episode length: 124.53
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 23.28s
                        Total time: 19557.48s
                               ETA: 1351948.8s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.416s, learning 0.209s)
               Value function loss: 1.7274
                    Surrogate loss: 0.0334
             Mean action noise std: 0.74
                       Mean reward: 10.70
               Mean episode length: 124.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 23.62s
                        Total time: 19581.11s
                               ETA: 1352619.6s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 707 steps/s (collection: 22.855s, learning 0.309s)
               Value function loss: 1.4034
                    Surrogate loss: 0.0206
             Mean action noise std: 0.74
                       Mean reward: 11.31
               Mean episode length: 124.53
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 23.16s
                        Total time: 19604.27s
                               ETA: 1353257.6s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.958s, learning 0.174s)
               Value function loss: 1.8213
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 10.28
               Mean episode length: 124.53
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 24.13s
                        Total time: 19628.40s
                               ETA: 1353961.6s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.274s, learning 0.175s)
               Value function loss: 4.1029
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 8.90
               Mean episode length: 124.53
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 23.45s
                        Total time: 19651.85s
                               ETA: 1354617.4s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.219s, learning 0.172s)
               Value function loss: 4.0265
                    Surrogate loss: 0.0010
             Mean action noise std: 0.74
                       Mean reward: 3.45
               Mean episode length: 124.44
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 23.39s
                        Total time: 19675.24s
                               ETA: 1355268.2s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.505s, learning 0.261s)
               Value function loss: 2.4992
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: 8.12
               Mean episode length: 124.56
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 23.77s
                        Total time: 19699.01s
                               ETA: 1355943.8s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 706 steps/s (collection: 23.021s, learning 0.185s)
               Value function loss: 2.7200
                    Surrogate loss: 0.0092
             Mean action noise std: 0.74
                       Mean reward: 6.08
               Mean episode length: 124.14
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 23.21s
                        Total time: 19722.22s
                               ETA: 1356580.1s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.582s, learning 0.171s)
               Value function loss: 3.1483
                    Surrogate loss: 0.0094
             Mean action noise std: 0.74
                       Mean reward: 9.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 23.75s
                        Total time: 19745.97s
                               ETA: 1357253.0s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.171s, learning 0.266s)
               Value function loss: 3.6642
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 9.72
               Mean episode length: 124.76
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 23.44s
                        Total time: 19769.40s
                               ETA: 1357903.2s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.694s, learning 0.179s)
               Value function loss: 4.4956
                    Surrogate loss: 0.0050
             Mean action noise std: 0.74
                       Mean reward: 5.36
               Mean episode length: 124.52
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 23.87s
                        Total time: 19793.28s
                               ETA: 1358582.5s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 682 steps/s (collection: 23.833s, learning 0.168s)
               Value function loss: 4.0678
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 11.10
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 24.00s
                        Total time: 19817.28s
                               ETA: 1359269.5s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 669 steps/s (collection: 24.265s, learning 0.208s)
               Value function loss: 31.4948
                    Surrogate loss: 0.0261
             Mean action noise std: 0.74
                       Mean reward: 10.48
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 4.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 24.47s
                        Total time: 19841.75s
                               ETA: 1359987.9s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.518s, learning 0.273s)
               Value function loss: 0.6279
                    Surrogate loss: -0.0173
             Mean action noise std: 0.74
                       Mean reward: 10.00
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 23.79s
                        Total time: 19865.54s
                               ETA: 1360658.6s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.172s, learning 0.170s)
               Value function loss: 0.7033
                    Surrogate loss: 0.0056
             Mean action noise std: 0.74
                       Mean reward: 9.87
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 23.34s
                        Total time: 19888.89s
                               ETA: 1361297.5s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.254s, learning 0.285s)
               Value function loss: 0.8035
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 9.42
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 23.54s
                        Total time: 19912.42s
                               ETA: 1361949.0s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.955s, learning 0.188s)
               Value function loss: 2.0762
                    Surrogate loss: 0.0012
             Mean action noise std: 0.74
                       Mean reward: 9.33
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 24.14s
                        Total time: 19936.57s
                               ETA: 1362640.9s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 681 steps/s (collection: 23.869s, learning 0.183s)
               Value function loss: 2.2778
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 8.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 24.05s
                        Total time: 19960.62s
                               ETA: 1363325.6s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 814 steps/s (collection: 19.952s, learning 0.173s)
               Value function loss: 1.6885
                    Surrogate loss: 0.0161
             Mean action noise std: 0.74
                       Mean reward: 9.45
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 20.13s
                        Total time: 19980.75s
                               ETA: 1363741.2s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.382s, learning 0.261s)
               Value function loss: 1.7906
                    Surrogate loss: 0.0089
             Mean action noise std: 0.74
                       Mean reward: 10.70
               Mean episode length: 124.56
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 12.64s
                        Total time: 19993.39s
                               ETA: 1363645.9s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.549s, learning 0.161s)
               Value function loss: 2.6347
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 9.81
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 12.71s
                        Total time: 20006.10s
                               ETA: 1363555.4s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.633s, learning 0.163s)
               Value function loss: 3.5179
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 8.55
               Mean episode length: 124.45
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 12.80s
                        Total time: 20018.89s
                               ETA: 1363470.7s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.395s, learning 0.233s)
               Value function loss: 4.4487
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 12.83
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 12.63s
                        Total time: 20031.52s
                               ETA: 1363374.8s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.705s, learning 0.162s)
               Value function loss: 7.4658
                    Surrogate loss: 0.0661
             Mean action noise std: 0.74
                       Mean reward: 9.28
               Mean episode length: 124.74
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 12.87s
                        Total time: 20044.39s
                               ETA: 1363295.1s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.458s, learning 0.156s)
               Value function loss: 4.0559
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 12.13
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 12.61s
                        Total time: 20057.00s
                               ETA: 1363198.5s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.172s, learning 0.165s)
               Value function loss: 3.9883
                    Surrogate loss: 0.0166
             Mean action noise std: 0.74
                       Mean reward: 10.35
               Mean episode length: 124.80
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 12.34s
                        Total time: 20069.34s
                               ETA: 1363083.0s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.233s, learning 0.190s)
               Value function loss: 4.1278
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 9.44
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 12.42s
                        Total time: 20081.76s
                               ETA: 1362973.6s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.254s, learning 0.180s)
               Value function loss: 26.0584
                    Surrogate loss: 0.0402
             Mean action noise std: 0.74
                       Mean reward: 12.10
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 12.43s
                        Total time: 20094.20s
                               ETA: 1362865.1s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1236 steps/s (collection: 13.052s, learning 0.199s)
               Value function loss: 24.3443
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 11.91
               Mean episode length: 125.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 13.25s
                        Total time: 20107.45s
                               ETA: 1362812.1s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.383s, learning 0.218s)
               Value function loss: 0.6637
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 11.95
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 12.60s
                        Total time: 20120.05s
                               ETA: 1362715.1s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.250s, learning 0.169s)
               Value function loss: 0.9095
                    Surrogate loss: 0.0007
             Mean action noise std: 0.74
                       Mean reward: 12.92
               Mean episode length: 125.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 12.42s
                        Total time: 20132.47s
                               ETA: 1362605.8s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.024s, learning 0.170s)
               Value function loss: 1.1657
                    Surrogate loss: 0.0174
             Mean action noise std: 0.74
                       Mean reward: 13.40
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 12.19s
                        Total time: 20144.66s
                               ETA: 1362481.5s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.719s, learning 0.161s)
               Value function loss: 1.7826
                    Surrogate loss: 0.0046
             Mean action noise std: 0.74
                       Mean reward: 11.56
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 12.88s
                        Total time: 20157.54s
                               ETA: 1362403.8s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.350s, learning 0.206s)
               Value function loss: 1.8695
                    Surrogate loss: 0.0221
             Mean action noise std: 0.74
                       Mean reward: 12.01
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 12.56s
                        Total time: 20170.10s
                               ETA: 1362304.2s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.711s, learning 0.163s)
               Value function loss: 1.5873
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 11.84
               Mean episode length: 124.51
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 12.87s
                        Total time: 20182.97s
                               ETA: 1362226.2s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.409s, learning 0.190s)
               Value function loss: 2.4015
                    Surrogate loss: 0.0099
             Mean action noise std: 0.74
                       Mean reward: 10.09
               Mean episode length: 124.51
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 12.60s
                        Total time: 20195.57s
                               ETA: 1362129.7s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.287s, learning 0.239s)
               Value function loss: 4.1801
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 9.53
               Mean episode length: 124.47
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 12.53s
                        Total time: 20208.10s
                               ETA: 1362028.5s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.073s, learning 0.209s)
               Value function loss: 5.8147
                    Surrogate loss: 0.0056
             Mean action noise std: 0.74
                       Mean reward: 5.86
               Mean episode length: 123.29
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 12.28s
                        Total time: 20220.38s
                               ETA: 1361911.0s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.523s, learning 0.168s)
               Value function loss: 3.0283
                    Surrogate loss: -0.0023
             Mean action noise std: 0.74
                       Mean reward: 12.48
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 11.69s
                        Total time: 20232.07s
                               ETA: 1361753.8s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.312s, learning 0.164s)
               Value function loss: 6.8917
                    Surrogate loss: 0.0015
             Mean action noise std: 0.74
                       Mean reward: 5.91
               Mean episode length: 124.12
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 12.48s
                        Total time: 20244.55s
                               ETA: 1361649.5s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.697s, learning 0.255s)
               Value function loss: 3.8613
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 7.50
               Mean episode length: 123.71
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 12.95s
                        Total time: 20257.50s
                               ETA: 1361577.4s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.441s, learning 0.164s)
               Value function loss: 4.5860
                    Surrogate loss: 0.0039
             Mean action noise std: 0.74
                       Mean reward: 10.39
               Mean episode length: 124.79
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 12.61s
                        Total time: 20270.10s
                               ETA: 1361482.1s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.205s, learning 0.160s)
               Value function loss: 4.2290
                    Surrogate loss: -0.0002
             Mean action noise std: 0.74
                       Mean reward: 7.04
               Mean episode length: 124.83
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 12.36s
                        Total time: 20282.47s
                               ETA: 1361370.8s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1191 steps/s (collection: 13.471s, learning 0.283s)
               Value function loss: 38.8349
                    Surrogate loss: 0.0264
             Mean action noise std: 0.74
                       Mean reward: 10.29
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 4.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 13.75s
                        Total time: 20296.22s
                               ETA: 1361352.8s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.917s, learning 0.157s)
               Value function loss: 1.0341
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 9.22
               Mean episode length: 124.38
                  Mean reward/step: -0.73
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 12.07s
                        Total time: 20308.30s
                               ETA: 1361222.2s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.121s, learning 0.226s)
               Value function loss: 0.8186
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 10.07
               Mean episode length: 124.38
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 12.35s
                        Total time: 20320.64s
                               ETA: 1361110.1s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.786s, learning 0.206s)
               Value function loss: 1.0155
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 9.92
               Mean episode length: 123.96
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 12.99s
                        Total time: 20333.63s
                               ETA: 1361041.2s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.418s, learning 0.157s)
               Value function loss: 1.6075
                    Surrogate loss: 0.0021
             Mean action noise std: 0.74
                       Mean reward: 9.03
               Mean episode length: 122.68
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 12.57s
                        Total time: 20346.21s
                               ETA: 1360944.5s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.577s, learning 0.164s)
               Value function loss: 1.9823
                    Surrogate loss: 0.0024
             Mean action noise std: 0.74
                       Mean reward: 8.78
               Mean episode length: 122.68
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 12.74s
                        Total time: 20358.95s
                               ETA: 1360859.1s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.283s, learning 0.200s)
               Value function loss: 1.6892
                    Surrogate loss: 0.0086
             Mean action noise std: 0.74
                       Mean reward: 7.95
               Mean episode length: 122.68
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 12.48s
                        Total time: 20371.43s
                               ETA: 1360756.5s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1239 steps/s (collection: 13.041s, learning 0.179s)
               Value function loss: 2.2669
                    Surrogate loss: 0.0226
             Mean action noise std: 0.74
                       Mean reward: 7.67
               Mean episode length: 122.45
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 13.22s
                        Total time: 20384.65s
                               ETA: 1360703.3s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.335s, learning 0.165s)
               Value function loss: 2.9049
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 5.75
               Mean episode length: 123.51
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 12.50s
                        Total time: 20397.15s
                               ETA: 1360602.1s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.542s, learning 0.208s)
               Value function loss: 4.4391
                    Surrogate loss: 0.0015
             Mean action noise std: 0.74
                       Mean reward: 5.20
               Mean episode length: 122.93
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 12.75s
                        Total time: 20409.90s
                               ETA: 1360517.6s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.583s, learning 0.272s)
               Value function loss: 2.7214
                    Surrogate loss: 0.0021
             Mean action noise std: 0.74
                       Mean reward: 8.01
               Mean episode length: 123.93
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 12.85s
                        Total time: 20422.76s
                               ETA: 1360440.2s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.668s, learning 0.169s)
               Value function loss: 3.2566
                    Surrogate loss: 0.0014
             Mean action noise std: 0.74
                       Mean reward: 8.41
               Mean episode length: 123.56
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 12.84s
                        Total time: 20435.60s
                               ETA: 1360361.7s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.349s, learning 0.181s)
               Value function loss: 3.6120
                    Surrogate loss: 0.0134
             Mean action noise std: 0.74
                       Mean reward: 3.16
               Mean episode length: 123.88
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 12.53s
                        Total time: 20448.13s
                               ETA: 1360263.0s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.391s, learning 0.208s)
               Value function loss: 3.6945
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: 8.89
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 12.60s
                        Total time: 20460.73s
                               ETA: 1360168.9s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.562s, learning 0.181s)
               Value function loss: 4.3458
                    Surrogate loss: 0.0055
             Mean action noise std: 0.74
                       Mean reward: 6.51
               Mean episode length: 124.70
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 12.74s
                        Total time: 20473.47s
                               ETA: 1360084.5s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.578s, learning 0.157s)
               Value function loss: 4.2562
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 7.95
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 11.73s
                        Total time: 20485.20s
                               ETA: 1359933.2s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1245 steps/s (collection: 12.970s, learning 0.188s)
               Value function loss: 31.8267
                    Surrogate loss: 0.0179
             Mean action noise std: 0.74
                       Mean reward: 10.48
               Mean episode length: 125.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 4.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 13.16s
                        Total time: 20498.36s
                               ETA: 1359876.5s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.742s, learning 0.184s)
               Value function loss: 0.9612
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 11.02
               Mean episode length: 125.00
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 12.93s
                        Total time: 20511.29s
                               ETA: 1359804.6s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.293s, learning 0.184s)
               Value function loss: 0.8165
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 9.54
               Mean episode length: 124.47
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 12.48s
                        Total time: 20523.77s
                               ETA: 1359702.9s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.006s, learning 0.166s)
               Value function loss: 1.5014
                    Surrogate loss: -0.0045
             Mean action noise std: 0.74
                       Mean reward: 9.12
               Mean episode length: 124.47
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 12.17s
                        Total time: 20535.94s
                               ETA: 1359581.2s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.078s, learning 0.156s)
               Value function loss: 1.8758
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 8.37
               Mean episode length: 124.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 12.23s
                        Total time: 20548.17s
                               ETA: 1359463.7s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.175s, learning 0.202s)
               Value function loss: 6.3470
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: 6.09
               Mean episode length: 123.51
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 12.38s
                        Total time: 20560.55s
                               ETA: 1359355.8s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.693s, learning 0.187s)
               Value function loss: 1.3222
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 6.43
               Mean episode length: 123.51
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 12.88s
                        Total time: 20573.43s
                               ETA: 1359281.3s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.957s, learning 0.162s)
               Value function loss: 1.8439
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 5.02
               Mean episode length: 123.33
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 12.12s
                        Total time: 20585.55s
                               ETA: 1359156.6s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.518s, learning 0.194s)
               Value function loss: 3.5625
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 8.79
               Mean episode length: 124.42
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 12.71s
                        Total time: 20598.26s
                               ETA: 1359071.2s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.895s, learning 0.159s)
               Value function loss: 4.9880
                    Surrogate loss: 0.0031
             Mean action noise std: 0.74
                       Mean reward: 8.59
               Mean episode length: 123.94
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 13.05s
                        Total time: 20611.31s
                               ETA: 1359008.4s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.343s, learning 0.175s)
               Value function loss: 2.7152
                    Surrogate loss: 0.0008
             Mean action noise std: 0.74
                       Mean reward: 7.85
               Mean episode length: 124.61
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 12.52s
                        Total time: 20623.83s
                               ETA: 1358910.4s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.672s, learning 0.168s)
               Value function loss: 3.3623
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 5.96
               Mean episode length: 124.03
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 12.84s
                        Total time: 20636.67s
                               ETA: 1358833.7s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.518s, learning 0.225s)
               Value function loss: 3.4265
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: 6.26
               Mean episode length: 124.73
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 12.74s
                        Total time: 20649.41s
                               ETA: 1358750.7s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.496s, learning 0.159s)
               Value function loss: 3.9300
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 5.75
               Mean episode length: 124.82
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 12.65s
                        Total time: 20662.07s
                               ETA: 1358662.0s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.179s, learning 0.160s)
               Value function loss: 4.2340
                    Surrogate loss: 0.0024
             Mean action noise std: 0.74
                       Mean reward: 8.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 12.34s
                        Total time: 20674.41s
                               ETA: 1358552.7s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1215 steps/s (collection: 13.306s, learning 0.172s)
               Value function loss: 51.6697
                    Surrogate loss: 0.0262
             Mean action noise std: 0.74
                       Mean reward: 8.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 13.48s
                        Total time: 20687.88s
                               ETA: 1358518.2s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.759s, learning 0.168s)
               Value function loss: 1.0820
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 8.42
               Mean episode length: 125.00
                  Mean reward/step: -0.76
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 12.93s
                        Total time: 20700.81s
                               ETA: 1358447.7s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.087s, learning 0.165s)
               Value function loss: 0.7291
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: 8.80
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 12.25s
                        Total time: 20713.06s
                               ETA: 1358332.9s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.758s, learning 0.237s)
               Value function loss: 0.8137
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 8.02
               Mean episode length: 125.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 12.99s
                        Total time: 20726.06s
                               ETA: 1358267.0s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.149s, learning 0.165s)
               Value function loss: 1.2861
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 7.78
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 12.31s
                        Total time: 20738.37s
                               ETA: 1358156.5s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.688s, learning 0.161s)
               Value function loss: 3.3697
                    Surrogate loss: 0.0099
             Mean action noise std: 0.74
                       Mean reward: 7.23
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 12.85s
                        Total time: 20751.22s
                               ETA: 1358081.2s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.552s, learning 0.243s)
               Value function loss: 3.5711
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 5.32
               Mean episode length: 124.62
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 12.80s
                        Total time: 20764.02s
                               ETA: 1358002.5s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.803s, learning 0.205s)
               Value function loss: 3.7917
                    Surrogate loss: -0.0045
             Mean action noise std: 0.74
                       Mean reward: 5.90
               Mean episode length: 124.62
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 13.01s
                        Total time: 20777.02s
                               ETA: 1357937.8s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.435s, learning 0.157s)
               Value function loss: 2.3024
                    Surrogate loss: 0.0603
             Mean action noise std: 0.74
                       Mean reward: 3.22
               Mean episode length: 123.38
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 12.59s
                        Total time: 20789.62s
                               ETA: 1357845.9s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.643s, learning 0.168s)
               Value function loss: 3.2941
                    Surrogate loss: -0.0001
             Mean action noise std: 0.74
                       Mean reward: 0.14
               Mean episode length: 124.27
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 12.81s
                        Total time: 20802.43s
                               ETA: 1357768.5s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.167s, learning 0.169s)
               Value function loss: 2.9545
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 0.25
               Mean episode length: 123.22
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 12.34s
                        Total time: 20814.76s
                               ETA: 1357660.1s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.216s, learning 0.159s)
               Value function loss: 2.3255
                    Surrogate loss: 0.0021
             Mean action noise std: 0.74
                       Mean reward: 0.30
               Mean episode length: 123.93
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 12.38s
                        Total time: 20827.14s
                               ETA: 1357554.5s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.348s, learning 0.183s)
               Value function loss: 2.9209
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 4.60
               Mean episode length: 124.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 12.53s
                        Total time: 20839.67s
                               ETA: 1357459.1s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.196s, learning 0.160s)
               Value function loss: 3.8705
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 1.33
               Mean episode length: 124.88
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 12.36s
                        Total time: 20852.03s
                               ETA: 1357352.5s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.409s, learning 0.161s)
               Value function loss: 6.4479
                    Surrogate loss: 0.0043
             Mean action noise std: 0.74
                       Mean reward: 3.29
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 12.57s
                        Total time: 20864.60s
                               ETA: 1357259.9s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1224 steps/s (collection: 13.081s, learning 0.295s)
               Value function loss: 3.6985
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 2.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 13.38s
                        Total time: 20877.97s
                               ETA: 1357219.8s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1231 steps/s (collection: 13.148s, learning 0.157s)
               Value function loss: 31.2751
                    Surrogate loss: 0.0122
             Mean action noise std: 0.74
                       Mean reward: 1.58
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 4.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 13.31s
                        Total time: 20891.28s
                               ETA: 1357175.1s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.315s, learning 0.162s)
               Value function loss: 0.5405
                    Surrogate loss: -0.0205
             Mean action noise std: 0.74
                       Mean reward: 1.25
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 12.48s
                        Total time: 20903.75s
                               ETA: 1357076.7s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.325s, learning 0.158s)
               Value function loss: 0.4683
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 0.76
               Mean episode length: 124.40
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 12.48s
                        Total time: 20916.24s
                               ETA: 1356978.8s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1247 steps/s (collection: 12.925s, learning 0.213s)
               Value function loss: 0.5387
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 0.33
               Mean episode length: 124.40
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 13.14s
                        Total time: 20929.38s
                               ETA: 1356923.5s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.690s, learning 0.208s)
               Value function loss: 0.8947
                    Surrogate loss: 0.0097
             Mean action noise std: 0.74
                       Mean reward: 1.02
               Mean episode length: 124.40
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 12.90s
                        Total time: 20942.27s
                               ETA: 1356852.7s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.641s, learning 0.168s)
               Value function loss: 1.4358
                    Surrogate loss: 0.0165
             Mean action noise std: 0.74
                       Mean reward: 0.75
               Mean episode length: 124.40
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 12.81s
                        Total time: 20955.08s
                               ETA: 1356776.2s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.309s, learning 0.180s)
               Value function loss: 1.1756
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 0.99
               Mean episode length: 124.40
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 12.49s
                        Total time: 20967.57s
                               ETA: 1356679.1s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.444s, learning 0.288s)
               Value function loss: 1.7453
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 1.49
               Mean episode length: 123.75
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 12.73s
                        Total time: 20980.30s
                               ETA: 1356597.8s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.865s, learning 0.166s)
               Value function loss: 2.8614
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 0.41
               Mean episode length: 123.74
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 13.03s
                        Total time: 20993.34s
                               ETA: 1356535.9s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.463s, learning 0.194s)
               Value function loss: 3.9705
                    Surrogate loss: -0.0016
             Mean action noise std: 0.74
                       Mean reward: 4.08
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 12.66s
                        Total time: 21005.99s
                               ETA: 1356449.9s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.927s, learning 0.160s)
               Value function loss: 3.0039
                    Surrogate loss: -0.0014
             Mean action noise std: 0.74
                       Mean reward: 1.95
               Mean episode length: 124.14
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 13.09s
                        Total time: 21019.08s
                               ETA: 1356391.7s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.509s, learning 0.242s)
               Value function loss: 5.4866
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 2.86
               Mean episode length: 123.14
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 12.75s
                        Total time: 21031.83s
                               ETA: 1356312.0s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.524s, learning 0.194s)
               Value function loss: 4.3410
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: 3.18
               Mean episode length: 124.14
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 12.72s
                        Total time: 21044.55s
                               ETA: 1356230.2s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.306s, learning 0.216s)
               Value function loss: 3.8172
                    Surrogate loss: -0.0004
             Mean action noise std: 0.74
                       Mean reward: 5.83
               Mean episode length: 124.56
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 12.52s
                        Total time: 21057.07s
                               ETA: 1356135.9s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.865s, learning 0.158s)
               Value function loss: 4.7212
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 5.81
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 12.02s
                        Total time: 21069.09s
                               ETA: 1356009.6s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.856s, learning 0.167s)
               Value function loss: 4.3772
                    Surrogate loss: 0.0017
             Mean action noise std: 0.74
                       Mean reward: 8.62
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 13.02s
                        Total time: 21082.12s
                               ETA: 1355947.7s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.582s, learning 0.164s)
               Value function loss: 37.0515
                    Surrogate loss: 0.0199
             Mean action noise std: 0.74
                       Mean reward: 10.26
               Mean episode length: 125.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 4.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 12.75s
                        Total time: 21094.86s
                               ETA: 1355868.1s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.225s, learning 0.162s)
               Value function loss: 0.7348
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 10.33
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 12.39s
                        Total time: 21107.25s
                               ETA: 1355765.6s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.777s, learning 0.162s)
               Value function loss: 0.8591
                    Surrogate loss: 0.0028
             Mean action noise std: 0.74
                       Mean reward: 9.49
               Mean episode length: 124.48
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 12.94s
                        Total time: 21120.19s
                               ETA: 1355698.6s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.437s, learning 0.171s)
               Value function loss: 1.2696
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 10.63
               Mean episode length: 124.48
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 12.61s
                        Total time: 21132.80s
                               ETA: 1355610.4s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.616s, learning 0.169s)
               Value function loss: 2.4809
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 10.38
               Mean episode length: 124.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 12.78s
                        Total time: 21145.58s
                               ETA: 1355533.6s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.718s, learning 0.166s)
               Value function loss: 2.7787
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 8.15
               Mean episode length: 124.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 12.88s
                        Total time: 21158.47s
                               ETA: 1355463.3s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.344s, learning 0.164s)
               Value function loss: 3.7686
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: 8.16
               Mean episode length: 123.39
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 12.51s
                        Total time: 21170.97s
                               ETA: 1355369.0s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.521s, learning 0.163s)
               Value function loss: 3.5849
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 9.49
               Mean episode length: 122.12
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 12.68s
                        Total time: 21183.66s
                               ETA: 1355286.1s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.136s, learning 0.163s)
               Value function loss: 7.5594
                    Surrogate loss: 0.0068
             Mean action noise std: 0.74
                       Mean reward: 12.67
               Mean episode length: 123.86
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 12.30s
                        Total time: 21195.96s
                               ETA: 1355178.6s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.238s, learning 0.164s)
               Value function loss: 4.6154
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 11.34
               Mean episode length: 124.01
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 12.40s
                        Total time: 21208.36s
                               ETA: 1355077.9s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.288s, learning 0.168s)
               Value function loss: 5.5256
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 10.67
               Mean episode length: 124.14
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 12.46s
                        Total time: 21220.81s
                               ETA: 1354980.7s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.755s, learning 0.163s)
               Value function loss: 4.6369
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 12.51
               Mean episode length: 124.34
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 12.92s
                        Total time: 21233.73s
                               ETA: 1354913.1s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.268s, learning 0.258s)
               Value function loss: 4.5406
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 14.90
               Mean episode length: 124.70
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 12.53s
                        Total time: 21246.26s
                               ETA: 1354820.5s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.594s, learning 0.156s)
               Value function loss: 6.2704
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 13.88
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 12.75s
                        Total time: 21259.01s
                               ETA: 1354742.4s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.405s, learning 0.166s)
               Value function loss: 5.7989
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 8.90
               Mean episode length: 124.90
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 12.57s
                        Total time: 21271.58s
                               ETA: 1354652.8s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1238 steps/s (collection: 13.012s, learning 0.213s)
               Value function loss: 59.8451
                    Surrogate loss: 0.0648
             Mean action noise std: 0.74
                       Mean reward: 14.86
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 13.22s
                        Total time: 21284.80s
                               ETA: 1354605.1s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.561s, learning 0.163s)
               Value function loss: 1.2388
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 14.64
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 12.72s
                        Total time: 21297.53s
                               ETA: 1354525.5s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.707s, learning 0.185s)
               Value function loss: 1.0873
                    Surrogate loss: 0.0002
             Mean action noise std: 0.74
                       Mean reward: 15.50
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 12.89s
                        Total time: 21310.42s
                               ETA: 1354456.7s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.993s, learning 0.204s)
               Value function loss: 1.1613
                    Surrogate loss: 0.0088
             Mean action noise std: 0.74
                       Mean reward: 14.89
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 12.20s
                        Total time: 21322.62s
                               ETA: 1354343.9s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.096s, learning 0.182s)
               Value function loss: 2.1554
                    Surrogate loss: 0.0117
             Mean action noise std: 0.74
                       Mean reward: 12.87
               Mean episode length: 124.35
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 12.28s
                        Total time: 21334.90s
                               ETA: 1354236.3s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1243 steps/s (collection: 13.014s, learning 0.167s)
               Value function loss: 2.5818
                    Surrogate loss: 0.0144
             Mean action noise std: 0.74
                       Mean reward: 13.45
               Mean episode length: 124.35
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 13.18s
                        Total time: 21348.08s
                               ETA: 1354186.0s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.171s, learning 0.168s)
               Value function loss: 2.0317
                    Surrogate loss: 0.0051
             Mean action noise std: 0.74
                       Mean reward: 11.15
               Mean episode length: 122.86
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 12.34s
                        Total time: 21360.42s
                               ETA: 1354082.5s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.871s, learning 0.203s)
               Value function loss: 3.1306
                    Surrogate loss: 0.0020
             Mean action noise std: 0.74
                       Mean reward: 11.65
               Mean episode length: 122.86
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 12.07s
                        Total time: 21372.49s
                               ETA: 1353962.3s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.472s, learning 0.163s)
               Value function loss: 3.1782
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 9.16
               Mean episode length: 120.47
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 12.64s
                        Total time: 21385.12s
                               ETA: 1353877.8s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.567s, learning 0.162s)
               Value function loss: 5.1639
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 11.99
               Mean episode length: 123.91
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 12.73s
                        Total time: 21397.85s
                               ETA: 1353799.2s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.161s, learning 0.159s)
               Value function loss: 3.6825
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: 11.17
               Mean episode length: 124.54
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 12.32s
                        Total time: 21410.17s
                               ETA: 1353695.0s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.149s, learning 0.164s)
               Value function loss: 4.4446
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: 9.98
               Mean episode length: 124.65
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 12.31s
                        Total time: 21422.49s
                               ETA: 1353590.4s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.338s, learning 0.205s)
               Value function loss: 5.9348
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 11.59
               Mean episode length: 124.01
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 12.54s
                        Total time: 21435.03s
                               ETA: 1353500.4s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.839s, learning 0.162s)
               Value function loss: 9.6733
                    Surrogate loss: 0.0107
             Mean action noise std: 0.74
                       Mean reward: 7.77
               Mean episode length: 124.35
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 13.00s
                        Total time: 21448.03s
                               ETA: 1353439.5s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.284s, learning 0.161s)
               Value function loss: 4.8053
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 12.77
               Mean episode length: 124.81
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 12.45s
                        Total time: 21460.48s
                               ETA: 1353343.5s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.683s, learning 0.206s)
               Value function loss: 4.5020
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 7.22
               Mean episode length: 124.91
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 12.89s
                        Total time: 21473.37s
                               ETA: 1353275.7s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.654s, learning 0.169s)
               Value function loss: 36.5482
                    Surrogate loss: 0.0220
             Mean action noise std: 0.74
                       Mean reward: 11.91
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 4.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 12.82s
                        Total time: 21486.19s
                               ETA: 1353203.7s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.652s, learning 0.164s)
               Value function loss: 0.8647
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 13.10
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 12.82s
                        Total time: 21499.00s
                               ETA: 1353131.4s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.465s, learning 0.191s)
               Value function loss: 0.8005
                    Surrogate loss: -0.0021
             Mean action noise std: 0.74
                       Mean reward: 12.50
               Mean episode length: 124.42
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 12.66s
                        Total time: 21511.66s
                               ETA: 1353049.0s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.180s, learning 0.157s)
               Value function loss: 1.0614
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 10.16
               Mean episode length: 123.81
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 12.34s
                        Total time: 21524.00s
                               ETA: 1352946.8s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.645s, learning 0.202s)
               Value function loss: 1.8129
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 8.48
               Mean episode length: 123.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 12.85s
                        Total time: 21536.84s
                               ETA: 1352876.7s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.597s, learning 0.170s)
               Value function loss: 1.8108
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 7.92
               Mean episode length: 123.42
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 12.77s
                        Total time: 21549.61s
                               ETA: 1352801.6s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.368s, learning 0.167s)
               Value function loss: 1.9668
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 5.83
               Mean episode length: 122.60
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 12.53s
                        Total time: 21562.15s
                               ETA: 1352712.0s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.293s, learning 0.160s)
               Value function loss: 2.5714
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 6.11
               Mean episode length: 122.15
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 12.45s
                        Total time: 21574.60s
                               ETA: 1352617.4s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.722s, learning 0.185s)
               Value function loss: 3.6530
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 9.00
               Mean episode length: 124.41
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 12.91s
                        Total time: 21587.51s
                               ETA: 1352551.3s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.109s, learning 0.189s)
               Value function loss: 4.6977
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 10.99
               Mean episode length: 123.49
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 12.30s
                        Total time: 21599.80s
                               ETA: 1352447.3s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.603s, learning 0.272s)
               Value function loss: 3.7532
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 10.58
               Mean episode length: 123.63
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 12.88s
                        Total time: 21612.68s
                               ETA: 1352379.4s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.582s, learning 0.205s)
               Value function loss: 4.8753
                    Surrogate loss: 0.0137
             Mean action noise std: 0.73
                       Mean reward: 9.20
               Mean episode length: 123.75
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 12.79s
                        Total time: 21625.47s
                               ETA: 1352306.1s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.677s, learning 0.208s)
               Value function loss: 3.5381
                    Surrogate loss: 0.0018
             Mean action noise std: 0.73
                       Mean reward: 8.29
               Mean episode length: 123.93
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 12.89s
                        Total time: 21638.35s
                               ETA: 1352239.1s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.752s, learning 0.230s)
               Value function loss: 3.9148
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 11.94
               Mean episode length: 124.81
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 12.98s
                        Total time: 21651.33s
                               ETA: 1352178.0s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.366s, learning 0.167s)
               Value function loss: 4.2609
                    Surrogate loss: 0.0152
             Mean action noise std: 0.73
                       Mean reward: 10.47
               Mean episode length: 124.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 12.53s
                        Total time: 21663.87s
                               ETA: 1352089.1s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.710s, learning 0.232s)
               Value function loss: 11.8716
                    Surrogate loss: 0.0326
             Mean action noise std: 0.73
                       Mean reward: 11.73
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 11.94s
                        Total time: 21675.81s
                               ETA: 1351963.3s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.887s, learning 0.166s)
               Value function loss: 25.3776
                    Surrogate loss: 0.0052
             Mean action noise std: 0.73
                       Mean reward: 12.69
               Mean episode length: 125.00
                  Mean reward/step: -0.72
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 13.05s
                        Total time: 21688.86s
                               ETA: 1351907.0s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.465s, learning 0.203s)
               Value function loss: 0.7558
                    Surrogate loss: 0.0000
             Mean action noise std: 0.73
                       Mean reward: 12.79
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 12.67s
                        Total time: 21701.53s
                               ETA: 1351826.8s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.215s, learning 0.221s)
               Value function loss: 0.7593
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 13.38
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 12.44s
                        Total time: 21713.97s
                               ETA: 1351732.2s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.755s, learning 0.162s)
               Value function loss: 1.1771
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 11.52
               Mean episode length: 124.58
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 12.92s
                        Total time: 21726.88s
                               ETA: 1351667.6s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.407s, learning 0.174s)
               Value function loss: 2.6622
                    Surrogate loss: 0.0036
             Mean action noise std: 0.73
                       Mean reward: 9.40
               Mean episode length: 124.07
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 12.58s
                        Total time: 21739.46s
                               ETA: 1351582.1s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.394s, learning 0.165s)
               Value function loss: 2.0791
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 9.29
               Mean episode length: 124.07
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 12.56s
                        Total time: 21752.02s
                               ETA: 1351495.4s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.313s, learning 0.165s)
               Value function loss: 1.9556
                    Surrogate loss: 0.0105
             Mean action noise std: 0.73
                       Mean reward: 11.03
               Mean episode length: 124.07
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 12.48s
                        Total time: 21764.50s
                               ETA: 1351403.8s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.499s, learning 0.173s)
               Value function loss: 2.1297
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 10.63
               Mean episode length: 123.97
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 12.67s
                        Total time: 21777.17s
                               ETA: 1351324.3s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.592s, learning 0.240s)
               Value function loss: 4.0720
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 11.27
               Mean episode length: 124.47
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 12.83s
                        Total time: 21790.00s
                               ETA: 1351254.9s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.808s, learning 0.247s)
               Value function loss: 5.5718
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 10.05
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 13.06s
                        Total time: 21803.06s
                               ETA: 1351199.3s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.155s, learning 0.209s)
               Value function loss: 3.8401
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 10.73
               Mean episode length: 124.56
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 12.36s
                        Total time: 21815.42s
                               ETA: 1351101.0s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.704s, learning 0.156s)
               Value function loss: 4.6260
                    Surrogate loss: 0.0049
             Mean action noise std: 0.73
                       Mean reward: 9.72
               Mean episode length: 124.40
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 12.86s
                        Total time: 21828.28s
                               ETA: 1351033.5s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.183s, learning 0.163s)
               Value function loss: 3.5600
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 10.01
               Mean episode length: 124.55
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 12.35s
                        Total time: 21840.63s
                               ETA: 1350934.2s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.536s, learning 0.246s)
               Value function loss: 4.6499
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 10.10
               Mean episode length: 124.82
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 12.78s
                        Total time: 21853.41s
                               ETA: 1350862.1s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.463s, learning 0.203s)
               Value function loss: 4.0733
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 10.15
               Mean episode length: 124.97
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 12.67s
                        Total time: 21866.08s
                               ETA: 1350782.8s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1230 steps/s (collection: 13.141s, learning 0.172s)
               Value function loss: 37.5965
                    Surrogate loss: 0.0295
             Mean action noise std: 0.73
                       Mean reward: 10.27
               Mean episode length: 124.48
                  Mean reward/step: -0.10
       Mean episode length/episode: 4.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 13.31s
                        Total time: 21879.39s
                               ETA: 1350743.5s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.188s, learning 0.178s)
               Value function loss: 1.1791
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 10.31
               Mean episode length: 124.48
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 12.37s
                        Total time: 21891.76s
                               ETA: 1350645.8s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.859s, learning 0.206s)
               Value function loss: 0.7309
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 9.56
               Mean episode length: 124.48
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 13.07s
                        Total time: 21904.82s
                               ETA: 1350591.5s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.309s, learning 0.179s)
               Value function loss: 0.7471
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 9.15
               Mean episode length: 123.99
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 12.49s
                        Total time: 21917.31s
                               ETA: 1350501.5s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.688s, learning 0.220s)
               Value function loss: 1.6295
                    Surrogate loss: 0.0015
             Mean action noise std: 0.73
                       Mean reward: 9.34
               Mean episode length: 123.99
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 12.91s
                        Total time: 21930.22s
                               ETA: 1350437.6s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.969s, learning 0.167s)
               Value function loss: 2.1373
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 9.36
               Mean episode length: 123.41
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 12.14s
                        Total time: 21942.35s
                               ETA: 1350326.2s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.425s, learning 0.201s)
               Value function loss: 1.9940
                    Surrogate loss: 0.0005
             Mean action noise std: 0.73
                       Mean reward: 8.10
               Mean episode length: 122.66
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 12.63s
                        Total time: 21954.98s
                               ETA: 1350245.0s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.403s, learning 0.170s)
               Value function loss: 2.6613
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 8.45
               Mean episode length: 122.58
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 12.57s
                        Total time: 21967.55s
                               ETA: 1350160.7s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.686s, learning 0.159s)
               Value function loss: 3.3293
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 8.98
               Mean episode length: 123.78
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 11.85s
                        Total time: 21979.40s
                               ETA: 1350031.7s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.343s, learning 0.256s)
               Value function loss: 5.2788
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 10.98
               Mean episode length: 123.95
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 12.60s
                        Total time: 21992.00s
                               ETA: 1349949.2s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.507s, learning 0.173s)
               Value function loss: 3.4627
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 9.45
               Mean episode length: 123.50
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 12.68s
                        Total time: 22004.68s
                               ETA: 1349871.7s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.684s, learning 0.170s)
               Value function loss: 4.2429
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 14.89
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 12.85s
                        Total time: 22017.53s
                               ETA: 1349805.0s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.323s, learning 0.174s)
               Value function loss: 4.0672
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 9.20
               Mean episode length: 124.10
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 12.50s
                        Total time: 22030.03s
                               ETA: 1349716.5s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.597s, learning 0.196s)
               Value function loss: 4.3602
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 12.42
               Mean episode length: 124.59
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 12.79s
                        Total time: 22042.82s
                               ETA: 1349646.1s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.845s, learning 0.156s)
               Value function loss: 5.0880
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: 13.27
               Mean episode length: 124.72
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 13.00s
                        Total time: 22055.82s
                               ETA: 1349588.6s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.693s, learning 0.194s)
               Value function loss: 4.9564
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 11.25
               Mean episode length: 124.90
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 12.89s
                        Total time: 22068.71s
                               ETA: 1349524.2s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1242 steps/s (collection: 13.016s, learning 0.172s)
               Value function loss: 35.5879
                    Surrogate loss: 0.0227
             Mean action noise std: 0.73
                       Mean reward: 15.81
               Mean episode length: 125.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 4.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 13.19s
                        Total time: 22081.90s
                               ETA: 1349478.2s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.872s, learning 0.216s)
               Value function loss: 0.9250
                    Surrogate loss: -0.0239
             Mean action noise std: 0.73
                       Mean reward: 15.11
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 13.09s
                        Total time: 22094.98s
                               ETA: 1349426.2s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.324s, learning 0.169s)
               Value function loss: 0.9348
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 15.18
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 12.49s
                        Total time: 22107.48s
                               ETA: 1349337.9s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.022s, learning 0.161s)
               Value function loss: 1.3344
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 14.64
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 12.18s
                        Total time: 22119.66s
                               ETA: 1349230.8s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.772s, learning 0.201s)
               Value function loss: 3.1959
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 15.12
               Mean episode length: 124.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 12.97s
                        Total time: 22132.63s
                               ETA: 1349171.9s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.785s, learning 0.175s)
               Value function loss: 2.9547
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 13.86
               Mean episode length: 123.81
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 12.96s
                        Total time: 22145.59s
                               ETA: 1349112.4s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.338s, learning 0.239s)
               Value function loss: 2.2361
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 14.19
               Mean episode length: 123.81
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 12.58s
                        Total time: 22158.17s
                               ETA: 1349029.5s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.320s, learning 0.183s)
               Value function loss: 3.3488
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 12.00
               Mean episode length: 123.81
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 12.50s
                        Total time: 22170.67s
                               ETA: 1348942.3s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.235s, learning 0.168s)
               Value function loss: 4.2324
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 8.78
               Mean episode length: 124.59
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 12.40s
                        Total time: 22183.08s
                               ETA: 1348849.0s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.269s, learning 0.162s)
               Value function loss: 4.5050
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 9.96
               Mean episode length: 124.01
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 12.43s
                        Total time: 22195.51s
                               ETA: 1348757.6s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.359s, learning 0.206s)
               Value function loss: 3.2677
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 12.71
               Mean episode length: 124.08
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 12.57s
                        Total time: 22208.07s
                               ETA: 1348674.4s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.533s, learning 0.162s)
               Value function loss: 5.4475
                    Surrogate loss: 0.0044
             Mean action noise std: 0.73
                       Mean reward: 14.88
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 12.69s
                        Total time: 22220.77s
                               ETA: 1348599.1s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.051s, learning 0.166s)
               Value function loss: 4.3909
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 11.52
               Mean episode length: 124.70
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 12.22s
                        Total time: 22232.99s
                               ETA: 1348495.0s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.269s, learning 0.168s)
               Value function loss: 4.6971
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 11.48
               Mean episode length: 124.57
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 12.44s
                        Total time: 22245.42s
                               ETA: 1348404.3s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.378s, learning 0.163s)
               Value function loss: 4.5740
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 14.07
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 12.54s
                        Total time: 22257.96s
                               ETA: 1348320.0s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1238 steps/s (collection: 13.069s, learning 0.160s)
               Value function loss: 77.7888
                    Surrogate loss: 0.0226
             Mean action noise std: 0.73
                       Mean reward: 16.38
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 13.23s
                        Total time: 22271.19s
                               ETA: 1348277.5s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.232s, learning 0.176s)
               Value function loss: 3.6786
                    Surrogate loss: -0.0192
             Mean action noise std: 0.73
                       Mean reward: 13.91
               Mean episode length: 124.51
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 12.41s
                        Total time: 22283.60s
                               ETA: 1348185.3s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.669s, learning 0.158s)
               Value function loss: 1.3888
                    Surrogate loss: 0.0049
             Mean action noise std: 0.73
                       Mean reward: 14.50
               Mean episode length: 124.51
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 12.83s
                        Total time: 22296.43s
                               ETA: 1348118.6s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.085s, learning 0.170s)
               Value function loss: 0.9102
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 13.22
               Mean episode length: 124.51
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 12.26s
                        Total time: 22308.68s
                               ETA: 1348017.3s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.723s, learning 0.159s)
               Value function loss: 1.1635
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 12.78
               Mean episode length: 124.02
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 12.88s
                        Total time: 22321.57s
                               ETA: 1347954.0s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.283s, learning 0.182s)
               Value function loss: 1.9773
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 10.96
               Mean episode length: 123.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 12.47s
                        Total time: 22334.03s
                               ETA: 1347865.7s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.724s, learning 0.163s)
               Value function loss: 2.3526
                    Surrogate loss: 0.0070
             Mean action noise std: 0.73
                       Mean reward: 10.73
               Mean episode length: 123.21
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 11.89s
                        Total time: 22345.92s
                               ETA: 1347742.5s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.478s, learning 0.158s)
               Value function loss: 2.3514
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 6.72
               Mean episode length: 122.20
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 12.64s
                        Total time: 22358.56s
                               ETA: 1347664.7s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.691s, learning 0.231s)
               Value function loss: 3.4184
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 9.54
               Mean episode length: 123.60
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 12.92s
                        Total time: 22371.48s
                               ETA: 1347604.1s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1238 steps/s (collection: 13.065s, learning 0.162s)
               Value function loss: 5.4536
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 9.11
               Mean episode length: 123.35
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 13.23s
                        Total time: 22384.70s
                               ETA: 1347562.0s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.631s, learning 0.203s)
               Value function loss: 4.5571
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 8.05
               Mean episode length: 124.15
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 12.83s
                        Total time: 22397.54s
                               ETA: 1347496.2s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.842s, learning 0.164s)
               Value function loss: 3.4305
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 13.34
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 13.01s
                        Total time: 22410.54s
                               ETA: 1347440.8s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.633s, learning 0.161s)
               Value function loss: 7.2977
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 9.38
               Mean episode length: 124.66
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 12.79s
                        Total time: 22423.34s
                               ETA: 1347372.8s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.602s, learning 0.171s)
               Value function loss: 4.9024
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 15.32
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 12.77s
                        Total time: 22436.11s
                               ETA: 1347303.6s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.480s, learning 0.159s)
               Value function loss: 4.8846
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 10.34
               Mean episode length: 124.69
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 12.64s
                        Total time: 22448.75s
                               ETA: 1347226.4s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.120s, learning 0.356s)
               Value function loss: 4.9842
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 8.22
               Mean episode length: 124.88
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 12.48s
                        Total time: 22461.23s
                               ETA: 1347139.4s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1226 steps/s (collection: 13.199s, learning 0.158s)
               Value function loss: 41.9933
                    Surrogate loss: 0.0297
             Mean action noise std: 0.73
                       Mean reward: 11.48
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 4.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 13.36s
                        Total time: 22474.58s
                               ETA: 1347105.4s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.340s, learning 0.253s)
               Value function loss: 1.8872
                    Surrogate loss: -0.0203
             Mean action noise std: 0.73
                       Mean reward: 12.94
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 12.59s
                        Total time: 22487.18s
                               ETA: 1347025.7s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.183s, learning 0.190s)
               Value function loss: 0.9253
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 12.19
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 12.37s
                        Total time: 22499.55s
                               ETA: 1346932.9s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.902s, learning 0.162s)
               Value function loss: 0.8566
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 11.48
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 12.06s
                        Total time: 22511.61s
                               ETA: 1346821.6s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.862s, learning 0.228s)
               Value function loss: 1.2888
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 10.07
               Mean episode length: 124.05
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 13.09s
                        Total time: 22524.70s
                               ETA: 1346771.8s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.574s, learning 0.167s)
               Value function loss: 1.8776
                    Surrogate loss: 0.0078
             Mean action noise std: 0.73
                       Mean reward: 9.48
               Mean episode length: 124.05
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 12.74s
                        Total time: 22537.44s
                               ETA: 1346701.3s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.522s, learning 0.205s)
               Value function loss: 1.6019
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 8.75
               Mean episode length: 124.05
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 12.73s
                        Total time: 22550.17s
                               ETA: 1346630.0s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.708s, learning 0.182s)
               Value function loss: 2.3969
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 8.92
               Mean episode length: 123.42
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 12.89s
                        Total time: 22563.06s
                               ETA: 1346568.4s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.393s, learning 0.156s)
               Value function loss: 3.9931
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 7.37
               Mean episode length: 122.49
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 12.55s
                        Total time: 22575.61s
                               ETA: 1346486.6s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.900s, learning 0.170s)
               Value function loss: 5.7720
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 8.72
               Mean episode length: 123.94
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 13.07s
                        Total time: 22588.68s
                               ETA: 1346435.9s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.122s, learning 0.176s)
               Value function loss: 3.7834
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 9.02
               Mean episode length: 124.07
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 12.30s
                        Total time: 22600.98s
                               ETA: 1346339.2s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.193s, learning 0.160s)
               Value function loss: 4.0297
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 7.25
               Mean episode length: 124.36
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 12.35s
                        Total time: 22613.33s
                               ETA: 1346246.0s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.466s, learning 0.162s)
               Value function loss: 4.0146
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 9.54
               Mean episode length: 124.46
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 12.63s
                        Total time: 22625.96s
                               ETA: 1346169.2s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.667s, learning 0.156s)
               Value function loss: 4.6830
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 8.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 12.82s
                        Total time: 22638.78s
                               ETA: 1346104.1s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.506s, learning 0.184s)
               Value function loss: 5.1138
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 10.15
               Mean episode length: 124.76
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 12.69s
                        Total time: 22651.47s
                               ETA: 1346031.1s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.622s, learning 0.210s)
               Value function loss: 5.2473
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 7.10
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 12.83s
                        Total time: 22664.30s
                               ETA: 1345966.6s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1184 steps/s (collection: 13.621s, learning 0.211s)
               Value function loss: 38.5357
                    Surrogate loss: 0.0113
             Mean action noise std: 0.73
                       Mean reward: 7.92
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 5.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 13.83s
                        Total time: 22678.13s
                               ETA: 1345961.6s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.341s, learning 0.212s)
               Value function loss: 1.2305
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 5.47
               Mean episode length: 124.16
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 12.55s
                        Total time: 22690.69s
                               ETA: 1345880.7s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.749s, learning 0.253s)
               Value function loss: 1.2401
                    Surrogate loss: 0.0028
             Mean action noise std: 0.73
                       Mean reward: 5.52
               Mean episode length: 124.16
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 13.00s
                        Total time: 22703.69s
                               ETA: 1345826.4s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.889s, learning 0.163s)
               Value function loss: 1.5986
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 6.44
               Mean episode length: 123.54
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 13.05s
                        Total time: 22716.74s
                               ETA: 1345775.2s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.617s, learning 0.156s)
               Value function loss: 5.9443
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: -2.34
               Mean episode length: 122.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 12.77s
                        Total time: 22729.51s
                               ETA: 1345707.6s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.957s, learning 0.168s)
               Value function loss: 4.4136
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: -1.45
               Mean episode length: 122.39
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 12.13s
                        Total time: 22741.64s
                               ETA: 1345601.6s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.543s, learning 0.227s)
               Value function loss: 2.6112
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: -1.66
               Mean episode length: 121.10
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 12.77s
                        Total time: 22754.41s
                               ETA: 1345533.9s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.463s, learning 0.162s)
               Value function loss: 2.7900
                    Surrogate loss: 0.0009
             Mean action noise std: 0.73
                       Mean reward: 4.67
               Mean episode length: 122.15
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 12.63s
                        Total time: 22767.03s
                               ETA: 1345457.7s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.250s, learning 0.173s)
               Value function loss: 4.9907
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 7.90
               Mean episode length: 124.45
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 12.42s
                        Total time: 22779.46s
                               ETA: 1345369.7s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.579s, learning 0.180s)
               Value function loss: 4.2822
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 8.48
               Mean episode length: 124.48
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 12.76s
                        Total time: 22792.21s
                               ETA: 1345301.6s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.670s, learning 0.167s)
               Value function loss: 3.2548
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 7.77
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 12.84s
                        Total time: 22805.05s
                               ETA: 1345238.1s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.390s, learning 0.180s)
               Value function loss: 4.1110
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 9.17
               Mean episode length: 124.86
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 12.57s
                        Total time: 22817.62s
                               ETA: 1345159.0s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.353s, learning 0.201s)
               Value function loss: 4.1950
                    Surrogate loss: 0.0030
             Mean action noise std: 0.73
                       Mean reward: 9.49
               Mean episode length: 124.25
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 12.55s
                        Total time: 22830.18s
                               ETA: 1345079.0s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.747s, learning 0.188s)
               Value function loss: 4.7252
                    Surrogate loss: 0.0036
             Mean action noise std: 0.73
                       Mean reward: 10.54
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 12.94s
                        Total time: 22843.11s
                               ETA: 1345021.5s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.222s, learning 0.180s)
               Value function loss: 4.4216
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 4.31
               Mean episode length: 124.72
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 12.40s
                        Total time: 22855.51s
                               ETA: 1344932.8s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.843s, learning 0.180s)
               Value function loss: 36.0832
                    Surrogate loss: 0.0292
             Mean action noise std: 0.73
                       Mean reward: 11.44
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 13.02s
                        Total time: 22868.54s
                               ETA: 1344880.6s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.659s, learning 0.173s)
               Value function loss: 1.1342
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 9.79
               Mean episode length: 124.45
                  Mean reward/step: -0.69
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 12.83s
                        Total time: 22881.37s
                               ETA: 1344817.2s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.879s, learning 0.163s)
               Value function loss: 0.9426
                    Surrogate loss: 0.0151
             Mean action noise std: 0.73
                       Mean reward: 6.10
               Mean episode length: 122.78
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 12.04s
                        Total time: 22893.41s
                               ETA: 1344707.5s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.405s, learning 0.160s)
               Value function loss: 0.9158
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: 5.28
               Mean episode length: 122.64
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 12.56s
                        Total time: 22905.97s
                               ETA: 1344628.6s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.683s, learning 0.159s)
               Value function loss: 1.5081
                    Surrogate loss: 0.0104
             Mean action noise std: 0.73
                       Mean reward: 1.02
               Mean episode length: 121.07
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 11.84s
                        Total time: 22917.82s
                               ETA: 1344507.4s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.625s, learning 0.217s)
               Value function loss: 2.5917
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: 1.25
               Mean episode length: 121.07
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 12.84s
                        Total time: 22930.66s
                               ETA: 1344444.9s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.718s, learning 0.179s)
               Value function loss: 2.3662
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 2.56
               Mean episode length: 121.07
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 12.90s
                        Total time: 22943.56s
                               ETA: 1344385.8s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.780s, learning 0.157s)
               Value function loss: 2.2583
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 5.26
               Mean episode length: 123.15
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 12.94s
                        Total time: 22956.49s
                               ETA: 1344329.0s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.551s, learning 0.182s)
               Value function loss: 5.2331
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: 6.55
               Mean episode length: 124.46
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 12.73s
                        Total time: 22969.23s
                               ETA: 1344260.3s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.460s, learning 0.159s)
               Value function loss: 6.6866
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 122.02
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 12.62s
                        Total time: 22981.85s
                               ETA: 1344185.0s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.679s, learning 0.158s)
               Value function loss: 5.9034
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 1.89
               Mean episode length: 121.81
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 12.84s
                        Total time: 22994.68s
                               ETA: 1344122.6s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.314s, learning 0.158s)
               Value function loss: 5.2527
                    Surrogate loss: 0.0022
             Mean action noise std: 0.73
                       Mean reward: 3.46
               Mean episode length: 123.93
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 12.47s
                        Total time: 23007.15s
                               ETA: 1344038.9s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.254s, learning 0.178s)
               Value function loss: 3.6077
                    Surrogate loss: -0.0000
             Mean action noise std: 0.73
                       Mean reward: 8.49
               Mean episode length: 124.67
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 12.43s
                        Total time: 23019.59s
                               ETA: 1343953.0s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.434s, learning 0.160s)
               Value function loss: 4.2707
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 9.17
               Mean episode length: 124.54
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 12.59s
                        Total time: 23032.18s
                               ETA: 1343876.5s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.725s, learning 0.158s)
               Value function loss: 4.4559
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 14.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 12.88s
                        Total time: 23045.06s
                               ETA: 1343817.1s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.499s, learning 0.159s)
               Value function loss: 4.5891
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 10.93
               Mean episode length: 124.91
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 12.66s
                        Total time: 23057.72s
                               ETA: 1343744.5s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1244 steps/s (collection: 13.001s, learning 0.164s)
               Value function loss: 39.3985
                    Surrogate loss: 0.0352
             Mean action noise std: 0.73
                       Mean reward: 8.07
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 5.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 13.16s
                        Total time: 23070.89s
                               ETA: 1343701.5s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.501s, learning 0.263s)
               Value function loss: 0.7302
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 8.58
               Mean episode length: 124.46
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 12.76s
                        Total time: 23083.65s
                               ETA: 1343635.3s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.279s, learning 0.246s)
               Value function loss: 0.8995
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 7.67
               Mean episode length: 124.05
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 12.53s
                        Total time: 23096.18s
                               ETA: 1343555.2s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1243 steps/s (collection: 12.913s, learning 0.259s)
               Value function loss: 1.0094
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 8.10
               Mean episode length: 124.05
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 13.17s
                        Total time: 23109.35s
                               ETA: 1343512.8s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.762s, learning 0.291s)
               Value function loss: 1.9547
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 6.49
               Mean episode length: 124.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 13.05s
                        Total time: 23122.40s
                               ETA: 1343463.5s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.543s, learning 0.159s)
               Value function loss: 2.9573
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 4.64
               Mean episode length: 124.05
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 12.70s
                        Total time: 23135.10s
                               ETA: 1343393.9s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.537s, learning 0.164s)
               Value function loss: 2.5310
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 3.57
               Mean episode length: 123.40
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 12.70s
                        Total time: 23147.81s
                               ETA: 1343324.2s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.099s, learning 0.159s)
               Value function loss: 3.3831
                    Surrogate loss: 0.0160
             Mean action noise std: 0.73
                       Mean reward: -0.01
               Mean episode length: 122.11
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 12.26s
                        Total time: 23160.06s
                               ETA: 1343229.0s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.385s, learning 0.247s)
               Value function loss: 4.5877
                    Surrogate loss: 0.0066
             Mean action noise std: 0.73
                       Mean reward: 3.81
               Mean episode length: 123.40
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 12.63s
                        Total time: 23172.70s
                               ETA: 1343155.6s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.662s, learning 0.178s)
               Value function loss: 5.8025
                    Surrogate loss: 0.0021
             Mean action noise std: 0.73
                       Mean reward: 3.04
               Mean episode length: 122.78
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 12.84s
                        Total time: 23185.54s
                               ETA: 1343094.2s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.721s, learning 0.205s)
               Value function loss: 3.8448
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 0.51
               Mean episode length: 122.78
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 12.93s
                        Total time: 23198.46s
                               ETA: 1343037.9s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.671s, learning 0.204s)
               Value function loss: 4.3609
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 4.44
               Mean episode length: 124.25
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 12.87s
                        Total time: 23211.34s
                               ETA: 1342978.7s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.443s, learning 0.159s)
               Value function loss: 4.3110
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 2.76
               Mean episode length: 124.72
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 12.60s
                        Total time: 23223.94s
                               ETA: 1342903.7s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.220s, learning 0.159s)
               Value function loss: 4.9792
                    Surrogate loss: 0.0162
             Mean action noise std: 0.73
                       Mean reward: 5.44
               Mean episode length: 124.82
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 12.38s
                        Total time: 23236.32s
                               ETA: 1342816.0s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.638s, learning 0.182s)
               Value function loss: 4.2070
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 7.49
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 11.82s
                        Total time: 23248.14s
                               ETA: 1342696.1s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.346s, learning 0.158s)
               Value function loss: 20.9480
                    Surrogate loss: 0.0354
             Mean action noise std: 0.73
                       Mean reward: 8.76
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 12.50s
                        Total time: 23260.64s
                               ETA: 1342615.7s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1222 steps/s (collection: 13.206s, learning 0.196s)
               Value function loss: 22.9295
                    Surrogate loss: 0.0014
             Mean action noise std: 0.73
                       Mean reward: 8.50
               Mean episode length: 125.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 13.40s
                        Total time: 23274.04s
                               ETA: 1342587.2s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.445s, learning 0.230s)
               Value function loss: 1.1185
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 8.73
               Mean episode length: 124.52
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 12.67s
                        Total time: 23286.72s
                               ETA: 1342516.9s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.754s, learning 0.164s)
               Value function loss: 1.0203
                    Surrogate loss: 0.0022
             Mean action noise std: 0.73
                       Mean reward: 8.32
               Mean episode length: 123.96
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 11.92s
                        Total time: 23298.64s
                               ETA: 1342403.0s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.343s, learning 0.159s)
               Value function loss: 1.0417
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 10.49
               Mean episode length: 123.96
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 12.50s
                        Total time: 23311.14s
                               ETA: 1342322.8s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.367s, learning 0.176s)
               Value function loss: 2.6064
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: 10.37
               Mean episode length: 123.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 12.54s
                        Total time: 23323.68s
                               ETA: 1342245.1s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.348s, learning 0.159s)
               Value function loss: 2.9645
                    Surrogate loss: 0.0119
             Mean action noise std: 0.73
                       Mean reward: 10.72
               Mean episode length: 123.41
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 12.51s
                        Total time: 23336.19s
                               ETA: 1342165.3s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.829s, learning 0.165s)
               Value function loss: 2.2290
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 9.19
               Mean episode length: 123.42
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 11.99s
                        Total time: 23348.18s
                               ETA: 1342056.3s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.367s, learning 0.159s)
               Value function loss: 2.7376
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 4.43
               Mean episode length: 123.36
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 12.53s
                        Total time: 23360.71s
                               ETA: 1341977.8s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1336 steps/s (collection: 11.981s, learning 0.281s)
               Value function loss: 4.5697
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 4.42
               Mean episode length: 123.86
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 12.26s
                        Total time: 23372.97s
                               ETA: 1341884.2s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.420s, learning 0.246s)
               Value function loss: 4.7904
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 9.07
               Mean episode length: 124.30
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 12.67s
                        Total time: 23385.64s
                               ETA: 1341814.0s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.590s, learning 0.221s)
               Value function loss: 3.5663
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 7.14
               Mean episode length: 123.96
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 12.81s
                        Total time: 23398.45s
                               ETA: 1341752.2s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.179s, learning 0.162s)
               Value function loss: 4.4574
                    Surrogate loss: 0.0022
             Mean action noise std: 0.73
                       Mean reward: 7.23
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 12.34s
                        Total time: 23410.79s
                               ETA: 1341663.4s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.665s, learning 0.194s)
               Value function loss: 4.3178
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 9.09
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 12.86s
                        Total time: 23423.65s
                               ETA: 1341604.4s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.910s, learning 0.157s)
               Value function loss: 5.2313
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 5.58
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 12.07s
                        Total time: 23435.71s
                               ETA: 1341500.2s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.089s, learning 0.161s)
               Value function loss: 4.9398
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 5.89
               Mean episode length: 124.44
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 12.25s
                        Total time: 23447.96s
                               ETA: 1341406.5s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.375s, learning 0.158s)
               Value function loss: 36.9092
                    Surrogate loss: 0.0352
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 5.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 12.53s
                        Total time: 23460.50s
                               ETA: 1341329.1s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.502s, learning 0.198s)
               Value function loss: 1.0084
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 12.70s
                        Total time: 23473.20s
                               ETA: 1341261.3s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.719s, learning 0.159s)
               Value function loss: 1.0144
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 4.11
               Mean episode length: 124.47
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 12.88s
                        Total time: 23486.08s
                               ETA: 1341203.7s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.939s, learning 0.197s)
               Value function loss: 1.2286
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 4.67
               Mean episode length: 124.02
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 12.14s
                        Total time: 23498.21s
                               ETA: 1341103.8s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.591s, learning 0.162s)
               Value function loss: 1.8644
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 1.26
               Mean episode length: 123.65
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 12.75s
                        Total time: 23510.96s
                               ETA: 1341039.2s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.373s, learning 0.159s)
               Value function loss: 3.4235
                    Surrogate loss: 0.0033
             Mean action noise std: 0.73
                       Mean reward: 1.94
               Mean episode length: 123.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 12.53s
                        Total time: 23523.50s
                               ETA: 1340962.0s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.948s, learning 0.157s)
               Value function loss: 3.3067
                    Surrogate loss: 0.0192
             Mean action noise std: 0.73
                       Mean reward: -0.41
               Mean episode length: 122.49
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 13.11s
                        Total time: 23536.60s
                               ETA: 1340917.7s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.063s, learning 0.199s)
               Value function loss: 3.0568
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: -1.94
               Mean episode length: 123.47
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 12.26s
                        Total time: 23548.86s
                               ETA: 1340825.3s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.605s, learning 0.162s)
               Value function loss: 4.5783
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: -4.42
               Mean episode length: 122.92
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 12.77s
                        Total time: 23561.63s
                               ETA: 1340761.8s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.531s, learning 0.160s)
               Value function loss: 6.2936
                    Surrogate loss: 0.0118
             Mean action noise std: 0.73
                       Mean reward: -2.79
               Mean episode length: 122.93
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 12.69s
                        Total time: 23574.32s
                               ETA: 1340694.0s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.886s, learning 0.204s)
               Value function loss: 3.8744
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: -1.11
               Mean episode length: 122.48
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 12.09s
                        Total time: 23586.41s
                               ETA: 1340592.1s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.605s, learning 0.158s)
               Value function loss: 3.6183
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 1.13
               Mean episode length: 124.24
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 12.76s
                        Total time: 23599.17s
                               ETA: 1340528.6s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.280s, learning 0.158s)
               Value function loss: 3.5800
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 3.87
               Mean episode length: 124.73
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 12.44s
                        Total time: 23611.61s
                               ETA: 1340446.6s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.472s, learning 0.158s)
               Value function loss: 4.0516
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 3.75
               Mean episode length: 124.52
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 12.63s
                        Total time: 23624.24s
                               ETA: 1340375.6s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.681s, learning 0.161s)
               Value function loss: 4.2347
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 12.84s
                        Total time: 23637.08s
                               ETA: 1340316.7s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.200s, learning 0.174s)
               Value function loss: 4.9395
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 6.65
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 12.37s
                        Total time: 23649.46s
                               ETA: 1340231.4s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.688s, learning 0.205s)
               Value function loss: 27.9611
                    Surrogate loss: 0.0255
             Mean action noise std: 0.73
                       Mean reward: 0.17
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 5.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 12.89s
                        Total time: 23662.35s
                               ETA: 1340175.5s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.658s, learning 0.223s)
               Value function loss: 1.1527
                    Surrogate loss: -0.0221
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 12.88s
                        Total time: 23675.23s
                               ETA: 1340119.0s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.369s, learning 0.172s)
               Value function loss: 1.1835
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: -1.71
               Mean episode length: 123.91
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 12.54s
                        Total time: 23687.77s
                               ETA: 1340043.3s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.247s, learning 0.178s)
               Value function loss: 1.1035
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: -1.71
               Mean episode length: 123.64
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 12.42s
                        Total time: 23700.20s
                               ETA: 1339961.1s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.669s, learning 0.167s)
               Value function loss: 2.2434
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -3.33
               Mean episode length: 122.74
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 12.84s
                        Total time: 23713.03s
                               ETA: 1339902.2s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.620s, learning 0.163s)
               Value function loss: 3.2783
                    Surrogate loss: 0.0207
             Mean action noise std: 0.73
                       Mean reward: -4.11
               Mean episode length: 122.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 12.78s
                        Total time: 23725.81s
                               ETA: 1339840.4s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.303s, learning 0.170s)
               Value function loss: 2.4992
                    Surrogate loss: 0.0254
             Mean action noise std: 0.73
                       Mean reward: -6.18
               Mean episode length: 122.74
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 12.47s
                        Total time: 23738.29s
                               ETA: 1339761.1s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.257s, learning 0.162s)
               Value function loss: 2.8381
                    Surrogate loss: 0.0123
             Mean action noise std: 0.73
                       Mean reward: -5.56
               Mean episode length: 124.10
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 12.42s
                        Total time: 23750.71s
                               ETA: 1339678.9s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.067s, learning 0.248s)
               Value function loss: 4.8616
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: -0.56
               Mean episode length: 124.41
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 12.32s
                        Total time: 23763.02s
                               ETA: 1339590.9s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.506s, learning 0.302s)
               Value function loss: 4.5227
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: -2.11
               Mean episode length: 124.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 12.81s
                        Total time: 23775.83s
                               ETA: 1339530.8s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.674s, learning 0.318s)
               Value function loss: 2.9449
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 0.60
               Mean episode length: 124.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 12.99s
                        Total time: 23788.82s
                               ETA: 1339481.0s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.513s, learning 0.177s)
               Value function loss: 3.3763
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -5.63
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 12.69s
                        Total time: 23801.51s
                               ETA: 1339414.3s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.666s, learning 0.172s)
               Value function loss: 3.6303
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: -0.97
               Mean episode length: 124.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 12.84s
                        Total time: 23814.35s
                               ETA: 1339356.0s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.341s, learning 0.208s)
               Value function loss: 3.9098
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: -5.02
               Mean episode length: 124.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 12.55s
                        Total time: 23826.90s
                               ETA: 1339281.6s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.696s, learning 0.158s)
               Value function loss: 3.9262
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: -3.08
               Mean episode length: 124.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 12.85s
                        Total time: 23839.75s
                               ETA: 1339224.3s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.757s, learning 0.184s)
               Value function loss: 33.8272
                    Surrogate loss: 0.0251
             Mean action noise std: 0.73
                       Mean reward: -6.49
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 5.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 12.94s
                        Total time: 23852.69s
                               ETA: 1339171.9s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.519s, learning 0.163s)
               Value function loss: 0.8248
                    Surrogate loss: -0.0219
             Mean action noise std: 0.73
                       Mean reward: -6.11
               Mean episode length: 125.00
                  Mean reward/step: -0.70
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 12.68s
                        Total time: 23865.37s
                               ETA: 1339105.1s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.024s, learning 0.200s)
               Value function loss: 0.7867
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: -5.72
               Mean episode length: 125.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 12.22s
                        Total time: 23877.60s
                               ETA: 1339012.7s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.048s, learning 0.211s)
               Value function loss: 1.0037
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: -5.08
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 12.26s
                        Total time: 23889.86s
                               ETA: 1338922.3s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.326s, learning 0.210s)
               Value function loss: 1.3721
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: -6.23
               Mean episode length: 124.83
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 12.54s
                        Total time: 23902.39s
                               ETA: 1338847.5s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.020s, learning 0.164s)
               Value function loss: 2.9564
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: -5.95
               Mean episode length: 124.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 12.18s
                        Total time: 23914.58s
                               ETA: 1338753.1s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.290s, learning 0.167s)
               Value function loss: 3.1454
                    Surrogate loss: 0.0063
             Mean action noise std: 0.73
                       Mean reward: -6.29
               Mean episode length: 123.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 12.46s
                        Total time: 23927.04s
                               ETA: 1338674.0s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.297s, learning 0.166s)
               Value function loss: 2.8354
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: -12.28
               Mean episode length: 122.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 12.46s
                        Total time: 23939.50s
                               ETA: 1338595.3s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.296s, learning 0.222s)
               Value function loss: 3.3756
                    Surrogate loss: 0.0068
             Mean action noise std: 0.73
                       Mean reward: -5.67
               Mean episode length: 124.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 12.52s
                        Total time: 23952.02s
                               ETA: 1338519.8s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.721s, learning 0.165s)
               Value function loss: 4.5602
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: -9.87
               Mean episode length: 124.45
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 12.89s
                        Total time: 23964.90s
                               ETA: 1338464.9s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.398s, learning 0.166s)
               Value function loss: 3.1620
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: -10.57
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 12.56s
                        Total time: 23977.47s
                               ETA: 1338392.1s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.741s, learning 0.166s)
               Value function loss: 2.2659
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: -11.91
               Mean episode length: 124.42
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 11.91s
                        Total time: 23989.37s
                               ETA: 1338282.8s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.117s, learning 0.222s)
               Value function loss: 2.6895
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: -12.15
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 12.34s
                        Total time: 24001.71s
                               ETA: 1338197.6s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.071s, learning 0.164s)
               Value function loss: 2.9170
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: -16.35
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 12.23s
                        Total time: 24013.95s
                               ETA: 1338106.6s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.068s, learning 0.210s)
               Value function loss: 3.3056
                    Surrogate loss: 0.0077
             Mean action noise std: 0.73
                       Mean reward: -14.52
               Mean episode length: 124.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 12.28s
                        Total time: 24026.22s
                               ETA: 1338018.2s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.106s, learning 0.285s)
               Value function loss: 3.0637
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: -13.75
               Mean episode length: 124.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 12.39s
                        Total time: 24038.62s
                               ETA: 1337936.2s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1234 steps/s (collection: 13.092s, learning 0.179s)
               Value function loss: 23.5028
                    Surrogate loss: 0.0160
             Mean action noise std: 0.73
                       Mean reward: -8.25
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 5.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 13.27s
                        Total time: 24051.89s
                               ETA: 1337903.2s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.830s, learning 0.187s)
               Value function loss: 0.5431
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: -7.35
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 13.02s
                        Total time: 24064.90s
                               ETA: 1337856.1s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.432s, learning 0.157s)
               Value function loss: 0.6761
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: -8.79
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 12.59s
                        Total time: 24077.49s
                               ETA: 1337785.2s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.667s, learning 0.166s)
               Value function loss: 0.6510
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: -8.99
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 12.83s
                        Total time: 24090.32s
                               ETA: 1337728.0s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.429s, learning 0.158s)
               Value function loss: 1.4358
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -9.31
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 12.59s
                        Total time: 24102.91s
                               ETA: 1337657.2s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.173s, learning 0.159s)
               Value function loss: 1.8885
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -11.43
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 12.33s
                        Total time: 24115.24s
                               ETA: 1337572.2s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.022s, learning 0.175s)
               Value function loss: 1.5626
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: -11.44
               Mean episode length: 124.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 12.20s
                        Total time: 24127.44s
                               ETA: 1337479.9s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.082s, learning 0.206s)
               Value function loss: 2.0541
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: -10.18
               Mean episode length: 123.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 12.29s
                        Total time: 24139.73s
                               ETA: 1337392.7s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.776s, learning 0.161s)
               Value function loss: 3.0516
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: -16.39
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 12.94s
                        Total time: 24152.67s
                               ETA: 1337341.5s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.485s, learning 0.232s)
               Value function loss: 4.6126
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: -13.29
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 12.72s
                        Total time: 24165.38s
                               ETA: 1337278.3s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.935s, learning 0.162s)
               Value function loss: 2.9929
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: -10.57
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 12.10s
                        Total time: 24177.48s
                               ETA: 1337180.7s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.248s, learning 0.181s)
               Value function loss: 2.8325
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: -13.18
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 12.43s
                        Total time: 24189.91s
                               ETA: 1337101.7s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.104s, learning 0.168s)
               Value function loss: 2.5038
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: -12.03
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 12.27s
                        Total time: 24202.18s
                               ETA: 1337014.0s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.185s, learning 0.167s)
               Value function loss: 3.0851
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: -9.10
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 12.35s
                        Total time: 24214.53s
                               ETA: 1336930.8s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.226s, learning 0.251s)
               Value function loss: 2.8013
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: -10.53
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 12.48s
                        Total time: 24227.01s
                               ETA: 1336854.6s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.763s, learning 0.157s)
               Value function loss: 3.2489
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: -11.55
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 11.92s
                        Total time: 24238.93s
                               ETA: 1336747.8s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1245 steps/s (collection: 12.988s, learning 0.164s)
               Value function loss: 24.4445
                    Surrogate loss: 0.0144
             Mean action noise std: 0.73
                       Mean reward: -11.65
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 5.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 13.15s
                        Total time: 24252.08s
                               ETA: 1336708.9s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1248 steps/s (collection: 12.954s, learning 0.169s)
               Value function loss: 0.5881
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: -9.28
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 13.12s
                        Total time: 24265.21s
                               ETA: 1336668.5s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.167s, learning 0.180s)
               Value function loss: 0.5937
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: -9.20
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 12.35s
                        Total time: 24277.55s
                               ETA: 1336585.4s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.610s, learning 0.262s)
               Value function loss: 0.6906
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: -9.18
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 12.87s
                        Total time: 24290.42s
                               ETA: 1336531.2s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.381s, learning 0.166s)
               Value function loss: 1.4818
                    Surrogate loss: 0.0102
             Mean action noise std: 0.73
                       Mean reward: -8.77
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 12.55s
                        Total time: 24302.97s
                               ETA: 1336459.3s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.226s, learning 0.262s)
               Value function loss: 1.2899
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: -9.79
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 12.49s
                        Total time: 24315.46s
                               ETA: 1336384.2s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.368s, learning 0.170s)
               Value function loss: 1.2351
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: -10.05
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 12.54s
                        Total time: 24328.00s
                               ETA: 1336311.9s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.703s, learning 0.161s)
               Value function loss: 1.8203
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: -9.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 12.86s
                        Total time: 24340.86s
                               ETA: 1336257.5s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.664s, learning 0.188s)
               Value function loss: 3.4441
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: -8.04
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 12.85s
                        Total time: 24353.71s
                               ETA: 1336202.6s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.463s, learning 0.194s)
               Value function loss: 3.1979
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: -10.48
               Mean episode length: 124.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 12.66s
                        Total time: 24366.37s
                               ETA: 1336137.0s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.970s, learning 0.189s)
               Value function loss: 2.0606
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: -8.31
               Mean episode length: 124.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 12.16s
                        Total time: 24378.53s
                               ETA: 1336044.1s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.681s, learning 0.165s)
               Value function loss: 2.4766
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: -7.47
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 12.85s
                        Total time: 24391.38s
                               ETA: 1335989.0s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.082s, learning 0.186s)
               Value function loss: 2.4585
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: -7.25
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 12.27s
                        Total time: 24403.64s
                               ETA: 1335902.3s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.486s, learning 0.163s)
               Value function loss: 2.5788
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: -9.04
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 12.65s
                        Total time: 24416.29s
                               ETA: 1335836.5s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.810s, learning 0.171s)
               Value function loss: 3.0300
                    Surrogate loss: 0.0041
             Mean action noise std: 0.73
                       Mean reward: -5.81
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 11.98s
                        Total time: 24428.27s
                               ETA: 1335734.2s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.730s, learning 0.159s)
               Value function loss: 24.5556
                    Surrogate loss: 0.0253
             Mean action noise std: 0.73
                       Mean reward: -5.80
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 5.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 12.89s
                        Total time: 24441.16s
                               ETA: 1335681.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.870s, learning 0.163s)
               Value function loss: 0.3877
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: -5.47
               Mean episode length: 125.00
                  Mean reward/step: -0.68
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 13.03s
                        Total time: 24454.20s
                               ETA: 1335637.1s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.327s, learning 0.193s)
               Value function loss: 0.4799
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: -6.24
               Mean episode length: 125.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 12.52s
                        Total time: 24466.72s
                               ETA: 1335564.5s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.348s, learning 0.157s)
               Value function loss: 0.5412
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: -6.39
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 12.50s
                        Total time: 24479.22s
                               ETA: 1335491.1s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.184s, learning 0.246s)
               Value function loss: 1.0233
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: -7.53
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 12.43s
                        Total time: 24491.65s
                               ETA: 1335413.7s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.358s, learning 0.287s)
               Value function loss: 1.0987
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: -7.77
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 12.64s
                        Total time: 24504.30s
                               ETA: 1335348.1s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.625s, learning 0.270s)
               Value function loss: 1.0602
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: -7.87
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 12.90s
                        Total time: 24517.19s
                               ETA: 1335296.2s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1245 steps/s (collection: 12.991s, learning 0.168s)
               Value function loss: 1.4184
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: -7.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 13.16s
                        Total time: 24530.35s
                               ETA: 1335258.7s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.320s, learning 0.357s)
               Value function loss: 2.2411
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: -7.12
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 12.68s
                        Total time: 24543.03s
                               ETA: 1335195.1s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.300s, learning 0.166s)
               Value function loss: 3.9017
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: -6.23
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 12.47s
                        Total time: 24555.49s
                               ETA: 1335120.0s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.862s, learning 0.249s)
               Value function loss: 2.7104
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: -6.54
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 13.11s
                        Total time: 24568.60s
                               ETA: 1335080.0s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.192s, learning 0.166s)
               Value function loss: 2.5344
                    Surrogate loss: 0.0074
             Mean action noise std: 0.73
                       Mean reward: -7.04
               Mean episode length: 124.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 12.36s
                        Total time: 24580.96s
                               ETA: 1334999.1s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.312s, learning 0.200s)
               Value function loss: 2.6832
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: -4.26
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 12.51s
                        Total time: 24593.47s
                               ETA: 1334926.7s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.041s, learning 0.159s)
               Value function loss: 2.9468
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: -2.75
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 12.20s
                        Total time: 24605.67s
                               ETA: 1334837.4s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.215s, learning 0.165s)
               Value function loss: 3.2814
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: -6.05
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 12.38s
                        Total time: 24618.05s
                               ETA: 1334757.9s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.887s, learning 0.159s)
               Value function loss: 3.7213
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: -6.81
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 12.05s
                        Total time: 24630.10s
                               ETA: 1334660.4s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.840s, learning 0.170s)
               Value function loss: 27.6240
                    Surrogate loss: 0.0410
             Mean action noise std: 0.73
                       Mean reward: -7.86
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 5.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 13.01s
                        Total time: 24643.11s
                               ETA: 1334615.3s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.970s, learning 0.211s)
               Value function loss: 0.6025
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: -8.68
               Mean episode length: 124.39
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 12.18s
                        Total time: 24655.29s
                               ETA: 1334525.3s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.540s, learning 0.217s)
               Value function loss: 0.6098
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: -8.60
               Mean episode length: 123.95
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 12.76s
                        Total time: 24668.05s
                               ETA: 1334466.5s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.109s, learning 0.157s)
               Value function loss: 0.6185
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: -7.60
               Mean episode length: 123.95
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 12.27s
                        Total time: 24680.31s
                               ETA: 1334381.3s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.556s, learning 0.199s)
               Value function loss: 1.0490
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: -6.40
               Mean episode length: 123.95
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 12.76s
                        Total time: 24693.07s
                               ETA: 1334322.6s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.605s, learning 0.189s)
               Value function loss: 1.1045
                    Surrogate loss: 0.0024
             Mean action noise std: 0.73
                       Mean reward: -5.69
               Mean episode length: 123.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 12.79s
                        Total time: 24705.86s
                               ETA: 1334266.0s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.118s, learning 0.207s)
               Value function loss: 1.2401
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: -3.19
               Mean episode length: 124.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 12.32s
                        Total time: 24718.19s
                               ETA: 1334184.1s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.529s, learning 0.157s)
               Value function loss: 1.8149
                    Surrogate loss: 0.0039
             Mean action noise std: 0.73
                       Mean reward: -1.82
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 11.69s
                        Total time: 24729.87s
                               ETA: 1334067.9s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1245 steps/s (collection: 12.899s, learning 0.259s)
               Value function loss: 3.3295
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: -3.00
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 13.16s
                        Total time: 24743.03s
                               ETA: 1334031.1s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.298s, learning 0.161s)
               Value function loss: 3.6357
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: -7.43
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 12.46s
                        Total time: 24755.49s
                               ETA: 1333956.7s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.261s, learning 0.165s)
               Value function loss: 2.8030
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: -5.39
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 12.43s
                        Total time: 24767.92s
                               ETA: 1333880.6s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.554s, learning 0.174s)
               Value function loss: 3.4139
                    Surrogate loss: -0.0018
             Mean action noise std: 0.73
                       Mean reward: -6.99
               Mean episode length: 124.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 12.73s
                        Total time: 24780.64s
                               ETA: 1333820.8s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.205s, learning 0.208s)
               Value function loss: 3.2245
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: -1.83
               Mean episode length: 124.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 12.41s
                        Total time: 24793.06s
                               ETA: 1333744.2s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.142s, learning 0.162s)
               Value function loss: 3.6621
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: -2.83
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 12.30s
                        Total time: 24805.36s
                               ETA: 1333661.7s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.291s, learning 0.160s)
               Value function loss: 3.5161
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: -6.24
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 12.45s
                        Total time: 24817.81s
                               ETA: 1333587.2s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.779s, learning 0.177s)
               Value function loss: 7.3406
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: -0.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 12.96s
                        Total time: 24830.77s
                               ETA: 1333539.9s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.741s, learning 0.164s)
               Value function loss: 22.8394
                    Surrogate loss: 0.0230
             Mean action noise std: 0.73
                       Mean reward: -0.68
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 12.91s
                        Total time: 24843.67s
                               ETA: 1333489.9s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.991s, learning 0.170s)
               Value function loss: 0.6235
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: -1.52
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 12.16s
                        Total time: 24855.83s
                               ETA: 1333400.0s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.321s, learning 0.163s)
               Value function loss: 0.6853
                    Surrogate loss: 0.0002
             Mean action noise std: 0.73
                       Mean reward: -1.26
               Mean episode length: 125.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 12.48s
                        Total time: 24868.32s
                               ETA: 1333327.6s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.902s, learning 0.164s)
               Value function loss: 0.5995
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: -2.78
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 12.07s
                        Total time: 24880.38s
                               ETA: 1333232.7s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.449s, learning 0.160s)
               Value function loss: 1.2167
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: -1.85
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 12.61s
                        Total time: 24892.99s
                               ETA: 1333167.1s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.771s, learning 0.172s)
               Value function loss: 1.0699
                    Surrogate loss: 0.0047
             Mean action noise std: 0.73
                       Mean reward: -1.44
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 12.94s
                        Total time: 24905.94s
                               ETA: 1333119.4s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.536s, learning 0.172s)
               Value function loss: 1.2375
                    Surrogate loss: 0.0002
             Mean action noise std: 0.73
                       Mean reward: -2.80
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 12.71s
                        Total time: 24918.64s
                               ETA: 1333059.2s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.094s, learning 0.164s)
               Value function loss: 2.2180
                    Surrogate loss: 0.0027
             Mean action noise std: 0.73
                       Mean reward: -2.39
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 12.26s
                        Total time: 24930.90s
                               ETA: 1332974.9s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.853s, learning 0.167s)
               Value function loss: 3.9122
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: -0.83
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 13.02s
                        Total time: 24943.92s
                               ETA: 1332931.5s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.697s, learning 0.166s)
               Value function loss: 3.9323
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: -0.45
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 12.86s
                        Total time: 24956.79s
                               ETA: 1332879.7s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.276s, learning 0.206s)
               Value function loss: 2.7905
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 0.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 12.48s
                        Total time: 24969.27s
                               ETA: 1332807.6s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.746s, learning 0.162s)
               Value function loss: 3.5446
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 12.91s
                        Total time: 24982.18s
                               ETA: 1332758.3s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.481s, learning 0.168s)
               Value function loss: 3.3380
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 2.17
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 12.65s
                        Total time: 24994.82s
                               ETA: 1332695.3s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.710s, learning 0.173s)
               Value function loss: 3.9026
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 0.37
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 12.88s
                        Total time: 25007.71s
                               ETA: 1332644.7s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.610s, learning 0.184s)
               Value function loss: 4.2935
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 0.08
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 12.79s
                        Total time: 25020.50s
                               ETA: 1332589.4s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1222 steps/s (collection: 13.235s, learning 0.169s)
               Value function loss: 35.7128
                    Surrogate loss: 0.0195
             Mean action noise std: 0.73
                       Mean reward: 2.76
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 5.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 13.40s
                        Total time: 25033.90s
                               ETA: 1332566.7s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.510s, learning 0.162s)
               Value function loss: 0.6090
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 3.44
               Mean episode length: 125.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 12.67s
                        Total time: 25046.58s
                               ETA: 1332505.0s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.320s, learning 0.189s)
               Value function loss: 0.6703
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 1.92
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 12.51s
                        Total time: 25059.09s
                               ETA: 1332434.7s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.989s, learning 0.233s)
               Value function loss: 0.6857
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 1.55
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 12.22s
                        Total time: 25071.31s
                               ETA: 1332349.3s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.446s, learning 0.158s)
               Value function loss: 1.2590
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 0.98
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 12.60s
                        Total time: 25083.91s
                               ETA: 1332284.2s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.456s, learning 0.156s)
               Value function loss: 1.4138
                    Surrogate loss: 0.0035
             Mean action noise std: 0.73
                       Mean reward: 1.75
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 12.61s
                        Total time: 25096.52s
                               ETA: 1332219.5s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.334s, learning 0.164s)
               Value function loss: 1.2259
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 2.60
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 12.50s
                        Total time: 25109.02s
                               ETA: 1332148.9s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.499s, learning 0.178s)
               Value function loss: 1.9248
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 3.15
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 12.68s
                        Total time: 25121.70s
                               ETA: 1332087.8s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.687s, learning 0.209s)
               Value function loss: 3.3805
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 4.05
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 11.90s
                        Total time: 25133.59s
                               ETA: 1331985.4s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.701s, learning 0.171s)
               Value function loss: 5.0530
                    Surrogate loss: 0.0005
             Mean action noise std: 0.73
                       Mean reward: 3.02
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 12.87s
                        Total time: 25146.47s
                               ETA: 1331934.8s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.550s, learning 0.157s)
               Value function loss: 3.2428
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 1.48
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 12.71s
                        Total time: 25159.17s
                               ETA: 1331875.6s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.167s, learning 0.166s)
               Value function loss: 3.5604
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 2.33
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 12.33s
                        Total time: 25171.51s
                               ETA: 1331796.6s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.366s, learning 0.167s)
               Value function loss: 3.4125
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 3.00
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 12.53s
                        Total time: 25184.04s
                               ETA: 1331728.2s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.546s, learning 0.269s)
               Value function loss: 3.8259
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 1.16
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 12.82s
                        Total time: 25196.85s
                               ETA: 1331674.7s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.418s, learning 0.267s)
               Value function loss: 4.1315
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: -1.50
               Mean episode length: 124.91
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 12.69s
                        Total time: 25209.54s
                               ETA: 1331614.5s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.720s, learning 0.216s)
               Value function loss: 4.0820
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 0.64
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 12.94s
                        Total time: 25222.47s
                               ETA: 1331567.6s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1221 steps/s (collection: 13.245s, learning 0.163s)
               Value function loss: 29.0238
                    Surrogate loss: 0.0273
             Mean action noise std: 0.73
                       Mean reward: 2.43
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 5.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 13.41s
                        Total time: 25235.88s
                               ETA: 1331545.6s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.032s, learning 0.167s)
               Value function loss: 0.6871
                    Surrogate loss: -0.0234
             Mean action noise std: 0.73
                       Mean reward: 3.16
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 12.20s
                        Total time: 25248.08s
                               ETA: 1331459.8s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.555s, learning 0.203s)
               Value function loss: 0.7891
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 4.34
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 12.76s
                        Total time: 25260.84s
                               ETA: 1331403.6s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.598s, learning 0.243s)
               Value function loss: 0.6812
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 3.74
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 12.84s
                        Total time: 25273.68s
                               ETA: 1331351.8s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.794s, learning 0.164s)
               Value function loss: 1.3298
                    Surrogate loss: 0.0121
             Mean action noise std: 0.73
                       Mean reward: 4.09
               Mean episode length: 125.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 12.96s
                        Total time: 25286.64s
                               ETA: 1331306.2s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.232s, learning 0.156s)
               Value function loss: 1.1669
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 1.62
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 12.39s
                        Total time: 25299.03s
                               ETA: 1331230.7s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.293s, learning 0.257s)
               Value function loss: 1.3738
                    Surrogate loss: 0.0055
             Mean action noise std: 0.73
                       Mean reward: 0.05
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 12.55s
                        Total time: 25311.58s
                               ETA: 1331163.7s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.084s, learning 0.166s)
               Value function loss: 2.0740
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: -2.17
               Mean episode length: 124.81
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 12.25s
                        Total time: 25323.83s
                               ETA: 1331081.1s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.442s, learning 0.207s)
               Value function loss: 3.6789
                    Surrogate loss: 0.0079
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 12.65s
                        Total time: 25336.48s
                               ETA: 1331019.5s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.994s, learning 0.162s)
               Value function loss: 3.7287
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 12.16s
                        Total time: 25348.63s
                               ETA: 1330932.0s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.299s, learning 0.174s)
               Value function loss: 2.7164
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 4.07
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 12.47s
                        Total time: 25361.10s
                               ETA: 1330861.3s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.578s, learning 0.192s)
               Value function loss: 3.4414
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 5.04
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 12.77s
                        Total time: 25373.87s
                               ETA: 1330806.1s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.516s, learning 0.158s)
               Value function loss: 3.7976
                    Surrogate loss: 0.0024
             Mean action noise std: 0.73
                       Mean reward: 3.21
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 12.67s
                        Total time: 25386.55s
                               ETA: 1330746.1s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.297s, learning 0.162s)
               Value function loss: 4.0341
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 4.06
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 12.46s
                        Total time: 25399.01s
                               ETA: 1330674.7s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.044s, learning 0.198s)
               Value function loss: 4.2694
                    Surrogate loss: 0.0028
             Mean action noise std: 0.73
                       Mean reward: 4.01
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 12.24s
                        Total time: 25411.25s
                               ETA: 1330592.1s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.162s, learning 0.163s)
               Value function loss: 32.2054
                    Surrogate loss: 0.0235
             Mean action noise std: 0.73
                       Mean reward: 1.47
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 5.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 16.32s
                        Total time: 25427.57s
                               ETA: 1330723.2s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.226s, learning 0.167s)
               Value function loss: 0.9101
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 2.13
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 23.39s
                        Total time: 25450.97s
                               ETA: 1331223.9s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.572s, learning 0.302s)
               Value function loss: 0.7242
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 2.54
               Mean episode length: 125.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 23.87s
                        Total time: 25474.84s
                               ETA: 1331749.1s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.243s, learning 0.174s)
               Value function loss: 0.7723
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 2.85
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 23.42s
                        Total time: 25498.26s
                               ETA: 1332250.0s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 673 steps/s (collection: 24.130s, learning 0.181s)
               Value function loss: 0.7779
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 2.43
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 24.31s
                        Total time: 25522.57s
                               ETA: 1332796.9s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.988s, learning 0.171s)
               Value function loss: 1.4509
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 3.55
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 24.16s
                        Total time: 25546.73s
                               ETA: 1333335.2s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 676 steps/s (collection: 23.937s, learning 0.291s)
               Value function loss: 1.6406
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 4.26
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 24.23s
                        Total time: 25570.95s
                               ETA: 1333876.6s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.494s, learning 0.204s)
               Value function loss: 2.9766
                    Surrogate loss: 0.0050
             Mean action noise std: 0.73
                       Mean reward: 4.06
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 23.70s
                        Total time: 25594.65s
                               ETA: 1334389.8s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.307s, learning 0.173s)
               Value function loss: 5.3677
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 2.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 23.48s
                        Total time: 25618.13s
                               ETA: 1334891.0s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.375s, learning 0.160s)
               Value function loss: 9.8499
                    Surrogate loss: 0.0049
             Mean action noise std: 0.73
                       Mean reward: 4.46
               Mean episode length: 124.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 23.53s
                        Total time: 25641.67s
                               ETA: 1335394.6s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.412s, learning 0.169s)
               Value function loss: 11.6506
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 23.58s
                        Total time: 25665.25s
                               ETA: 1335900.0s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.673s, learning 0.191s)
               Value function loss: 12.7346
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: 3.28
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 23.86s
                        Total time: 25689.11s
                               ETA: 1336419.5s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 706 steps/s (collection: 23.039s, learning 0.163s)
               Value function loss: 17.3687
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 5.11
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 23.20s
                        Total time: 25712.31s
                               ETA: 1336904.1s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.720s, learning 0.173s)
               Value function loss: 21.1741
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: 1.22
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 23.89s
                        Total time: 25736.21s
                               ETA: 1337424.0s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.167s, learning 0.184s)
               Value function loss: 23.0503
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 3.43
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 23.35s
                        Total time: 25759.56s
                               ETA: 1337915.1s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 674 steps/s (collection: 24.024s, learning 0.254s)
               Value function loss: 25.5194
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 4.83
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 24.28s
                        Total time: 25783.84s
                               ETA: 1338453.9s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 667 steps/s (collection: 24.337s, learning 0.218s)
               Value function loss: 43.2319
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: 6.61
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 5.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 24.55s
                        Total time: 25808.39s
                               ETA: 1339006.4s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.552s, learning 0.191s)
               Value function loss: 1.0917
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 5.71
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 23.74s
                        Total time: 25832.13s
                               ETA: 1339516.3s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.957s, learning 0.170s)
               Value function loss: 1.0979
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 5.10
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 23.13s
                        Total time: 25855.26s
                               ETA: 1339993.6s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.300s, learning 0.187s)
               Value function loss: 1.0007
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 5.51
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 23.49s
                        Total time: 25878.75s
                               ETA: 1340489.0s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.680s, learning 0.177s)
               Value function loss: 1.7637
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 3.76
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 23.86s
                        Total time: 25902.60s
                               ETA: 1341003.1s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 659 steps/s (collection: 24.598s, learning 0.231s)
               Value function loss: 1.4481
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: 3.61
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 24.83s
                        Total time: 25927.43s
                               ETA: 1341566.9s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 672 steps/s (collection: 24.194s, learning 0.176s)
               Value function loss: 1.1919
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 5.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 24.37s
                        Total time: 25951.80s
                               ETA: 1342106.3s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 675 steps/s (collection: 24.055s, learning 0.185s)
               Value function loss: 2.2500
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 7.23
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 24.24s
                        Total time: 25976.04s
                               ETA: 1342638.4s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.228s, learning 0.163s)
               Value function loss: 3.2390
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 7.27
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 23.39s
                        Total time: 25999.43s
                               ETA: 1343126.1s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 676 steps/s (collection: 24.033s, learning 0.177s)
               Value function loss: 4.4300
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 6.84
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 24.21s
                        Total time: 26023.64s
                               ETA: 1343655.5s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.185s, learning 0.168s)
               Value function loss: 3.2247
                    Surrogate loss: 0.0031
             Mean action noise std: 0.73
                       Mean reward: 5.29
               Mean episode length: 124.55
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 23.35s
                        Total time: 26047.00s
                               ETA: 1344140.2s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.534s, learning 0.217s)
               Value function loss: 3.6460
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 5.08
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 23.75s
                        Total time: 26070.75s
                               ETA: 1344644.8s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 706 steps/s (collection: 23.013s, learning 0.187s)
               Value function loss: 3.5329
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: 3.05
               Mean episode length: 124.70
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 23.20s
                        Total time: 26093.95s
                               ETA: 1345120.4s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.101s, learning 0.193s)
               Value function loss: 3.8176
                    Surrogate loss: 0.0147
             Mean action noise std: 0.73
                       Mean reward: 4.18
               Mean episode length: 124.39
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 23.29s
                        Total time: 26117.24s
                               ETA: 1345600.4s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 674 steps/s (collection: 24.116s, learning 0.164s)
               Value function loss: 3.6338
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 4.72
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 24.28s
                        Total time: 26141.52s
                               ETA: 1346130.6s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.882s, learning 0.211s)
               Value function loss: 4.1742
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: 3.85
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 24.09s
                        Total time: 26165.62s
                               ETA: 1346650.6s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 667 steps/s (collection: 24.367s, learning 0.162s)
               Value function loss: 33.3963
                    Surrogate loss: 0.0103
             Mean action noise std: 0.73
                       Mean reward: 1.96
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 5.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 24.53s
                        Total time: 26190.15s
                               ETA: 1347192.5s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.643s, learning 0.159s)
               Value function loss: 0.6889
                    Surrogate loss: -0.0155
             Mean action noise std: 0.73
                       Mean reward: 1.90
               Mean episode length: 125.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 23.80s
                        Total time: 26213.95s
                               ETA: 1347696.4s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.259s, learning 0.168s)
               Value function loss: 0.9792
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 23.43s
                        Total time: 26237.37s
                               ETA: 1348180.5s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.217s, learning 0.188s)
               Value function loss: 1.0328
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 2.76
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 23.41s
                        Total time: 26260.78s
                               ETA: 1348662.9s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.222s, learning 0.236s)
               Value function loss: 2.1729
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 4.19
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 23.46s
                        Total time: 26284.24s
                               ETA: 1349147.5s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 684 steps/s (collection: 23.765s, learning 0.164s)
               Value function loss: 1.7324
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 2.89
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 23.93s
                        Total time: 26308.17s
                               ETA: 1349655.7s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 951 steps/s (collection: 16.946s, learning 0.280s)
               Value function loss: 1.7359
                    Surrogate loss: 0.0508
             Mean action noise std: 0.73
                       Mean reward: 1.15
               Mean episode length: 124.39
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 17.23s
                        Total time: 26325.39s
                               ETA: 1349819.7s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.395s, learning 0.203s)
               Value function loss: 2.3125
                    Surrogate loss: 0.0104
             Mean action noise std: 0.73
                       Mean reward: -3.10
               Mean episode length: 124.39
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 12.60s
                        Total time: 26337.99s
                               ETA: 1349746.3s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.338s, learning 0.163s)
               Value function loss: 4.6055
                    Surrogate loss: 0.0041
             Mean action noise std: 0.73
                       Mean reward: -1.28
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 12.50s
                        Total time: 26350.49s
                               ETA: 1349668.0s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.938s, learning 0.166s)
               Value function loss: 4.1748
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: -1.85
               Mean episode length: 124.95
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 12.10s
                        Total time: 26362.60s
                               ETA: 1349569.5s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.971s, learning 0.164s)
               Value function loss: 2.8462
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: -4.39
               Mean episode length: 124.51
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 12.13s
                        Total time: 26374.73s
                               ETA: 1349472.6s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.262s, learning 0.183s)
               Value function loss: 3.7693
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: -3.49
               Mean episode length: 124.64
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 12.45s
                        Total time: 26387.18s
                               ETA: 1349391.7s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.025s, learning 0.164s)
               Value function loss: 3.7799
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -1.30
               Mean episode length: 124.22
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 12.19s
                        Total time: 26399.37s
                               ETA: 1349297.8s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.206s, learning 0.168s)
               Value function loss: 4.1947
                    Surrogate loss: 0.0035
             Mean action noise std: 0.73
                       Mean reward: -1.28
               Mean episode length: 124.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 12.37s
                        Total time: 26411.74s
                               ETA: 1349213.4s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.721s, learning 0.159s)
               Value function loss: 3.7016
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: -2.69
               Mean episode length: 124.30
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 12.88s
                        Total time: 26424.62s
                               ETA: 1349155.0s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.903s, learning 0.202s)
               Value function loss: 36.9071
                    Surrogate loss: 0.0466
             Mean action noise std: 0.73
                       Mean reward: 0.57
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 5.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 13.11s
                        Total time: 26437.72s
                               ETA: 1349108.0s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.214s, learning 0.157s)
               Value function loss: 1.0940
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: -1.29
               Mean episode length: 124.83
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 12.37s
                        Total time: 26450.10s
                               ETA: 1349023.7s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.296s, learning 0.202s)
               Value function loss: 0.9153
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: -3.52
               Mean episode length: 124.83
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 12.50s
                        Total time: 26462.59s
                               ETA: 1348945.8s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.583s, learning 0.158s)
               Value function loss: 1.2993
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: -4.05
               Mean episode length: 124.83
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 12.74s
                        Total time: 26475.34s
                               ETA: 1348880.5s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.452s, learning 0.165s)
               Value function loss: 1.5914
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -6.31
               Mean episode length: 124.83
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 12.62s
                        Total time: 26487.95s
                               ETA: 1348808.9s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.097s, learning 0.201s)
               Value function loss: 1.9030
                    Surrogate loss: 0.0031
             Mean action noise std: 0.73
                       Mean reward: -6.13
               Mean episode length: 124.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 12.30s
                        Total time: 26500.25s
                               ETA: 1348721.1s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.517s, learning 0.167s)
               Value function loss: 1.4747
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: -6.77
               Mean episode length: 124.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 12.68s
                        Total time: 26512.93s
                               ETA: 1348653.0s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.726s, learning 0.169s)
               Value function loss: 2.6192
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: -4.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 12.90s
                        Total time: 26525.83s
                               ETA: 1348595.7s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.120s, learning 0.165s)
               Value function loss: 2.8231
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: -2.82
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 12.29s
                        Total time: 26538.11s
                               ETA: 1348507.5s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.457s, learning 0.157s)
               Value function loss: 3.9128
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: -2.96
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 12.61s
                        Total time: 26550.73s
                               ETA: 1348436.0s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.641s, learning 0.157s)
               Value function loss: 3.1365
                    Surrogate loss: 0.0095
             Mean action noise std: 0.73
                       Mean reward: -1.98
               Mean episode length: 124.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 12.80s
                        Total time: 26563.53s
                               ETA: 1348373.9s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.288s, learning 0.201s)
               Value function loss: 2.3705
                    Surrogate loss: 0.0061
             Mean action noise std: 0.73
                       Mean reward: -2.23
               Mean episode length: 124.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 12.49s
                        Total time: 26576.02s
                               ETA: 1348296.3s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.467s, learning 0.260s)
               Value function loss: 2.3375
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: -0.80
               Mean episode length: 124.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 12.73s
                        Total time: 26588.74s
                               ETA: 1348230.7s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.598s, learning 0.195s)
               Value function loss: 2.6981
                    Surrogate loss: -0.0018
             Mean action noise std: 0.73
                       Mean reward: -6.04
               Mean episode length: 124.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 12.79s
                        Total time: 26601.54s
                               ETA: 1348168.6s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.199s, learning 0.200s)
               Value function loss: 2.8116
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: -4.33
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 12.40s
                        Total time: 26613.93s
                               ETA: 1348086.5s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.295s, learning 0.162s)
               Value function loss: 2.7794
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: -3.84
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 12.46s
                        Total time: 26626.39s
                               ETA: 1348007.4s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.935s, learning 0.162s)
               Value function loss: 21.8427
                    Surrogate loss: 0.0281
             Mean action noise std: 0.73
                       Mean reward: -4.22
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 5.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 13.10s
                        Total time: 26639.49s
                               ETA: 1347960.8s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1243 steps/s (collection: 13.009s, learning 0.169s)
               Value function loss: 0.5335
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: -3.60
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 13.18s
                        Total time: 26652.67s
                               ETA: 1347918.3s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.625s, learning 0.166s)
               Value function loss: 0.6989
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: -0.97
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 12.79s
                        Total time: 26665.46s
                               ETA: 1347856.3s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.158s, learning 0.179s)
               Value function loss: 0.6383
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 0.58
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 12.34s
                        Total time: 26677.79s
                               ETA: 1347771.5s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.162s, learning 0.160s)
               Value function loss: 1.1066
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: -1.20
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 12.32s
                        Total time: 26690.12s
                               ETA: 1347685.9s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.513s, learning 0.156s)
               Value function loss: 1.0683
                    Surrogate loss: 0.0038
             Mean action noise std: 0.73
                       Mean reward: -0.94
               Mean episode length: 124.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 12.67s
                        Total time: 26702.78s
                               ETA: 1347617.9s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.532s, learning 0.173s)
               Value function loss: 1.0137
                    Surrogate loss: 0.0057
             Mean action noise std: 0.73
                       Mean reward: -3.62
               Mean episode length: 124.70
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 12.71s
                        Total time: 26715.49s
                               ETA: 1347551.8s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.684s, learning 0.162s)
               Value function loss: 1.5880
                    Surrogate loss: 0.0193
             Mean action noise std: 0.73
                       Mean reward: -5.31
               Mean episode length: 124.70
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 12.85s
                        Total time: 26728.33s
                               ETA: 1347492.9s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.092s, learning 0.156s)
               Value function loss: 2.8462
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: -6.84
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 12.25s
                        Total time: 26740.58s
                               ETA: 1347403.8s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.125s, learning 0.167s)
               Value function loss: 3.5941
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: -5.38
               Mean episode length: 124.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 12.29s
                        Total time: 26752.87s
                               ETA: 1347317.1s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.912s, learning 0.166s)
               Value function loss: 2.6138
                    Surrogate loss: 0.0139
             Mean action noise std: 0.73
                       Mean reward: -5.41
               Mean episode length: 124.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 12.08s
                        Total time: 26764.95s
                               ETA: 1347219.6s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.877s, learning 0.164s)
               Value function loss: 2.8100
                    Surrogate loss: 0.0030
             Mean action noise std: 0.73
                       Mean reward: -4.35
               Mean episode length: 125.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 12.04s
                        Total time: 26776.99s
                               ETA: 1347120.4s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.819s, learning 0.156s)
               Value function loss: 2.6217
                    Surrogate loss: 0.0048
             Mean action noise std: 0.73
                       Mean reward: -6.69
               Mean episode length: 124.74
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 12.98s
                        Total time: 26789.97s
                               ETA: 1347068.3s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.248s, learning 0.168s)
               Value function loss: 3.0240
                    Surrogate loss: 0.0040
             Mean action noise std: 0.73
                       Mean reward: -10.47
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 12.42s
                        Total time: 26802.38s
                               ETA: 1346988.1s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.412s, learning 0.159s)
               Value function loss: 2.5154
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: -11.93
               Mean episode length: 124.84
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 12.57s
                        Total time: 26814.96s
                               ETA: 1346915.7s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.679s, learning 0.185s)
               Value function loss: 5.4098
                    Surrogate loss: 0.0341
             Mean action noise std: 0.73
                       Mean reward: -7.62
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 12.86s
                        Total time: 26827.82s
                               ETA: 1346858.2s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1247 steps/s (collection: 12.923s, learning 0.207s)
               Value function loss: 16.7741
                    Surrogate loss: 0.0173
             Mean action noise std: 0.73
                       Mean reward: -7.78
               Mean episode length: 124.46
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 13.13s
                        Total time: 26840.95s
                               ETA: 1346814.1s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.294s, learning 0.160s)
               Value function loss: 0.5261
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: -7.87
               Mean episode length: 124.46
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 12.45s
                        Total time: 26853.40s
                               ETA: 1346736.0s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.750s, learning 0.176s)
               Value function loss: 0.6847
                    Surrogate loss: 0.0015
             Mean action noise std: 0.73
                       Mean reward: -7.85
               Mean episode length: 124.46
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 12.93s
                        Total time: 26866.33s
                               ETA: 1346681.7s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.492s, learning 0.162s)
               Value function loss: 0.7259
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -9.02
               Mean episode length: 123.84
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 12.65s
                        Total time: 26878.98s
                               ETA: 1346613.8s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.592s, learning 0.165s)
               Value function loss: 1.2531
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: -9.55
               Mean episode length: 123.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 12.76s
                        Total time: 26891.74s
                               ETA: 1346551.1s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.512s, learning 0.161s)
               Value function loss: 1.0520
                    Surrogate loss: 0.0116
             Mean action noise std: 0.73
                       Mean reward: -8.01
               Mean episode length: 124.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 12.67s
                        Total time: 26904.42s
                               ETA: 1346484.3s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.355s, learning 0.176s)
               Value function loss: 1.0683
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: -9.18
               Mean episode length: 124.38
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 12.53s
                        Total time: 26916.95s
                               ETA: 1346410.4s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.848s, learning 0.166s)
               Value function loss: 1.5963
                    Surrogate loss: 0.0142
             Mean action noise std: 0.73
                       Mean reward: -9.62
               Mean episode length: 124.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 13.01s
                        Total time: 26929.96s
                               ETA: 1346360.7s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.305s, learning 0.171s)
               Value function loss: 2.7705
                    Surrogate loss: 0.0052
             Mean action noise std: 0.73
                       Mean reward: -11.52
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 12.48s
                        Total time: 26942.44s
                               ETA: 1346284.2s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.874s, learning 0.162s)
               Value function loss: 2.5968
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: -11.19
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 12.04s
                        Total time: 26954.47s
                               ETA: 1346185.8s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.806s, learning 0.193s)
               Value function loss: 1.7884
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: -12.44
               Mean episode length: 123.86
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 13.00s
                        Total time: 26967.47s
                               ETA: 1346135.5s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.211s, learning 0.167s)
               Value function loss: 2.1102
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: -13.70
               Mean episode length: 124.64
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 12.38s
                        Total time: 26979.85s
                               ETA: 1346054.2s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.119s, learning 0.186s)
               Value function loss: 2.0675
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: -14.14
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 12.31s
                        Total time: 26992.16s
                               ETA: 1345969.4s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.797s, learning 0.179s)
               Value function loss: 2.3327
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: -13.92
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 12.98s
                        Total time: 27005.13s
                               ETA: 1345918.2s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.395s, learning 0.158s)
               Value function loss: 2.5854
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: -12.78
               Mean episode length: 124.77
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 12.55s
                        Total time: 27017.69s
                               ETA: 1345845.9s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.778s, learning 0.162s)
               Value function loss: 24.4617
                    Surrogate loss: 0.0183
             Mean action noise std: 0.73
                       Mean reward: -14.76
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 5.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 12.94s
                        Total time: 27030.63s
                               ETA: 1345792.9s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.329s, learning 0.185s)
               Value function loss: 0.4887
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -15.34
               Mean episode length: 124.37
                  Mean reward/step: -0.64
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 12.51s
                        Total time: 27043.14s
                               ETA: 1345718.8s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.273s, learning 0.249s)
               Value function loss: 0.4164
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: -16.20
               Mean episode length: 124.37
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 12.52s
                        Total time: 27055.66s
                               ETA: 1345645.0s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.513s, learning 0.278s)
               Value function loss: 0.5082
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -15.44
               Mean episode length: 124.37
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 12.79s
                        Total time: 27068.45s
                               ETA: 1345584.8s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.588s, learning 0.234s)
               Value function loss: 0.7160
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: -16.35
               Mean episode length: 124.37
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 12.82s
                        Total time: 27081.27s
                               ETA: 1345526.1s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.505s, learning 0.158s)
               Value function loss: 0.8527
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: -15.60
               Mean episode length: 123.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 12.66s
                        Total time: 27093.94s
                               ETA: 1345459.6s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.273s, learning 0.163s)
               Value function loss: 0.8762
                    Surrogate loss: 0.0059
             Mean action noise std: 0.73
                       Mean reward: -13.71
               Mean episode length: 124.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 12.44s
                        Total time: 27106.37s
                               ETA: 1345381.8s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.690s, learning 0.182s)
               Value function loss: 1.3158
                    Surrogate loss: 0.0044
             Mean action noise std: 0.73
                       Mean reward: -12.91
               Mean episode length: 124.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 12.87s
                        Total time: 27119.24s
                               ETA: 1345325.8s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.526s, learning 0.191s)
               Value function loss: 1.8790
                    Surrogate loss: 0.0057
             Mean action noise std: 0.73
                       Mean reward: -12.37
               Mean episode length: 124.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 12.72s
                        Total time: 27131.96s
                               ETA: 1345262.2s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.365s, learning 0.170s)
               Value function loss: 2.6619
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: -14.91
               Mean episode length: 125.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 12.54s
                        Total time: 27144.50s
                               ETA: 1345189.5s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.171s, learning 0.268s)
               Value function loss: 1.8732
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: -16.66
               Mean episode length: 124.09
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 12.44s
                        Total time: 27156.93s
                               ETA: 1345112.2s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.465s, learning 0.173s)
               Value function loss: 1.8213
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: -13.61
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 12.64s
                        Total time: 27169.57s
                               ETA: 1345044.8s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.296s, learning 0.249s)
               Value function loss: 1.8291
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: -14.80
               Mean episode length: 124.70
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 12.55s
                        Total time: 27182.12s
                               ETA: 1344972.8s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.408s, learning 0.164s)
               Value function loss: 1.9381
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: -13.18
               Mean episode length: 124.80
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 12.57s
                        Total time: 27194.69s
                               ETA: 1344902.3s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.821s, learning 0.168s)
               Value function loss: 1.9050
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: -15.77
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 12.99s
                        Total time: 27207.68s
                               ETA: 1344852.4s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.370s, learning 0.185s)
               Value function loss: 1.9255
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -13.23
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 12.55s
                        Total time: 27220.23s
                               ETA: 1344781.1s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.910s, learning 0.160s)
               Value function loss: 16.8417
                    Surrogate loss: 0.0234
             Mean action noise std: 0.73
                       Mean reward: -11.27
               Mean episode length: 125.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 5.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 13.07s
                        Total time: 27233.30s
                               ETA: 1344735.3s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.618s, learning 0.158s)
               Value function loss: 0.4056
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: -11.61
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 12.78s
                        Total time: 27246.08s
                               ETA: 1344675.0s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.478s, learning 0.201s)
               Value function loss: 0.5876
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: -11.54
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 12.68s
                        Total time: 27258.76s
                               ETA: 1344609.9s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.292s, learning 0.193s)
               Value function loss: 0.6218
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: -12.53
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 12.49s
                        Total time: 27271.24s
                               ETA: 1344535.4s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.498s, learning 0.158s)
               Value function loss: 1.1449
                    Surrogate loss: 0.0007
             Mean action noise std: 0.73
                       Mean reward: -14.00
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 12.66s
                        Total time: 27283.90s
                               ETA: 1344469.4s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.248s, learning 0.169s)
               Value function loss: 0.9528
                    Surrogate loss: 0.0077
             Mean action noise std: 0.73
                       Mean reward: -14.45
               Mean episode length: 124.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 12.42s
                        Total time: 27296.32s
                               ETA: 1344391.7s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1263 steps/s (collection: 12.758s, learning 0.208s)
               Value function loss: 0.9874
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: -14.81
               Mean episode length: 123.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 12.97s
                        Total time: 27309.28s
                               ETA: 1344341.0s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.226s, learning 0.211s)
               Value function loss: 1.4395
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: -13.97
               Mean episode length: 123.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 12.44s
                        Total time: 27321.72s
                               ETA: 1344264.3s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.099s, learning 0.216s)
               Value function loss: 2.3533
                    Surrogate loss: 0.0114
             Mean action noise std: 0.73
                       Mean reward: -12.87
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 12.31s
                        Total time: 27334.04s
                               ETA: 1344181.7s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.172s, learning 0.167s)
               Value function loss: 2.5568
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: -14.36
               Mean episode length: 124.53
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 12.34s
                        Total time: 27346.37s
                               ETA: 1344100.4s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.254s, learning 0.228s)
               Value function loss: 1.8032
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: -16.13
               Mean episode length: 123.96
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 12.48s
                        Total time: 27358.86s
                               ETA: 1344026.1s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.984s, learning 0.185s)
               Value function loss: 2.0066
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: -11.97
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 12.17s
                        Total time: 27371.03s
                               ETA: 1343936.6s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.060s, learning 0.159s)
               Value function loss: 1.9189
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: -17.74
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 12.22s
                        Total time: 27383.24s
                               ETA: 1343849.5s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.210s, learning 0.212s)
               Value function loss: 2.2021
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: -13.94
               Mean episode length: 124.65
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 12.42s
                        Total time: 27395.67s
                               ETA: 1343772.5s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.676s, learning 0.258s)
               Value function loss: 2.2719
                    Surrogate loss: 0.0051
             Mean action noise std: 0.73
                       Mean reward: -14.27
               Mean episode length: 124.89
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 12.93s
                        Total time: 27408.60s
                               ETA: 1343720.7s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.870s, learning 0.159s)
               Value function loss: 16.7566
                    Surrogate loss: 0.0531
             Mean action noise std: 0.73
                       Mean reward: -14.92
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 5.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 13.03s
                        Total time: 27421.63s
                               ETA: 1343673.6s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.242s, learning 0.254s)
               Value function loss: 0.3373
                    Surrogate loss: -0.0175
             Mean action noise std: 0.73
                       Mean reward: -14.81
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 12.50s
                        Total time: 27434.13s
                               ETA: 1343600.4s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.865s, learning 0.189s)
               Value function loss: 0.3839
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: -15.69
               Mean episode length: 125.00
                  Mean reward/step: -0.47
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 13.05s
                        Total time: 27447.18s
                               ETA: 1343554.6s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.339s, learning 0.166s)
               Value function loss: 0.4892
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -16.38
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 12.50s
                        Total time: 27459.68s
                               ETA: 1343481.9s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.347s, learning 0.250s)
               Value function loss: 0.5914
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: -17.67
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 12.60s
                        Total time: 27472.28s
                               ETA: 1343413.7s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.396s, learning 0.162s)
               Value function loss: 1.1154
                    Surrogate loss: 0.0083
             Mean action noise std: 0.73
                       Mean reward: -17.07
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 12.56s
                        Total time: 27484.84s
                               ETA: 1343343.8s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.077s, learning 0.161s)
               Value function loss: 0.9340
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: -15.92
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 12.24s
                        Total time: 27497.08s
                               ETA: 1343258.3s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.352s, learning 0.168s)
               Value function loss: 1.2230
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: -16.44
               Mean episode length: 124.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 12.52s
                        Total time: 27509.60s
                               ETA: 1343186.6s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.273s, learning 0.168s)
               Value function loss: 1.6435
                    Surrogate loss: 0.0129
             Mean action noise std: 0.73
                       Mean reward: -15.08
               Mean episode length: 124.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 12.44s
                        Total time: 27522.04s
                               ETA: 1343111.1s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.770s, learning 0.168s)
               Value function loss: 2.5762
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -16.45
               Mean episode length: 124.46
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 12.94s
                        Total time: 27534.98s
                               ETA: 1343059.9s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.268s, learning 0.171s)
               Value function loss: 2.2289
                    Surrogate loss: 0.0033
             Mean action noise std: 0.73
                       Mean reward: -14.86
               Mean episode length: 124.53
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 12.44s
                        Total time: 27547.41s
                               ETA: 1342984.4s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.027s, learning 0.204s)
               Value function loss: 1.7347
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: -12.80
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 12.23s
                        Total time: 27559.65s
                               ETA: 1342898.9s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.604s, learning 0.165s)
               Value function loss: 1.7400
                    Surrogate loss: 0.0188
             Mean action noise std: 0.73
                       Mean reward: -16.32
               Mean episode length: 124.85
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 12.77s
                        Total time: 27572.41s
                               ETA: 1342839.6s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.833s, learning 0.177s)
               Value function loss: 1.6793
                    Surrogate loss: 0.0031
             Mean action noise std: 0.73
                       Mean reward: -16.47
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 12.01s
                        Total time: 27584.42s
                               ETA: 1342743.5s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.448s, learning 0.187s)
               Value function loss: 1.7686
                    Surrogate loss: 0.0038
             Mean action noise std: 0.73
                       Mean reward: -16.39
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 12.64s
                        Total time: 27597.06s
                               ETA: 1342677.8s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.512s, learning 0.227s)
               Value function loss: 1.7729
                    Surrogate loss: 0.0105
             Mean action noise std: 0.73
                       Mean reward: -16.24
               Mean episode length: 124.69
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 12.74s
                        Total time: 27609.80s
                               ETA: 1342617.3s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.947s, learning 0.167s)
               Value function loss: 14.7243
                    Surrogate loss: 0.0108
             Mean action noise std: 0.73
                       Mean reward: -18.42
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 5.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 13.11s
                        Total time: 27622.91s
                               ETA: 1342575.0s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.731s, learning 0.156s)
               Value function loss: 0.2645
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: -18.26
               Mean episode length: 125.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 12.89s
                        Total time: 27635.80s
                               ETA: 1342521.7s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.271s, learning 0.212s)
               Value function loss: 0.3991
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: -18.42
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 12.48s
                        Total time: 27648.28s
                               ETA: 1342448.8s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.945s, learning 0.245s)
               Value function loss: 0.4580
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: -17.64
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 12.19s
                        Total time: 27660.47s
                               ETA: 1342361.8s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.594s, learning 0.258s)
               Value function loss: 0.8086
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: -19.19
               Mean episode length: 124.72
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 12.85s
                        Total time: 27673.32s
                               ETA: 1342307.0s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.549s, learning 0.233s)
               Value function loss: 0.6848
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: -18.64
               Mean episode length: 124.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 12.78s
                        Total time: 27686.11s
                               ETA: 1342248.7s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.590s, learning 0.229s)
               Value function loss: 0.7403
                    Surrogate loss: 0.0106
             Mean action noise std: 0.73
                       Mean reward: -19.11
               Mean episode length: 124.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 12.82s
                        Total time: 27698.92s
                               ETA: 1342192.4s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1336 steps/s (collection: 11.975s, learning 0.283s)
               Value function loss: 1.1031
                    Surrogate loss: 0.0070
             Mean action noise std: 0.73
                       Mean reward: -18.18
               Mean episode length: 124.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 12.26s
                        Total time: 27711.18s
                               ETA: 1342108.9s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.601s, learning 0.256s)
               Value function loss: 1.6493
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: -17.01
               Mean episode length: 125.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 12.86s
                        Total time: 27724.04s
                               ETA: 1342054.5s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.473s, learning 0.243s)
               Value function loss: 2.2527
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: -16.62
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 12.72s
                        Total time: 27736.76s
                               ETA: 1341993.3s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.420s, learning 0.257s)
               Value function loss: 1.6183
                    Surrogate loss: 0.0175
             Mean action noise std: 0.73
                       Mean reward: -19.62
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 12.68s
                        Total time: 27749.43s
                               ETA: 1341930.3s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.240s, learning 0.246s)
               Value function loss: 1.6270
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: -19.06
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 12.49s
                        Total time: 27761.92s
                               ETA: 1341858.1s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.266s, learning 0.251s)
               Value function loss: 1.4879
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: -19.53
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 12.52s
                        Total time: 27774.44s
                               ETA: 1341787.4s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.350s, learning 0.232s)
               Value function loss: 1.7592
                    Surrogate loss: 0.0148
             Mean action noise std: 0.73
                       Mean reward: -19.00
               Mean episode length: 124.81
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 12.58s
                        Total time: 27787.02s
                               ETA: 1341720.0s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.412s, learning 0.237s)
               Value function loss: 1.5511
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: -19.14
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 12.65s
                        Total time: 27799.67s
                               ETA: 1341655.8s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.435s, learning 0.158s)
               Value function loss: 1.7645
                    Surrogate loss: -0.0018
             Mean action noise std: 0.73
                       Mean reward: -19.33
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 12.59s
                        Total time: 27812.26s
                               ETA: 1341588.9s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1227 steps/s (collection: 13.136s, learning 0.211s)
               Value function loss: 12.3136
                    Surrogate loss: 0.0500
             Mean action noise std: 0.73
                       Mean reward: -20.06
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 5.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 13.35s
                        Total time: 27825.61s
                               ETA: 1341558.5s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.387s, learning 0.201s)
               Value function loss: 0.4482
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: -21.66
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 12.59s
                        Total time: 27838.19s
                               ETA: 1341491.5s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.551s, learning 0.249s)
               Value function loss: 0.4135
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: -22.36
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 12.80s
                        Total time: 27851.00s
                               ETA: 1341434.8s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.650s, learning 0.204s)
               Value function loss: 0.4741
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: -22.31
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 12.85s
                        Total time: 27863.85s
                               ETA: 1341380.8s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.315s, learning 0.161s)
               Value function loss: 1.0430
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: -21.87
               Mean episode length: 124.91
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 12.48s
                        Total time: 27876.33s
                               ETA: 1341308.6s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.121s, learning 0.206s)
               Value function loss: 0.9057
                    Surrogate loss: 0.0117
             Mean action noise std: 0.73
                       Mean reward: -22.28
               Mean episode length: 124.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 12.33s
                        Total time: 27888.65s
                               ETA: 1341229.3s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.371s, learning 0.228s)
               Value function loss: 0.8348
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: -20.89
               Mean episode length: 124.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 12.60s
                        Total time: 27901.25s
                               ETA: 1341163.1s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.590s, learning 0.162s)
               Value function loss: 1.2164
                    Surrogate loss: 0.0027
             Mean action noise std: 0.73
                       Mean reward: -20.60
               Mean episode length: 124.80
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 12.75s
                        Total time: 27914.00s
                               ETA: 1341104.3s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.405s, learning 0.200s)
               Value function loss: 1.9633
                    Surrogate loss: 0.0045
             Mean action noise std: 0.73
                       Mean reward: -21.29
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 12.61s
                        Total time: 27926.61s
                               ETA: 1341038.5s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.292s, learning 0.268s)
               Value function loss: 1.9304
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: -20.67
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 12.56s
                        Total time: 27939.17s
                               ETA: 1340970.6s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.317s, learning 0.180s)
               Value function loss: 1.4134
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: -18.36
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 12.50s
                        Total time: 27951.67s
                               ETA: 1340899.7s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.878s, learning 0.209s)
               Value function loss: 1.6083
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: -21.25
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 12.09s
                        Total time: 27963.75s
                               ETA: 1340809.3s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.085s, learning 0.193s)
               Value function loss: 1.5879
                    Surrogate loss: 0.0134
             Mean action noise std: 0.73
                       Mean reward: -21.62
               Mean episode length: 124.78
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 12.28s
                        Total time: 27976.03s
                               ETA: 1340728.0s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.363s, learning 0.162s)
               Value function loss: 1.5072
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: -22.99
               Mean episode length: 124.79
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 12.53s
                        Total time: 27988.56s
                               ETA: 1340658.7s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.405s, learning 0.194s)
               Value function loss: 1.5725
                    Surrogate loss: 0.0054
             Mean action noise std: 0.73
                       Mean reward: -20.10
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 12.60s
                        Total time: 28001.15s
                               ETA: 1340592.9s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1245 steps/s (collection: 12.991s, learning 0.162s)
               Value function loss: 15.1450
                    Surrogate loss: 0.0111
             Mean action noise std: 0.73
                       Mean reward: -20.79
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 5.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 13.15s
                        Total time: 28014.31s
                               ETA: 1340553.7s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.255s, learning 0.188s)
               Value function loss: 0.5144
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: -20.97
               Mean episode length: 125.00
                  Mean reward/step: -0.66
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 12.44s
                        Total time: 28026.75s
                               ETA: 1340480.6s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.654s, learning 0.205s)
               Value function loss: 0.3373
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: -21.32
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 12.86s
                        Total time: 28039.61s
                               ETA: 1340427.4s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.276s, learning 0.216s)
               Value function loss: 0.3466
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: -22.19
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 12.49s
                        Total time: 28052.10s
                               ETA: 1340356.7s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.051s, learning 0.163s)
               Value function loss: 0.5281
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: -22.49
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 12.21s
                        Total time: 28064.31s
                               ETA: 1340272.8s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.613s, learning 0.191s)
               Value function loss: 0.7982
                    Surrogate loss: 0.0062
             Mean action noise std: 0.73
                       Mean reward: -23.09
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 12.80s
                        Total time: 28077.12s
                               ETA: 1340217.2s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.619s, learning 0.163s)
               Value function loss: 0.9475
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: -23.49
               Mean episode length: 125.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 12.78s
                        Total time: 28089.90s
                               ETA: 1340160.5s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.379s, learning 0.175s)
               Value function loss: 1.0916
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: -24.23
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 12.55s
                        Total time: 28102.45s
                               ETA: 1340093.0s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.499s, learning 0.158s)
               Value function loss: 1.4284
                    Surrogate loss: 0.0198
             Mean action noise std: 0.73
                       Mean reward: -24.49
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 12.66s
                        Total time: 28115.11s
                               ETA: 1340030.5s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.531s, learning 0.182s)
               Value function loss: 2.1477
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: -21.87
               Mean episode length: 124.46
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 12.71s
                        Total time: 28127.82s
                               ETA: 1339970.7s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.179s, learning 0.161s)
               Value function loss: 1.7331
                    Surrogate loss: 0.0041
             Mean action noise std: 0.73
                       Mean reward: -21.50
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 12.34s
                        Total time: 28140.16s
                               ETA: 1339893.1s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.967s, learning 0.182s)
               Value function loss: 1.5356
                    Surrogate loss: 0.0058
             Mean action noise std: 0.73
                       Mean reward: -24.74
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 12.15s
                        Total time: 28152.31s
                               ETA: 1339806.5s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.486s, learning 0.246s)
               Value function loss: 1.3765
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: -21.71
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 12.73s
                        Total time: 28165.04s
                               ETA: 1339747.8s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.851s, learning 0.166s)
               Value function loss: 1.5887
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: -26.81
               Mean episode length: 124.51
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 13.02s
                        Total time: 28178.06s
                               ETA: 1339702.7s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.694s, learning 0.172s)
               Value function loss: 1.6150
                    Surrogate loss: 0.0025
             Mean action noise std: 0.73
                       Mean reward: -20.75
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 11.87s
                        Total time: 28189.93s
                               ETA: 1339602.9s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.234s, learning 0.166s)
               Value function loss: 1.4115
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: -23.42
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 12.40s
                        Total time: 28202.33s
                               ETA: 1339528.5s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1232 steps/s (collection: 13.097s, learning 0.194s)
               Value function loss: 13.6270
                    Surrogate loss: 0.0197
             Mean action noise std: 0.73
                       Mean reward: -26.82
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 5.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 13.29s
                        Total time: 28215.62s
                               ETA: 1339496.5s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.271s, learning 0.199s)
               Value function loss: 0.3120
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: -26.36
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 12.47s
                        Total time: 28228.09s
                               ETA: 1339425.6s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.208s, learning 0.165s)
               Value function loss: 0.3460
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: -26.32
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 12.37s
                        Total time: 28240.46s
                               ETA: 1339350.1s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.402s, learning 0.164s)
               Value function loss: 0.3651
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -25.90
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 12.57s
                        Total time: 28253.03s
                               ETA: 1339283.8s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.241s, learning 0.173s)
               Value function loss: 0.6436
                    Surrogate loss: 0.0129
             Mean action noise std: 0.73
                       Mean reward: -23.19
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 12.41s
                        Total time: 28265.44s
                               ETA: 1339210.3s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.896s, learning 0.163s)
               Value function loss: 0.7194
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: -22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 12.06s
                        Total time: 28277.50s
                               ETA: 1339120.2s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.890s, learning 0.200s)
               Value function loss: 0.7462
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: -20.54
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 12.09s
                        Total time: 28289.59s
                               ETA: 1339031.5s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.416s, learning 0.168s)
               Value function loss: 1.0971
                    Surrogate loss: 0.0247
             Mean action noise std: 0.73
                       Mean reward: -21.45
               Mean episode length: 124.91
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 12.58s
                        Total time: 28302.17s
                               ETA: 1338966.3s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.431s, learning 0.161s)
               Value function loss: 1.6009
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: -23.25
               Mean episode length: 124.86
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 12.59s
                        Total time: 28314.77s
                               ETA: 1338901.5s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.432s, learning 0.158s)
               Value function loss: 2.0037
                    Surrogate loss: 0.0119
             Mean action noise std: 0.73
                       Mean reward: -22.81
               Mean episode length: 124.51
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 12.59s
                        Total time: 28327.36s
                               ETA: 1338836.7s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.559s, learning 0.184s)
               Value function loss: 1.4228
                    Surrogate loss: -0.0023
             Mean action noise std: 0.73
                       Mean reward: -21.73
               Mean episode length: 124.66
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 12.74s
                        Total time: 28340.10s
                               ETA: 1338779.2s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.317s, learning 0.159s)
               Value function loss: 1.6125
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: -20.40
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 12.48s
                        Total time: 28352.57s
                               ETA: 1338709.0s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.093s, learning 0.207s)
               Value function loss: 1.5856
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: -19.76
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 12.30s
                        Total time: 28364.87s
                               ETA: 1338630.7s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.842s, learning 0.156s)
               Value function loss: 1.8173
                    Surrogate loss: 0.0142
             Mean action noise std: 0.73
                       Mean reward: -21.13
               Mean episode length: 124.79
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 13.00s
                        Total time: 28377.87s
                               ETA: 1338585.3s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.323s, learning 0.225s)
               Value function loss: 1.5289
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: -21.05
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 12.55s
                        Total time: 28390.42s
                               ETA: 1338518.8s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.399s, learning 0.201s)
               Value function loss: 2.7786
                    Surrogate loss: 0.0018
             Mean action noise std: 0.73
                       Mean reward: -21.27
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 12.60s
                        Total time: 28403.02s
                               ETA: 1338454.7s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.525s, learning 0.168s)
               Value function loss: 14.1297
                    Surrogate loss: 0.0471
             Mean action noise std: 0.73
                       Mean reward: -21.38
               Mean episode length: 125.00
                  Mean reward/step: -0.67
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 12.69s
                        Total time: 28415.71s
                               ETA: 1338395.1s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.105s, learning 0.184s)
               Value function loss: 0.3027
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: -22.42
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 12.29s
                        Total time: 28428.00s
                               ETA: 1338316.6s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.144s, learning 0.182s)
               Value function loss: 0.3272
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -23.63
               Mean episode length: 125.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 12.33s
                        Total time: 28440.33s
                               ETA: 1338239.8s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.556s, learning 0.208s)
               Value function loss: 0.3362
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: -22.43
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 12.76s
                        Total time: 28453.09s
                               ETA: 1338183.7s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.264s, learning 0.164s)
               Value function loss: 0.7890
                    Surrogate loss: 0.0065
             Mean action noise std: 0.73
                       Mean reward: -20.43
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 12.43s
                        Total time: 28465.52s
                               ETA: 1338111.8s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.869s, learning 0.161s)
               Value function loss: 0.6738
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: -22.80
               Mean episode length: 124.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 12.03s
                        Total time: 28477.55s
                               ETA: 1338021.3s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.163s, learning 0.183s)
               Value function loss: 0.7678
                    Surrogate loss: 0.0010
             Mean action noise std: 0.73
                       Mean reward: -22.00
               Mean episode length: 124.67
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 12.35s
                        Total time: 28489.90s
                               ETA: 1337945.7s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.269s, learning 0.181s)
               Value function loss: 1.1622
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: -22.27
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 12.45s
                        Total time: 28502.35s
                               ETA: 1337875.0s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.252s, learning 0.173s)
               Value function loss: 2.1357
                    Surrogate loss: 0.0259
             Mean action noise std: 0.73
                       Mean reward: -20.75
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 12.42s
                        Total time: 28514.77s
                               ETA: 1337803.3s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.240s, learning 0.164s)
               Value function loss: 1.7649
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: -21.54
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 12.40s
                        Total time: 28527.18s
                               ETA: 1337730.6s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.661s, learning 0.219s)
               Value function loss: 1.4293
                    Surrogate loss: 0.0276
             Mean action noise std: 0.73
                       Mean reward: -20.88
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 12.88s
                        Total time: 28540.06s
                               ETA: 1337680.2s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.319s, learning 0.163s)
               Value function loss: 1.5394
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: -22.76
               Mean episode length: 124.90
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 12.48s
                        Total time: 28552.54s
                               ETA: 1337611.3s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.695s, learning 0.168s)
               Value function loss: 1.5743
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: -23.71
               Mean episode length: 124.94
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 12.86s
                        Total time: 28565.40s
                               ETA: 1337560.2s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.886s, learning 0.158s)
               Value function loss: 2.3620
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: -22.81
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 12.04s
                        Total time: 28577.45s
                               ETA: 1337470.9s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.897s, learning 0.192s)
               Value function loss: 2.9023
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: -20.04
               Mean episode length: 124.93
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 12.09s
                        Total time: 28589.53s
                               ETA: 1337383.7s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.871s, learning 0.166s)
               Value function loss: 20.0260
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: -21.65
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 5.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 13.04s
                        Total time: 28602.57s
                               ETA: 1337340.9s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.700s, learning 0.196s)
               Value function loss: 0.4872
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: -21.28
               Mean episode length: 125.00
                  Mean reward/step: -0.63
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 12.90s
                        Total time: 28615.47s
                               ETA: 1337291.6s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.343s, learning 0.186s)
               Value function loss: 0.2977
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: -20.71
               Mean episode length: 125.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 12.53s
                        Total time: 28628.00s
                               ETA: 1337225.2s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.439s, learning 0.166s)
               Value function loss: 0.3142
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: -20.74
               Mean episode length: 125.00
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 12.61s
                        Total time: 28640.60s
                               ETA: 1337162.4s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.198s, learning 0.156s)
               Value function loss: 0.4218
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: -20.56
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 12.35s
                        Total time: 28652.96s
                               ETA: 1337087.9s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.533s, learning 0.172s)
               Value function loss: 0.5177
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: -19.55
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 12.70s
                        Total time: 28665.66s
                               ETA: 1337029.8s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.982s, learning 0.158s)
               Value function loss: 0.4846
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: -18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 12.14s
                        Total time: 28677.80s
                               ETA: 1336945.4s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.853s, learning 0.171s)
               Value function loss: 0.7540
                    Surrogate loss: 0.0041
             Mean action noise std: 0.73
                       Mean reward: -20.30
               Mean episode length: 125.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 12.02s
                        Total time: 28689.82s
                               ETA: 1336855.7s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.392s, learning 0.196s)
               Value function loss: 1.4802
                    Surrogate loss: 0.0078
             Mean action noise std: 0.73
                       Mean reward: -19.21
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 12.59s
                        Total time: 28702.41s
                               ETA: 1336792.3s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.401s, learning 0.170s)
               Value function loss: 2.2783
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: -21.57
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 12.57s
                        Total time: 28714.98s
                               ETA: 1336728.2s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.773s, learning 0.186s)
               Value function loss: 1.6546
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: -21.78
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 12.96s
                        Total time: 28727.94s
                               ETA: 1336682.3s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.239s, learning 0.157s)
               Value function loss: 1.6933
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: -23.55
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 12.40s
                        Total time: 28740.34s
                               ETA: 1336610.1s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.960s, learning 0.188s)
               Value function loss: 1.4969
                    Surrogate loss: 0.0027
             Mean action noise std: 0.73
                       Mean reward: -22.93
               Mean episode length: 124.72
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 12.15s
                        Total time: 28752.49s
                               ETA: 1336526.5s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.453s, learning 0.178s)
               Value function loss: 12.0769
                    Surrogate loss: 0.0039
             Mean action noise std: 0.73
                       Mean reward: -25.45
               Mean episode length: 124.78
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 12.63s
                        Total time: 28765.12s
                               ETA: 1336465.4s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.547s, learning 0.196s)
               Value function loss: 1.3663
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: -21.36
               Mean episode length: 124.87
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 12.74s
                        Total time: 28777.86s
                               ETA: 1336409.5s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.955s, learning 0.163s)
               Value function loss: 1.4342
                    Surrogate loss: 0.0033
             Mean action noise std: 0.73
                       Mean reward: -22.12
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 12.12s
                        Total time: 28789.98s
                               ETA: 1336324.7s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.690s, learning 0.172s)
               Value function loss: 11.8865
                    Surrogate loss: 0.0168
             Mean action noise std: 0.73
                       Mean reward: -23.88
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 5.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 12.86s
                        Total time: 28802.84s
                               ETA: 1336274.4s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.582s, learning 0.252s)
               Value function loss: 0.2762
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: -24.07
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 12.83s
                        Total time: 28815.68s
                               ETA: 1336222.9s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.275s, learning 0.183s)
               Value function loss: 0.3363
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: -23.67
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 12.46s
                        Total time: 28828.13s
                               ETA: 1336154.0s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.070s, learning 0.161s)
               Value function loss: 0.3710
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: -23.23
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 12.23s
                        Total time: 28840.36s
                               ETA: 1336074.6s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.602s, learning 0.258s)
               Value function loss: 0.8548
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: -22.88
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 12.86s
                        Total time: 28853.23s
                               ETA: 1336024.4s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.110s, learning 0.157s)
               Value function loss: 0.8564
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: -22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 12.27s
                        Total time: 28865.49s
                               ETA: 1335946.8s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.526s, learning 0.159s)
               Value function loss: 1.0376
                    Surrogate loss: 0.0102
             Mean action noise std: 0.73
                       Mean reward: -22.10
               Mean episode length: 124.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 12.69s
                        Total time: 28878.18s
                               ETA: 1335888.6s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.407s, learning 0.192s)
               Value function loss: 1.0675
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: -21.80
               Mean episode length: 124.62
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 12.60s
                        Total time: 28890.78s
                               ETA: 1335826.5s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.211s, learning 0.256s)
               Value function loss: 1.8089
                    Surrogate loss: 0.0035
             Mean action noise std: 0.73
                       Mean reward: -21.08
               Mean episode length: 124.43
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 12.47s
                        Total time: 28903.24s
                               ETA: 1335758.3s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.442s, learning 0.167s)
               Value function loss: 2.2443
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: -20.57
               Mean episode length: 124.47
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 12.61s
                        Total time: 28915.85s
                               ETA: 1335696.7s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.069s, learning 0.219s)
               Value function loss: 1.9513
                    Surrogate loss: 0.0145
             Mean action noise std: 0.73
                       Mean reward: -20.26
               Mean episode length: 124.57
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 12.29s
                        Total time: 28928.14s
                               ETA: 1335620.4s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.550s, learning 0.157s)
               Value function loss: 1.6976
                    Surrogate loss: 0.0067
             Mean action noise std: 0.73
                       Mean reward: -19.94
               Mean episode length: 124.89
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 12.71s
                        Total time: 28940.85s
                               ETA: 1335563.4s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.074s, learning 0.172s)
               Value function loss: 3.1113
                    Surrogate loss: 0.0072
             Mean action noise std: 0.73
                       Mean reward: -21.67
               Mean episode length: 124.73
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 12.25s
                        Total time: 28953.09s
                               ETA: 1335485.3s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.950s, learning 0.170s)
               Value function loss: 1.7075
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: -20.78
               Mean episode length: 125.00
                  Mean reward/step: -0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 12.12s
                        Total time: 28965.21s
                               ETA: 1335401.4s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.997s, learning 0.157s)
               Value function loss: 1.5486
                    Surrogate loss: 0.0015
             Mean action noise std: 0.73
                       Mean reward: -20.23
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 12.15s
                        Total time: 28977.37s
                               ETA: 1335319.1s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.831s, learning 0.159s)
               Value function loss: 17.5935
                    Surrogate loss: 0.0308
             Mean action noise std: 0.73
                       Mean reward: -17.70
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 5.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 12.99s
                        Total time: 28990.36s
                               ETA: 1335275.4s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.409s, learning 0.168s)
               Value function loss: 0.3184
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: -18.50
               Mean episode length: 125.00
                  Mean reward/step: -0.65
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 12.58s
                        Total time: 29002.93s
                               ETA: 1335212.7s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.266s, learning 0.162s)
               Value function loss: 0.3713
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: -18.74
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 12.43s
                        Total time: 29015.36s
                               ETA: 1335143.2s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.163s, learning 0.163s)
               Value function loss: 0.5029
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: -19.40
               Mean episode length: 124.80
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 12.33s
                        Total time: 29027.69s
                               ETA: 1335069.1s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.995s, learning 0.175s)
               Value function loss: 0.7313
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: -18.60
               Mean episode length: 124.80
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 12.17s
                        Total time: 29039.86s
                               ETA: 1334987.9s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.412s, learning 0.166s)
               Value function loss: 1.0845
                    Surrogate loss: 0.0024
             Mean action noise std: 0.73
                       Mean reward: -18.12
               Mean episode length: 124.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 12.58s
                        Total time: 29052.44s
                               ETA: 1334925.4s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.118s, learning 0.161s)
               Value function loss: 0.9187
                    Surrogate loss: 0.0083
             Mean action noise std: 0.73
                       Mean reward: -17.61
               Mean episode length: 124.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 12.28s
                        Total time: 29064.72s
                               ETA: 1334849.3s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.482s, learning 0.176s)
               Value function loss: 1.0542
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: -16.53
               Mean episode length: 124.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 12.66s
                        Total time: 29077.38s
                               ETA: 1334790.6s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.957s, learning 0.158s)
               Value function loss: 1.9387
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: -17.05
               Mean episode length: 124.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 12.12s
                        Total time: 29089.49s
                               ETA: 1334707.1s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.998s, learning 0.158s)
               Value function loss: 2.8649
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: -19.02
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 12.16s
                        Total time: 29101.65s
                               ETA: 1334625.5s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.972s, learning 0.159s)
               Value function loss: 2.0802
                    Surrogate loss: 0.0089
             Mean action noise std: 0.73
                       Mean reward: -16.72
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 12.13s
                        Total time: 29113.78s
                               ETA: 1334542.9s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.676s, learning 0.181s)
               Value function loss: 1.3161
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: -17.17
               Mean episode length: 124.84
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 12.86s
                        Total time: 29126.64s
                               ETA: 1334493.5s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.968s, learning 0.158s)
               Value function loss: 1.5453
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: -16.17
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 12.13s
                        Total time: 29138.76s
                               ETA: 1334410.7s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1347 steps/s (collection: 12.000s, learning 0.158s)
               Value function loss: 1.5432
                    Surrogate loss: 0.0090
             Mean action noise std: 0.73
                       Mean reward: -17.55
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 12.16s
                        Total time: 29150.92s
                               ETA: 1334329.4s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.024s, learning 0.159s)
               Value function loss: 1.6000
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: -14.68
               Mean episode length: 124.81
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 12.18s
                        Total time: 29163.10s
                               ETA: 1334249.4s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.217s, learning 0.186s)
               Value function loss: 1.6400
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: -18.55
               Mean episode length: 124.90
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 12.40s
                        Total time: 29175.50s
                               ETA: 1334179.4s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.502s, learning 0.257s)
               Value function loss: 14.3722
                    Surrogate loss: 0.0122
             Mean action noise std: 0.72
                       Mean reward: -16.39
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 5.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 12.76s
                        Total time: 29188.26s
                               ETA: 1334125.9s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.087s, learning 0.160s)
               Value function loss: 0.2575
                    Surrogate loss: -0.0122
             Mean action noise std: 0.72
                       Mean reward: -17.13
               Mean episode length: 125.00
                  Mean reward/step: -0.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 12.25s
                        Total time: 29200.51s
                               ETA: 1334048.9s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.150s, learning 0.159s)
               Value function loss: 0.3897
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: -16.92
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 12.31s
                        Total time: 29212.82s
                               ETA: 1333974.9s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.117s, learning 0.183s)
               Value function loss: 0.3843
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -17.02
               Mean episode length: 124.62
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 12.30s
                        Total time: 29225.12s
                               ETA: 1333900.4s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.289s, learning 0.159s)
               Value function loss: 0.7454
                    Surrogate loss: 0.0100
             Mean action noise std: 0.72
                       Mean reward: -16.97
               Mean episode length: 124.62
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 12.45s
                        Total time: 29237.57s
                               ETA: 1333832.8s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.303s, learning 0.161s)
               Value function loss: 0.8292
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -15.02
               Mean episode length: 124.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 12.46s
                        Total time: 29250.03s
                               ETA: 1333766.0s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.147s, learning 0.160s)
               Value function loss: 1.1893
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: -14.14
               Mean episode length: 124.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 12.31s
                        Total time: 29262.34s
                               ETA: 1333692.1s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.922s, learning 0.158s)
               Value function loss: 1.7080
                    Surrogate loss: -0.0058
             Mean action noise std: 0.72
                       Mean reward: -16.07
               Mean episode length: 124.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 12.08s
                        Total time: 29274.42s
                               ETA: 1333607.8s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.201s, learning 0.161s)
               Value function loss: 4.1610
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: -16.75
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 12.36s
                        Total time: 29286.78s
                               ETA: 1333536.5s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.450s, learning 0.181s)
               Value function loss: 4.9643
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: -16.50
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 12.63s
                        Total time: 29299.41s
                               ETA: 1333477.5s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.196s, learning 0.159s)
               Value function loss: 5.4946
                    Surrogate loss: 0.0106
             Mean action noise std: 0.72
                       Mean reward: -18.00
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 12.36s
                        Total time: 29311.77s
                               ETA: 1333406.0s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.422s, learning 0.158s)
               Value function loss: 1.5639
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: -16.52
               Mean episode length: 124.91
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 12.58s
                        Total time: 29324.35s
                               ETA: 1333344.7s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.270s, learning 0.164s)
               Value function loss: 1.4874
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: -17.04
               Mean episode length: 124.75
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 12.43s
                        Total time: 29336.78s
                               ETA: 1333276.9s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.567s, learning 0.289s)
               Value function loss: 1.7524
                    Surrogate loss: 0.0402
             Mean action noise std: 0.72
                       Mean reward: -13.66
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 12.86s
                        Total time: 29349.63s
                               ETA: 1333228.3s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.592s, learning 0.207s)
               Value function loss: 1.5294
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: -15.89
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 12.80s
                        Total time: 29362.43s
                               ETA: 1333177.1s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.007s, learning 0.216s)
               Value function loss: 1.6802
                    Surrogate loss: 0.0167
             Mean action noise std: 0.72
                       Mean reward: -16.01
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 12.22s
                        Total time: 29374.66s
                               ETA: 1333099.9s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1247 steps/s (collection: 12.947s, learning 0.185s)
               Value function loss: 13.3406
                    Surrogate loss: 0.0230
             Mean action noise std: 0.72
                       Mean reward: -16.51
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 5.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 13.13s
                        Total time: 29387.79s
                               ETA: 1333063.9s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.237s, learning 0.162s)
               Value function loss: 0.3691
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: -17.56
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 12.40s
                        Total time: 29400.19s
                               ETA: 1332994.7s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.714s, learning 0.161s)
               Value function loss: 0.4924
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: -17.00
               Mean episode length: 124.47
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 12.87s
                        Total time: 29413.06s
                               ETA: 1332947.1s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.108s, learning 0.159s)
               Value function loss: 0.5438
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: -17.18
               Mean episode length: 124.47
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 12.27s
                        Total time: 29425.33s
                               ETA: 1332872.1s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.607s, learning 0.158s)
               Value function loss: 1.7963
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: -15.87
               Mean episode length: 124.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 12.76s
                        Total time: 29438.09s
                               ETA: 1332819.6s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.592s, learning 0.158s)
               Value function loss: 1.2788
                    Surrogate loss: 0.0095
             Mean action noise std: 0.72
                       Mean reward: -14.28
               Mean episode length: 124.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 12.75s
                        Total time: 29450.84s
                               ETA: 1332766.5s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.773s, learning 0.198s)
               Value function loss: 1.1507
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: -12.52
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 11.97s
                        Total time: 29462.81s
                               ETA: 1332678.2s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.814s, learning 0.159s)
               Value function loss: 1.2759
                    Surrogate loss: 0.0067
             Mean action noise std: 0.72
                       Mean reward: -11.95
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 12.97s
                        Total time: 29475.79s
                               ETA: 1332635.2s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.371s, learning 0.159s)
               Value function loss: 2.3940
                    Surrogate loss: 0.0038
             Mean action noise std: 0.72
                       Mean reward: -12.96
               Mean episode length: 124.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 12.53s
                        Total time: 29488.32s
                               ETA: 1332572.3s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.360s, learning 0.160s)
               Value function loss: 2.2624
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -14.34
               Mean episode length: 124.73
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 12.52s
                        Total time: 29500.84s
                               ETA: 1332509.0s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.155s, learning 0.159s)
               Value function loss: 1.7066
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: -14.75
               Mean episode length: 124.56
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 12.31s
                        Total time: 29513.15s
                               ETA: 1332436.4s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.586s, learning 0.198s)
               Value function loss: 2.4920
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -13.47
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 12.78s
                        Total time: 29525.94s
                               ETA: 1332385.1s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.135s, learning 0.197s)
               Value function loss: 3.7711
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: -17.95
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 12.33s
                        Total time: 29538.27s
                               ETA: 1332313.5s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1237 steps/s (collection: 13.073s, learning 0.167s)
               Value function loss: 3.6599
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: -16.63
               Mean episode length: 124.81
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 13.24s
                        Total time: 29551.51s
                               ETA: 1332282.8s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.282s, learning 0.180s)
               Value function loss: 3.1500
                    Surrogate loss: 0.0331
             Mean action noise std: 0.72
                       Mean reward: -15.45
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 12.46s
                        Total time: 29563.97s
                               ETA: 1332217.1s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1243 steps/s (collection: 12.925s, learning 0.251s)
               Value function loss: 13.8458
                    Surrogate loss: 0.0074
             Mean action noise std: 0.72
                       Mean reward: -14.37
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 5.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 13.18s
                        Total time: 29577.15s
                               ETA: 1332183.5s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.469s, learning 0.169s)
               Value function loss: 0.3227
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: -15.37
               Mean episode length: 125.00
                  Mean reward/step: -0.61
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 12.64s
                        Total time: 29589.78s
                               ETA: 1332125.8s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.420s, learning 0.159s)
               Value function loss: 0.3348
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: -15.46
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 12.58s
                        Total time: 29602.36s
                               ETA: 1332065.5s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.448s, learning 0.160s)
               Value function loss: 0.3899
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: -16.12
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 12.61s
                        Total time: 29614.97s
                               ETA: 1332006.6s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.680s, learning 0.225s)
               Value function loss: 0.5190
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: -16.24
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 12.90s
                        Total time: 29627.88s
                               ETA: 1331961.0s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.506s, learning 0.159s)
               Value function loss: 0.8164
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: -16.78
               Mean episode length: 124.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 12.67s
                        Total time: 29640.54s
                               ETA: 1331904.7s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.061s, learning 0.229s)
               Value function loss: 0.8034
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: -16.57
               Mean episode length: 124.74
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 12.29s
                        Total time: 29652.83s
                               ETA: 1331831.6s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.681s, learning 0.185s)
               Value function loss: 1.0904
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: -15.75
               Mean episode length: 124.74
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 12.87s
                        Total time: 29665.70s
                               ETA: 1331784.3s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.402s, learning 0.157s)
               Value function loss: 1.8224
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: -15.68
               Mean episode length: 124.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 12.56s
                        Total time: 29678.26s
                               ETA: 1331723.4s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1244 steps/s (collection: 12.997s, learning 0.165s)
               Value function loss: 2.6130
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -11.48
               Mean episode length: 124.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 13.16s
                        Total time: 29691.42s
                               ETA: 1331689.5s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.691s, learning 0.158s)
               Value function loss: 2.3520
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -10.93
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 12.85s
                        Total time: 29704.27s
                               ETA: 1331641.6s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.617s, learning 0.184s)
               Value function loss: 3.3065
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: -13.63
               Mean episode length: 124.65
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 12.80s
                        Total time: 29717.07s
                               ETA: 1331591.6s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.625s, learning 0.198s)
               Value function loss: 2.6388
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: -13.27
               Mean episode length: 124.65
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 12.82s
                        Total time: 29729.89s
                               ETA: 1331542.6s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.469s, learning 0.158s)
               Value function loss: 3.7019
                    Surrogate loss: 0.0067
             Mean action noise std: 0.72
                       Mean reward: -13.68
               Mean episode length: 124.79
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 12.63s
                        Total time: 29742.52s
                               ETA: 1331484.9s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.728s, learning 0.157s)
               Value function loss: 2.1231
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: -14.30
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 12.88s
                        Total time: 29755.41s
                               ETA: 1331438.7s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.253s, learning 0.239s)
               Value function loss: 1.9512
                    Surrogate loss: 0.0032
             Mean action noise std: 0.72
                       Mean reward: -12.83
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 12.49s
                        Total time: 29767.90s
                               ETA: 1331375.0s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1252 steps/s (collection: 12.793s, learning 0.286s)
               Value function loss: 15.4496
                    Surrogate loss: 0.0187
             Mean action noise std: 0.72
                       Mean reward: -15.97
               Mean episode length: 125.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 5.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 13.08s
                        Total time: 29780.98s
                               ETA: 1331337.5s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.400s, learning 0.158s)
               Value function loss: 0.4087
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: -15.88
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 12.56s
                        Total time: 29793.53s
                               ETA: 1331276.9s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.752s, learning 0.266s)
               Value function loss: 0.4768
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: -15.05
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 13.02s
                        Total time: 29806.55s
                               ETA: 1331236.8s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.569s, learning 0.348s)
               Value function loss: 0.6296
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: -15.78
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 12.92s
                        Total time: 29819.47s
                               ETA: 1331192.3s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.843s, learning 0.270s)
               Value function loss: 0.7192
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: -14.61
               Mean episode length: 124.75
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 13.11s
                        Total time: 29832.58s
                               ETA: 1331156.5s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.369s, learning 0.158s)
               Value function loss: 0.8119
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -14.51
               Mean episode length: 124.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 12.53s
                        Total time: 29845.11s
                               ETA: 1331094.6s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.755s, learning 0.159s)
               Value function loss: 0.8528
                    Surrogate loss: 0.0078
             Mean action noise std: 0.72
                       Mean reward: -14.80
               Mean episode length: 124.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 11.91s
                        Total time: 29857.02s
                               ETA: 1331005.4s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.449s, learning 0.159s)
               Value function loss: 1.0147
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: -15.40
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 12.61s
                        Total time: 29869.63s
                               ETA: 1330947.2s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.575s, learning 0.223s)
               Value function loss: 1.7098
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: -14.03
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 12.80s
                        Total time: 29882.43s
                               ETA: 1330897.6s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.302s, learning 0.189s)
               Value function loss: 2.0804
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: -14.01
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 12.49s
                        Total time: 29894.92s
                               ETA: 1330834.3s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.249s, learning 0.247s)
               Value function loss: 1.6714
                    Surrogate loss: 0.0095
             Mean action noise std: 0.72
                       Mean reward: -12.64
               Mean episode length: 124.53
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 12.50s
                        Total time: 29907.42s
                               ETA: 1330771.2s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.339s, learning 0.162s)
               Value function loss: 1.7237
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: -15.94
               Mean episode length: 124.42
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 12.50s
                        Total time: 29919.92s
                               ETA: 1330708.4s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.458s, learning 0.214s)
               Value function loss: 1.7563
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: -15.11
               Mean episode length: 124.64
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 12.67s
                        Total time: 29932.59s
                               ETA: 1330653.2s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.035s, learning 0.187s)
               Value function loss: 1.7849
                    Surrogate loss: 0.0039
             Mean action noise std: 0.72
                       Mean reward: -13.86
               Mean episode length: 124.79
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 12.22s
                        Total time: 29944.81s
                               ETA: 1330578.2s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.951s, learning 0.161s)
               Value function loss: 1.7361
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: -10.22
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 12.11s
                        Total time: 29956.92s
                               ETA: 1330498.2s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.286s, learning 0.174s)
               Value function loss: 4.6992
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: -11.79
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 12.46s
                        Total time: 29969.38s
                               ETA: 1330433.8s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1243 steps/s (collection: 13.000s, learning 0.171s)
               Value function loss: 13.9648
                    Surrogate loss: 0.0224
             Mean action noise std: 0.72
                       Mean reward: -11.68
               Mean episode length: 125.00
                  Mean reward/step: -0.62
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 13.17s
                        Total time: 29982.55s
                               ETA: 1330401.0s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.810s, learning 0.255s)
               Value function loss: 0.4030
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -12.67
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 13.06s
                        Total time: 29995.62s
                               ETA: 1330363.5s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.464s, learning 0.196s)
               Value function loss: 0.4602
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: -12.93
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 12.66s
                        Total time: 30008.28s
                               ETA: 1330308.1s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.423s, learning 0.159s)
               Value function loss: 0.4617
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: -12.06
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 12.58s
                        Total time: 30020.86s
                               ETA: 1330249.2s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.358s, learning 0.161s)
               Value function loss: 0.8364
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: -14.22
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 12.52s
                        Total time: 30033.38s
                               ETA: 1330187.6s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.275s, learning 0.203s)
               Value function loss: 0.8405
                    Surrogate loss: 0.0048
             Mean action noise std: 0.72
                       Mean reward: -15.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 12.48s
                        Total time: 30045.86s
                               ETA: 1330124.3s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.952s, learning 0.157s)
               Value function loss: 0.9453
                    Surrogate loss: 0.0122
             Mean action noise std: 0.72
                       Mean reward: -14.70
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 13.11s
                        Total time: 30058.97s
                               ETA: 1330088.9s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.579s, learning 0.164s)
               Value function loss: 1.0988
                    Surrogate loss: 0.0080
             Mean action noise std: 0.72
                       Mean reward: -13.83
               Mean episode length: 124.37
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 12.74s
                        Total time: 30071.71s
                               ETA: 1330037.3s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.316s, learning 0.171s)
               Value function loss: 2.0596
                    Surrogate loss: 0.0054
             Mean action noise std: 0.72
                       Mean reward: -8.12
               Mean episode length: 124.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 12.49s
                        Total time: 30084.20s
                               ETA: 1329974.5s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.432s, learning 0.159s)
               Value function loss: 1.9303
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -13.44
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 12.59s
                        Total time: 30096.79s
                               ETA: 1329916.3s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.738s, learning 0.281s)
               Value function loss: 1.4609
                    Surrogate loss: 0.0048
             Mean action noise std: 0.72
                       Mean reward: -11.57
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 13.02s
                        Total time: 30109.81s
                               ETA: 1329877.0s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.301s, learning 0.163s)
               Value function loss: 1.5999
                    Surrogate loss: 0.0092
             Mean action noise std: 0.72
                       Mean reward: -11.74
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 12.46s
                        Total time: 30122.27s
                               ETA: 1329813.2s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.809s, learning 0.165s)
               Value function loss: 1.4802
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: -11.68
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 12.97s
                        Total time: 30135.24s
                               ETA: 1329772.0s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1242 steps/s (collection: 13.032s, learning 0.159s)
               Value function loss: 1.6452
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -10.70
               Mean episode length: 124.87
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 13.19s
                        Total time: 30148.43s
                               ETA: 1329740.4s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.318s, learning 0.168s)
               Value function loss: 1.6118
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: -9.28
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 12.49s
                        Total time: 30160.92s
                               ETA: 1329677.8s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1233 steps/s (collection: 13.116s, learning 0.162s)
               Value function loss: 13.6112
                    Surrogate loss: 0.0195
             Mean action noise std: 0.72
                       Mean reward: -11.60
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 5.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 13.28s
                        Total time: 30174.20s
                               ETA: 1329650.0s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.069s, learning 0.157s)
               Value function loss: 0.3391
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: -12.19
               Mean episode length: 124.94
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 12.23s
                        Total time: 30186.42s
                               ETA: 1329576.0s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.824s, learning 0.164s)
               Value function loss: 0.4145
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -11.13
               Mean episode length: 124.94
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 11.99s
                        Total time: 30198.41s
                               ETA: 1329491.6s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.902s, learning 0.169s)
               Value function loss: 0.5990
                    Surrogate loss: 0.0049
             Mean action noise std: 0.72
                       Mean reward: -10.91
               Mean episode length: 124.94
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 12.07s
                        Total time: 30210.48s
                               ETA: 1329410.9s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.205s, learning 0.234s)
               Value function loss: 0.7439
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -12.31
               Mean episode length: 124.94
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 12.44s
                        Total time: 30222.92s
                               ETA: 1329346.4s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.103s, learning 0.272s)
               Value function loss: 1.3581
                    Surrogate loss: 0.0079
             Mean action noise std: 0.72
                       Mean reward: -12.67
               Mean episode length: 124.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 12.38s
                        Total time: 30235.30s
                               ETA: 1329279.1s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.808s, learning 0.243s)
               Value function loss: 1.0487
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: -10.77
               Mean episode length: 124.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 13.05s
                        Total time: 30248.35s
                               ETA: 1329241.6s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.780s, learning 0.212s)
               Value function loss: 1.7461
                    Surrogate loss: 0.0047
             Mean action noise std: 0.72
                       Mean reward: -10.87
               Mean episode length: 124.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 12.99s
                        Total time: 30261.34s
                               ETA: 1329201.6s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.460s, learning 0.191s)
               Value function loss: 1.9403
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: -12.70
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 12.65s
                        Total time: 30273.99s
                               ETA: 1329146.5s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.938s, learning 0.163s)
               Value function loss: 2.5287
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: -12.44
               Mean episode length: 124.50
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 12.10s
                        Total time: 30286.09s
                               ETA: 1329067.4s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.531s, learning 0.159s)
               Value function loss: 2.2735
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: -13.04
               Mean episode length: 124.54
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 12.69s
                        Total time: 30298.78s
                               ETA: 1329014.2s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.461s, learning 0.195s)
               Value function loss: 1.7241
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: -12.00
               Mean episode length: 124.59
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 12.66s
                        Total time: 30311.44s
                               ETA: 1328959.6s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.454s, learning 0.189s)
               Value function loss: 1.9671
                    Surrogate loss: 0.0049
             Mean action noise std: 0.72
                       Mean reward: -12.42
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 12.64s
                        Total time: 30324.08s
                               ETA: 1328904.4s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.004s, learning 0.197s)
               Value function loss: 1.7411
                    Surrogate loss: 0.0062
             Mean action noise std: 0.72
                       Mean reward: -14.39
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 12.20s
                        Total time: 30336.28s
                               ETA: 1328829.8s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.617s, learning 0.167s)
               Value function loss: 1.5649
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: -12.20
               Mean episode length: 124.97
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 12.78s
                        Total time: 30349.07s
                               ETA: 1328780.9s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.930s, learning 0.165s)
               Value function loss: 1.6687
                    Surrogate loss: 0.0030
             Mean action noise std: 0.72
                       Mean reward: -12.79
               Mean episode length: 124.98
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 12.09s
                        Total time: 30361.16s
                               ETA: 1328701.8s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1208 steps/s (collection: 13.353s, learning 0.205s)
               Value function loss: 12.8641
                    Surrogate loss: 0.0172
             Mean action noise std: 0.72
                       Mean reward: -14.25
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 5.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 13.56s
                        Total time: 30374.72s
                               ETA: 1328686.8s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.903s, learning 0.170s)
               Value function loss: 0.4833
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: -13.59
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 13.07s
                        Total time: 30387.79s
                               ETA: 1328650.6s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.185s, learning 0.181s)
               Value function loss: 0.5087
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: -11.96
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 12.37s
                        Total time: 30400.16s
                               ETA: 1328583.5s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.479s, learning 0.161s)
               Value function loss: 0.4849
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: -13.13
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 12.64s
                        Total time: 30412.80s
                               ETA: 1328528.4s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.202s, learning 0.159s)
               Value function loss: 0.7906
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: -10.51
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 12.36s
                        Total time: 30425.16s
                               ETA: 1328461.1s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.345s, learning 0.165s)
               Value function loss: 1.0874
                    Surrogate loss: 0.0061
             Mean action noise std: 0.72
                       Mean reward: -9.14
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 12.51s
                        Total time: 30437.67s
                               ETA: 1328400.4s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.023s, learning 0.207s)
               Value function loss: 1.2773
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: -9.55
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 12.23s
                        Total time: 30449.90s
                               ETA: 1328327.6s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.622s, learning 0.164s)
               Value function loss: 1.5196
                    Surrogate loss: 0.0084
             Mean action noise std: 0.72
                       Mean reward: -10.23
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 12.79s
                        Total time: 30462.69s
                               ETA: 1328279.1s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.505s, learning 0.166s)
               Value function loss: 2.1463
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: -11.66
               Mean episode length: 124.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 12.67s
                        Total time: 30475.36s
                               ETA: 1328225.5s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.246s, learning 0.187s)
               Value function loss: 2.9712
                    Surrogate loss: 0.0091
             Mean action noise std: 0.72
                       Mean reward: -9.89
               Mean episode length: 124.62
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 12.43s
                        Total time: 30487.79s
                               ETA: 1328161.7s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.936s, learning 0.172s)
               Value function loss: 1.6856
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: -11.89
               Mean episode length: 124.61
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 13.11s
                        Total time: 30500.90s
                               ETA: 1328127.3s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.611s, learning 0.207s)
               Value function loss: 2.0622
                    Surrogate loss: 0.0090
             Mean action noise std: 0.72
                       Mean reward: -12.79
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 11.82s
                        Total time: 30512.72s
                               ETA: 1328036.7s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.661s, learning 0.232s)
               Value function loss: 1.6157
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -12.07
               Mean episode length: 124.73
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 12.89s
                        Total time: 30525.61s
                               ETA: 1327993.0s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.944s, learning 0.170s)
               Value function loss: 1.7384
                    Surrogate loss: 0.0136
             Mean action noise std: 0.72
                       Mean reward: -12.56
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 12.11s
                        Total time: 30537.72s
                               ETA: 1327915.4s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.331s, learning 0.190s)
               Value function loss: 1.5639
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: -10.88
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 12.52s
                        Total time: 30550.24s
                               ETA: 1327855.6s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.530s, learning 0.165s)
               Value function loss: 16.5424
                    Surrogate loss: 0.0092
             Mean action noise std: 0.72
                       Mean reward: -10.31
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 5.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 12.69s
                        Total time: 30562.94s
                               ETA: 1327803.4s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.479s, learning 0.166s)
               Value function loss: 0.3253
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: -10.59
               Mean episode length: 125.00
                  Mean reward/step: -0.60
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 12.64s
                        Total time: 30575.58s
                               ETA: 1327749.0s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.213s, learning 0.161s)
               Value function loss: 0.3290
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: -10.65
               Mean episode length: 125.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 12.37s
                        Total time: 30587.96s
                               ETA: 1327683.0s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.106s, learning 0.186s)
               Value function loss: 0.4355
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: -11.44
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 12.29s
                        Total time: 30600.25s
                               ETA: 1327613.4s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.242s, learning 0.162s)
               Value function loss: 0.5067
                    Surrogate loss: 0.0035
             Mean action noise std: 0.72
                       Mean reward: -11.39
               Mean episode length: 125.00
                  Mean reward/step: -0.09
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 12.40s
                        Total time: 30612.65s
                               ETA: 1327548.7s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.512s, learning 0.167s)
               Value function loss: 0.8665
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: -10.17
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 12.68s
                        Total time: 30625.33s
                               ETA: 1327496.0s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.875s, learning 0.169s)
               Value function loss: 0.9759
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: -10.84
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 13.04s
                        Total time: 30638.37s
                               ETA: 1327459.2s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.316s, learning 0.178s)
               Value function loss: 1.7556
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: -9.67
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 12.49s
                        Total time: 30650.87s
                               ETA: 1327398.5s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.560s, learning 0.187s)
               Value function loss: 3.3899
                    Surrogate loss: 0.0039
             Mean action noise std: 0.72
                       Mean reward: -11.42
               Mean episode length: 124.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 12.75s
                        Total time: 30663.62s
                               ETA: 1327348.9s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.364s, learning 0.161s)
               Value function loss: 2.8186
                    Surrogate loss: 0.0101
             Mean action noise std: 0.72
                       Mean reward: -10.94
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 12.53s
                        Total time: 30676.14s
                               ETA: 1327289.7s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.119s, learning 0.163s)
               Value function loss: 2.4291
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -7.82
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 12.28s
                        Total time: 30688.42s
                               ETA: 1327220.0s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.377s, learning 0.187s)
               Value function loss: 1.9715
                    Surrogate loss: 0.0099
             Mean action noise std: 0.72
                       Mean reward: -10.03
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 11.56s
                        Total time: 30699.99s
                               ETA: 1327119.3s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.904s, learning 0.207s)
               Value function loss: 2.0089
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: -10.61
               Mean episode length: 124.66
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 12.11s
                        Total time: 30712.10s
                               ETA: 1327042.4s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.024s, learning 0.166s)
               Value function loss: 2.1806
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -10.02
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 12.19s
                        Total time: 30724.29s
                               ETA: 1326968.9s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.522s, learning 0.294s)
               Value function loss: 2.4235
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -7.59
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 12.82s
                        Total time: 30737.10s
                               ETA: 1326922.4s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.569s, learning 0.190s)
               Value function loss: 6.1049
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: -8.26
               Mean episode length: 125.00
                  Mean reward/step: -0.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 12.76s
                        Total time: 30749.86s
                               ETA: 1326873.6s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.696s, learning 0.185s)
               Value function loss: 30.8351
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: -17.11
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 5.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 12.88s
                        Total time: 30762.74s
                               ETA: 1326830.0s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1231 steps/s (collection: 13.103s, learning 0.198s)
               Value function loss: 0.8150
                    Surrogate loss: 0.0187
             Mean action noise std: 0.72
                       Mean reward: -16.97
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 13.30s
                        Total time: 30776.05s
                               ETA: 1326804.6s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.459s, learning 0.182s)
               Value function loss: 0.5903
                    Surrogate loss: 0.0033
             Mean action noise std: 0.72
                       Mean reward: -16.04
               Mean episode length: 125.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 12.64s
                        Total time: 30788.69s
                               ETA: 1326750.7s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.703s, learning 0.186s)
               Value function loss: 0.5399
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: -14.54
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 12.89s
                        Total time: 30801.57s
                               ETA: 1326707.6s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.473s, learning 0.160s)
               Value function loss: 0.7668
                    Surrogate loss: 0.0042
             Mean action noise std: 0.72
                       Mean reward: -6.61
               Mean episode length: 125.00
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 12.63s
                        Total time: 30814.21s
                               ETA: 1326653.5s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.393s, learning 0.184s)
               Value function loss: 0.7654
                    Surrogate loss: 0.0083
             Mean action noise std: 0.72
                       Mean reward: -7.96
               Mean episode length: 124.43
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 12.58s
                        Total time: 30826.78s
                               ETA: 1326596.9s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.811s, learning 0.170s)
               Value function loss: 0.8090
                    Surrogate loss: 0.0083
             Mean action noise std: 0.72
                       Mean reward: -8.19
               Mean episode length: 124.43
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 12.98s
                        Total time: 30839.77s
                               ETA: 1326557.9s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.339s, learning 0.164s)
               Value function loss: 1.1135
                    Surrogate loss: 0.0062
             Mean action noise std: 0.72
                       Mean reward: -6.57
               Mean episode length: 124.43
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 12.50s
                        Total time: 30852.27s
                               ETA: 1326498.3s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.690s, learning 0.160s)
               Value function loss: 1.6545
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: -8.85
               Mean episode length: 123.93
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 11.85s
                        Total time: 30864.12s
                               ETA: 1326410.6s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.172s, learning 0.167s)
               Value function loss: 2.2345
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: -9.56
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 12.34s
                        Total time: 30876.46s
                               ETA: 1326344.1s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.605s, learning 0.198s)
               Value function loss: 1.7104
                    Surrogate loss: 0.0092
             Mean action noise std: 0.72
                       Mean reward: -5.80
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 12.80s
                        Total time: 30889.26s
                               ETA: 1326297.5s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.601s, learning 0.189s)
               Value function loss: 1.7415
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: -6.33
               Mean episode length: 125.00
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 12.79s
                        Total time: 30902.05s
                               ETA: 1326250.4s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.185s, learning 0.169s)
               Value function loss: 1.5968
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: -8.04
               Mean episode length: 124.72
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 12.35s
                        Total time: 30914.41s
                               ETA: 1326184.6s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.650s, learning 0.199s)
               Value function loss: 1.7970
                    Surrogate loss: 0.0147
             Mean action noise std: 0.72
                       Mean reward: -5.36
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 12.85s
                        Total time: 30927.25s
                               ETA: 1326140.0s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.716s, learning 0.204s)
               Value function loss: 1.5968
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: -4.41
               Mean episode length: 124.85
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 12.92s
                        Total time: 30940.17s
                               ETA: 1326098.5s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.663s, learning 0.159s)
               Value function loss: 1.8105
                    Surrogate loss: 0.0073
             Mean action noise std: 0.72
                       Mean reward: -5.35
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 12.82s
                        Total time: 30952.99s
                               ETA: 1326052.9s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1211 steps/s (collection: 13.273s, learning 0.254s)
               Value function loss: 12.4843
                    Surrogate loss: 0.0217
             Mean action noise std: 0.72
                       Mean reward: -5.19
               Mean episode length: 124.63
                  Mean reward/step: -0.52
       Mean episode length/episode: 5.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 13.53s
                        Total time: 30966.52s
                               ETA: 1326037.4s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.878s, learning 0.181s)
               Value function loss: 0.3915
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: -5.86
               Mean episode length: 124.63
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 13.06s
                        Total time: 30979.58s
                               ETA: 1326002.0s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.978s, learning 0.162s)
               Value function loss: 0.4299
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -7.12
               Mean episode length: 124.63
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 12.14s
                        Total time: 30991.72s
                               ETA: 1325927.3s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.256s, learning 0.162s)
               Value function loss: 0.4372
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: -6.29
               Mean episode length: 124.63
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 12.42s
                        Total time: 31004.14s
                               ETA: 1325864.5s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.112s, learning 0.185s)
               Value function loss: 0.8713
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -4.13
               Mean episode length: 124.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 12.30s
                        Total time: 31016.43s
                               ETA: 1325796.5s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.380s, learning 0.158s)
               Value function loss: 0.8685
                    Surrogate loss: 0.0187
             Mean action noise std: 0.72
                       Mean reward: -4.06
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 12.54s
                        Total time: 31028.97s
                               ETA: 1325738.9s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.371s, learning 0.208s)
               Value function loss: 0.7929
                    Surrogate loss: 0.0077
             Mean action noise std: 0.72
                       Mean reward: -3.62
               Mean episode length: 124.55
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 12.58s
                        Total time: 31041.55s
                               ETA: 1325683.2s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.838s, learning 0.254s)
               Value function loss: 1.2061
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: -2.96
               Mean episode length: 124.30
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 13.09s
                        Total time: 31054.64s
                               ETA: 1325649.3s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.641s, learning 0.305s)
               Value function loss: 2.2324
                    Surrogate loss: 0.0051
             Mean action noise std: 0.72
                       Mean reward: -2.73
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 12.95s
                        Total time: 31067.59s
                               ETA: 1325609.3s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.594s, learning 0.251s)
               Value function loss: 2.1001
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: -4.74
               Mean episode length: 124.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 12.85s
                        Total time: 31080.43s
                               ETA: 1325564.9s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.351s, learning 0.205s)
               Value function loss: 1.7429
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: -4.04
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 12.56s
                        Total time: 31092.99s
                               ETA: 1325508.3s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.749s, learning 0.162s)
               Value function loss: 1.9312
                    Surrogate loss: 0.0049
             Mean action noise std: 0.72
                       Mean reward: -2.83
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 12.91s
                        Total time: 31105.90s
                               ETA: 1325466.8s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.259s, learning 0.159s)
               Value function loss: 1.6789
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -6.04
               Mean episode length: 124.71
                  Mean reward/step: 0.00
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 12.42s
                        Total time: 31118.32s
                               ETA: 1325404.4s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.999s, learning 0.186s)
               Value function loss: 1.7904
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: -4.98
               Mean episode length: 124.85
                  Mean reward/step: -0.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 12.19s
                        Total time: 31130.50s
                               ETA: 1325332.1s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.525s, learning 0.158s)
               Value function loss: 1.7222
                    Surrogate loss: 0.0102
             Mean action noise std: 0.72
                       Mean reward: -1.30
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 12.68s
                        Total time: 31143.19s
                               ETA: 1325281.0s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.664s, learning 0.158s)
               Value function loss: 14.5317
                    Surrogate loss: 0.0221
             Mean action noise std: 0.72
                       Mean reward: -1.65
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 5.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 12.82s
                        Total time: 31156.01s
                               ETA: 1325235.8s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.574s, learning 0.203s)
               Value function loss: 0.3202
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: -2.16
               Mean episode length: 125.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 12.78s
                        Total time: 31168.79s
                               ETA: 1325188.8s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.849s, learning 0.213s)
               Value function loss: 0.3821
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: -2.97
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 13.06s
                        Total time: 31181.85s
                               ETA: 1325154.0s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.578s, learning 0.160s)
               Value function loss: 0.4699
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: -2.21
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 12.74s
                        Total time: 31194.59s
                               ETA: 1325105.3s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.565s, learning 0.159s)
               Value function loss: 0.6560
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: -4.48
               Mean episode length: 124.52
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 12.72s
                        Total time: 31207.31s
                               ETA: 1325056.2s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.780s, learning 0.224s)
               Value function loss: 0.9229
                    Surrogate loss: 0.0116
             Mean action noise std: 0.72
                       Mean reward: -5.23
               Mean episode length: 124.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 13.00s
                        Total time: 31220.31s
                               ETA: 1325018.9s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.394s, learning 0.198s)
               Value function loss: 0.7537
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: -4.87
               Mean episode length: 124.52
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 12.59s
                        Total time: 31232.91s
                               ETA: 1324964.2s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.488s, learning 0.200s)
               Value function loss: 1.1442
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: -4.16
               Mean episode length: 124.96
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 12.69s
                        Total time: 31245.60s
                               ETA: 1324913.6s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.743s, learning 0.184s)
               Value function loss: 1.7292
                    Surrogate loss: 0.0061
             Mean action noise std: 0.72
                       Mean reward: -2.59
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 12.93s
                        Total time: 31258.52s
                               ETA: 1324873.2s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.499s, learning 0.161s)
               Value function loss: 2.5603
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: -4.24
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 12.66s
                        Total time: 31271.18s
                               ETA: 1324821.4s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.643s, learning 0.243s)
               Value function loss: 2.2595
                    Surrogate loss: 0.0079
             Mean action noise std: 0.72
                       Mean reward: -3.10
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 12.89s
                        Total time: 31284.07s
                               ETA: 1324779.3s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.196s, learning 0.158s)
               Value function loss: 1.6501
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: -3.01
               Mean episode length: 124.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 12.35s
                        Total time: 31296.42s
                               ETA: 1324714.6s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.345s, learning 0.163s)
               Value function loss: 2.0744
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: -4.33
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 12.51s
                        Total time: 31308.93s
                               ETA: 1324656.6s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.072s, learning 0.212s)
               Value function loss: 3.2982
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: -1.58
               Mean episode length: 124.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 12.28s
                        Total time: 31321.21s
                               ETA: 1324589.1s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.379s, learning 0.160s)
               Value function loss: 3.3039
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: -3.90
               Mean episode length: 124.69
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 12.54s
                        Total time: 31333.75s
                               ETA: 1324532.4s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.165s, learning 0.161s)
               Value function loss: 1.9213
                    Surrogate loss: 0.0343
             Mean action noise std: 0.72
                       Mean reward: -1.82
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 12.33s
                        Total time: 31346.08s
                               ETA: 1324466.7s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.630s, learning 0.164s)
               Value function loss: 14.5411
                    Surrogate loss: 0.0122
             Mean action noise std: 0.72
                       Mean reward: -3.85
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 5.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 12.79s
                        Total time: 31358.87s
                               ETA: 1324420.9s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.659s, learning 0.191s)
               Value function loss: 0.3527
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: -3.87
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 12.85s
                        Total time: 31371.72s
                               ETA: 1324377.5s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.382s, learning 0.207s)
               Value function loss: 0.5406
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: -4.23
               Mean episode length: 124.45
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 12.59s
                        Total time: 31384.31s
                               ETA: 1324323.1s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.502s, learning 0.192s)
               Value function loss: 0.4751
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: -3.33
               Mean episode length: 124.45
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 12.69s
                        Total time: 31397.01s
                               ETA: 1324273.1s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.747s, learning 0.244s)
               Value function loss: 0.8365
                    Surrogate loss: 0.0011
             Mean action noise std: 0.72
                       Mean reward: -4.47
               Mean episode length: 124.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 12.99s
                        Total time: 31410.00s
                               ETA: 1324235.7s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.329s, learning 0.162s)
               Value function loss: 0.9320
                    Surrogate loss: 0.0064
             Mean action noise std: 0.72
                       Mean reward: -5.05
               Mean episode length: 124.02
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 12.49s
                        Total time: 31422.49s
                               ETA: 1324177.2s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.477s, learning 0.182s)
               Value function loss: 0.9538
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: -3.67
               Mean episode length: 124.57
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 12.66s
                        Total time: 31435.15s
                               ETA: 1324125.9s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.475s, learning 0.172s)
               Value function loss: 1.5620
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: -2.11
               Mean episode length: 124.57
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 12.65s
                        Total time: 31447.79s
                               ETA: 1324074.1s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.035s, learning 0.166s)
               Value function loss: 2.2973
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: -2.47
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 12.20s
                        Total time: 31459.99s
                               ETA: 1324003.6s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.178s, learning 0.165s)
               Value function loss: 2.5415
                    Surrogate loss: 0.0057
             Mean action noise std: 0.72
                       Mean reward: -3.32
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 12.34s
                        Total time: 31472.34s
                               ETA: 1323939.0s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.265s, learning 0.183s)
               Value function loss: 1.9243
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -1.78
               Mean episode length: 124.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 12.45s
                        Total time: 31484.79s
                               ETA: 1323879.0s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.741s, learning 0.221s)
               Value function loss: 1.8622
                    Surrogate loss: 0.0052
             Mean action noise std: 0.72
                       Mean reward: -0.45
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.96s
                        Total time: 31496.75s
                               ETA: 1323798.6s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.804s, learning 0.157s)
               Value function loss: 1.7807
                    Surrogate loss: 0.0100
             Mean action noise std: 0.72
                       Mean reward: -0.56
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 11.96s
                        Total time: 31508.71s
                               ETA: 1323718.1s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.516s, learning 0.161s)
               Value function loss: 2.0628
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: -3.34
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 12.68s
                        Total time: 31521.39s
                               ETA: 1323667.8s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.361s, learning 0.162s)
               Value function loss: 1.7850
                    Surrogate loss: 0.0120
             Mean action noise std: 0.72
                       Mean reward: -3.43
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 12.52s
                        Total time: 31533.91s
                               ETA: 1323611.1s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.202s, learning 0.160s)
               Value function loss: 3.2209
                    Surrogate loss: 0.0099
             Mean action noise std: 0.72
                       Mean reward: 1.03
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 12.36s
                        Total time: 31546.27s
                               ETA: 1323547.6s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1209 steps/s (collection: 13.384s, learning 0.163s)
               Value function loss: 13.4574
                    Surrogate loss: 0.0133
             Mean action noise std: 0.72
                       Mean reward: 0.52
               Mean episode length: 125.00
                  Mean reward/step: -0.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 13.55s
                        Total time: 31559.82s
                               ETA: 1323534.0s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.615s, learning 0.173s)
               Value function loss: 0.4026
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: 0.84
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 12.79s
                        Total time: 31572.61s
                               ETA: 1323488.4s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.580s, learning 0.165s)
               Value function loss: 0.4935
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 1.95
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 12.75s
                        Total time: 31585.35s
                               ETA: 1323441.1s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.264s, learning 0.236s)
               Value function loss: 0.6082
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 2.20
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 12.50s
                        Total time: 31597.85s
                               ETA: 1323383.6s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.818s, learning 0.169s)
               Value function loss: 1.2224
                    Surrogate loss: 0.0059
             Mean action noise std: 0.72
                       Mean reward: 2.71
               Mean episode length: 124.73
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 12.99s
                        Total time: 31610.84s
                               ETA: 1323346.5s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.289s, learning 0.170s)
               Value function loss: 0.7984
                    Surrogate loss: 0.0039
             Mean action noise std: 0.72
                       Mean reward: 3.06
               Mean episode length: 124.60
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 12.46s
                        Total time: 31623.30s
                               ETA: 1323287.3s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.443s, learning 0.179s)
               Value function loss: 0.9014
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 124.60
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 12.62s
                        Total time: 31635.92s
                               ETA: 1323235.0s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1247 steps/s (collection: 12.931s, learning 0.202s)
               Value function loss: 1.3938
                    Surrogate loss: 0.0046
             Mean action noise std: 0.72
                       Mean reward: 2.42
               Mean episode length: 124.81
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 13.13s
                        Total time: 31649.05s
                               ETA: 1323204.1s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.363s, learning 0.164s)
               Value function loss: 2.6634
                    Surrogate loss: 0.0025
             Mean action noise std: 0.72
                       Mean reward: 0.51
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 12.53s
                        Total time: 31661.58s
                               ETA: 1323147.9s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.169s, learning 0.173s)
               Value function loss: 2.2778
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 2.77
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 12.34s
                        Total time: 31673.92s
                               ETA: 1323083.9s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.217s, learning 0.170s)
               Value function loss: 1.7781
                    Surrogate loss: 0.0112
             Mean action noise std: 0.72
                       Mean reward: -0.22
               Mean episode length: 124.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 12.39s
                        Total time: 31686.31s
                               ETA: 1323021.9s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.464s, learning 0.298s)
               Value function loss: 2.0901
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 124.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 12.76s
                        Total time: 31699.07s
                               ETA: 1322975.6s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.436s, learning 0.162s)
               Value function loss: 2.1643
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 124.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 12.60s
                        Total time: 31711.67s
                               ETA: 1322922.5s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.375s, learning 0.163s)
               Value function loss: 2.1558
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 0.08
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 12.54s
                        Total time: 31724.21s
                               ETA: 1322866.9s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.704s, learning 0.238s)
               Value function loss: 2.1581
                    Surrogate loss: 0.0290
             Mean action noise std: 0.72
                       Mean reward: 2.52
               Mean episode length: 124.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 12.94s
                        Total time: 31737.15s
                               ETA: 1322828.2s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 893 steps/s (collection: 18.156s, learning 0.177s)
               Value function loss: 15.8496
                    Surrogate loss: 0.0260
             Mean action noise std: 0.72
                       Mean reward: 3.75
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 5.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 18.33s
                        Total time: 31755.48s
                               ETA: 1323014.1s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.912s, learning 0.162s)
               Value function loss: 0.5361
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: 4.28
               Mean episode length: 125.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 24.07s
                        Total time: 31779.56s
                               ETA: 1323438.9s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 673 steps/s (collection: 24.129s, learning 0.205s)
               Value function loss: 0.5322
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 3.25
               Mean episode length: 124.59
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 24.33s
                        Total time: 31803.89s
                               ETA: 1323874.2s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 668 steps/s (collection: 24.311s, learning 0.205s)
               Value function loss: 0.6025
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 2.54
               Mean episode length: 124.59
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 24.52s
                        Total time: 31828.40s
                               ETA: 1324316.6s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 684 steps/s (collection: 23.759s, learning 0.191s)
               Value function loss: 1.0463
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 2.75
               Mean episode length: 124.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 23.95s
                        Total time: 31852.36s
                               ETA: 1324735.1s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.250s, learning 0.176s)
               Value function loss: 1.3065
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 2.54
               Mean episode length: 124.07
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 23.43s
                        Total time: 31875.78s
                               ETA: 1325131.4s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.208s, learning 0.166s)
               Value function loss: 1.0259
                    Surrogate loss: 0.1088
             Mean action noise std: 0.72
                       Mean reward: 1.72
               Mean episode length: 124.29
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 23.37s
                        Total time: 31899.16s
                               ETA: 1325525.3s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 675 steps/s (collection: 24.026s, learning 0.211s)
               Value function loss: 1.2274
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 2.75
               Mean episode length: 124.13
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 24.24s
                        Total time: 31923.39s
                               ETA: 1325954.6s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 673 steps/s (collection: 24.107s, learning 0.203s)
               Value function loss: 1.7869
                    Surrogate loss: 0.0082
             Mean action noise std: 0.72
                       Mean reward: 1.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 24.31s
                        Total time: 31947.70s
                               ETA: 1326386.6s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 675 steps/s (collection: 24.045s, learning 0.205s)
               Value function loss: 2.8270
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.19
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 24.25s
                        Total time: 31971.95s
                               ETA: 1326815.6s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.189s, learning 0.160s)
               Value function loss: 1.9945
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 4.02
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 23.35s
                        Total time: 31995.30s
                               ETA: 1327206.9s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 672 steps/s (collection: 24.159s, learning 0.203s)
               Value function loss: 2.2882
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 2.82
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 24.36s
                        Total time: 32019.66s
                               ETA: 1327639.9s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 673 steps/s (collection: 24.106s, learning 0.213s)
               Value function loss: 2.4781
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: 5.23
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 24.32s
                        Total time: 32043.98s
                               ETA: 1328070.7s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.723s, learning 0.161s)
               Value function loss: 2.4866
                    Surrogate loss: -0.0058
             Mean action noise std: 0.72
                       Mean reward: 2.02
               Mean episode length: 124.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 23.88s
                        Total time: 32067.87s
                               ETA: 1328483.1s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 673 steps/s (collection: 24.159s, learning 0.170s)
               Value function loss: 2.5592
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 2.04
               Mean episode length: 124.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 24.33s
                        Total time: 32092.19s
                               ETA: 1328913.6s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.925s, learning 0.187s)
               Value function loss: 2.5292
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 23.11s
                        Total time: 32115.31s
                               ETA: 1329293.3s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 674 steps/s (collection: 24.130s, learning 0.162s)
               Value function loss: 15.2386
                    Surrogate loss: 0.0208
             Mean action noise std: 0.72
                       Mean reward: 2.37
               Mean episode length: 124.71
                  Mean reward/step: -0.42
       Mean episode length/episode: 5.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 24.29s
                        Total time: 32139.60s
                               ETA: 1329721.4s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.539s, learning 0.189s)
               Value function loss: 0.5144
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 0.86
               Mean episode length: 124.53
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 23.73s
                        Total time: 32163.33s
                               ETA: 1330125.9s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.749s, learning 0.162s)
               Value function loss: 0.6582
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: -0.16
               Mean episode length: 124.53
                  Mean reward/step: -0.27
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 23.91s
                        Total time: 32187.24s
                               ETA: 1330537.6s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 687 steps/s (collection: 23.674s, learning 0.165s)
               Value function loss: 0.6168
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: 0.55
               Mean episode length: 124.53
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 23.84s
                        Total time: 32211.08s
                               ETA: 1330945.9s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.451s, learning 0.237s)
               Value function loss: 1.1206
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: -1.17
               Mean episode length: 124.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 23.69s
                        Total time: 32234.77s
                               ETA: 1331347.6s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 666 steps/s (collection: 24.398s, learning 0.194s)
               Value function loss: 1.0778
                    Surrogate loss: 0.0197
             Mean action noise std: 0.72
                       Mean reward: -1.17
               Mean episode length: 124.63
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 24.59s
                        Total time: 32259.36s
                               ETA: 1331786.3s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 675 steps/s (collection: 24.073s, learning 0.169s)
               Value function loss: 1.0222
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: -1.11
               Mean episode length: 124.63
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 24.24s
                        Total time: 32283.60s
                               ETA: 1332210.2s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 658 steps/s (collection: 24.657s, learning 0.235s)
               Value function loss: 1.5201
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 1.07
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 24.89s
                        Total time: 32308.49s
                               ETA: 1332660.4s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 677 steps/s (collection: 24.004s, learning 0.183s)
               Value function loss: 2.5513
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: -0.61
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 24.19s
                        Total time: 32332.68s
                               ETA: 1333081.3s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 679 steps/s (collection: 23.901s, learning 0.209s)
               Value function loss: 2.7607
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 0.63
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 24.11s
                        Total time: 32356.79s
                               ETA: 1333498.5s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 674 steps/s (collection: 24.127s, learning 0.164s)
               Value function loss: 1.9035
                    Surrogate loss: 0.0048
             Mean action noise std: 0.72
                       Mean reward: -0.18
               Mean episode length: 124.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 24.29s
                        Total time: 32381.08s
                               ETA: 1333922.9s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 669 steps/s (collection: 24.285s, learning 0.203s)
               Value function loss: 2.1084
                    Surrogate loss: 0.0042
             Mean action noise std: 0.72
                       Mean reward: 0.89
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 24.49s
                        Total time: 32405.57s
                               ETA: 1334355.0s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.560s, learning 0.166s)
               Value function loss: 2.1712
                    Surrogate loss: 0.0088
             Mean action noise std: 0.72
                       Mean reward: 0.58
               Mean episode length: 124.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 23.73s
                        Total time: 32429.30s
                               ETA: 1334755.3s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.599s, learning 0.170s)
               Value function loss: 2.5052
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: 2.84
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 23.77s
                        Total time: 32453.06s
                               ETA: 1335157.1s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.475s, learning 0.211s)
               Value function loss: 2.1857
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 0.18
               Mean episode length: 124.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 23.69s
                        Total time: 32476.75s
                               ETA: 1335555.1s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 664 steps/s (collection: 24.460s, learning 0.204s)
               Value function loss: 17.1834
                    Surrogate loss: 0.0288
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 5.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 24.66s
                        Total time: 32501.42s
                               ETA: 1335992.9s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.943s, learning 0.200s)
               Value function loss: 0.5398
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 1.02
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 24.14s
                        Total time: 32525.56s
                               ETA: 1336408.9s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.269s, learning 0.162s)
               Value function loss: 0.4464
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 0.10
               Mean episode length: 124.47
                  Mean reward/step: -0.38
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 23.43s
                        Total time: 32548.99s
                               ETA: 1336795.3s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.311s, learning 0.159s)
               Value function loss: 0.5774
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: -0.92
               Mean episode length: 124.47
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 23.47s
                        Total time: 32572.46s
                               ETA: 1337183.0s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.596s, learning 0.167s)
               Value function loss: 0.8085
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: -0.80
               Mean episode length: 124.47
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 23.76s
                        Total time: 32596.22s
                               ETA: 1337582.3s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.222s, learning 0.252s)
               Value function loss: 1.2832
                    Surrogate loss: 0.0038
             Mean action noise std: 0.72
                       Mean reward: 0.79
               Mean episode length: 124.47
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 23.47s
                        Total time: 32619.70s
                               ETA: 1337969.4s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.893s, learning 0.243s)
               Value function loss: 0.9991
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: 3.71
               Mean episode length: 124.42
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 24.14s
                        Total time: 32643.83s
                               ETA: 1338383.4s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1149 steps/s (collection: 14.050s, learning 0.199s)
               Value function loss: 1.6167
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 4.08
               Mean episode length: 124.42
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 14.25s
                        Total time: 32658.08s
                               ETA: 1338391.8s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.484s, learning 0.160s)
               Value function loss: 2.1069
                    Surrogate loss: 0.0165
             Mean action noise std: 0.72
                       Mean reward: 2.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 12.64s
                        Total time: 32670.72s
                               ETA: 1338334.4s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1244 steps/s (collection: 13.001s, learning 0.165s)
               Value function loss: 2.6940
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 124.72
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 13.17s
                        Total time: 32683.89s
                               ETA: 1338298.4s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.061s, learning 0.163s)
               Value function loss: 2.3191
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 4.09
               Mean episode length: 124.56
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 12.22s
                        Total time: 32696.12s
                               ETA: 1338223.9s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.099s, learning 0.167s)
               Value function loss: 1.9209
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: 2.47
               Mean episode length: 124.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 12.27s
                        Total time: 32708.38s
                               ETA: 1338151.1s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.157s, learning 0.157s)
               Value function loss: 2.4613
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 4.21
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 12.31s
                        Total time: 32720.70s
                               ETA: 1338080.4s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.636s, learning 0.168s)
               Value function loss: 2.7172
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 1.05
               Mean episode length: 124.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 12.80s
                        Total time: 32733.50s
                               ETA: 1338029.8s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.779s, learning 0.163s)
               Value function loss: 2.5086
                    Surrogate loss: 0.0074
             Mean action noise std: 0.72
                       Mean reward: 4.40
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 12.94s
                        Total time: 32746.44s
                               ETA: 1337984.8s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.242s, learning 0.160s)
               Value function loss: 2.3742
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: 5.58
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 12.40s
                        Total time: 32758.84s
                               ETA: 1337917.8s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1244 steps/s (collection: 12.980s, learning 0.187s)
               Value function loss: 21.8399
                    Surrogate loss: 0.0300
             Mean action noise std: 0.72
                       Mean reward: 4.06
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 5.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 13.17s
                        Total time: 32772.01s
                               ETA: 1337882.0s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1218 steps/s (collection: 13.230s, learning 0.215s)
               Value function loss: 0.7678
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 4.69
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 13.45s
                        Total time: 32785.46s
                               ETA: 1337857.7s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.363s, learning 0.177s)
               Value function loss: 0.7126
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 5.53
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 12.54s
                        Total time: 32798.00s
                               ETA: 1337796.4s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.543s, learning 0.163s)
               Value function loss: 0.7069
                    Surrogate loss: 0.0073
             Mean action noise std: 0.72
                       Mean reward: 5.01
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 12.71s
                        Total time: 32810.70s
                               ETA: 1337741.9s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.798s, learning 0.196s)
               Value function loss: 1.0655
                    Surrogate loss: 0.0103
             Mean action noise std: 0.72
                       Mean reward: 4.05
               Mean episode length: 124.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 12.99s
                        Total time: 32823.70s
                               ETA: 1337699.3s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.330s, learning 0.159s)
               Value function loss: 1.1742
                    Surrogate loss: 0.0041
             Mean action noise std: 0.72
                       Mean reward: 0.81
               Mean episode length: 124.64
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 12.49s
                        Total time: 32836.19s
                               ETA: 1337636.0s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.480s, learning 0.192s)
               Value function loss: 0.9960
                    Surrogate loss: 0.0105
             Mean action noise std: 0.72
                       Mean reward: -0.30
               Mean episode length: 124.64
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 12.67s
                        Total time: 32848.86s
                               ETA: 1337580.3s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.356s, learning 0.191s)
               Value function loss: 1.5188
                    Surrogate loss: 0.0108
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 12.55s
                        Total time: 32861.41s
                               ETA: 1337519.5s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1241 steps/s (collection: 13.034s, learning 0.159s)
               Value function loss: 2.2754
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 124.77
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 13.19s
                        Total time: 32874.60s
                               ETA: 1337485.0s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.892s, learning 0.196s)
               Value function loss: 3.1405
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: -0.67
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 13.09s
                        Total time: 32887.69s
                               ETA: 1337446.3s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.424s, learning 0.160s)
               Value function loss: 2.1605
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: 3.80
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 12.58s
                        Total time: 32900.27s
                               ETA: 1337387.1s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.626s, learning 0.177s)
               Value function loss: 2.1800
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 1.01
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 12.80s
                        Total time: 32913.07s
                               ETA: 1337336.8s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.760s, learning 0.281s)
               Value function loss: 2.0708
                    Surrogate loss: 0.0264
             Mean action noise std: 0.72
                       Mean reward: 5.26
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 13.04s
                        Total time: 32926.11s
                               ETA: 1337296.2s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.618s, learning 0.193s)
               Value function loss: 2.2044
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 12.81s
                        Total time: 32938.92s
                               ETA: 1337246.4s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.700s, learning 0.281s)
               Value function loss: 1.9027
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: 1.04
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 12.98s
                        Total time: 32951.90s
                               ETA: 1337203.4s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.484s, learning 0.224s)
               Value function loss: 2.3185
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: 2.52
               Mean episode length: 124.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 12.71s
                        Total time: 32964.61s
                               ETA: 1337149.4s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1241 steps/s (collection: 13.038s, learning 0.161s)
               Value function loss: 16.1607
                    Surrogate loss: 0.0222
             Mean action noise std: 0.72
                       Mean reward: 0.22
               Mean episode length: 124.19
                  Mean reward/step: -0.50
       Mean episode length/episode: 5.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 13.20s
                        Total time: 32977.81s
                               ETA: 1337115.3s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.368s, learning 0.231s)
               Value function loss: 0.5914
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: 0.45
               Mean episode length: 124.19
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 12.60s
                        Total time: 32990.41s
                               ETA: 1337057.0s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.498s, learning 0.161s)
               Value function loss: 0.7020
                    Surrogate loss: 0.0047
             Mean action noise std: 0.72
                       Mean reward: 0.58
               Mean episode length: 124.19
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 12.66s
                        Total time: 33003.07s
                               ETA: 1337001.1s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.724s, learning 0.169s)
               Value function loss: 0.4632
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: -0.55
               Mean episode length: 124.19
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 12.89s
                        Total time: 33015.96s
                               ETA: 1336954.7s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.538s, learning 0.232s)
               Value function loss: 1.0661
                    Surrogate loss: 0.0117
             Mean action noise std: 0.72
                       Mean reward: -1.52
               Mean episode length: 124.19
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 12.77s
                        Total time: 33028.73s
                               ETA: 1336903.4s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.637s, learning 0.162s)
               Value function loss: 0.9048
                    Surrogate loss: 0.0116
             Mean action noise std: 0.72
                       Mean reward: -2.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 12.80s
                        Total time: 33041.53s
                               ETA: 1336853.3s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.135s, learning 0.158s)
               Value function loss: 0.8989
                    Surrogate loss: 0.0043
             Mean action noise std: 0.72
                       Mean reward: -2.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 12.29s
                        Total time: 33053.83s
                               ETA: 1336782.7s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.560s, learning 0.159s)
               Value function loss: 1.3062
                    Surrogate loss: 0.0124
             Mean action noise std: 0.72
                       Mean reward: -0.51
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 12.72s
                        Total time: 33066.54s
                               ETA: 1336729.4s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.438s, learning 0.157s)
               Value function loss: 2.3674
                    Surrogate loss: 0.0021
             Mean action noise std: 0.72
                       Mean reward: -0.54
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 12.60s
                        Total time: 33079.14s
                               ETA: 1336671.2s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.405s, learning 0.271s)
               Value function loss: 2.3609
                    Surrogate loss: 0.0047
             Mean action noise std: 0.72
                       Mean reward: -0.51
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 12.68s
                        Total time: 33091.82s
                               ETA: 1336616.2s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1248 steps/s (collection: 12.921s, learning 0.199s)
               Value function loss: 1.6989
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: -0.47
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 13.12s
                        Total time: 33104.93s
                               ETA: 1336579.2s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.424s, learning 0.158s)
               Value function loss: 2.0959
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 2.24
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 12.58s
                        Total time: 33117.52s
                               ETA: 1336520.6s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.214s, learning 0.161s)
               Value function loss: 2.0535
                    Surrogate loss: 0.0044
             Mean action noise std: 0.72
                       Mean reward: -0.68
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 12.38s
                        Total time: 33129.89s
                               ETA: 1336453.6s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.757s, learning 0.166s)
               Value function loss: 2.1179
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: -1.43
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 12.92s
                        Total time: 33142.82s
                               ETA: 1336408.7s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.629s, learning 0.171s)
               Value function loss: 2.1744
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: -1.07
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 12.80s
                        Total time: 33155.62s
                               ETA: 1336358.9s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1220 steps/s (collection: 13.247s, learning 0.174s)
               Value function loss: 22.8270
                    Surrogate loss: 0.0319
             Mean action noise std: 0.72
                       Mean reward: 2.47
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 5.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 13.42s
                        Total time: 33169.04s
                               ETA: 1336334.2s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.891s, learning 0.202s)
               Value function loss: 0.7431
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 2.66
               Mean episode length: 125.00
                  Mean reward/step: -0.55
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 13.09s
                        Total time: 33182.13s
                               ETA: 1336296.2s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.767s, learning 0.173s)
               Value function loss: 0.4144
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 2.09
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 12.94s
                        Total time: 33195.07s
                               ETA: 1336252.2s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.712s, learning 0.161s)
               Value function loss: 0.4638
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 0.99
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 12.87s
                        Total time: 33207.94s
                               ETA: 1336205.4s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.456s, learning 0.207s)
               Value function loss: 0.6298
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 12.66s
                        Total time: 33220.61s
                               ETA: 1336150.3s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.792s, learning 0.201s)
               Value function loss: 0.9605
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: -1.11
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 12.99s
                        Total time: 33233.60s
                               ETA: 1336108.4s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.796s, learning 0.160s)
               Value function loss: 0.8306
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 0.07
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 12.96s
                        Total time: 33246.55s
                               ETA: 1336065.1s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.812s, learning 0.190s)
               Value function loss: 1.0644
                    Surrogate loss: 0.0095
             Mean action noise std: 0.72
                       Mean reward: -2.47
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 13.00s
                        Total time: 33259.56s
                               ETA: 1336023.7s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.865s, learning 0.189s)
               Value function loss: 1.4037
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: -1.86
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 13.05s
                        Total time: 33272.61s
                               ETA: 1335984.3s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.407s, learning 0.271s)
               Value function loss: 2.5208
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: -1.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 12.68s
                        Total time: 33285.29s
                               ETA: 1335929.9s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.839s, learning 0.162s)
               Value function loss: 2.4423
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: -0.75
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 13.00s
                        Total time: 33298.29s
                               ETA: 1335888.5s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.687s, learning 0.203s)
               Value function loss: 1.9274
                    Surrogate loss: 0.0031
             Mean action noise std: 0.72
                       Mean reward: -2.96
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 12.89s
                        Total time: 33311.18s
                               ETA: 1335842.7s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.832s, learning 0.208s)
               Value function loss: 2.1409
                    Surrogate loss: 0.0192
             Mean action noise std: 0.72
                       Mean reward: -3.53
               Mean episode length: 124.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 13.04s
                        Total time: 33324.22s
                               ETA: 1335802.9s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.710s, learning 0.174s)
               Value function loss: 1.9720
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: -4.05
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 12.88s
                        Total time: 33337.11s
                               ETA: 1335756.9s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.458s, learning 0.164s)
               Value function loss: 2.1455
                    Surrogate loss: 0.0095
             Mean action noise std: 0.72
                       Mean reward: -1.71
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 12.62s
                        Total time: 33349.73s
                               ETA: 1335700.4s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.613s, learning 0.160s)
               Value function loss: 2.0490
                    Surrogate loss: 0.0046
             Mean action noise std: 0.72
                       Mean reward: -2.96
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 12.77s
                        Total time: 33362.50s
                               ETA: 1335650.0s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1241 steps/s (collection: 13.041s, learning 0.160s)
               Value function loss: 15.2097
                    Surrogate loss: 0.0216
             Mean action noise std: 0.72
                       Mean reward: -0.19
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 5.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 13.20s
                        Total time: 33375.70s
                               ETA: 1335616.7s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.805s, learning 0.210s)
               Value function loss: 0.4359
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: -0.17
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 12.02s
                        Total time: 33387.72s
                               ETA: 1335536.0s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.234s, learning 0.165s)
               Value function loss: 0.5235
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: -0.58
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 12.40s
                        Total time: 33400.12s
                               ETA: 1335470.8s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.026s, learning 0.189s)
               Value function loss: 0.4786
                    Surrogate loss: 0.0053
             Mean action noise std: 0.72
                       Mean reward: -1.26
               Mean episode length: 125.00
                  Mean reward/step: -0.16
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 12.21s
                        Total time: 33412.33s
                               ETA: 1335398.2s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.499s, learning 0.216s)
               Value function loss: 1.0054
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: -1.74
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 12.71s
                        Total time: 33425.05s
                               ETA: 1335345.6s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1240 steps/s (collection: 12.998s, learning 0.214s)
               Value function loss: 1.0447
                    Surrogate loss: 0.0052
             Mean action noise std: 0.72
                       Mean reward: -3.08
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 13.21s
                        Total time: 33438.26s
                               ETA: 1335313.0s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.580s, learning 0.160s)
               Value function loss: 0.9846
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: -2.52
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 12.74s
                        Total time: 33451.00s
                               ETA: 1335261.5s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.367s, learning 0.156s)
               Value function loss: 1.2366
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: -2.42
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 12.52s
                        Total time: 33463.52s
                               ETA: 1335201.3s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.653s, learning 0.166s)
               Value function loss: 2.2895
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: -4.50
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 12.82s
                        Total time: 33476.34s
                               ETA: 1335153.0s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.794s, learning 0.187s)
               Value function loss: 2.4024
                    Surrogate loss: 0.0180
             Mean action noise std: 0.72
                       Mean reward: -1.55
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 12.98s
                        Total time: 33489.32s
                               ETA: 1335111.2s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.284s, learning 0.192s)
               Value function loss: 1.9399
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: -0.39
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 12.48s
                        Total time: 33501.80s
                               ETA: 1335049.3s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.702s, learning 0.194s)
               Value function loss: 2.0867
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -1.45
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 12.90s
                        Total time: 33514.69s
                               ETA: 1335004.2s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.861s, learning 0.161s)
               Value function loss: 2.1415
                    Surrogate loss: 0.0140
             Mean action noise std: 0.72
                       Mean reward: -3.22
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 13.02s
                        Total time: 33527.72s
                               ETA: 1334964.1s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.627s, learning 0.286s)
               Value function loss: 2.0541
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: -3.20
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 12.91s
                        Total time: 33540.63s
                               ETA: 1334919.8s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.334s, learning 0.164s)
               Value function loss: 1.9125
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: -3.90
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 12.50s
                        Total time: 33553.13s
                               ETA: 1334858.9s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.479s, learning 0.159s)
               Value function loss: 2.6705
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: -2.66
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 12.64s
                        Total time: 33565.76s
                               ETA: 1334803.6s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1217 steps/s (collection: 13.293s, learning 0.168s)
               Value function loss: 12.6765
                    Surrogate loss: 0.0204
             Mean action noise std: 0.72
                       Mean reward: -3.18
               Mean episode length: 125.00
                  Mean reward/step: -0.58
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 13.46s
                        Total time: 33579.23s
                               ETA: 1334781.1s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.750s, learning 0.172s)
               Value function loss: 0.5666
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: -2.97
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 12.92s
                        Total time: 33592.15s
                               ETA: 1334737.1s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.866s, learning 0.162s)
               Value function loss: 0.5437
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: -3.88
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 13.03s
                        Total time: 33605.18s
                               ETA: 1334697.5s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.421s, learning 0.186s)
               Value function loss: 0.7079
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: -5.23
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 12.61s
                        Total time: 33617.78s
                               ETA: 1334641.1s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.863s, learning 0.164s)
               Value function loss: 1.1103
                    Surrogate loss: 0.0071
             Mean action noise std: 0.72
                       Mean reward: -6.18
               Mean episode length: 124.34
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 13.03s
                        Total time: 33630.81s
                               ETA: 1334601.4s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.878s, learning 0.226s)
               Value function loss: 0.9536
                    Surrogate loss: 0.0119
             Mean action noise std: 0.72
                       Mean reward: -6.39
               Mean episode length: 124.34
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 13.10s
                        Total time: 33643.92s
                               ETA: 1334564.8s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.607s, learning 0.186s)
               Value function loss: 1.0986
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: -5.06
               Mean episode length: 123.91
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 12.79s
                        Total time: 33656.71s
                               ETA: 1334515.9s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.252s, learning 0.186s)
               Value function loss: 1.3923
                    Surrogate loss: -0.0058
             Mean action noise std: 0.72
                       Mean reward: -3.65
               Mean episode length: 124.57
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 12.44s
                        Total time: 33669.15s
                               ETA: 1334452.9s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.356s, learning 0.160s)
               Value function loss: 3.1713
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: -5.37
               Mean episode length: 124.92
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 12.52s
                        Total time: 33681.66s
                               ETA: 1334393.0s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.543s, learning 0.209s)
               Value function loss: 2.8213
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: -1.83
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 12.75s
                        Total time: 33694.41s
                               ETA: 1334342.6s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.673s, learning 0.202s)
               Value function loss: 1.9075
                    Surrogate loss: 0.0520
             Mean action noise std: 0.72
                       Mean reward: -4.08
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 12.88s
                        Total time: 33707.29s
                               ETA: 1334297.1s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.674s, learning 0.220s)
               Value function loss: 1.8900
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: -0.24
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 12.89s
                        Total time: 33720.18s
                               ETA: 1334252.3s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.468s, learning 0.230s)
               Value function loss: 1.7275
                    Surrogate loss: 0.0076
             Mean action noise std: 0.72
                       Mean reward: -3.76
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 12.70s
                        Total time: 33732.88s
                               ETA: 1334199.8s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1263 steps/s (collection: 12.668s, learning 0.304s)
               Value function loss: 1.9410
                    Surrogate loss: 0.0060
             Mean action noise std: 0.72
                       Mean reward: -2.58
               Mean episode length: 124.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 12.97s
                        Total time: 33745.85s
                               ETA: 1334158.2s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.442s, learning 0.159s)
               Value function loss: 1.9407
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: -0.77
               Mean episode length: 124.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 12.60s
                        Total time: 33758.46s
                               ETA: 1334101.9s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1206 steps/s (collection: 13.416s, learning 0.158s)
               Value function loss: 16.1131
                    Surrogate loss: 0.0236
             Mean action noise std: 0.72
                       Mean reward: -3.03
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 5.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 13.57s
                        Total time: 33772.03s
                               ETA: 1334084.1s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.462s, learning 0.184s)
               Value function loss: 0.4142
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: -3.02
               Mean episode length: 125.00
                  Mean reward/step: -0.53
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 12.65s
                        Total time: 33784.68s
                               ETA: 1334029.7s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.367s, learning 0.241s)
               Value function loss: 0.4871
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: -0.66
               Mean episode length: 125.00
                  Mean reward/step: -0.33
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 12.61s
                        Total time: 33797.28s
                               ETA: 1333973.8s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.610s, learning 0.221s)
               Value function loss: 0.4763
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: -0.40
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 12.83s
                        Total time: 33810.12s
                               ETA: 1333926.7s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.610s, learning 0.201s)
               Value function loss: 0.8577
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: 0.46
               Mean episode length: 125.00
                  Mean reward/step: -0.02
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 12.81s
                        Total time: 33822.93s
                               ETA: 1333878.9s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.457s, learning 0.174s)
               Value function loss: 1.0815
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: -0.39
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 12.63s
                        Total time: 33835.56s
                               ETA: 1333824.0s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.535s, learning 0.185s)
               Value function loss: 0.9816
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: -2.40
               Mean episode length: 124.38
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 12.72s
                        Total time: 33848.28s
                               ETA: 1333772.6s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.202s, learning 0.261s)
               Value function loss: 1.3686
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: -4.07
               Mean episode length: 124.38
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 12.46s
                        Total time: 33860.74s
                               ETA: 1333711.2s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.780s, learning 0.164s)
               Value function loss: 2.0870
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: -2.88
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 12.94s
                        Total time: 33873.69s
                               ETA: 1333668.7s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.628s, learning 0.160s)
               Value function loss: 2.5958
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: -1.77
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 12.79s
                        Total time: 33886.47s
                               ETA: 1333620.1s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.431s, learning 0.162s)
               Value function loss: 1.9212
                    Surrogate loss: 0.0082
             Mean action noise std: 0.72
                       Mean reward: -4.06
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 12.59s
                        Total time: 33899.07s
                               ETA: 1333563.9s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.725s, learning 0.167s)
               Value function loss: 1.9566
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: -2.34
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 12.89s
                        Total time: 33911.96s
                               ETA: 1333519.4s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.581s, learning 0.165s)
               Value function loss: 1.9982
                    Surrogate loss: 0.0038
             Mean action noise std: 0.72
                       Mean reward: -3.32
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 12.75s
                        Total time: 33924.71s
                               ETA: 1333469.3s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.109s, learning 0.220s)
               Value function loss: 2.2759
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: -1.54
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 12.33s
                        Total time: 33937.03s
                               ETA: 1333402.7s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.301s, learning 0.160s)
               Value function loss: 2.9232
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: -6.96
               Mean episode length: 124.85
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 12.46s
                        Total time: 33949.49s
                               ETA: 1333341.4s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.326s, learning 0.159s)
               Value function loss: 2.1466
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: -2.17
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 12.49s
                        Total time: 33961.98s
                               ETA: 1333281.2s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1232 steps/s (collection: 13.107s, learning 0.182s)
               Value function loss: 13.7894
                    Surrogate loss: 0.0251
             Mean action noise std: 0.72
                       Mean reward: -3.10
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 5.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 13.29s
                        Total time: 33975.27s
                               ETA: 1333252.4s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.683s, learning 0.177s)
               Value function loss: 0.4566
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: -3.35
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 11.86s
                        Total time: 33987.13s
                               ETA: 1333167.7s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.340s, learning 0.163s)
               Value function loss: 0.5126
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: -3.38
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 12.50s
                        Total time: 33999.63s
                               ETA: 1333108.2s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.314s, learning 0.159s)
               Value function loss: 0.4931
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: -2.60
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 12.47s
                        Total time: 34012.11s
                               ETA: 1333047.6s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.531s, learning 0.163s)
               Value function loss: 1.1644
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: -2.03
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 12.69s
                        Total time: 34024.80s
                               ETA: 1332995.7s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.643s, learning 0.160s)
               Value function loss: 0.8757
                    Surrogate loss: 0.0032
             Mean action noise std: 0.72
                       Mean reward: -1.62
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 12.80s
                        Total time: 34037.60s
                               ETA: 1332948.1s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1249 steps/s (collection: 12.944s, learning 0.171s)
               Value function loss: 0.8484
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: -1.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 13.11s
                        Total time: 34050.72s
                               ETA: 1332912.7s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.256s, learning 0.163s)
               Value function loss: 1.2512
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: -0.94
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 12.42s
                        Total time: 34063.14s
                               ETA: 1332850.1s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.424s, learning 0.165s)
               Value function loss: 2.0684
                    Surrogate loss: 0.0068
             Mean action noise std: 0.72
                       Mean reward: -2.76
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 12.59s
                        Total time: 34075.73s
                               ETA: 1332794.2s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.772s, learning 0.167s)
               Value function loss: 2.2206
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -1.52
               Mean episode length: 124.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 12.94s
                        Total time: 34088.67s
                               ETA: 1332752.0s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.973s, learning 0.158s)
               Value function loss: 1.7448
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 1.80
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 12.13s
                        Total time: 34100.80s
                               ETA: 1332678.3s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.231s, learning 0.162s)
               Value function loss: 1.7626
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: -1.88
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 12.39s
                        Total time: 34113.19s
                               ETA: 1332614.8s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.396s, learning 0.209s)
               Value function loss: 1.8543
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 12.60s
                        Total time: 34125.79s
                               ETA: 1332559.6s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.685s, learning 0.168s)
               Value function loss: 1.8925
                    Surrogate loss: 0.0052
             Mean action noise std: 0.72
                       Mean reward: -1.82
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 12.85s
                        Total time: 34138.65s
                               ETA: 1332514.2s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.608s, learning 0.160s)
               Value function loss: 1.8150
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: -0.76
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 12.77s
                        Total time: 34151.41s
                               ETA: 1332465.5s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1222 steps/s (collection: 13.246s, learning 0.160s)
               Value function loss: 16.6491
                    Surrogate loss: 0.0266
             Mean action noise std: 0.72
                       Mean reward: -3.08
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 5.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 13.41s
                        Total time: 34164.82s
                               ETA: 1332441.7s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.439s, learning 0.168s)
               Value function loss: 0.4927
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: -2.12
               Mean episode length: 125.00
                  Mean reward/step: -0.56
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 12.61s
                        Total time: 34177.43s
                               ETA: 1332386.7s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.355s, learning 0.178s)
               Value function loss: 0.4855
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: -2.18
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 12.53s
                        Total time: 34189.96s
                               ETA: 1332328.9s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.637s, learning 0.186s)
               Value function loss: 0.4184
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: -1.94
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 12.82s
                        Total time: 34202.78s
                               ETA: 1332282.4s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.297s, learning 0.174s)
               Value function loss: 0.6512
                    Surrogate loss: 0.0173
             Mean action noise std: 0.72
                       Mean reward: -2.10
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 12.47s
                        Total time: 34215.25s
                               ETA: 1332222.3s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.587s, learning 0.163s)
               Value function loss: 0.9633
                    Surrogate loss: 0.0039
             Mean action noise std: 0.72
                       Mean reward: -2.93
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 12.75s
                        Total time: 34228.01s
                               ETA: 1332173.1s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.583s, learning 0.158s)
               Value function loss: 0.8448
                    Surrogate loss: 0.0133
             Mean action noise std: 0.72
                       Mean reward: -2.42
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 12.74s
                        Total time: 34240.75s
                               ETA: 1332123.5s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.267s, learning 0.158s)
               Value function loss: 1.0225
                    Surrogate loss: 0.0032
             Mean action noise std: 0.72
                       Mean reward: -2.16
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 12.43s
                        Total time: 34253.17s
                               ETA: 1332061.7s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.141s, learning 0.176s)
               Value function loss: 1.3493
                    Surrogate loss: 0.0126
             Mean action noise std: 0.72
                       Mean reward: -0.46
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 12.32s
                        Total time: 34265.49s
                               ETA: 1331995.7s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.922s, learning 0.170s)
               Value function loss: 2.3060
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: -2.64
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 13.09s
                        Total time: 34278.58s
                               ETA: 1331959.9s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.478s, learning 0.162s)
               Value function loss: 1.9616
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: -1.86
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 12.64s
                        Total time: 34291.22s
                               ETA: 1331906.5s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.490s, learning 0.170s)
               Value function loss: 1.5133
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: -2.74
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 12.66s
                        Total time: 34303.88s
                               ETA: 1331853.9s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.303s, learning 0.206s)
               Value function loss: 1.8612
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: -3.34
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 12.51s
                        Total time: 34316.39s
                               ETA: 1331795.5s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.196s, learning 0.179s)
               Value function loss: 2.0179
                    Surrogate loss: 0.0095
             Mean action noise std: 0.72
                       Mean reward: -1.71
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 12.38s
                        Total time: 34328.76s
                               ETA: 1331732.0s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.904s, learning 0.158s)
               Value function loss: 1.8171
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: -3.03
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 12.06s
                        Total time: 34340.83s
                               ETA: 1331656.4s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.354s, learning 0.167s)
               Value function loss: 1.8923
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: -2.48
               Mean episode length: 125.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 12.52s
                        Total time: 34353.35s
                               ETA: 1331598.6s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1235 steps/s (collection: 13.102s, learning 0.161s)
               Value function loss: 14.5793
                    Surrogate loss: 0.0451
             Mean action noise std: 0.72
                       Mean reward: -3.36
               Mean episode length: 124.73
                  Mean reward/step: -0.27
       Mean episode length/episode: 5.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 13.26s
                        Total time: 34366.61s
                               ETA: 1331569.6s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1252 steps/s (collection: 12.924s, learning 0.159s)
               Value function loss: 0.5318
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: -3.97
               Mean episode length: 124.73
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 13.08s
                        Total time: 34379.69s
                               ETA: 1331533.6s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.109s, learning 0.251s)
               Value function loss: 0.6014
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: -5.39
               Mean episode length: 124.73
                  Mean reward/step: -0.28
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 12.36s
                        Total time: 34392.05s
                               ETA: 1331469.7s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.512s, learning 0.274s)
               Value function loss: 0.5082
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: -4.68
               Mean episode length: 124.73
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 12.79s
                        Total time: 34404.84s
                               ETA: 1331422.2s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.210s, learning 0.162s)
               Value function loss: 0.9021
                    Surrogate loss: 0.0030
             Mean action noise std: 0.72
                       Mean reward: -4.48
               Mean episode length: 124.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 12.37s
                        Total time: 34417.21s
                               ETA: 1331358.8s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.926s, learning 0.192s)
               Value function loss: 1.0972
                    Surrogate loss: 0.0094
             Mean action noise std: 0.72
                       Mean reward: -3.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 12.12s
                        Total time: 34429.33s
                               ETA: 1331285.6s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.508s, learning 0.188s)
               Value function loss: 0.8959
                    Surrogate loss: 0.0156
             Mean action noise std: 0.72
                       Mean reward: -1.23
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 12.70s
                        Total time: 34442.03s
                               ETA: 1331234.8s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.529s, learning 0.220s)
               Value function loss: 1.3205
                    Surrogate loss: 0.0046
             Mean action noise std: 0.72
                       Mean reward: 2.29
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 12.75s
                        Total time: 34454.77s
                               ETA: 1331186.1s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.302s, learning 0.196s)
               Value function loss: 1.8163
                    Surrogate loss: 0.0058
             Mean action noise std: 0.72
                       Mean reward: 2.81
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 12.50s
                        Total time: 34467.27s
                               ETA: 1331127.7s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.772s, learning 0.266s)
               Value function loss: 2.6095
                    Surrogate loss: 0.0032
             Mean action noise std: 0.72
                       Mean reward: 2.83
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 13.04s
                        Total time: 34480.31s
                               ETA: 1331090.2s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.929s, learning 0.169s)
               Value function loss: 1.9875
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 2.91
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 12.10s
                        Total time: 34492.41s
                               ETA: 1331016.4s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.220s, learning 0.202s)
               Value function loss: 2.1203
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: -0.29
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 12.42s
                        Total time: 34504.83s
                               ETA: 1330955.3s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.412s, learning 0.172s)
               Value function loss: 1.9616
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 1.96
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 12.58s
                        Total time: 34517.41s
                               ETA: 1330900.3s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.618s, learning 0.162s)
               Value function loss: 2.2980
                    Surrogate loss: 0.0034
             Mean action noise std: 0.72
                       Mean reward: 1.00
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 12.78s
                        Total time: 34530.19s
                               ETA: 1330853.0s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.532s, learning 0.186s)
               Value function loss: 1.9851
                    Surrogate loss: 0.0076
             Mean action noise std: 0.72
                       Mean reward: 4.73
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 12.72s
                        Total time: 34542.91s
                               ETA: 1330803.2s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.784s, learning 0.205s)
               Value function loss: 2.2395
                    Surrogate loss: 0.0025
             Mean action noise std: 0.72
                       Mean reward: 3.95
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 12.99s
                        Total time: 34555.90s
                               ETA: 1330764.0s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1205 steps/s (collection: 13.424s, learning 0.165s)
               Value function loss: 17.4941
                    Surrogate loss: 0.0135
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 5.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 13.59s
                        Total time: 34569.49s
                               ETA: 1330747.9s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.533s, learning 0.208s)
               Value function loss: 0.4373
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 3.34
               Mean episode length: 125.00
                  Mean reward/step: -0.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 12.74s
                        Total time: 34582.23s
                               ETA: 1330699.2s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.289s, learning 0.162s)
               Value function loss: 0.5692
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 125.00
                  Mean reward/step: -0.22
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 12.45s
                        Total time: 34594.68s
                               ETA: 1330639.3s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.130s, learning 0.158s)
               Value function loss: 0.5148
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 124.62
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 12.29s
                        Total time: 34606.97s
                               ETA: 1330573.1s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.465s, learning 0.187s)
               Value function loss: 1.5637
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 124.62
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 12.65s
                        Total time: 34619.62s
                               ETA: 1330521.1s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.455s, learning 0.161s)
               Value function loss: 1.1397
                    Surrogate loss: 0.0028
             Mean action noise std: 0.72
                       Mean reward: 1.82
               Mean episode length: 124.37
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 12.62s
                        Total time: 34632.24s
                               ETA: 1330467.7s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.739s, learning 0.157s)
               Value function loss: 0.9682
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 124.37
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 12.90s
                        Total time: 34645.13s
                               ETA: 1330425.0s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.379s, learning 0.193s)
               Value function loss: 1.5280
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 2.53
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 12.57s
                        Total time: 34657.71s
                               ETA: 1330370.0s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.780s, learning 0.202s)
               Value function loss: 2.7765
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 4.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 12.98s
                        Total time: 34670.69s
                               ETA: 1330330.7s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.035s, learning 0.166s)
               Value function loss: 2.7952
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 3.65
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 12.20s
                        Total time: 34682.89s
                               ETA: 1330261.4s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.087s, learning 0.165s)
               Value function loss: 1.8928
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 2.68
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 12.25s
                        Total time: 34695.14s
                               ETA: 1330194.3s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.277s, learning 0.194s)
               Value function loss: 2.9649
                    Surrogate loss: 0.0100
             Mean action noise std: 0.72
                       Mean reward: 0.75
               Mean episode length: 124.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 12.47s
                        Total time: 34707.61s
                               ETA: 1330135.5s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.396s, learning 0.165s)
               Value function loss: 2.2705
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 124.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 12.56s
                        Total time: 34720.17s
                               ETA: 1330080.2s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.310s, learning 0.198s)
               Value function loss: 2.2098
                    Surrogate loss: 0.0151
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 12.51s
                        Total time: 34732.68s
                               ETA: 1330022.9s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.137s, learning 0.239s)
               Value function loss: 1.9926
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 12.38s
                        Total time: 34745.06s
                               ETA: 1329960.6s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1230 steps/s (collection: 13.149s, learning 0.164s)
               Value function loss: 20.6829
                    Surrogate loss: 0.0167
             Mean action noise std: 0.72
                       Mean reward: 4.02
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 5.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 13.31s
                        Total time: 34758.37s
                               ETA: 1329934.1s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.728s, learning 0.286s)
               Value function loss: 0.7429
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 4.66
               Mean episode length: 125.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 13.01s
                        Total time: 34771.39s
                               ETA: 1329896.3s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.214s, learning 0.171s)
               Value function loss: 0.4922
                    Surrogate loss: 0.0076
             Mean action noise std: 0.72
                       Mean reward: 5.52
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 12.39s
                        Total time: 34783.77s
                               ETA: 1329834.4s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.464s, learning 0.180s)
               Value function loss: 0.4584
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 5.97
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 12.64s
                        Total time: 34796.41s
                               ETA: 1329782.5s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.347s, learning 0.385s)
               Value function loss: 0.7149
                    Surrogate loss: 0.0058
             Mean action noise std: 0.72
                       Mean reward: 5.45
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 12.73s
                        Total time: 34809.15s
                               ETA: 1329734.0s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.785s, learning 0.198s)
               Value function loss: 1.1145
                    Surrogate loss: 0.0060
             Mean action noise std: 0.72
                       Mean reward: 4.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 12.98s
                        Total time: 34822.13s
                               ETA: 1329695.0s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.262s, learning 0.235s)
               Value function loss: 0.8762
                    Surrogate loss: 0.0206
             Mean action noise std: 0.72
                       Mean reward: 3.95
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 12.50s
                        Total time: 34834.63s
                               ETA: 1329637.6s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.255s, learning 0.157s)
               Value function loss: 1.0305
                    Surrogate loss: 0.0121
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 12.41s
                        Total time: 34847.04s
                               ETA: 1329576.9s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.417s, learning 0.163s)
               Value function loss: 1.5654
                    Surrogate loss: 0.0449
             Mean action noise std: 0.72
                       Mean reward: 2.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 12.58s
                        Total time: 34859.62s
                               ETA: 1329522.7s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1246 steps/s (collection: 12.945s, learning 0.198s)
               Value function loss: 2.6899
                    Surrogate loss: -0.0017
             Mean action noise std: 0.72
                       Mean reward: 2.16
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 13.14s
                        Total time: 34872.76s
                               ETA: 1329489.9s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.336s, learning 0.251s)
               Value function loss: 2.0445
                    Surrogate loss: 0.0037
             Mean action noise std: 0.72
                       Mean reward: 2.44
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 12.59s
                        Total time: 34885.35s
                               ETA: 1329436.0s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.828s, learning 0.167s)
               Value function loss: 1.9444
                    Surrogate loss: 0.0097
             Mean action noise std: 0.72
                       Mean reward: 2.38
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 11.99s
                        Total time: 34897.34s
                               ETA: 1329359.6s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1235 steps/s (collection: 13.090s, learning 0.176s)
               Value function loss: 2.0408
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: 5.61
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 13.27s
                        Total time: 34910.61s
                               ETA: 1329331.6s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.005s, learning 0.158s)
               Value function loss: 2.4711
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 12.16s
                        Total time: 34922.77s
                               ETA: 1329261.7s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.809s, learning 0.168s)
               Value function loss: 2.2484
                    Surrogate loss: 0.0027
             Mean action noise std: 0.72
                       Mean reward: 4.36
               Mean episode length: 124.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 12.98s
                        Total time: 34935.75s
                               ETA: 1329222.8s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.345s, learning 0.207s)
               Value function loss: 2.1847
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 7.32
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 12.55s
                        Total time: 34948.30s
                               ETA: 1329167.7s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.866s, learning 0.162s)
               Value function loss: 16.6034
                    Surrogate loss: 0.0302
             Mean action noise std: 0.72
                       Mean reward: 5.34
               Mean episode length: 125.00
                  Mean reward/step: -0.31
       Mean episode length/episode: 5.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 13.03s
                        Total time: 34961.33s
                               ETA: 1329130.8s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.812s, learning 0.201s)
               Value function loss: 0.8992
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 5.98
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 13.01s
                        Total time: 34974.34s
                               ETA: 1329093.3s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.592s, learning 0.165s)
               Value function loss: 0.8810
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 6.40
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 12.76s
                        Total time: 34987.10s
                               ETA: 1329046.0s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.635s, learning 0.219s)
               Value function loss: 0.5948
                    Surrogate loss: 0.0043
             Mean action noise std: 0.72
                       Mean reward: 7.00
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 12.85s
                        Total time: 34999.95s
                               ETA: 1329002.5s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.909s, learning 0.163s)
               Value function loss: 1.0854
                    Surrogate loss: -0.0064
             Mean action noise std: 0.72
                       Mean reward: 8.15
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 12.07s
                        Total time: 35012.03s
                               ETA: 1328929.4s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.861s, learning 0.160s)
               Value function loss: 1.1728
                    Surrogate loss: 0.0021
             Mean action noise std: 0.72
                       Mean reward: 8.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 13.02s
                        Total time: 35025.05s
                               ETA: 1328892.3s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.594s, learning 0.159s)
               Value function loss: 1.2848
                    Surrogate loss: 0.0560
             Mean action noise std: 0.72
                       Mean reward: 7.98
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 12.75s
                        Total time: 35037.80s
                               ETA: 1328845.0s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.985s, learning 0.170s)
               Value function loss: 1.3584
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: 9.10
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 12.15s
                        Total time: 35049.95s
                               ETA: 1328775.1s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.395s, learning 0.179s)
               Value function loss: 2.2945
                    Surrogate loss: 0.0061
             Mean action noise std: 0.72
                       Mean reward: 7.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 12.57s
                        Total time: 35062.53s
                               ETA: 1328721.2s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.450s, learning 0.178s)
               Value function loss: 2.6638
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: 5.59
               Mean episode length: 124.75
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 12.63s
                        Total time: 35075.16s
                               ETA: 1328669.3s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.296s, learning 0.220s)
               Value function loss: 2.0158
                    Surrogate loss: 0.0022
             Mean action noise std: 0.72
                       Mean reward: 3.06
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 12.52s
                        Total time: 35087.67s
                               ETA: 1328613.2s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.806s, learning 0.212s)
               Value function loss: 2.0692
                    Surrogate loss: 0.0096
             Mean action noise std: 0.72
                       Mean reward: 5.93
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 13.02s
                        Total time: 35100.69s
                               ETA: 1328576.1s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.232s, learning 0.186s)
               Value function loss: 2.1781
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 6.74
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 12.42s
                        Total time: 35113.11s
                               ETA: 1328516.4s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1258 steps/s (collection: 12.860s, learning 0.163s)
               Value function loss: 2.5951
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 5.94
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 13.02s
                        Total time: 35126.13s
                               ETA: 1328479.5s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.229s, learning 0.163s)
               Value function loss: 2.2353
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 4.14
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 12.39s
                        Total time: 35138.52s
                               ETA: 1328418.8s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.442s, learning 0.160s)
               Value function loss: 4.1718
                    Surrogate loss: 0.0195
             Mean action noise std: 0.72
                       Mean reward: 5.13
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 12.60s
                        Total time: 35151.12s
                               ETA: 1328366.2s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1217 steps/s (collection: 13.164s, learning 0.291s)
               Value function loss: 13.4817
                    Surrogate loss: 0.0148
             Mean action noise std: 0.72
                       Mean reward: 5.55
               Mean episode length: 125.00
                  Mean reward/step: -0.54
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 13.45s
                        Total time: 35164.58s
                               ETA: 1328345.7s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1275 steps/s (collection: 12.684s, learning 0.165s)
               Value function loss: 0.4337
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 5.49
               Mean episode length: 125.00
                  Mean reward/step: -0.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 12.85s
                        Total time: 35177.43s
                               ETA: 1328302.4s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.457s, learning 0.163s)
               Value function loss: 0.5361
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 5.65
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 12.62s
                        Total time: 35190.05s
                               ETA: 1328250.4s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.406s, learning 0.157s)
               Value function loss: 0.5432
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 3.74
               Mean episode length: 125.00
                  Mean reward/step: -0.06
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 12.56s
                        Total time: 35202.61s
                               ETA: 1328196.4s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.040s, learning 0.181s)
               Value function loss: 1.4232
                    Surrogate loss: 0.0132
             Mean action noise std: 0.72
                       Mean reward: 1.93
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 12.22s
                        Total time: 35214.83s
                               ETA: 1328129.5s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.653s, learning 0.218s)
               Value function loss: 1.0350
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 2.33
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 12.87s
                        Total time: 35227.70s
                               ETA: 1328087.1s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1289 steps/s (collection: 12.546s, learning 0.162s)
               Value function loss: 1.2480
                    Surrogate loss: 0.0057
             Mean action noise std: 0.72
                       Mean reward: 2.35
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 12.71s
                        Total time: 35240.41s
                               ETA: 1328038.6s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.577s, learning 0.260s)
               Value function loss: 2.1034
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 6.52
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 12.84s
                        Total time: 35253.25s
                               ETA: 1327995.0s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.574s, learning 0.214s)
               Value function loss: 2.8781
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 6.91
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 12.79s
                        Total time: 35266.03s
                               ETA: 1327949.6s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.219s, learning 0.230s)
               Value function loss: 2.4622
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 7.93
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 12.45s
                        Total time: 35278.48s
                               ETA: 1327891.4s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.244s, learning 0.216s)
               Value function loss: 2.0687
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 9.66
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 12.46s
                        Total time: 35290.94s
                               ETA: 1327833.7s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1234 steps/s (collection: 13.048s, learning 0.220s)
               Value function loss: 2.7842
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 5.27
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 13.27s
                        Total time: 35304.21s
                               ETA: 1327806.4s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.435s, learning 0.296s)
               Value function loss: 2.9024
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 8.19
               Mean episode length: 124.76
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 12.73s
                        Total time: 35316.94s
                               ETA: 1327759.0s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1297 steps/s (collection: 12.435s, learning 0.192s)
               Value function loss: 2.4228
                    Surrogate loss: 0.0045
             Mean action noise std: 0.72
                       Mean reward: 8.62
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 12.63s
                        Total time: 35329.57s
                               ETA: 1327707.6s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.582s, learning 0.163s)
               Value function loss: 2.3030
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 4.12
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 12.74s
                        Total time: 35342.31s
                               ETA: 1327660.7s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1232 steps/s (collection: 13.139s, learning 0.157s)
               Value function loss: 17.9299
                    Surrogate loss: 0.0248
             Mean action noise std: 0.72
                       Mean reward: 6.50
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 5.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 13.30s
                        Total time: 35355.61s
                               ETA: 1327634.5s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1228 steps/s (collection: 13.155s, learning 0.186s)
               Value function loss: 0.5238
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 5.83
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 13.34s
                        Total time: 35368.95s
                               ETA: 1327610.0s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.295s, learning 0.167s)
               Value function loss: 0.5088
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 4.67
               Mean episode length: 125.00
                  Mean reward/step: -0.30
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 12.46s
                        Total time: 35381.41s
                               ETA: 1327552.6s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.247s, learning 0.171s)
               Value function loss: 0.4450
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 12.42s
                        Total time: 35393.83s
                               ETA: 1327493.6s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.634s, learning 0.161s)
               Value function loss: 1.0189
                    Surrogate loss: 0.0062
             Mean action noise std: 0.72
                       Mean reward: 3.82
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 12.80s
                        Total time: 35406.63s
                               ETA: 1327448.7s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.212s, learning 0.176s)
               Value function loss: 1.3074
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 12.39s
                        Total time: 35419.02s
                               ETA: 1327388.6s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.541s, learning 0.203s)
               Value function loss: 1.0524
                    Surrogate loss: 0.0119
             Mean action noise std: 0.72
                       Mean reward: 2.34
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 12.74s
                        Total time: 35431.76s
                               ETA: 1327341.8s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.313s, learning 0.157s)
               Value function loss: 1.2919
                    Surrogate loss: 0.0024
             Mean action noise std: 0.72
                       Mean reward: 1.03
               Mean episode length: 124.99
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 12.47s
                        Total time: 35444.23s
                               ETA: 1327284.8s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.323s, learning 0.165s)
               Value function loss: 2.0164
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 0.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 12.49s
                        Total time: 35456.72s
                               ETA: 1327228.5s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.296s, learning 0.161s)
               Value function loss: 2.8503
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: -0.88
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 12.46s
                        Total time: 35469.17s
                               ETA: 1327171.2s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.355s, learning 0.256s)
               Value function loss: 2.1223
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: 0.84
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 12.61s
                        Total time: 35481.78s
                               ETA: 1327119.5s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.950s, learning 0.156s)
               Value function loss: 2.3920
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 0.94
               Mean episode length: 125.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 13.11s
                        Total time: 35494.89s
                               ETA: 1327086.5s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1278 steps/s (collection: 12.661s, learning 0.158s)
               Value function loss: 3.5366
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: -0.74
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 12.82s
                        Total time: 35507.71s
                               ETA: 1327042.7s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.234s, learning 0.156s)
               Value function loss: 3.7648
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: -1.44
               Mean episode length: 124.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 12.39s
                        Total time: 35520.10s
                               ETA: 1326982.9s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.233s, learning 0.159s)
               Value function loss: 2.0992
                    Surrogate loss: 0.0749
             Mean action noise std: 0.72
                       Mean reward: 0.37
               Mean episode length: 125.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 12.39s
                        Total time: 35532.49s
                               ETA: 1326923.2s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.269s, learning 0.158s)
               Value function loss: 1.9406
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 2.06
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 12.43s
                        Total time: 35544.92s
                               ETA: 1326864.9s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.877s, learning 0.163s)
               Value function loss: 14.8733
                    Surrogate loss: 0.0377
             Mean action noise std: 0.72
                       Mean reward: 4.30
               Mean episode length: 125.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 5.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 13.04s
                        Total time: 35557.96s
                               ETA: 1326829.5s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.936s, learning 0.165s)
               Value function loss: 0.4462
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 3.06
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 13.10s
                        Total time: 35571.06s
                               ETA: 1326796.4s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.330s, learning 0.213s)
               Value function loss: 0.6044
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 12.54s
                        Total time: 35583.60s
                               ETA: 1326742.5s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.789s, learning 0.161s)
               Value function loss: 0.5608
                    Surrogate loss: 0.0108
             Mean action noise std: 0.72
                       Mean reward: 1.87
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 11.95s
                        Total time: 35595.55s
                               ETA: 1326666.5s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.481s, learning 0.206s)
               Value function loss: 1.4868
                    Surrogate loss: 0.0135
             Mean action noise std: 0.72
                       Mean reward: -0.86
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 12.69s
                        Total time: 35608.24s
                               ETA: 1326618.0s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.850s, learning 0.158s)
               Value function loss: 1.6160
                    Surrogate loss: 0.0089
             Mean action noise std: 0.72
                       Mean reward: -2.98
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 13.01s
                        Total time: 35621.25s
                               ETA: 1326581.5s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.532s, learning 0.193s)
               Value function loss: 1.1914
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: -3.01
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 12.73s
                        Total time: 35633.97s
                               ETA: 1326534.5s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.291s, learning 0.201s)
               Value function loss: 1.5927
                    Surrogate loss: 0.0049
             Mean action noise std: 0.72
                       Mean reward: -4.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 12.49s
                        Total time: 35646.46s
                               ETA: 1326478.9s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.227s, learning 0.202s)
               Value function loss: 2.7959
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: -1.53
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 12.43s
                        Total time: 35658.89s
                               ETA: 1326420.9s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.695s, learning 0.181s)
               Value function loss: 3.2730
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 12.88s
                        Total time: 35671.77s
                               ETA: 1326379.6s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.454s, learning 0.164s)
               Value function loss: 2.5301
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 12.62s
                        Total time: 35684.39s
                               ETA: 1326328.7s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.165s, learning 0.163s)
               Value function loss: 2.8723
                    Surrogate loss: 0.0039
             Mean action noise std: 0.72
                       Mean reward: 4.29
               Mean episode length: 124.63
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 12.33s
                        Total time: 35696.71s
                               ETA: 1326267.1s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.363s, learning 0.159s)
               Value function loss: 2.8023
                    Surrogate loss: 0.0028
             Mean action noise std: 0.72
                       Mean reward: 5.34
               Mean episode length: 125.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 12.52s
                        Total time: 35709.24s
                               ETA: 1326212.7s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.743s, learning 0.288s)
               Value function loss: 2.7978
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 13.03s
                        Total time: 35722.27s
                               ETA: 1326177.3s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.337s, learning 0.189s)
               Value function loss: 2.6645
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 2.22
               Mean episode length: 124.92
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 12.53s
                        Total time: 35734.79s
                               ETA: 1326123.1s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1242 steps/s (collection: 13.010s, learning 0.172s)
               Value function loss: 22.4460
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: 4.62
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 5.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 13.18s
                        Total time: 35747.97s
                               ETA: 1326093.2s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.310s, learning 0.167s)
               Value function loss: 1.6847
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: 1.13
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 12.48s
                        Total time: 35760.45s
                               ETA: 1326037.3s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.283s, learning 0.207s)
               Value function loss: 0.7815
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: 1.06
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 12.49s
                        Total time: 35772.94s
                               ETA: 1325981.9s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.501s, learning 0.227s)
               Value function loss: 0.6678
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 0.60
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 12.73s
                        Total time: 35785.67s
                               ETA: 1325935.3s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.345s, learning 0.158s)
               Value function loss: 0.7337
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 1.79
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 12.50s
                        Total time: 35798.17s
                               ETA: 1325880.5s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.844s, learning 0.167s)
               Value function loss: 1.2979
                    Surrogate loss: 0.0102
             Mean action noise std: 0.72
                       Mean reward: 6.09
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 13.01s
                        Total time: 35811.18s
                               ETA: 1325844.4s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.076s, learning 0.162s)
               Value function loss: 1.0303
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 10.37
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 12.24s
                        Total time: 35823.42s
                               ETA: 1325779.8s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.806s, learning 0.207s)
               Value function loss: 1.4170
                    Surrogate loss: 0.0208
             Mean action noise std: 0.72
                       Mean reward: 12.79
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 12.01s
                        Total time: 35835.44s
                               ETA: 1325706.9s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.878s, learning 0.182s)
               Value function loss: 1.9369
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 12.60
               Mean episode length: 124.34
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 12.06s
                        Total time: 35847.50s
                               ETA: 1325635.8s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.560s, learning 0.229s)
               Value function loss: 3.6415
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: 14.97
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 12.79s
                        Total time: 35860.29s
                               ETA: 1325591.6s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.575s, learning 0.163s)
               Value function loss: 3.1059
                    Surrogate loss: 0.0055
             Mean action noise std: 0.72
                       Mean reward: 13.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 12.74s
                        Total time: 35873.02s
                               ETA: 1325545.7s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.018s, learning 0.203s)
               Value function loss: 2.3916
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: 15.31
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 12.22s
                        Total time: 35885.24s
                               ETA: 1325480.6s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.171s, learning 0.159s)
               Value function loss: 2.5751
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 12.11
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 12.33s
                        Total time: 35897.57s
                               ETA: 1325419.6s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.592s, learning 0.164s)
               Value function loss: 2.6134
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 11.10
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 12.76s
                        Total time: 35910.33s
                               ETA: 1325374.3s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.276s, learning 0.162s)
               Value function loss: 2.8519
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 13.19
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 12.44s
                        Total time: 35922.77s
                               ETA: 1325317.4s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.165s, learning 0.160s)
               Value function loss: 2.9203
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: 16.36
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 12.32s
                        Total time: 35935.09s
                               ETA: 1325256.3s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1262 steps/s (collection: 12.789s, learning 0.183s)
               Value function loss: 18.3208
                    Surrogate loss: 0.0439
             Mean action noise std: 0.72
                       Mean reward: 15.26
               Mean episode length: 125.00
                  Mean reward/step: -0.20
       Mean episode length/episode: 5.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 12.97s
                        Total time: 35948.07s
                               ETA: 1325219.1s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.577s, learning 0.199s)
               Value function loss: 0.4928
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 15.38
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 12.78s
                        Total time: 35960.84s
                               ETA: 1325174.7s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.585s, learning 0.156s)
               Value function loss: 0.6399
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 12.97
               Mean episode length: 124.47
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 12.74s
                        Total time: 35973.58s
                               ETA: 1325129.0s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.675s, learning 0.158s)
               Value function loss: 0.5695
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 13.03
               Mean episode length: 124.47
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 12.83s
                        Total time: 35986.41s
                               ETA: 1325086.7s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.324s, learning 0.160s)
               Value function loss: 1.2824
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 11.98
               Mean episode length: 124.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 12.48s
                        Total time: 35998.90s
                               ETA: 1325031.7s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.152s, learning 0.167s)
               Value function loss: 1.0998
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: 10.17
               Mean episode length: 124.47
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 12.32s
                        Total time: 36011.22s
                               ETA: 1324970.6s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.287s, learning 0.158s)
               Value function loss: 1.1584
                    Surrogate loss: 0.0032
             Mean action noise std: 0.72
                       Mean reward: 11.38
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 12.44s
                        Total time: 36023.66s
                               ETA: 1324914.1s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.296s, learning 0.186s)
               Value function loss: 1.4884
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 8.18
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 12.48s
                        Total time: 36036.14s
                               ETA: 1324859.1s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1256 steps/s (collection: 12.866s, learning 0.178s)
               Value function loss: 2.3688
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 5.57
               Mean episode length: 124.67
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 13.04s
                        Total time: 36049.19s
                               ETA: 1324824.7s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.549s, learning 0.259s)
               Value function loss: 2.7776
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 6.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 12.81s
                        Total time: 36062.00s
                               ETA: 1324781.7s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.489s, learning 0.161s)
               Value function loss: 2.2597
                    Surrogate loss: 0.0057
             Mean action noise std: 0.72
                       Mean reward: 9.83
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 12.65s
                        Total time: 36074.65s
                               ETA: 1324732.9s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.445s, learning 0.200s)
               Value function loss: 2.2111
                    Surrogate loss: 0.0118
             Mean action noise std: 0.72
                       Mean reward: 9.40
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 12.65s
                        Total time: 36087.29s
                               ETA: 1324684.0s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1272 steps/s (collection: 12.714s, learning 0.160s)
               Value function loss: 2.1773
                    Surrogate loss: 0.0187
             Mean action noise std: 0.72
                       Mean reward: 9.59
               Mean episode length: 124.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 12.87s
                        Total time: 36100.17s
                               ETA: 1324643.5s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.725s, learning 0.163s)
               Value function loss: 2.8197
                    Surrogate loss: 0.0030
             Mean action noise std: 0.72
                       Mean reward: 7.95
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 12.89s
                        Total time: 36113.06s
                               ETA: 1324603.5s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.420s, learning 0.199s)
               Value function loss: 2.4285
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 6.75
               Mean episode length: 124.74
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 12.62s
                        Total time: 36125.68s
                               ETA: 1324553.7s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.809s, learning 0.164s)
               Value function loss: 2.6143
                    Surrogate loss: 0.0098
             Mean action noise std: 0.72
                       Mean reward: 9.65
               Mean episode length: 124.92
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 11.97s
                        Total time: 36137.65s
                               ETA: 1324480.2s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1217 steps/s (collection: 13.249s, learning 0.208s)
               Value function loss: 18.2157
                    Surrogate loss: 0.0272
             Mean action noise std: 0.72
                       Mean reward: 8.32
               Mean episode length: 125.00
                  Mean reward/step: -0.44
       Mean episode length/episode: 5.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 13.46s
                        Total time: 36151.11s
                               ETA: 1324461.1s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.608s, learning 0.190s)
               Value function loss: 0.5946
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 9.72
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 12.80s
                        Total time: 36163.90s
                               ETA: 1324417.9s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.340s, learning 0.164s)
               Value function loss: 0.6966
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 9.15
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 12.50s
                        Total time: 36176.41s
                               ETA: 1324364.0s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.328s, learning 0.223s)
               Value function loss: 0.8121
                    Surrogate loss: 0.0106
             Mean action noise std: 0.72
                       Mean reward: 8.65
               Mean episode length: 124.68
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 12.55s
                        Total time: 36188.96s
                               ETA: 1324311.8s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.573s, learning 0.185s)
               Value function loss: 1.8030
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 7.76
               Mean episode length: 124.68
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 12.76s
                        Total time: 36201.72s
                               ETA: 1324267.2s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.179s, learning 0.162s)
               Value function loss: 1.1589
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 7.42
               Mean episode length: 124.42
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 12.34s
                        Total time: 36214.06s
                               ETA: 1324207.4s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.961s, learning 0.225s)
               Value function loss: 1.2763
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: 8.63
               Mean episode length: 124.58
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 12.19s
                        Total time: 36226.24s
                               ETA: 1324142.0s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.663s, learning 0.193s)
               Value function loss: 1.7534
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 9.21
               Mean episode length: 124.84
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 12.86s
                        Total time: 36239.10s
                               ETA: 1324101.0s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.868s, learning 0.160s)
               Value function loss: 3.4187
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 6.77
               Mean episode length: 124.66
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 12.03s
                        Total time: 36251.13s
                               ETA: 1324029.9s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.678s, learning 0.161s)
               Value function loss: 4.5171
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 8.71
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 12.84s
                        Total time: 36263.97s
                               ETA: 1323988.4s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.560s, learning 0.181s)
               Value function loss: 3.0892
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 10.96
               Mean episode length: 124.80
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 12.74s
                        Total time: 36276.71s
                               ETA: 1323943.4s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.700s, learning 0.191s)
               Value function loss: 3.4145
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 8.84
               Mean episode length: 124.77
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 12.89s
                        Total time: 36289.60s
                               ETA: 1323903.9s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.448s, learning 0.166s)
               Value function loss: 2.8084
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 9.38
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 12.61s
                        Total time: 36302.21s
                               ETA: 1323854.2s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.733s, learning 0.161s)
               Value function loss: 2.9375
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 10.00
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 11.89s
                        Total time: 36314.11s
                               ETA: 1323778.4s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.968s, learning 0.159s)
               Value function loss: 2.6878
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 10.14
               Mean episode length: 124.94
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 12.13s
                        Total time: 36326.23s
                               ETA: 1323711.1s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1243 steps/s (collection: 12.986s, learning 0.192s)
               Value function loss: 19.3178
                    Surrogate loss: 0.0367
             Mean action noise std: 0.72
                       Mean reward: 10.50
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 5.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 13.18s
                        Total time: 36339.41s
                               ETA: 1323682.2s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.593s, learning 0.176s)
               Value function loss: 0.5370
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 9.96
               Mean episode length: 125.00
                  Mean reward/step: -0.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 12.77s
                        Total time: 36352.18s
                               ETA: 1323638.3s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.334s, learning 0.160s)
               Value function loss: 0.6272
                    Surrogate loss: 0.0042
             Mean action noise std: 0.72
                       Mean reward: 10.59
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 12.49s
                        Total time: 36364.68s
                               ETA: 1323584.4s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.274s, learning 0.158s)
               Value function loss: 0.5788
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 11.08
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 12.43s
                        Total time: 36377.11s
                               ETA: 1323528.4s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.157s, learning 0.186s)
               Value function loss: 1.1292
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: 9.92
               Mean episode length: 125.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 12.34s
                        Total time: 36389.45s
                               ETA: 1323469.1s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.198s, learning 0.159s)
               Value function loss: 1.2892
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 8.90
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 12.36s
                        Total time: 36401.81s
                               ETA: 1323410.4s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.398s, learning 0.167s)
               Value function loss: 0.9993
                    Surrogate loss: 0.0131
             Mean action noise std: 0.72
                       Mean reward: 8.51
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 12.57s
                        Total time: 36414.37s
                               ETA: 1323359.3s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.289s, learning 0.232s)
               Value function loss: 1.2253
                    Surrogate loss: 0.0130
             Mean action noise std: 0.72
                       Mean reward: 9.29
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 12.52s
                        Total time: 36426.90s
                               ETA: 1323306.6s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.091s, learning 0.164s)
               Value function loss: 2.1239
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 11.73
               Mean episode length: 124.59
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 12.25s
                        Total time: 36439.15s
                               ETA: 1323244.2s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.566s, learning 0.165s)
               Value function loss: 3.2869
                    Surrogate loss: 0.0065
             Mean action noise std: 0.72
                       Mean reward: 10.78
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 12.73s
                        Total time: 36451.88s
                               ETA: 1323199.2s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.085s, learning 0.166s)
               Value function loss: 2.3931
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 10.10
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 12.25s
                        Total time: 36464.13s
                               ETA: 1323136.8s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.096s, learning 0.160s)
               Value function loss: 2.5112
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 12.34
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 12.26s
                        Total time: 36476.39s
                               ETA: 1323074.6s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.133s, learning 0.185s)
               Value function loss: 2.2815
                    Surrogate loss: 0.0037
             Mean action noise std: 0.72
                       Mean reward: 7.93
               Mean episode length: 124.66
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 12.32s
                        Total time: 36488.71s
                               ETA: 1323014.7s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.587s, learning 0.172s)
               Value function loss: 2.6942
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 10.28
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 11.76s
                        Total time: 36500.46s
                               ETA: 1322934.5s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.329s, learning 0.158s)
               Value function loss: 2.9281
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 7.28
               Mean episode length: 124.87
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 12.49s
                        Total time: 36512.95s
                               ETA: 1322880.8s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.237s, learning 0.180s)
               Value function loss: 2.7749
                    Surrogate loss: 0.0046
             Mean action noise std: 0.72
                       Mean reward: 12.49
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 12.42s
                        Total time: 36525.37s
                               ETA: 1322824.6s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.944s, learning 0.159s)
               Value function loss: 17.7937
                    Surrogate loss: 0.0408
             Mean action noise std: 0.72
                       Mean reward: 9.83
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 5.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 13.10s
                        Total time: 36538.47s
                               ETA: 1322793.3s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.208s, learning 0.204s)
               Value function loss: 0.6443
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 10.41
               Mean episode length: 125.00
                  Mean reward/step: -0.41
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 12.41s
                        Total time: 36550.88s
                               ETA: 1322736.9s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.962s, learning 0.158s)
               Value function loss: 0.8492
                    Surrogate loss: 0.0005
             Mean action noise std: 0.72
                       Mean reward: 11.52
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 12.12s
                        Total time: 36563.00s
                               ETA: 1322670.1s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.195s, learning 0.186s)
               Value function loss: 0.7263
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 11.13
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 12.38s
                        Total time: 36575.38s
                               ETA: 1322612.7s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.380s, learning 0.159s)
               Value function loss: 1.4946
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 11.67
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 12.54s
                        Total time: 36587.92s
                               ETA: 1322561.0s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.515s, learning 0.180s)
               Value function loss: 1.4036
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 9.22
               Mean episode length: 124.92
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 12.70s
                        Total time: 36600.62s
                               ETA: 1322515.1s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.267s, learning 0.301s)
               Value function loss: 1.1610
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 9.37
               Mean episode length: 124.92
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 12.57s
                        Total time: 36613.19s
                               ETA: 1322464.5s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.222s, learning 0.157s)
               Value function loss: 1.3990
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 9.99
               Mean episode length: 124.92
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 12.38s
                        Total time: 36625.57s
                               ETA: 1322407.2s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.432s, learning 0.165s)
               Value function loss: 2.6786
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 10.39
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 12.60s
                        Total time: 36638.16s
                               ETA: 1322357.7s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.423s, learning 0.157s)
               Value function loss: 3.1116
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 10.98
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 12.58s
                        Total time: 36650.74s
                               ETA: 1322307.7s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.468s, learning 0.188s)
               Value function loss: 2.3644
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 9.54
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 12.66s
                        Total time: 36663.40s
                               ETA: 1322260.5s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.214s, learning 0.159s)
               Value function loss: 2.4915
                    Surrogate loss: 0.0034
             Mean action noise std: 0.72
                       Mean reward: 11.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 12.37s
                        Total time: 36675.77s
                               ETA: 1322203.0s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.557s, learning 0.158s)
               Value function loss: 2.4125
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 9.48
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 12.72s
                        Total time: 36688.49s
                               ETA: 1322158.0s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.463s, learning 0.156s)
               Value function loss: 3.1140
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 12.03
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 12.62s
                        Total time: 36701.11s
                               ETA: 1322109.5s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1285 steps/s (collection: 12.589s, learning 0.156s)
               Value function loss: 2.6931
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: 11.41
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 12.74s
                        Total time: 36713.85s
                               ETA: 1322065.5s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.383s, learning 0.157s)
               Value function loss: 8.7371
                    Surrogate loss: 0.0529
             Mean action noise std: 0.72
                       Mean reward: 11.00
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 12.54s
                        Total time: 36726.39s
                               ETA: 1322014.3s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.803s, learning 0.183s)
               Value function loss: 16.7732
                    Surrogate loss: 0.0108
             Mean action noise std: 0.72
                       Mean reward: 10.89
               Mean episode length: 125.00
                  Mean reward/step: -0.52
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 12.99s
                        Total time: 36739.38s
                               ETA: 1321979.1s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.421s, learning 0.168s)
               Value function loss: 0.5760
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 9.10
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 12.59s
                        Total time: 36751.97s
                               ETA: 1321929.6s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.622s, learning 0.176s)
               Value function loss: 0.6787
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 9.31
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 12.80s
                        Total time: 36764.77s
                               ETA: 1321887.6s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.845s, learning 0.157s)
               Value function loss: 0.6959
                    Surrogate loss: 0.0079
             Mean action noise std: 0.72
                       Mean reward: 8.90
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 13.00s
                        Total time: 36777.77s
                               ETA: 1321853.1s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.233s, learning 0.183s)
               Value function loss: 1.4009
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 10.20
               Mean episode length: 124.84
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 12.42s
                        Total time: 36790.19s
                               ETA: 1321797.4s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.289s, learning 0.158s)
               Value function loss: 1.0558
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 11.15
               Mean episode length: 124.84
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 12.45s
                        Total time: 36802.63s
                               ETA: 1321743.0s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.747s, learning 0.198s)
               Value function loss: 1.1119
                    Surrogate loss: 0.0125
             Mean action noise std: 0.72
                       Mean reward: 9.74
               Mean episode length: 124.84
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 12.95s
                        Total time: 36815.58s
                               ETA: 1321706.4s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.435s, learning 0.159s)
               Value function loss: 1.7496
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: 7.68
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 12.59s
                        Total time: 36828.17s
                               ETA: 1321657.2s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.775s, learning 0.182s)
               Value function loss: 3.4036
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 8.53
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 12.96s
                        Total time: 36841.13s
                               ETA: 1321621.1s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.113s, learning 0.160s)
               Value function loss: 2.9285
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 6.70
               Mean episode length: 124.50
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 12.27s
                        Total time: 36853.40s
                               ETA: 1321560.5s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.027s, learning 0.161s)
               Value function loss: 2.6745
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 11.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 12.19s
                        Total time: 36865.59s
                               ETA: 1321496.8s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.779s, learning 0.176s)
               Value function loss: 3.4149
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 11.41
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 11.96s
                        Total time: 36877.54s
                               ETA: 1321424.9s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.838s, learning 0.156s)
               Value function loss: 2.6213
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 7.28
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 12.99s
                        Total time: 36890.54s
                               ETA: 1321390.2s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.299s, learning 0.199s)
               Value function loss: 2.9538
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 10.78
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 12.50s
                        Total time: 36903.04s
                               ETA: 1321337.8s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.505s, learning 0.160s)
               Value function loss: 2.9955
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 10.81
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 12.66s
                        Total time: 36915.70s
                               ETA: 1321291.4s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1253 steps/s (collection: 12.917s, learning 0.159s)
               Value function loss: 25.2357
                    Surrogate loss: 0.0254
             Mean action noise std: 0.72
                       Mean reward: 12.01
               Mean episode length: 125.00
                  Mean reward/step: -0.10
       Mean episode length/episode: 5.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 13.08s
                        Total time: 36928.78s
                               ETA: 1321259.7s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.231s, learning 0.179s)
               Value function loss: 1.0601
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 11.85
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 12.41s
                        Total time: 36941.19s
                               ETA: 1321204.2s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1260 steps/s (collection: 12.704s, learning 0.290s)
               Value function loss: 0.5892
                    Surrogate loss: 0.0196
             Mean action noise std: 0.72
                       Mean reward: 12.60
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 12.99s
                        Total time: 36954.18s
                               ETA: 1321169.6s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.215s, learning 0.256s)
               Value function loss: 0.7301
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 11.59
               Mean episode length: 125.00
                  Mean reward/step: -0.12
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 12.47s
                        Total time: 36966.65s
                               ETA: 1321116.4s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.371s, learning 0.293s)
               Value function loss: 1.0189
                    Surrogate loss: 0.0176
             Mean action noise std: 0.72
                       Mean reward: 12.95
               Mean episode length: 125.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 12.66s
                        Total time: 36979.31s
                               ETA: 1321070.0s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.095s, learning 0.157s)
               Value function loss: 1.2362
                    Surrogate loss: 0.0012
             Mean action noise std: 0.72
                       Mean reward: 12.66
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 12.25s
                        Total time: 36991.57s
                               ETA: 1321009.0s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1347 steps/s (collection: 12.007s, learning 0.156s)
               Value function loss: 1.1630
                    Surrogate loss: 0.0023
             Mean action noise std: 0.72
                       Mean reward: 12.16
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 12.16s
                        Total time: 37003.73s
                               ETA: 1320944.9s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.497s, learning 0.179s)
               Value function loss: 1.5150
                    Surrogate loss: 0.0042
             Mean action noise std: 0.72
                       Mean reward: 10.11
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 11.68s
                        Total time: 37015.41s
                               ETA: 1320863.4s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.642s, learning 0.160s)
               Value function loss: 2.9011
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 11.85
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 12.80s
                        Total time: 37028.21s
                               ETA: 1320822.1s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1283 steps/s (collection: 12.502s, learning 0.263s)
               Value function loss: 4.2894
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 13.19
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 12.77s
                        Total time: 37040.97s
                               ETA: 1320779.5s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.795s, learning 0.161s)
               Value function loss: 3.5048
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 11.09
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 11.96s
                        Total time: 37052.93s
                               ETA: 1320708.1s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.129s, learning 0.198s)
               Value function loss: 3.7701
                    Surrogate loss: 0.0076
             Mean action noise std: 0.72
                       Mean reward: 14.05
               Mean episode length: 124.82
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 12.33s
                        Total time: 37065.26s
                               ETA: 1320650.0s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.889s, learning 0.159s)
               Value function loss: 3.9045
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 13.06
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 12.05s
                        Total time: 37077.30s
                               ETA: 1320582.0s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.237s, learning 0.165s)
               Value function loss: 4.0854
                    Surrogate loss: 0.0045
             Mean action noise std: 0.72
                       Mean reward: 12.67
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 12.40s
                        Total time: 37089.71s
                               ETA: 1320526.6s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.948s, learning 0.158s)
               Value function loss: 3.9748
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: 15.03
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 12.11s
                        Total time: 37101.81s
                               ETA: 1320460.7s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.535s, learning 0.177s)
               Value function loss: 4.1662
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 7.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 12.71s
                        Total time: 37114.52s
                               ETA: 1320416.4s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1261 steps/s (collection: 12.761s, learning 0.226s)
               Value function loss: 21.6481
                    Surrogate loss: 0.0144
             Mean action noise std: 0.72
                       Mean reward: 14.37
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 5.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 12.99s
                        Total time: 37127.51s
                               ETA: 1320381.9s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.391s, learning 0.166s)
               Value function loss: 0.5847
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 15.27
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 12.56s
                        Total time: 37140.07s
                               ETA: 1320332.1s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.868s, learning 0.159s)
               Value function loss: 0.6674
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 15.03
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 12.03s
                        Total time: 37152.09s
                               ETA: 1320263.5s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.083s, learning 0.181s)
               Value function loss: 0.5926
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 15.08
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 12.26s
                        Total time: 37164.36s
                               ETA: 1320203.4s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.406s, learning 0.171s)
               Value function loss: 1.7585
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 14.42
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 12.58s
                        Total time: 37176.94s
                               ETA: 1320154.5s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1268 steps/s (collection: 12.758s, learning 0.159s)
               Value function loss: 1.1923
                    Surrogate loss: 0.0014
             Mean action noise std: 0.72
                       Mean reward: 14.96
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 12.92s
                        Total time: 37189.85s
                               ETA: 1320117.6s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.411s, learning 0.163s)
               Value function loss: 1.0999
                    Surrogate loss: 0.0095
             Mean action noise std: 0.72
                       Mean reward: 15.49
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 12.57s
                        Total time: 37202.43s
                               ETA: 1320068.6s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.450s, learning 0.184s)
               Value function loss: 1.7924
                    Surrogate loss: 0.0163
             Mean action noise std: 0.72
                       Mean reward: 16.36
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 12.63s
                        Total time: 37215.06s
                               ETA: 1320021.8s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.247s, learning 0.174s)
               Value function loss: 3.1856
                    Surrogate loss: 0.0040
             Mean action noise std: 0.72
                       Mean reward: 17.62
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 12.42s
                        Total time: 37227.48s
                               ETA: 1319967.4s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.912s, learning 0.165s)
               Value function loss: 3.4452
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 15.57
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 12.08s
                        Total time: 37239.56s
                               ETA: 1319900.8s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.272s, learning 0.232s)
               Value function loss: 2.3254
                    Surrogate loss: -0.0052
             Mean action noise std: 0.72
                       Mean reward: 13.50
               Mean episode length: 124.58
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 12.50s
                        Total time: 37252.06s
                               ETA: 1319849.4s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.447s, learning 0.166s)
               Value function loss: 2.6814
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 14.26
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 12.61s
                        Total time: 37264.68s
                               ETA: 1319802.0s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.383s, learning 0.190s)
               Value function loss: 2.7631
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 14.58
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 12.57s
                        Total time: 37277.25s
                               ETA: 1319753.1s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.831s, learning 0.181s)
               Value function loss: 3.0468
                    Surrogate loss: 0.0006
             Mean action noise std: 0.72
                       Mean reward: 17.14
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 12.01s
                        Total time: 37289.26s
                               ETA: 1319684.4s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.340s, learning 0.234s)
               Value function loss: 2.8930
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 13.24
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 12.57s
                        Total time: 37301.84s
                               ETA: 1319635.6s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.481s, learning 0.249s)
               Value function loss: 20.7178
                    Surrogate loss: 0.0275
             Mean action noise std: 0.72
                       Mean reward: 15.52
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 5.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 12.73s
                        Total time: 37314.57s
                               ETA: 1319592.3s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.970s, learning 0.163s)
               Value function loss: 0.7109
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: 15.43
               Mean episode length: 125.00
                  Mean reward/step: -0.49
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 12.13s
                        Total time: 37326.70s
                               ETA: 1319528.0s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.861s, learning 0.159s)
               Value function loss: 0.6408
                    Surrogate loss: 0.0121
             Mean action noise std: 0.72
                       Mean reward: 15.51
               Mean episode length: 125.00
                  Mean reward/step: -0.29
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 12.02s
                        Total time: 37338.72s
                               ETA: 1319459.7s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.974s, learning 0.184s)
               Value function loss: 0.7772
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 16.24
               Mean episode length: 125.00
                  Mean reward/step: -0.15
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 12.16s
                        Total time: 37350.88s
                               ETA: 1319396.3s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.073s, learning 0.245s)
               Value function loss: 1.5414
                    Surrogate loss: 0.0016
             Mean action noise std: 0.72
                       Mean reward: 15.29
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 12.32s
                        Total time: 37363.19s
                               ETA: 1319338.6s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.127s, learning 0.160s)
               Value function loss: 1.5989
                    Surrogate loss: 0.0074
             Mean action noise std: 0.72
                       Mean reward: 12.74
               Mean episode length: 124.58
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 12.29s
                        Total time: 37375.48s
                               ETA: 1319279.9s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.203s, learning 0.184s)
               Value function loss: 1.5718
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 9.49
               Mean episode length: 123.78
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 12.39s
                        Total time: 37387.87s
                               ETA: 1319224.7s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.885s, learning 0.163s)
               Value function loss: 1.4785
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 9.61
               Mean episode length: 123.33
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 12.05s
                        Total time: 37399.92s
                               ETA: 1319157.6s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.182s, learning 0.242s)
               Value function loss: 2.1448
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 11.35
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 12.42s
                        Total time: 37412.34s
                               ETA: 1319103.8s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.879s, learning 0.156s)
               Value function loss: 3.8224
                    Surrogate loss: -0.0013
             Mean action noise std: 0.71
                       Mean reward: 11.34
               Mean episode length: 124.48
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 12.03s
                        Total time: 37424.37s
                               ETA: 1319036.3s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.454s, learning 0.165s)
               Value function loss: 3.1248
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 11.35
               Mean episode length: 124.73
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 12.62s
                        Total time: 37436.99s
                               ETA: 1318989.4s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.991s, learning 0.175s)
               Value function loss: 2.6786
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 12.29
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 12.17s
                        Total time: 37449.16s
                               ETA: 1318926.6s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.673s, learning 0.157s)
               Value function loss: 2.8313
                    Surrogate loss: -0.0048
             Mean action noise std: 0.71
                       Mean reward: 12.43
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 11.83s
                        Total time: 37460.99s
                               ETA: 1318852.0s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.239s, learning 0.166s)
               Value function loss: 2.8877
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 11.44
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 12.40s
                        Total time: 37473.39s
                               ETA: 1318797.7s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.123s, learning 0.156s)
               Value function loss: 3.0783
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 11.96
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 12.28s
                        Total time: 37485.67s
                               ETA: 1318738.9s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1254 steps/s (collection: 12.901s, learning 0.163s)
               Value function loss: 3.2505
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 15.57
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 13.06s
                        Total time: 37498.74s
                               ETA: 1318707.9s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.354s, learning 0.173s)
               Value function loss: 21.4036
                    Surrogate loss: 0.0229
             Mean action noise std: 0.71
                       Mean reward: 13.21
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 5.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 12.53s
                        Total time: 37511.27s
                               ETA: 1318657.9s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.001s, learning 0.164s)
               Value function loss: 0.7871
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 13.12
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 12.17s
                        Total time: 37523.43s
                               ETA: 1318595.3s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.597s, learning 0.160s)
               Value function loss: 0.6566
                    Surrogate loss: 0.0183
             Mean action noise std: 0.71
                       Mean reward: 12.69
               Mean episode length: 125.00
                  Mean reward/step: -0.23
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 11.76s
                        Total time: 37535.19s
                               ETA: 1318518.4s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.179s, learning 0.187s)
               Value function loss: 0.5902
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 11.88
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 12.37s
                        Total time: 37547.55s
                               ETA: 1318462.9s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.341s, learning 0.157s)
               Value function loss: 1.5968
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 11.49
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 12.50s
                        Total time: 37560.05s
                               ETA: 1318412.0s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.620s, learning 0.168s)
               Value function loss: 1.0457
                    Surrogate loss: 0.0039
             Mean action noise std: 0.71
                       Mean reward: 12.26
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 12.79s
                        Total time: 37572.84s
                               ETA: 1318371.4s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.468s, learning 0.169s)
               Value function loss: 1.1597
                    Surrogate loss: 0.0145
             Mean action noise std: 0.71
                       Mean reward: 12.67
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 12.64s
                        Total time: 37585.48s
                               ETA: 1318325.5s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.159s, learning 0.162s)
               Value function loss: 1.4498
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 12.58
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 12.32s
                        Total time: 37597.80s
                               ETA: 1318268.5s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.424s, learning 0.185s)
               Value function loss: 2.2963
                    Surrogate loss: 0.0200
             Mean action noise std: 0.71
                       Mean reward: 11.05
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 12.61s
                        Total time: 37610.41s
                               ETA: 1318221.7s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.359s, learning 0.163s)
               Value function loss: 3.2796
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 13.13
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 12.52s
                        Total time: 37622.93s
                               ETA: 1318171.8s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.231s, learning 0.167s)
               Value function loss: 2.4170
                    Surrogate loss: 0.0129
             Mean action noise std: 0.71
                       Mean reward: 8.48
               Mean episode length: 124.55
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 12.40s
                        Total time: 37635.33s
                               ETA: 1318117.6s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.256s, learning 0.161s)
               Value function loss: 2.7579
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 9.20
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 12.42s
                        Total time: 37647.74s
                               ETA: 1318064.1s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.496s, learning 0.232s)
               Value function loss: 2.4894
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 11.25
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 12.73s
                        Total time: 37660.47s
                               ETA: 1318021.5s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.556s, learning 0.184s)
               Value function loss: 2.9012
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 7.77
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 12.74s
                        Total time: 37673.21s
                               ETA: 1317979.4s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.313s, learning 0.234s)
               Value function loss: 2.7669
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 12.51
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 12.55s
                        Total time: 37685.76s
                               ETA: 1317930.6s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.339s, learning 0.164s)
               Value function loss: 3.1719
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 12.96
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 12.50s
                        Total time: 37698.26s
                               ETA: 1317880.2s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.365s, learning 0.156s)
               Value function loss: 21.9502
                    Surrogate loss: 0.0258
             Mean action noise std: 0.71
                       Mean reward: 11.26
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 5.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 12.52s
                        Total time: 37710.78s
                               ETA: 1317830.4s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.399s, learning 0.206s)
               Value function loss: 0.6168
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 11.26
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 12.61s
                        Total time: 37723.39s
                               ETA: 1317783.7s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.079s, learning 0.161s)
               Value function loss: 0.7119
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 12.22
               Mean episode length: 125.00
                  Mean reward/step: -0.17
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 12.24s
                        Total time: 37735.63s
                               ETA: 1317724.3s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.279s, learning 0.157s)
               Value function loss: 0.7490
                    Surrogate loss: 0.0000
             Mean action noise std: 0.71
                       Mean reward: 12.73
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 12.44s
                        Total time: 37748.06s
                               ETA: 1317671.7s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.433s, learning 0.159s)
               Value function loss: 1.6975
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 12.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 12.59s
                        Total time: 37760.65s
                               ETA: 1317624.5s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.046s, learning 0.174s)
               Value function loss: 1.2355
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 12.14
               Mean episode length: 124.99
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 12.22s
                        Total time: 37772.87s
                               ETA: 1317564.5s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.588s, learning 0.164s)
               Value function loss: 1.1305
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 11.71
               Mean episode length: 124.99
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 12.75s
                        Total time: 37785.63s
                               ETA: 1317523.0s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.408s, learning 0.166s)
               Value function loss: 2.0242
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 14.04
               Mean episode length: 124.99
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 12.57s
                        Total time: 37798.20s
                               ETA: 1317475.3s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.235s, learning 0.181s)
               Value function loss: 3.9589
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 14.12
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 12.42s
                        Total time: 37810.62s
                               ETA: 1317422.1s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.190s, learning 0.181s)
               Value function loss: 3.8057
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 13.67
               Mean episode length: 124.52
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 12.37s
                        Total time: 37822.99s
                               ETA: 1317367.4s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.059s, learning 0.184s)
               Value function loss: 2.8999
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 13.90
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 12.24s
                        Total time: 37835.23s
                               ETA: 1317308.3s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1267 steps/s (collection: 12.722s, learning 0.205s)
               Value function loss: 3.2350
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 15.31
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 12.93s
                        Total time: 37848.16s
                               ETA: 1317273.0s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.470s, learning 0.174s)
               Value function loss: 3.1860
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 14.63
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 12.64s
                        Total time: 37860.80s
                               ETA: 1317227.9s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.982s, learning 0.175s)
               Value function loss: 3.1992
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 15.64
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 12.16s
                        Total time: 37872.96s
                               ETA: 1317165.8s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.509s, learning 0.189s)
               Value function loss: 3.1011
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 14.65
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 11.70s
                        Total time: 37884.65s
                               ETA: 1317087.9s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1255 steps/s (collection: 12.862s, learning 0.184s)
               Value function loss: 23.6182
                    Surrogate loss: 0.0380
             Mean action noise std: 0.71
                       Mean reward: 15.05
               Mean episode length: 125.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 5.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 13.05s
                        Total time: 37897.70s
                               ETA: 1317056.8s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1288 steps/s (collection: 12.498s, learning 0.222s)
               Value function loss: 0.8151
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 14.14
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 12.72s
                        Total time: 37910.42s
                               ETA: 1317014.5s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.358s, learning 0.260s)
               Value function loss: 0.7580
                    Surrogate loss: 0.0045
             Mean action noise std: 0.71
                       Mean reward: 13.72
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 12.62s
                        Total time: 37923.04s
                               ETA: 1316968.6s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.649s, learning 0.250s)
               Value function loss: 0.7751
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 13.95
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 12.90s
                        Total time: 37935.94s
                               ETA: 1316932.5s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.405s, learning 0.207s)
               Value function loss: 1.2274
                    Surrogate loss: 0.0053
             Mean action noise std: 0.71
                       Mean reward: 13.02
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 12.61s
                        Total time: 37948.55s
                               ETA: 1316886.4s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.343s, learning 0.164s)
               Value function loss: 1.2180
                    Surrogate loss: 0.0122
             Mean action noise std: 0.71
                       Mean reward: 12.46
               Mean episode length: 124.67
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 12.51s
                        Total time: 37961.06s
                               ETA: 1316836.8s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.017s, learning 0.298s)
               Value function loss: 1.1926
                    Surrogate loss: 0.0059
             Mean action noise std: 0.71
                       Mean reward: 13.42
               Mean episode length: 124.67
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 12.31s
                        Total time: 37973.37s
                               ETA: 1316780.5s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.040s, learning 0.163s)
               Value function loss: 1.4000
                    Surrogate loss: 0.0043
             Mean action noise std: 0.71
                       Mean reward: 13.64
               Mean episode length: 124.67
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 12.20s
                        Total time: 37985.57s
                               ETA: 1316720.3s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.012s, learning 0.165s)
               Value function loss: 2.5109
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 15.13
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 12.18s
                        Total time: 37997.75s
                               ETA: 1316659.3s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.164s, learning 0.273s)
               Value function loss: 4.1093
                    Surrogate loss: 0.0021
             Mean action noise std: 0.71
                       Mean reward: 13.79
               Mean episode length: 125.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 12.44s
                        Total time: 38010.19s
                               ETA: 1316607.4s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1247 steps/s (collection: 12.874s, learning 0.257s)
               Value function loss: 3.1341
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 12.86
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 13.13s
                        Total time: 38023.32s
                               ETA: 1316579.4s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.353s, learning 0.281s)
               Value function loss: 3.1160
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 15.23
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 12.63s
                        Total time: 38035.95s
                               ETA: 1316534.3s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.395s, learning 0.170s)
               Value function loss: 3.1811
                    Surrogate loss: 0.0036
             Mean action noise std: 0.71
                       Mean reward: 11.49
               Mean episode length: 124.41
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 12.56s
                        Total time: 38048.52s
                               ETA: 1316486.9s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.150s, learning 0.248s)
               Value function loss: 3.2432
                    Surrogate loss: 0.0099
             Mean action noise std: 0.71
                       Mean reward: 13.67
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 12.40s
                        Total time: 38060.92s
                               ETA: 1316433.6s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.174s, learning 0.265s)
               Value function loss: 2.9659
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 12.44s
                        Total time: 38073.36s
                               ETA: 1316381.9s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.403s, learning 0.202s)
               Value function loss: 3.1302
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 12.38
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 12.61s
                        Total time: 38085.96s
                               ETA: 1316335.9s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.724s, learning 0.193s)
               Value function loss: 24.8676
                    Surrogate loss: 0.0272
             Mean action noise std: 0.71
                       Mean reward: 16.80
               Mean episode length: 125.00
                  Mean reward/step: -0.25
       Mean episode length/episode: 5.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 21.92s
                        Total time: 38107.88s
                               ETA: 1316611.6s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.128s, learning 0.171s)
               Value function loss: 0.6474
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 16.80
               Mean episode length: 125.00
                  Mean reward/step: -0.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 23.30s
                        Total time: 38131.18s
                               ETA: 1316934.8s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 677 steps/s (collection: 24.023s, learning 0.177s)
               Value function loss: 0.8228
                    Surrogate loss: -0.0038
             Mean action noise std: 0.71
                       Mean reward: 16.81
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 24.20s
                        Total time: 38155.38s
                               ETA: 1317288.9s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 673 steps/s (collection: 24.160s, learning 0.166s)
               Value function loss: 0.6544
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 16.88
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 24.33s
                        Total time: 38179.70s
                               ETA: 1317647.1s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.349s, learning 0.186s)
               Value function loss: 1.6119
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 15.04
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 23.53s
                        Total time: 38203.24s
                               ETA: 1317977.8s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 707 steps/s (collection: 22.907s, learning 0.261s)
               Value function loss: 1.1911
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 14.10
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 23.17s
                        Total time: 38226.40s
                               ETA: 1318295.5s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 695 steps/s (collection: 23.393s, learning 0.175s)
               Value function loss: 1.0501
                    Surrogate loss: 0.0063
             Mean action noise std: 0.71
                       Mean reward: 13.94
               Mean episode length: 125.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 23.57s
                        Total time: 38249.97s
                               ETA: 1318626.7s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 713 steps/s (collection: 22.770s, learning 0.186s)
               Value function loss: 1.7317
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 13.84
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 22.96s
                        Total time: 38272.93s
                               ETA: 1318936.7s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 687 steps/s (collection: 23.634s, learning 0.183s)
               Value function loss: 3.6373
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 15.22
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 23.82s
                        Total time: 38296.74s
                               ETA: 1319276.0s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 705 steps/s (collection: 23.012s, learning 0.209s)
               Value function loss: 4.1370
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 12.31
               Mean episode length: 124.89
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 23.22s
                        Total time: 38319.97s
                               ETA: 1319594.6s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 684 steps/s (collection: 23.737s, learning 0.186s)
               Value function loss: 3.4961
                    Surrogate loss: 0.0027
             Mean action noise std: 0.71
                       Mean reward: 12.37
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 23.92s
                        Total time: 38343.89s
                               ETA: 1319937.1s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.563s, learning 0.202s)
               Value function loss: 3.3557
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 12.74
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 23.77s
                        Total time: 38367.65s
                               ETA: 1320273.9s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.487s, learning 0.158s)
               Value function loss: 3.1561
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 13.71
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 23.64s
                        Total time: 38391.30s
                               ETA: 1320606.3s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.178s, learning 0.264s)
               Value function loss: 3.1495
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 13.14
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 23.44s
                        Total time: 38414.74s
                               ETA: 1320931.5s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.737s, learning 0.178s)
               Value function loss: 3.0476
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 13.54
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 23.92s
                        Total time: 38438.66s
                               ETA: 1321272.7s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.527s, learning 0.163s)
               Value function loss: 6.6101
                    Surrogate loss: 0.0204
             Mean action noise std: 0.71
                       Mean reward: 10.24
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 23.69s
                        Total time: 38462.35s
                               ETA: 1321605.9s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.777s, learning 0.184s)
               Value function loss: 16.7183
                    Surrogate loss: 0.0117
             Mean action noise std: 0.71
                       Mean reward: 11.05
               Mean episode length: 125.00
                  Mean reward/step: -0.51
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 23.96s
                        Total time: 38486.31s
                               ETA: 1321948.2s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.305s, learning 0.194s)
               Value function loss: 0.4695
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 11.23
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 23.50s
                        Total time: 38509.81s
                               ETA: 1322274.3s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.237s, learning 0.159s)
               Value function loss: 0.6157
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 11.34
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 23.40s
                        Total time: 38533.20s
                               ETA: 1322596.7s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.407s, learning 0.188s)
               Value function loss: 0.7055
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 11.75
               Mean episode length: 125.00
                  Mean reward/step: -0.04
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 23.59s
                        Total time: 38556.80s
                               ETA: 1322925.6s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.285s, learning 0.164s)
               Value function loss: 1.5817
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 8.62
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 23.45s
                        Total time: 38580.25s
                               ETA: 1323249.3s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.768s, learning 0.204s)
               Value function loss: 0.9713
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 8.49
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 23.97s
                        Total time: 38604.22s
                               ETA: 1323590.7s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.466s, learning 0.213s)
               Value function loss: 1.2227
                    Surrogate loss: 0.0076
             Mean action noise std: 0.71
                       Mean reward: 9.96
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 23.68s
                        Total time: 38627.90s
                               ETA: 1323921.8s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.323s, learning 0.167s)
               Value function loss: 1.8267
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 9.25
               Mean episode length: 124.73
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 23.49s
                        Total time: 38651.39s
                               ETA: 1324246.1s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.098s, learning 0.164s)
               Value function loss: 3.3945
                    Surrogate loss: 0.0012
             Mean action noise std: 0.71
                       Mean reward: 7.47
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 23.26s
                        Total time: 38674.65s
                               ETA: 1324562.4s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 705 steps/s (collection: 23.046s, learning 0.168s)
               Value function loss: 3.1768
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 10.61
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 23.21s
                        Total time: 38697.86s
                               ETA: 1324876.8s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.703s, learning 0.161s)
               Value function loss: 2.5713
                    Surrogate loss: -0.0041
             Mean action noise std: 0.71
                       Mean reward: 8.88
               Mean episode length: 124.63
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 23.86s
                        Total time: 38721.72s
                               ETA: 1325213.2s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 682 steps/s (collection: 23.813s, learning 0.209s)
               Value function loss: 2.9149
                    Surrogate loss: 0.0069
             Mean action noise std: 0.71
                       Mean reward: 9.46
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 24.02s
                        Total time: 38745.75s
                               ETA: 1325554.8s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.371s, learning 0.162s)
               Value function loss: 2.9094
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 11.11
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 23.53s
                        Total time: 38769.28s
                               ETA: 1325879.3s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.292s, learning 0.186s)
               Value function loss: 3.1015
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 10.12
               Mean episode length: 124.71
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 23.48s
                        Total time: 38792.76s
                               ETA: 1326201.8s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.066s, learning 0.217s)
               Value function loss: 3.1530
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 10.45
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 23.28s
                        Total time: 38816.04s
                               ETA: 1326517.3s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 680 steps/s (collection: 23.893s, learning 0.185s)
               Value function loss: 22.0672
                    Surrogate loss: 0.0222
             Mean action noise std: 0.71
                       Mean reward: 9.61
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 5.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 24.08s
                        Total time: 38840.12s
                               ETA: 1326859.8s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.632s, learning 0.157s)
               Value function loss: 0.5447
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 10.65
               Mean episode length: 125.00
                  Mean reward/step: -0.45
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 23.79s
                        Total time: 38863.91s
                               ETA: 1327192.2s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 690 steps/s (collection: 23.499s, learning 0.219s)
               Value function loss: 0.5555
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 10.71
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 23.72s
                        Total time: 38887.63s
                               ETA: 1327521.9s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.681s, learning 0.187s)
               Value function loss: 0.6360
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: 9.94
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 23.87s
                        Total time: 38911.49s
                               ETA: 1327856.4s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.203s, learning 0.167s)
               Value function loss: 1.1708
                    Surrogate loss: 0.0084
             Mean action noise std: 0.71
                       Mean reward: 10.49
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 23.37s
                        Total time: 38934.86s
                               ETA: 1328173.7s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 675 steps/s (collection: 24.066s, learning 0.203s)
               Value function loss: 1.1762
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 10.83
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 24.27s
                        Total time: 38959.13s
                               ETA: 1328521.5s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 734 steps/s (collection: 22.076s, learning 0.224s)
               Value function loss: 0.8755
                    Surrogate loss: 0.0083
             Mean action noise std: 0.71
                       Mean reward: 8.24
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 22.30s
                        Total time: 38981.43s
                               ETA: 1328801.8s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.109s, learning 0.204s)
               Value function loss: 1.2980
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 7.17
               Mean episode length: 125.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 12.31s
                        Total time: 38993.75s
                               ETA: 1328741.6s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.879s, learning 0.210s)
               Value function loss: 2.4958
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 10.77
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 12.09s
                        Total time: 39005.83s
                               ETA: 1328673.8s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.746s, learning 0.167s)
               Value function loss: 4.0396
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 10.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 11.91s
                        Total time: 39017.75s
                               ETA: 1328600.1s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1274 steps/s (collection: 12.687s, learning 0.163s)
               Value function loss: 4.0024
                    Surrogate loss: 0.0137
             Mean action noise std: 0.71
                       Mean reward: 11.10
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 12.85s
                        Total time: 39030.60s
                               ETA: 1328558.3s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.906s, learning 0.181s)
               Value function loss: 3.2347
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 9.44
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 12.09s
                        Total time: 39042.68s
                               ETA: 1328490.6s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.396s, learning 0.159s)
               Value function loss: 3.4502
                    Surrogate loss: -0.0040
             Mean action noise std: 0.71
                       Mean reward: 13.45
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 12.56s
                        Total time: 39055.24s
                               ETA: 1328438.8s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.544s, learning 0.187s)
               Value function loss: 5.5008
                    Surrogate loss: 0.0093
             Mean action noise std: 0.71
                       Mean reward: 10.29
               Mean episode length: 124.80
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 12.73s
                        Total time: 39067.97s
                               ETA: 1328393.0s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.477s, learning 0.262s)
               Value function loss: 2.7724
                    Surrogate loss: 0.0286
             Mean action noise std: 0.71
                       Mean reward: 11.69
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 12.74s
                        Total time: 39080.71s
                               ETA: 1328347.5s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.182s, learning 0.165s)
               Value function loss: 2.9578
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 11.19
               Mean episode length: 124.93
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 12.35s
                        Total time: 39093.06s
                               ETA: 1328288.8s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.157s, learning 0.160s)
               Value function loss: 19.1402
                    Surrogate loss: 0.0412
             Mean action noise std: 0.71
                       Mean reward: 8.10
               Mean episode length: 125.00
                  Mean reward/step: -0.36
       Mean episode length/episode: 5.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 12.32s
                        Total time: 39105.37s
                               ETA: 1328229.0s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.421s, learning 0.166s)
               Value function loss: 0.6671
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 7.04
               Mean episode length: 125.00
                  Mean reward/step: -0.37
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 12.59s
                        Total time: 39117.96s
                               ETA: 1328178.4s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.666s, learning 0.169s)
               Value function loss: 0.7013
                    Surrogate loss: 0.0058
             Mean action noise std: 0.71
                       Mean reward: 6.92
               Mean episode length: 125.00
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 12.83s
                        Total time: 39130.79s
                               ETA: 1328136.3s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.558s, learning 0.198s)
               Value function loss: 0.6002
                    Surrogate loss: 0.0046
             Mean action noise std: 0.71
                       Mean reward: 6.80
               Mean episode length: 125.00
                  Mean reward/step: -0.08
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 12.76s
                        Total time: 39143.55s
                               ETA: 1328091.6s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1250 steps/s (collection: 12.943s, learning 0.156s)
               Value function loss: 1.8950
                    Surrogate loss: 0.0042
             Mean action noise std: 0.71
                       Mean reward: 7.69
               Mean episode length: 124.95
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 13.10s
                        Total time: 39156.65s
                               ETA: 1328058.5s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.376s, learning 0.157s)
               Value function loss: 1.0979
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 9.01
               Mean episode length: 124.95
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 12.53s
                        Total time: 39169.18s
                               ETA: 1328006.2s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.404s, learning 0.187s)
               Value function loss: 1.0801
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 10.06
               Mean episode length: 124.95
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 12.59s
                        Total time: 39181.77s
                               ETA: 1327955.9s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.667s, learning 0.160s)
               Value function loss: 1.8217
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 10.60
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 12.83s
                        Total time: 39194.60s
                               ETA: 1327913.6s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.066s, learning 0.156s)
               Value function loss: 3.7497
                    Surrogate loss: 0.0083
             Mean action noise std: 0.71
                       Mean reward: 11.25
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 12.22s
                        Total time: 39206.82s
                               ETA: 1327850.9s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.232s, learning 0.208s)
               Value function loss: 3.7641
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 9.91
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 12.44s
                        Total time: 39219.26s
                               ETA: 1327795.6s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.883s, learning 0.158s)
               Value function loss: 2.9215
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 9.96
               Mean episode length: 124.18
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 12.04s
                        Total time: 39231.31s
                               ETA: 1327726.8s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.545s, learning 0.281s)
               Value function loss: 3.0734
                    Surrogate loss: 0.0072
             Mean action noise std: 0.71
                       Mean reward: 11.38
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 12.83s
                        Total time: 39244.13s
                               ETA: 1327684.6s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.066s, learning 0.294s)
               Value function loss: 3.0464
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 7.87
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 12.36s
                        Total time: 39256.49s
                               ETA: 1327626.6s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.541s, learning 0.252s)
               Value function loss: 3.3179
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 9.91
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 12.79s
                        Total time: 39269.28s
                               ETA: 1327583.4s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.617s, learning 0.205s)
               Value function loss: 2.8971
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 9.49
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 12.82s
                        Total time: 39282.11s
                               ETA: 1327541.1s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1259 steps/s (collection: 12.851s, learning 0.160s)
               Value function loss: 20.9431
                    Surrogate loss: 0.0258
             Mean action noise std: 0.71
                       Mean reward: 9.93
               Mean episode length: 125.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 5.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 13.01s
                        Total time: 39295.12s
                               ETA: 1327505.3s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.297s, learning 0.206s)
               Value function loss: 0.6308
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 10.24
               Mean episode length: 125.00
                  Mean reward/step: -0.50
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 12.50s
                        Total time: 39307.62s
                               ETA: 1327452.3s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1276 steps/s (collection: 12.664s, learning 0.168s)
               Value function loss: 0.4990
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 12.08
               Mean episode length: 125.00
                  Mean reward/step: -0.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 12.83s
                        Total time: 39320.45s
                               ETA: 1327410.4s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.407s, learning 0.188s)
               Value function loss: 0.6130
                    Surrogate loss: 0.0046
             Mean action noise std: 0.71
                       Mean reward: 12.54
               Mean episode length: 125.00
                  Mean reward/step: -0.18
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 12.59s
                        Total time: 39333.05s
                               ETA: 1327360.5s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1265 steps/s (collection: 12.778s, learning 0.169s)
               Value function loss: 0.7205
                    Surrogate loss: 0.0039
             Mean action noise std: 0.71
                       Mean reward: 10.18
               Mean episode length: 125.00
                  Mean reward/step: -0.03
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 12.95s
                        Total time: 39346.00s
                               ETA: 1327322.6s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1294 steps/s (collection: 12.440s, learning 0.213s)
               Value function loss: 1.2974
                    Surrogate loss: 0.0107
             Mean action noise std: 0.71
                       Mean reward: 4.60
               Mean episode length: 124.79
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 12.65s
                        Total time: 39358.65s
                               ETA: 1327274.8s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.646s, learning 0.158s)
               Value function loss: 1.0054
                    Surrogate loss: 0.0094
             Mean action noise std: 0.71
                       Mean reward: 3.55
               Mean episode length: 124.79
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 12.80s
                        Total time: 39371.45s
                               ETA: 1327232.0s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.006s, learning 0.158s)
               Value function loss: 1.2320
                    Surrogate loss: 0.0016
             Mean action noise std: 0.71
                       Mean reward: 5.32
               Mean episode length: 124.79
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 12.16s
                        Total time: 39383.62s
                               ETA: 1327167.8s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.548s, learning 0.203s)
               Value function loss: 2.0406
                    Surrogate loss: 0.0032
             Mean action noise std: 0.71
                       Mean reward: 7.72
               Mean episode length: 125.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 12.75s
                        Total time: 39396.37s
                               ETA: 1327123.3s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1270 steps/s (collection: 12.732s, learning 0.166s)
               Value function loss: 3.5625
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 7.05
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 12.90s
                        Total time: 39409.27s
                               ETA: 1327083.8s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1279 steps/s (collection: 12.592s, learning 0.210s)
               Value function loss: 3.0538
                    Surrogate loss: 0.0094
             Mean action noise std: 0.71
                       Mean reward: 8.67
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 12.80s
                        Total time: 39422.07s
                               ETA: 1327041.1s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.717s, learning 0.314s)
               Value function loss: 2.4906
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 8.73
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 13.03s
                        Total time: 39435.10s
                               ETA: 1327006.1s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.285s, learning 0.210s)
               Value function loss: 2.5991
                    Surrogate loss: 0.0058
             Mean action noise std: 0.71
                       Mean reward: 10.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 12.49s
                        Total time: 39447.59s
                               ETA: 1326953.1s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1277 steps/s (collection: 12.639s, learning 0.183s)
               Value function loss: 2.8034
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 5.31
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 12.82s
                        Total time: 39460.42s
                               ETA: 1326911.1s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.276s, learning 0.170s)
               Value function loss: 3.1170
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 7.77
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 12.45s
                        Total time: 39472.86s
                               ETA: 1326856.5s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.996s, learning 0.160s)
               Value function loss: 2.9807
                    Surrogate loss: 0.0061
             Mean action noise std: 0.71
                       Mean reward: 9.52
               Mean episode length: 124.75
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 12.16s
                        Total time: 39485.02s
                               ETA: 1326792.2s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.604s, learning 0.169s)
               Value function loss: 19.0025
                    Surrogate loss: 0.0494
             Mean action noise std: 0.71
                       Mean reward: 10.10
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 5.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 12.77s
                        Total time: 39497.79s
                               ETA: 1326748.7s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.604s, learning 0.194s)
               Value function loss: 0.5593
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 9.04
               Mean episode length: 125.00
                  Mean reward/step: -0.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 12.80s
                        Total time: 39510.59s
                               ETA: 1326706.0s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.383s, learning 0.164s)
               Value function loss: 0.7015
                    Surrogate loss: 0.0119
             Mean action noise std: 0.71
                       Mean reward: 8.50
               Mean episode length: 125.00
                  Mean reward/step: -0.24
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 12.55s
                        Total time: 39523.14s
                               ETA: 1326654.9s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.316s, learning 0.176s)
               Value function loss: 0.6216
                    Surrogate loss: 0.0065
             Mean action noise std: 0.71
                       Mean reward: 7.60
               Mean episode length: 125.00
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 12.49s
                        Total time: 39535.63s
                               ETA: 1326602.0s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.428s, learning 0.177s)
               Value function loss: 1.3653
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 8.90
               Mean episode length: 125.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 12.60s
                        Total time: 39548.23s
                               ETA: 1326552.9s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.016s, learning 0.177s)
               Value function loss: 1.2580
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 10.19
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 12.19s
                        Total time: 39560.43s
                               ETA: 1326490.0s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.402s, learning 0.187s)
               Value function loss: 1.2268
                    Surrogate loss: 0.0034
             Mean action noise std: 0.71
                       Mean reward: 12.48
               Mean episode length: 125.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 12.59s
                        Total time: 39573.01s
                               ETA: 1326440.5s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.018s, learning 0.240s)
               Value function loss: 1.5828
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 11.27
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 12.26s
                        Total time: 39585.27s
                               ETA: 1326379.8s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.071s, learning 0.236s)
               Value function loss: 3.1398
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 12.60
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 12.31s
                        Total time: 39597.58s
                               ETA: 1326320.9s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.287s, learning 0.164s)
               Value function loss: 3.7867
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 10.81
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 12.45s
                        Total time: 39610.03s
                               ETA: 1326266.7s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.021s, learning 0.185s)
               Value function loss: 2.9452
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 10.78
               Mean episode length: 124.53
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 12.21s
                        Total time: 39622.24s
                               ETA: 1326204.5s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.175s, learning 0.174s)
               Value function loss: 3.2978
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 11.27
               Mean episode length: 124.69
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 12.35s
                        Total time: 39634.59s
                               ETA: 1326147.0s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.120s, learning 0.161s)
               Value function loss: 2.9236
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 11.15
               Mean episode length: 124.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 12.28s
                        Total time: 39646.87s
                               ETA: 1326087.3s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.235s, learning 0.159s)
               Value function loss: 3.1233
                    Surrogate loss: 0.0022
             Mean action noise std: 0.71
                       Mean reward: 9.81
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 12.39s
                        Total time: 39659.26s
                               ETA: 1326031.4s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.190s, learning 0.203s)
               Value function loss: 2.8112
                    Surrogate loss: 0.0152
             Mean action noise std: 0.71
                       Mean reward: 12.13
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 12.39s
                        Total time: 39671.65s
                               ETA: 1325975.5s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.895s, learning 0.244s)
               Value function loss: 3.1605
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 11.68
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 12.14s
                        Total time: 39683.79s
                               ETA: 1325911.1s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1266 steps/s (collection: 12.771s, learning 0.169s)
               Value function loss: 21.1856
                    Surrogate loss: 0.0202
             Mean action noise std: 0.71
                       Mean reward: 11.33
               Mean episode length: 125.00
                  Mean reward/step: -0.43
       Mean episode length/episode: 5.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 12.94s
                        Total time: 39696.73s
                               ETA: 1325873.6s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.421s, learning 0.179s)
               Value function loss: 0.5017
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 11.36
               Mean episode length: 125.00
                  Mean reward/step: -0.34
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 12.60s
                        Total time: 39709.33s
                               ETA: 1325824.7s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.502s, learning 0.194s)
               Value function loss: 0.6855
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 12.85
               Mean episode length: 125.00
                  Mean reward/step: -0.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 12.70s
                        Total time: 39722.03s
                               ETA: 1325779.0s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.568s, learning 0.168s)
               Value function loss: 0.8390
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 12.19
               Mean episode length: 125.00
                  Mean reward/step: -0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 12.74s
                        Total time: 39734.76s
                               ETA: 1325734.7s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.232s, learning 0.159s)
               Value function loss: 1.6660
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 10.88
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 12.39s
                        Total time: 39747.16s
                               ETA: 1325678.9s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.293s, learning 0.159s)
               Value function loss: 1.2989
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 9.45
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 12.45s
                        Total time: 39759.61s
                               ETA: 1325625.2s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.487s, learning 0.207s)
               Value function loss: 1.2638
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 8.96
               Mean episode length: 125.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 11.69s
                        Total time: 39771.30s
                               ETA: 1325546.2s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.624s, learning 0.162s)
               Value function loss: 2.2897
                    Surrogate loss: 0.0039
             Mean action noise std: 0.71
                       Mean reward: 12.65
               Mean episode length: 125.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 12.79s
                        Total time: 39784.09s
                               ETA: 1325503.7s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.195s, learning 0.159s)
               Value function loss: 3.3910
                    Surrogate loss: -0.0005
             Mean action noise std: 0.71
                       Mean reward: 8.98
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 12.35s
                        Total time: 39796.44s
                               ETA: 1325446.8s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.076s, learning 0.234s)
               Value function loss: 3.2864
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 10.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 12.31s
                        Total time: 39808.75s
                               ETA: 1325388.5s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.994s, learning 0.174s)
               Value function loss: 2.4809
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 10.57
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 12.17s
                        Total time: 39820.92s
                               ETA: 1325325.5s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.390s, learning 0.167s)
               Value function loss: 3.2003
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 9.65
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 12.56s
                        Total time: 39833.48s
                               ETA: 1325275.4s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.323s, learning 0.210s)
               Value function loss: 2.8709
                    Surrogate loss: 0.0257
             Mean action noise std: 0.71
                       Mean reward: 11.10
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 12.53s
                        Total time: 39846.01s
                               ETA: 1325224.5s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1293 steps/s (collection: 12.497s, learning 0.169s)
               Value function loss: 2.8427
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 9.94
               Mean episode length: 124.83
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 12.67s
                        Total time: 39858.68s
                               ETA: 1325178.2s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.076s, learning 0.191s)
               Value function loss: 2.8996
                    Surrogate loss: -0.0055
             Mean action noise std: 0.71
                       Mean reward: 11.16
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 12.27s
                        Total time: 39870.94s
                               ETA: 1325118.5s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1251 steps/s (collection: 12.924s, learning 0.172s)
               Value function loss: 26.4465
                    Surrogate loss: 0.0677
             Mean action noise std: 0.71
                       Mean reward: 9.57
               Mean episode length: 125.00
                  Mean reward/step: -0.01
       Mean episode length/episode: 5.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 13.10s
                        Total time: 39884.04s
                               ETA: 1325086.5s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.646s, learning 0.164s)
               Value function loss: 0.7992
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 9.76
               Mean episode length: 125.00
                  Mean reward/step: -0.46
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 11.81s
                        Total time: 39895.85s
                               ETA: 1325011.7s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.179s, learning 0.183s)
               Value function loss: 0.6189
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 9.75
               Mean episode length: 125.00
                  Mean reward/step: -0.26
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 12.36s
                        Total time: 39908.21s
                               ETA: 1324955.4s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.241s, learning 0.174s)
               Value function loss: 0.6686
                    Surrogate loss: 0.0111
             Mean action noise std: 0.71
                       Mean reward: 10.14
               Mean episode length: 125.00
                  Mean reward/step: -0.13
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 12.42s
                        Total time: 39920.63s
                               ETA: 1324900.8s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.311s, learning 0.171s)
               Value function loss: 0.7819
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 12.55
               Mean episode length: 125.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 12.48s
                        Total time: 39933.11s
                               ETA: 1324848.5s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1295 steps/s (collection: 12.456s, learning 0.196s)
               Value function loss: 1.0704
                    Surrogate loss: -0.0081
             Mean action noise std: 0.71
                       Mean reward: 12.23
               Mean episode length: 125.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 12.65s
                        Total time: 39945.76s
                               ETA: 1324801.8s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.510s, learning 0.215s)
               Value function loss: 1.1160
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 12.94
               Mean episode length: 125.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 12.73s
                        Total time: 39958.49s
                               ETA: 1324757.6s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.070s, learning 0.168s)
               Value function loss: 1.3048
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 12.32
               Mean episode length: 125.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 12.24s
                        Total time: 39970.72s
                               ETA: 1324697.2s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.186s, learning 0.253s)
               Value function loss: 2.5840
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 12.44s
                        Total time: 39983.16s
                               ETA: 1324643.5s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1281 steps/s (collection: 12.591s, learning 0.198s)
               Value function loss: 4.1256
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 11.75
               Mean episode length: 125.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 12.79s
                        Total time: 39995.95s
                               ETA: 1324601.5s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.917s, learning 0.161s)
               Value function loss: 3.1583
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 12.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 12.08s
                        Total time: 40008.03s
                               ETA: 1324535.9s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1286 steps/s (collection: 12.470s, learning 0.264s)
               Value function loss: 2.9953
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 14.30
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 12.73s
                        Total time: 40020.76s
                               ETA: 1324492.1s
